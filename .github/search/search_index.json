{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to the Athomic","text":"<p>Athomic is an opinionated, production-ready engine for building resilient, observable, and scalable microservices in Python. It provides a robust foundation\u2014the Athomic Layer\u2014that handles cross-cutting concerns, allowing developers to focus purely on business logic.</p> <p>Our philosophy is grounded in battle-tested software engineering principles like SOLID, Single Responsibility (SRP), and Dependency Injection (DI), ensuring that applications built on this engine are maintainable, scalable, and easy to test.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Follow these steps to get a local development environment up and running.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker &amp; Docker Compose</li> <li>Python 3.11+</li> <li>Poetry</li> </ul>"},{"location":"#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone [https://github.com/guandaline/athomic-docs.git](https://github.com/guandaline/athomic-docs.git)\ncd athomic-docs\n</code></pre>"},{"location":"#2-start-infrastructure-services","title":"2. Start Infrastructure Services","text":"<p>This command will start all necessary services defined in <code>docker-compose.yml</code>, such as MongoDB, Redis, and Vault.</p> <pre><code>docker-compose up -d\n</code></pre>"},{"location":"#3-install-dependencies","title":"3. Install Dependencies","text":"<p>Install the project's Python dependencies using Poetry.</p> <pre><code>poetry install\n</code></pre>"},{"location":"#4-run-the-application","title":"4. Run the Application","text":"<p>Execute the application using <code>uvicorn</code>. The server will start on <code>http://127.0.0.1:8000</code>.</p> <pre><code>poetry run uvicorn nala.api.main:app --reload\n</code></pre> <p>You can now access the API documentation at <code>http://127.0.0.1:8000/docs</code>.</p>"},{"location":"#explore-the-documentation","title":"Explore the Documentation","text":"<ul> <li>Architecture Overview: Dive deep into the layered architecture and design principles.</li> <li>Dependency Injection: Understand how services are managed.</li> </ul>"},{"location":"#athomic-layer-modules","title":"Athomic Layer Modules","text":""},{"location":"#core-lifecycle","title":"Core &amp; Lifecycle","text":"<ul> <li>Services: The base service lifecycle.</li> <li>Lifecycle Management: How services are started and stopped in order.</li> <li>Plugins: The extensible plugin system.</li> </ul>"},{"location":"#configuration-context","title":"Configuration &amp; Context","text":"<ul> <li>Configuration Management: The <code>Dynaconf</code> and <code>Pydantic</code> based configuration system.</li> <li>Context Management: Handling request-scoped context for tracing and multi-tenancy.</li> </ul>"},{"location":"#data-persistence","title":"Data &amp; Persistence","text":"<ul> <li>Connection Management: The central manager for all data store connections.</li> <li>Document Stores: Abstraction for document databases like MongoDB.</li> <li>Key-Value Stores: Abstraction for KV stores like Redis, with a powerful wrapper system.</li> <li>Transactional Outbox: For guaranteeing at-least-once event delivery.</li> <li>Database Migrations: Version-controlled schema management.</li> <li>File Storage: Abstraction for object storage like GCS or local files.</li> </ul>"},{"location":"#security","title":"Security","text":"<ul> <li>Secrets Management: Securely resolve secrets at runtime from backends like Vault.</li> <li>Authentication &amp; Authorization: Policy-based security for endpoints using JWT or API Keys.</li> <li>Cryptography: High-level abstraction for symmetric encryption.</li> </ul>"},{"location":"#observability","title":"Observability","text":"<ul> <li>Structured Logging: <code>Loguru</code>-based logging with automatic sensitive data masking.</li> <li>Distributed Tracing: End-to-end tracing with OpenTelemetry.</li> <li>Metrics: Out-of-the-box instrumentation with Prometheus.</li> <li>Health &amp; Readiness: Extensible readiness probes for Kubernetes.</li> </ul>"},{"location":"#cross-cutting-concerns","title":"Cross-Cutting Concerns","text":"<ul> <li>Serializer: Pluggable serializers (JSON, Protobuf) for data conversion.</li> <li>Payload Processing Pipeline: A \"Pipes and Filters\" system for composing transformations like encryption and compression.</li> <li>Notifications: Resiliently send emails via SMTP or other providers.</li> <li>Internal Event Bus: In-process Pub/Sub for decoupling internal components.</li> <li>Resilient HTTP Client: A factory for creating pre-configured, resilient clients.</li> </ul>"},{"location":"#resilience-patterns","title":"Resilience Patterns","text":"<ul> <li>Retry</li> <li>Circuit Breaker</li> <li>Rate Limiter</li> <li>Timeout &amp; Cancellation</li> <li>Bulkhead</li> <li>Fallback</li> <li>Distributed Locking</li> <li>Idempotency</li> <li>Distributed Leasing</li> <li>Workload Sharding</li> <li>Backpressure</li> <li>Exponential Backoff</li> <li>Sagas</li> <li>Adaptive Throttling</li> </ul>"},{"location":"README-onboarding/","title":"\ud83d\ude80 Onboarding Guide \u2014 athomic-docs","text":"<p>This guide helps new developers quickly set up the development environment for the <code>athomic-docs</code> project.</p>"},{"location":"README-onboarding/#local-setup","title":"\u2699\ufe0f Local Setup","text":"<ol> <li>Give execution permission to the setup script:</li> </ol> <pre><code>chmod +x bin/setup.sh\n</code></pre> <ol> <li>Run the script with Make:</li> </ol> <pre><code>make setup\n</code></pre> <p>This command will: - Create a local <code>.venv</code> virtual environment with Poetry - Activate the virtual environment (via <code>source</code>) - Install project dependencies - Create the <code>.env</code> file from <code>.env.example</code> (if needed) - Load the <code>GH_TOKEN</code> or <code>CI_JOB_TOKEN</code> variable - Start the application with <code>uvicorn</code></p> <p>\u2139\ufe0f If you prefer to run manually: <code>bash source bin/setup.sh</code></p>"},{"location":"README-onboarding/#versioning-with-semantic-release","title":"\ud83d\udce6 Versioning with Semantic Release","text":"<p>This project uses <code>python-semantic-release</code> for automated version control.</p>"},{"location":"README-onboarding/#run-release-manually","title":"Run release manually:","text":"<pre><code>make release\n</code></pre> <p>Or:</p> <pre><code>poetry run dotenv run -- poetry run semantic-release version\n</code></pre>"},{"location":"README-onboarding/#setting-up-gh_token-on-github","title":"\ud83d\udd10 Setting up GH_TOKEN on GitHub","text":"<ol> <li>Go to: <code>Settings &gt; Secrets and variables &gt; Actions</code></li> <li>Click New repository secret</li> <li>Name: <code>GH_TOKEN</code></li> <li>Value: your personal token with <code>repo</code>, <code>workflow</code>, <code>write:packages</code> scopes</li> </ol>"},{"location":"README-onboarding/#optional-set-up-gh_token-via-github-cli","title":"\ud83d\udcbb (Optional) Set up GH_TOKEN via GitHub CLI","text":"<pre><code>gh auth login\ngh secret set GH_TOKEN -b\"your_token_here\"\n</code></pre>"},{"location":"README-onboarding/#integration-with-gitlab-cicd-future-ready","title":"\ud83d\udee0\ufe0f Integration with GitLab CI/CD (future-ready)","text":"<p>In GitLab CI, the <code>CI_JOB_TOKEN</code> will be automatically used as <code>GH_TOKEN</code>.</p>"},{"location":"README-onboarding/#useful-make-commands","title":"\ud83e\uddf0 Useful Make Commands","text":""},{"location":"README-onboarding/#local-setup_1","title":"\ud83d\udd27 Local setup","text":"<pre><code>make setup\n</code></pre>"},{"location":"adr/ADR-000-sample-doc/","title":"ADR-XXX: [Short Decision Title]","text":"<ul> <li>Status: [Proposed | Accepted | Deprecated | Superseded by ADR-YYY]</li> <li>Date: [YYYY-MM-DD]</li> </ul>"},{"location":"adr/ADR-000-sample-doc/#context","title":"Context","text":"<p>[Describe here the problem or architectural force being addressed. What was the situation? What alternatives were considered? What were the constraints (technical, time, business)?]</p>"},{"location":"adr/ADR-000-sample-doc/#decision","title":"Decision","text":"<p>[Describe the chosen solution clearly and concisely. What decision was made? How will it be implemented?]</p>"},{"location":"adr/ADR-000-sample-doc/#consequences","title":"Consequences","text":"<p>[List the expected or observed results of making this decision. It is important to be honest about the pros and cons.]</p> <ul> <li>Positive:<ul> <li>[Benefit 1]</li> <li>[Benefit 2]</li> </ul> </li> <li>Negative:<ul> <li>[Disadvantage or trade-off 1]</li> <li>[Risk or limitation 1]</li> </ul> </li> <li>Neutral/Other:<ul> <li>[Implication or future consideration]</li> </ul> </li> </ul>"},{"location":"adr/ADR-001-dynaconf-with-pydantic/","title":"ADR-001: Use of Dynaconf + Pydantic for Configuration Management","text":"<ul> <li>Status: Accepted (Implemented)</li> <li>Date: 2025-04-13 (Note: Ideally, use the date the decision was made)</li> </ul>"},{"location":"adr/ADR-001-dynaconf-with-pydantic/#context","title":"Context","text":"<ul> <li>Problem: Modern applications need to manage complex configurations (database, cache, external services, feature flags) that vary across environments (development, testing, production). Using only environment variables or simple files (<code>.ini</code>, <code>.json</code>) becomes difficult to manage, validate, and ensure correct typing.</li> <li>Needs:<ul> <li>Load configurations from multiple sources (files like <code>.toml</code>, <code>.env</code>, environment variables, and potentially Vault in the future [source: 9]).</li> <li>Separate configurations by environment (<code>default</code>, <code>development</code>, <code>production</code>). [source: 18, 20, 21, 22]</li> <li>Rigorously validate configuration values and structure at startup to prevent runtime errors. [source: 235]</li> <li>Provide type-safe access to configurations within the application code.</li> </ul> </li> <li>Alternatives Considered:<ul> <li>Pure Environment Variables: Simple, but impractical for nested structures and lacks intrinsic validation/typing.</li> <li>Pydantic <code>BaseSettings</code>: Excellent for validation and typing, but more limited in loading sources (focuses on env vars and <code>.env</code>).</li> <li><code>.ini</code>/<code>.json</code> files + <code>ConfigParser</code>/<code>json</code>: Less expressive than TOML for complex structures, lack strong integrated validation.</li> <li>Python Modules (<code>config.py</code>): Difficult to override externally (via env vars) and can violate the separation of code and configuration.</li> </ul> </li> </ul>"},{"location":"adr/ADR-001-dynaconf-with-pydantic/#decision","title":"Decision","text":"<ul> <li>Adopt the <code>Dynaconf</code> library as the primary configuration loader, leveraging its ability to read and merge multiple sources and environments. [source: 4, 775-792]</li> <li>Utilize <code>.toml</code> files to define default configurations and environment-specific overrides (<code>settings/settings.toml</code>, <code>settings/development.toml</code>, <code>settings/production.toml</code>). [source: 18, 20-22]</li> <li>Allow the use of <code>.env</code> files for local overrides or secrets during development. [source: 17, 784]</li> <li>Allow environment variables (prefixed with <code>NALA_</code> [source: 775]) to override any values defined in the files.</li> <li>Define the entire expected configuration structure using <code>Pydantic</code> models (starting with <code>AppSettings</code> [source: 817] and including nested models like <code>CacheSettings</code> [source: 845], <code>SecretsSettings</code> [source: 820], <code>DatabaseSettings</code> [source: 846], etc.). [source: 767]</li> <li>Implement a centralized access point (<code>nala.athomic.config.settings.get_settings</code>) [source: 768] that:<ol> <li>Uses <code>DynaconfLoader</code> [source: 769] to load raw configuration data into a dictionary.</li> <li>Validates this dictionary against the <code>AppSettings</code> Pydantic model. [source: 771-774]</li> <li>Returns the validated, type-safe instance of <code>AppSettings</code>.</li> <li>Utilizes <code>@lru_cache</code> [source: 712, 768] to ensure loading and validation occur only once.</li> </ol> </li> </ul>"},{"location":"adr/ADR-001-dynaconf-with-pydantic/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Flexibility: Configuration loading from diverse sources with clear precedence (env var &gt; .env &gt; environment file &gt; default file).</li> <li>Early Validation: Configuration errors (wrong types, missing values) are caught at application startup, not at runtime.</li> <li>Type Safety: Accessing configurations in code (e.g., <code>settings.cache.ttl</code>) is type-safe, aided by type hints and IDE auto-completion.</li> <li>Clear Structure: The configuration structure is explicitly defined and documented by the Pydantic models.</li> <li>Maintainability: Facilitates refactoring and understanding of available configurations.</li> <li>Extensibility: Prepared for adding new sources (like Vault) in the future, integrating them with Dynaconf or as additional providers validated by Pydantic.</li> </ul> </li> <li>Negative:<ul> <li>Dependencies: Adds two significant dependencies (<code>Dynaconf</code>, <code>Pydantic</code>) to the project.</li> <li>Learning Curve: Requires familiarity with Dynaconf's loading rules and Pydantic model definition/validation.</li> <li>Dual Definition: It's necessary to maintain the structure in both the <code>.toml</code> files and the Pydantic models (although aliases help [source: 818, 824]).</li> </ul> </li> <li>Neutral/Other:<ul> <li>Configuration is loaded and validated once at the start (due to <code>@lru_cache</code>), making subsequent access very fast but requiring an application restart to reflect changes (unless a live-reload mechanism is implemented).</li> </ul> </li> </ul>"},{"location":"adr/ADR-002-abstract-layer/","title":"ADR-002: Creation of the <code>athomic</code> Abstraction Layer","text":"<ul> <li>Status: Accepted (Implemented)</li> <li>Date: 2025-04-13 (Note: Ideally, use the date the decision was made)</li> </ul>"},{"location":"adr/ADR-002-abstract-layer/#context","title":"Context","text":"<ul> <li>Problem: Modern APIs rely heavily on cross-cutting infrastructure concerns like caching, configuration management, secrets handling, rate limiting, logging, observability, and resilience patterns (retry, fallback). Mixing this logic directly with the API endpoint handlers (e.g., in FastAPI routes) or business logic leads to high coupling, code duplication, difficulty in testing, and hinders reusability across different projects or API frameworks.</li> <li>Goal: Create a reusable, framework-agnostic core library (<code>athomic-docs</code>) that encapsulates these infrastructure best practices, allowing application developers to focus on business logic. [source: 7, 15, 16]</li> <li>Alternatives Considered:<ul> <li>No Abstraction: Implement infrastructure logic directly within the API framework (e.g., using FastAPI dependencies and middleware). Drawback: High coupling to the framework, poor reusability, harder testing.</li> <li>Utility Functions: Place infrastructure logic in simple utility functions. Drawback: Less organized, harder to enforce consistency, doesn't easily support stateful components or different providers.</li> <li>Multiple Smaller Libraries: Create separate libraries for cache, secrets, etc. Drawback: Increases dependency management overhead, harder to ensure cohesive integration and consistent patterns across libraries.</li> </ul> </li> </ul>"},{"location":"adr/ADR-002-abstract-layer/#decision","title":"Decision","text":"<ul> <li>Create a distinct top-level Python package named <code>nala.athomic</code> within the <code>src/nala/</code> directory. [source: 8]</li> <li>This <code>athomic</code> layer will house all core, reusable infrastructure components, designed to be as independent as possible from specific web frameworks (like FastAPI).</li> <li>Sub-modules within <code>athomic</code> will be organized by concern: <code>cache</code>, <code>config</code>, <code>db</code>, <code>log</code> (safelogger), <code>observability</code>, <code>rate_limiter</code>, <code>security</code>, <code>service_discovery</code>, <code>utils</code>. [source: 8-10, 689-1127]</li> <li>Components within <code>athomic</code> will primarily expose functionality through well-defined interfaces (Protocols/ABCs) [source: 443, 689, 855, 918, 1000, 1065] and decorators [source: 445, 726-735, 942-959, 962, 1092].</li> <li>The <code>nala.api</code> layer (containing FastAPI specifics like routes, middleware, request/response models) will depend on and consume the services provided by the <code>nala.athomic</code> layer.</li> </ul>"},{"location":"adr/ADR-002-abstract-layer/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>High Reusability: <code>athomic</code> components can potentially be reused in other Python applications or services, not just FastAPI APIs. [source: 14, 16]</li> <li>Improved Testability: Core infrastructure logic in <code>athomic</code> can be tested independently of the web framework, often with simpler unit tests.</li> <li>Clear Separation of Concerns: Enforces a clean architecture, separating infrastructure logic from API handling and business rules.</li> <li>Enhanced Maintainability: Changes to infrastructure (e.g., switching cache providers) are localized within <code>athomic</code>, minimizing impact on the API layer.</li> <li>Scalability: Promotes building specialized, robust core components.</li> </ul> </li> <li>Negative:<ul> <li>Increased Indirection: Adds a layer of abstraction, which might slightly increase complexity for simple use cases or for developers unfamiliar with the structure.</li> <li>Potential for Over-Engineering: Need to be mindful not to abstract prematurely or create overly complex interfaces if the benefit isn't clear.</li> </ul> </li> <li>Neutral/Other:<ul> <li>Requires clear documentation explaining the purpose and usage of the <code>athomic</code> layer and its components. [source: 7-16, 175-616]</li> <li>Establishes a strong architectural pattern that should guide future development within the <code>athomic-docs</code> ecosystem.</li> </ul> </li> </ul>"},{"location":"adr/ADR-003-provider-registry-for-extension/","title":"ADR-003: Provider/Registry/Factory Pattern for Extensibility","text":"<ul> <li>Status: Accepted (Implemented in several modules)</li> <li>Date: 2025-04-13 (Note: Ideally, use the date the decision was made)</li> </ul>"},{"location":"adr/ADR-003-provider-registry-for-extension/#context","title":"Context","text":"<ul> <li>Problem: The <code>athomic-docs</code> needs to interact with various external systems or support multiple strategies for core functionalities (e.g., different cache backends like Redis/Memory [source: 715, 716]; different secrets managers like Vault/AWS/Local [source: 873, 898]; different auth methods; different rate limiting storages [source: 1075]; different databases [source: 1007]). Hardcoding specific implementations would make the system rigid, difficult to test, and hard to extend or adapt to different deployment environments or future technologies.</li> <li>Goal: Allow easy swapping and addition of implementations for key infrastructure components without modifying the core consuming logic. Enable configuration-driven selection of implementations. Promote testability by allowing mock providers to be injected. [source: 14, 16]</li> <li>Alternatives Considered:<ul> <li>Conditional Logic (if/else): Directly checking configuration values in the code and executing different logic paths. Drawback: Violates Open/Closed Principle, leads to complex conditional blocks, hard to add new options.</li> <li>Strategy Pattern (Simple): Using classes for strategies but without a central registry or factory. Drawback: Requires manual instantiation and passing of the correct strategy class throughout the application, configuration logic might be scattered.</li> <li>Dependency Injection Framework: Using a full DI framework (like <code>python-dependency-injector</code>). Drawback: Adds a potentially heavier dependency, might be overkill if the primary need is swappable providers based on simple configuration keys.</li> </ul> </li> </ul>"},{"location":"adr/ADR-003-provider-registry-for-extension/#decision","title":"Decision","text":"<ul> <li>Implement a consistent \"Provider/Registry/Factory\" pattern for key extensible components within the <code>athomic</code> layer (Cache, Secrets, Auth, Rate Limiting, DB Repositories, potentially Config loading itself).</li> <li>Provider (Interface): Define a clear interface (using <code>abc.ABC</code> or <code>typing.Protocol</code>) for each component, specifying the required methods and their signatures (e.g., <code>CacheProtocol</code>, <code>SecretsProvider</code>, <code>AuthProvider</code>, <code>AbstractRateLimiter</code>, <code>IRepository</code>). [source: 443, 689, 855, 918, 1000, 1065]</li> <li>Provider (Implementations): Create concrete classes that implement these interfaces for specific technologies or strategies (e.g., <code>RedisCacheProvider</code> [source: 739], <code>VaultSecretsProvider</code> [source: 898], <code>JWTAuthProvider</code> [source: 929], <code>LimitsRateLimiter</code> [source: 1099], <code>MongoUserRepository</code> [source: 1030]).</li> <li>Registry (Optional but common): Maintain a mapping (often a simple dictionary) from a configuration key (e.g., backend name like \"redis\", \"vault\") to the corresponding Provider class (e.g., <code>SecretsRegistry</code> [source: 857], <code>STORAGE_REGISTRY</code> in rate limiter [source: 1075]). This centralizes the knowledge of available implementations.</li> <li>Factory (often Singleton): Create a factory function (e.g., <code>get_cache</code> [source: 712], <code>get_secrets_provider</code> [source: 859], <code>get_repository</code> [source: 1009], <code>get_auth_provider</code> [source: 938]) responsible for:<ol> <li>Reading the relevant configuration (via <code>get_settings</code>).</li> <li>Determining the desired provider implementation based on the configuration key (using the Registry if applicable).</li> <li>Instantiating the chosen Provider class, passing necessary configuration.</li> <li>Often using <code>@lru_cache</code> to return a singleton instance of the configured provider, ensuring efficiency and consistent state.</li> </ol> </li> </ul>"},{"location":"adr/ADR-003-provider-registry-for-extension/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>High Extensibility: Adding support for a new backend (e.g., a new cache provider) involves creating a new class implementing the interface and registering it, with minimal changes to consuming code.</li> <li>Configurability: The active provider is determined by configuration settings, allowing easy adaptation to different environments (e.g., use <code>MemoryCache</code> in tests, <code>RedisCache</code> in production).</li> <li>Improved Testability: Consuming code depends on the interface, making it easy to inject mock providers during unit testing.</li> <li>Decoupling: Core logic interacts with the interface, unaware of the specific implementation details.</li> <li>Consistency: Provides a standard way to handle pluggable components across the framework.</li> </ul> </li> <li>Negative:<ul> <li>Boilerplate: Introduces some boilerplate code (interfaces, registries, factories) for each extensible component.</li> <li>Indirection: Adds a layer of indirection compared to direct instantiation.</li> </ul> </li> <li>Neutral/Other:<ul> <li>Relies heavily on the configuration system (ADR-001) being robust.</li> <li>Requires developers to understand the pattern and where to register new providers.</li> </ul> </li> </ul>"},{"location":"adr/ADR-004-safelogger-with-masking/","title":"ADR-003: Provider/Registry/Factory Pattern for Extensibility","text":"<ul> <li>Status: Accepted (Implemented in several modules)</li> <li>Date: 2025-04-13 (Note: Ideally, use the date the decision was made)</li> </ul>"},{"location":"adr/ADR-004-safelogger-with-masking/#context","title":"Context","text":"<ul> <li>Problem: The <code>athomic-docs</code> needs to interact with various external systems or support multiple strategies for core functionalities (e.g., different cache backends like Redis/Memory [source: 715, 716]; different secrets managers like Vault/AWS/Local [source: 873, 898]; different auth methods; different rate limiting storages [source: 1075]; different databases [source: 1007]). Hardcoding specific implementations would make the system rigid, difficult to test, and hard to extend or adapt to different deployment environments or future technologies.</li> <li>Goal: Allow easy swapping and addition of implementations for key infrastructure components without modifying the core consuming logic. Enable configuration-driven selection of implementations. Promote testability by allowing mock providers to be injected. [source: 14, 16]</li> <li>Alternatives Considered:<ul> <li>Conditional Logic (if/else): Directly checking configuration values in the code and executing different logic paths. Drawback: Violates Open/Closed Principle, leads to complex conditional blocks, hard to add new options.</li> <li>Strategy Pattern (Simple): Using classes for strategies but without a central registry or factory. Drawback: Requires manual instantiation and passing of the correct strategy class throughout the application, configuration logic might be scattered.</li> <li>Dependency Injection Framework: Using a full DI framework (like <code>python-dependency-injector</code>). Drawback: Adds a potentially heavier dependency, might be overkill if the primary need is swappable providers based on simple configuration keys.</li> </ul> </li> </ul>"},{"location":"adr/ADR-004-safelogger-with-masking/#decision","title":"Decision","text":"<ul> <li>Implement a consistent \"Provider/Registry/Factory\" pattern for key extensible components within the <code>athomic</code> layer (Cache, Secrets, Auth, Rate Limiting, DB Repositories, potentially Config loading itself).</li> <li>Provider (Interface): Define a clear interface (using <code>abc.ABC</code> or <code>typing.Protocol</code>) for each component, specifying the required methods and their signatures (e.g., <code>CacheProtocol</code>, <code>SecretsProvider</code>, <code>AuthProvider</code>, <code>AbstractRateLimiter</code>, <code>IRepository</code>). [source: 443, 689, 855, 918, 1000, 1065]</li> <li>Provider (Implementations): Create concrete classes that implement these interfaces for specific technologies or strategies (e.g., <code>RedisCacheProvider</code> [source: 739], <code>VaultSecretsProvider</code> [source: 898], <code>JWTAuthProvider</code> [source: 929], <code>LimitsRateLimiter</code> [source: 1099], <code>MongoUserRepository</code> [source: 1030]).</li> <li>Registry (Optional but common): Maintain a mapping (often a simple dictionary) from a configuration key (e.g., backend name like \"redis\", \"vault\") to the corresponding Provider class (e.g., <code>SecretsRegistry</code> [source: 857], <code>STORAGE_REGISTRY</code> in rate limiter [source: 1075]). This centralizes the knowledge of available implementations.</li> <li>Factory (often Singleton): Create a factory function (e.g., <code>get_cache</code> [source: 712], <code>get_secrets_provider</code> [source: 859], <code>get_repository</code> [source: 1009], <code>get_auth_provider</code> [source: 938]) responsible for:<ol> <li>Reading the relevant configuration (via <code>get_settings</code>).</li> <li>Determining the desired provider implementation based on the configuration key (using the Registry if applicable).</li> <li>Instantiating the chosen Provider class, passing necessary configuration.</li> <li>Often using <code>@lru_cache</code> to return a singleton instance of the configured provider, ensuring efficiency and consistent state.</li> </ol> </li> </ul>"},{"location":"adr/ADR-004-safelogger-with-masking/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>High Extensibility: Adding support for a new backend (e.g., a new cache provider) involves creating a new class implementing the interface and registering it, with minimal changes to consuming code.</li> <li>Configurability: The active provider is determined by configuration settings, allowing easy adaptation to different environments (e.g., use <code>MemoryCache</code> in tests, <code>RedisCache</code> in production).</li> <li>Improved Testability: Consuming code depends on the interface, making it easy to inject mock providers during unit testing.</li> <li>Decoupling: Core logic interacts with the interface, unaware of the specific implementation details.</li> <li>Consistency: Provides a standard way to handle pluggable components across the framework.</li> </ul> </li> <li>Negative:<ul> <li>Boilerplate: Introduces some boilerplate code (interfaces, registries, factories) for each extensible component.</li> <li>Indirection: Adds a layer of indirection compared to direct instantiation.</li> </ul> </li> <li>Neutral/Other:<ul> <li>Relies heavily on the configuration system (ADR-001) being robust.</li> <li>Requires developers to understand the pattern and where to register new providers.</li> </ul> </li> </ul>"},{"location":"adr/ADR-005-cache-strategy/","title":"ADR-005: Advanced Caching Strategy with Decorators and Multiple Backends","text":"<ul> <li>Status: Accepted (Implemented)</li> <li>Date: 2025-04-13 (Note: Using current date as placeholder)</li> </ul>"},{"location":"adr/ADR-005-cache-strategy/#context","title":"Context","text":"<ul> <li>Problem: In distributed systems and high-load APIs, frequent access to slow data sources (databases, external service calls) can become a significant performance and availability bottleneck. Simple caching helps, but doesn't address common large-scale caching issues, such as:<ul> <li>Cache Stampede (Thundering Herd): Multiple concurrent requests/processes attempting to recalculate an expired cache value simultaneously, overwhelming the underlying data source.</li> <li>Expiration Latency: When a cached item expires, the next request needing it must wait for the recalculation time, increasing perceived latency.</li> <li>Need for Multiple Backends &amp; Fallback: The requirement to use different cache types (e.g., local in-memory for ultra-fast access of small/frequent data, Redis for distributed/persistent cache) and having a fallback strategy if the primary cache fails. </li> <li>Code Clutter: Manually implementing cache checking, storage, and invalidation logic within business functions pollutes the core logic, making it repetitive and error-prone.</li> </ul> </li> <li>Goal: Implement a robust, flexible, and easy-to-use caching system for <code>athomic-docs</code> that mitigates the problems above, offers high performance and resilience, and integrates cleanly into the codebase, preferably via decorators.</li> <li>Alternatives Considered:<ul> <li>Manual Caching: Requiring developers to manually implement <code>cache.get()</code>, <code>compute_value()</code>, <code>cache.set()</code> logic at each necessary point. Drawback: Highly repetitive, error-prone, difficult to maintain consistency, doesn't easily accommodate advanced features.</li> <li>Simple Caching (Get/Set Only): Implementing only basic cache operations. Drawback: Fails to solve cache stampede or abrupt expiration issues.</li> <li>Direct Use of Caching Libraries: Using libraries like <code>cachetools</code> or the <code>redis-py</code> client directly within business logic. Drawback: Would require manual implementation of Jitter, Locking, Refresh Ahead, Fallback logic, and integration with configuration/observability.</li> <li>Web Framework Caching (HTTP Level): Relying solely on framework mechanisms for caching full HTTP responses. Drawback: Less granular, not applicable to internal calculations or service calls, doesn't cache raw data or intermediate results effectively.</li> </ul> </li> </ul>"},{"location":"adr/ADR-005-cache-strategy/#decision","title":"Decision","text":"<ul> <li>Implement the <code>nala.athomic.performance.cache</code> module following the Provider/Registry/Factory pattern (ADR-003) to support multiple backends.</li> <li>Initially support <code>MemoryCacheProvider</code> (based on <code>cachetools.TTLCache</code>) and <code>RedisCacheProvider</code> (using async <code>redis-py</code>), both adhering to the <code>CacheProtocol</code> interface.</li> <li>Implement a <code>FallbackCacheProvider</code> allowing a configurable chain of providers (e.g., try Redis, if it fails, try Memory), enhancing resilience. </li> <li>Create a primary decorator <code>@cache_result</code>  as the main way to apply caching to (async) functions.</li> <li>Embed advanced features within <code>@cache_result</code>, controllable via parameters:<ul> <li><code>ttl</code>: Configurable time-to-live for the cached item.</li> <li><code>use_jitter</code>: Apply random variation (jitter) to the final TTL (<code>apply_jitter</code>) to prevent mass simultaneous expirations.</li> <li><code>use_lock</code>: Implement a lock (mutex) mechanism per cache key to prevent cache stampedes. Only the first request/task hitting an expired/missing key acquires the lock to recompute; others wait for the result or lock release. (Note: Current implementation uses <code>asyncio.Lock</code>, only safe for single-instance).</li> <li><code>refresh_ahead</code>: Allow the value to be recomputed in the background before it expires (based on <code>refresh_threshold</code>). Current requests receive the old (stale) value while the new one is generated, ensuring low latency. The recomputed value replaces the old one for subsequent requests. </li> </ul> </li> <li>Create an <code>@invalidate_cache</code> decorator to easily remove specific cache keys before a function executes (useful after write/update/delete operations).</li> <li>Implement a <code>hash_key</code> utility  to generate deterministic, unique cache keys based on the cached function's name, arguments (<code>*args</code>, <code>**kwargs</code>), and a configurable prefix (global or per-decorator), avoiding collisions.</li> <li>Integrate the caching system with the Observability module by emitting Prometheus metrics for cache hits, misses, and errors (<code>cache_hit_counter</code>, <code>cache_miss_counter</code>, <code>cache_error_counter</code>) via a wrapper function like <code>observed_get</code>. </li> </ul>"},{"location":"adr/ADR-005-cache-strategy/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Significant Performance Improvement: Reduces latency and load on slower backend data sources for frequently accessed data.</li> <li>High Resilience &amp; Availability: Mitigates cache stampedes (with <code>use_lock</code>), smooths out cache expiration impacts (with <code>refresh_ahead</code>, <code>use_jitter</code>), and increases overall availability via <code>FallbackCacheProvider</code>.</li> <li>Cleaner &amp; Declarative Code: Caching complexity is encapsulated within the decorators and the <code>athomic.cache</code> module, keeping business logic focused.</li> <li>Ease of Use: Applying advanced caching strategies becomes as simple as adding and configuring the <code>@cache_result</code> decorator to an async function.</li> <li>Granular Configurability: Developers control TTL, expiration strategies (jitter), stampede protection (lock), proactive updates (refresh ahead), backends, and fallbacks via configuration and decorator parameters.</li> <li>Integrated Observability: Built-in metrics provide visibility into cache effectiveness and health.</li> </ul> </li> <li>Negative:<ul> <li>Intrinsic Complexity: The cache module itself is relatively complex due to the combination of advanced features.</li> <li>Cache Consistency Management: As with any caching system, correct cache invalidation (using <code>@invalidate_cache</code> or other strategies) is crucial to avoid serving stale data after the underlying data has been modified. The <code>refresh_ahead</code> strategy intentionally serves stale data for a short period.</li> <li>Debugging Challenges: The introduction of state (the cache) and asynchronous background logic (refresh ahead) can occasionally make debugging certain flows more complex.</li> <li>Distributed Lock Required for Scale: The <code>use_lock=True</code> feature in the current implementation (<code>asyncio.Lock</code> [source: 766]) only provides stampede protection within a single application instance. For a multi-instance deployment, a distributed lock implementation (e.g., using Redis) would be required for this feature to be fully effective (not currently implemented).</li> </ul> </li> <li>Neutral/Other:<ul> <li>The effectiveness of the cache fundamentally depends on choosing the right operations to cache, setting appropriate TTLs, and having a well-thought-out cache invalidation strategy.</li> </ul> </li> </ul>"},{"location":"adr/ADR-006-rate-limit-with-limits-lib/","title":"ADR-006: Choice of <code>limits</code> Library for Rate Limiting","text":"<ul> <li>Status: Accepted (Implemented)</li> <li>Date: 2025-04-13 (Note: Using current date as placeholder)</li> </ul>"},{"location":"adr/ADR-006-rate-limit-with-limits-lib/#context","title":"Context","text":"<ul> <li>Problem: APIs, whether public or internal, require protection against abuse (intentional or accidental) and overload. A mechanism is needed to limit the number of requests a client (identified by IP, user, token, etc.) can make within a specific time period. Implementing this logic from scratch is complex, requiring management of counters, time windows (fixed, sliding), and state storage (in-memory, Redis, etc.).</li> <li>Goal: Adopt a robust, flexible, well-maintained rate limiting solution for <code>athomic-docs</code> that integrates well with the project's asynchronous Python architecture, supporting different limiting strategies and storage backends. [source: 184]</li> <li>Alternatives Considered:<ul> <li>Manual Implementation: Building all the logic for counting, time windows, and storage internally. Drawback: Very complex to get right (especially sliding window algorithms and distributed consistency), high development and maintenance cost, reinvents the wheel.</li> <li>Framework-Specific Rate Limiting Libraries: Using solutions coupled to specific web frameworks, like <code>slowapi</code> for FastAPI. Drawback: Ties the rate limiting logic to the web framework, making it difficult to reuse outside the FastAPI context (e.g., background tasks, internal calls) or to migrate to another framework later.</li> <li>Other Generic Python Libraries: Evaluating alternatives like <code>pyrate-limiter</code>. Drawback: At the time of evaluation, the <code>limits</code> library appeared more mature, offering more strategies, support for multiple storage backends [source: 1075], and good documentation.</li> <li>Rate Limiting at the Infrastructure Level (Gateway/Infra): Relying solely on rate limiting configured in API Gateways (Kong, Nginx, Cloudflare, etc.) or Load Balancers. Drawback: Offers less granularity (can be hard to apply specific limits per authenticated user or per complex operation within the API), the logic resides outside the application, harder to unit test. Often used as a complementary layer, but doesn't replace the need for application-level rate limiting in many scenarios.</li> </ul> </li> </ul>"},{"location":"adr/ADR-006-rate-limit-with-limits-lib/#decision","title":"Decision","text":"<ul> <li>Adopt the <code>limits</code> Python library [source: 4, 12, 220] as the foundation for rate limiting functionality within <code>athomic-docs</code>.</li> <li>Create a dedicated module <code>nala.athomic.rate_limiter</code> to encapsulate interaction with the <code>limits</code> library.</li> <li>Define an <code>AbstractRateLimiter</code> interface [source: 1065] to standardize interaction with the rate limiting system.</li> <li>Implement a <code>LimitsRateLimiter</code> provider [source: 1099] that utilizes the <code>limits</code> library internally. This provider supports:<ul> <li>Configurable strategies (<code>fixed</code> window, <code>moving</code> window). [source: 1099]</li> <li>Multiple storage backends supported by the library, configured via a <code>storage_uri</code> [source: 853, 1099] (e.g., <code>memory://</code>, <code>redis://</code>, <code>memcached://</code> [source: 1075]).</li> </ul> </li> <li>Expose the functionality through:<ul> <li>An <code>@rate_limited</code> decorator [source: 1092] for easy application to async functions.</li> <li>A <code>core.py</code> module [source: 1076] for more programmatic usage if needed (e.g., in middleware).</li> </ul> </li> <li>Manage configuration (default backend, strategy, storage URI, key prefix) via the <code>RateLimiterSettings</code> Pydantic model [source: 853] within the centralized configuration system (ADR-001).</li> </ul>"},{"location":"adr/ADR-006-rate-limit-with-limits-lib/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Reuses Robust Logic: Leverages a well-tested and maintained library (<code>limits</code>) that correctly implements complex rate limiting algorithms (fixed window, moving/sliding window).</li> <li>Flexibility of Strategy &amp; Backend: Allows choosing the limiting strategy and storage backend via configuration without changing application code. [source: 1075, 1099]</li> <li>Clean Integration: Using the <code>AbstractRateLimiter</code> interface and the <code>@rate_limited</code> decorator keeps business logic decoupled from rate limiting implementation details.</li> <li>Asynchronous Support: The <code>limits</code> library has good support for <code>asyncio</code>.</li> <li>Community &amp; Documentation: <code>limits</code> is reasonably well-established and documented within the Python community.</li> </ul> </li> <li>Negative:<ul> <li>External Dependency: Adds the <code>limits</code> library as a project dependency.</li> <li>Library Limitations: <code>athomic-docs</code> inherits the features and limitations of the <code>limits</code> library. For instance, the difficulty in implementing <code>clear</code> for a specific key [source: 1103] is an inherited limitation.</li> <li>Abstraction Layer: Creating the <code>LimitsRateLimiter</code> wrapper adds a small layer of indirection over the original library.</li> </ul> </li> <li>Neutral/Other:<ul> <li>Correct configuration of the <code>storage_uri</code> and strategy is crucial for expected behavior in different environments (single-instance vs. multi-instance).</li> <li>Requires developers to understand basic rate limiting concepts (limit, window, strategies) to use the feature effectively.</li> </ul> </li> </ul>"},{"location":"adr/ADR-007-abstraction-repository-and-beaine-adoption/","title":"ADR-007: Repository Abstraction and Beanie ODM for MongoDB","text":"<ul> <li>Status: Accepted (Implemented)</li> <li>Date: 2025-04-13 (Placeholder)</li> </ul>"},{"location":"adr/ADR-007-abstraction-repository-and-beaine-adoption/#context","title":"Context","text":"<ul> <li>Problem: Applications need to interact with data persistence layers (databases). Directly using database driver code (e.g., <code>motor</code> for async MongoDB [source: 5]) or even a specific Object-Document Mapper (ODM) / Object-Relational Mapper (ORM) (like <code>Beanie</code> [source: 5], SQLAlchemy) within business logic (services) or API handlers creates tight coupling between the application logic and the specific database technology and its access patterns. This coupling makes it difficult to:<ul> <li>Switch database technologies in the future (e.g., migrating from MongoDB to PostgreSQL) without extensive refactoring.</li> <li>Unit test business logic in isolation without needing a live database connection or complex mocking of the database driver/ODM.</li> <li>Maintain consistency in how data is accessed and manipulated across different parts of the application.</li> <li>Evolve the data model and access logic independently of the business rules.</li> </ul> </li> <li>Goal: Abstract the data access logic behind a stable, consistent interface using the Repository pattern. This aims to decouple the business logic from the specific database implementation, thereby improving testability, maintainability, and the potential for future database migrations. Additionally, leverage an asynchronous ODM for MongoDB to simplify database interactions, provide schema validation close to the database, and integrate well with the FastAPI/asyncio ecosystem.</li> <li>Alternatives Considered:<ul> <li>Direct Driver/ODM Usage: Using <code>motor</code> or <code>Beanie</code> directly within service classes or FastAPI route functions. Drawback: Leads to high coupling, poor testability, and mixes data access concerns with business/API logic.</li> <li>Generic ORM/ODM Only (e.g., SQLAlchemy Core, Pymongo): Using lower-level database interaction tools. Drawback: Requires significantly more boilerplate code for common CRUD (Create, Read, Update, Delete) operations compared to a higher-level ODM like Beanie. Type safety for data models might be less integrated.</li> <li>Data Access Objects (DAOs): A pattern similar to Repositories, but sometimes considered more focused on simple CRUD operations per table/collection. Decision: The Repository pattern felt more aligned with encapsulating operations related to a whole business entity/aggregate, potentially including more complex queries beyond simple CRUD.</li> <li>Other Async ODMs for MongoDB: Evaluating alternatives to Beanie (e.g., <code>MongoEngine</code> - though async support might be less mature, <code>ODMantic</code> - another Pydantic-based option). Decision: Beanie was chosen due to its strong integration with Pydantic and FastAPI, its active development, and its focus on async. [source: 5]</li> </ul> </li> </ul>"},{"location":"adr/ADR-007-abstraction-repository-and-beaine-adoption/#decision","title":"Decision","text":"<ul> <li>Implement the Repository Pattern as the standard way to abstract data persistence logic within <code>athomic-docs</code>.</li> <li>Define a generic <code>IRepository</code> Protocol [source: 1000] using <code>typing.Protocol</code> to specify the common contract for data access, including methods like <code>get_by_id</code>, <code>save</code>, <code>delete</code>, <code>find</code>, <code>find_one</code>.</li> <li>Define specific repository interfaces (e.g., <code>IUserRepository</code> [source: 1006]) that inherit from <code>IRepository</code> for each core domain entity (like User). These specific interfaces can include methods relevant only to that entity (e.g., <code>get_by_email</code>).</li> <li>Utilize the <code>Beanie</code> ODM [source: 5] as the chosen tool for interacting with MongoDB. Beanie leverages <code>motor</code> for asynchronous operations and <code>Pydantic</code> for defining document models (<code>beanie.Document</code>) [source: 1030, 1142], providing data validation and a convenient API.</li> <li>Implement concrete repository classes specifically for the MongoDB backend using Beanie. This includes:<ul> <li>A generic <code>MongoBeanieRepository</code> [source: 1031] class that implements the <code>IRepository</code> protocol using Beanie's methods.</li> <li>Specific repository classes (e.g., <code>MongoUserRepository</code> [source: 1035]) that inherit from the generic Beanie repository and the specific domain interface (e.g., <code>IUserRepository</code>), implementing any additional required methods using Beanie queries.</li> </ul> </li> <li>Utilize the Factory Pattern via the <code>get_repository</code> function [source: 1009]. This function:<ol> <li>Reads the configured database backend from settings (<code>settings.database.backend</code> [source: 846, 1010]).</li> <li>Uses a mapping (<code>_REPOSITORY_IMPLEMENTATIONS</code> [source: 1007]) to look up the appropriate concrete repository class based on the requested interface type and the configured backend.</li> <li>Instantiates and returns the concrete repository class (often cached via <code>@lru_cache</code> for efficiency).</li> </ol> </li> <li>Centralize the MongoDB connection and Beanie initialization logic in <code>nala.athomic.db.mongo.beanie_init.py</code> [source: 1036]. This initialization (<code>init_database_connection</code> [source: 1038]) is designed to be called once during the application startup sequence (e.g., within the FastAPI <code>lifespan</code> manager [source: 1151]).</li> </ul>"},{"location":"adr/ADR-007-abstraction-repository-and-beaine-adoption/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Decoupling: Business logic components (services) depend only on the repository interfaces, making them theoretically agnostic to the underlying database technology (MongoDB/Beanie in this implementation).</li> <li>Improved Testability: Business logic can be easily unit-tested by injecting mock repository objects that conform to the defined interfaces, eliminating the need for a live database in unit tests.</li> <li>Centralized Data Access Logic: All database interaction logic for a given entity is concentrated within its repository implementation, improving code organization, maintainability, and consistency.</li> <li>Developer Productivity (for MongoDB): Beanie significantly simplifies common MongoDB operations (CRUD, indexing, potentially data migrations) and integrates seamlessly with Pydantic for data validation at the ODM level. [source: 1031-1034]</li> <li>Extensibility: Adding support for a different database technology (e.g., PostgreSQL with SQLAlchemy) would primarily involve creating new repository classes implementing the existing interfaces and updating the factory mapping, with minimal changes to the business logic layer. [source: 1008]</li> </ul> </li> <li>Negative:<ul> <li>Layer of Indirection: Introduces an extra layer of abstraction (interfaces, factory) compared to using the ODM or database driver directly.</li> <li>Boilerplate Code: Requires defining interface files and implementation classes for each repository, adding some boilerplate.</li> <li>Potential for Leaky Abstraction: Very complex or database-specific queries might be awkward or inefficient to express through the generic repository interface. This could necessitate adding specialized methods to interfaces or accepting that some database-specific constructs (like Beanie's query syntax) might leak into the criteria passed to methods like <code>find</code> or <code>find_one</code> [source: 1034].</li> <li>Dependency on Beanie (for Mongo): While the business logic is decoupled from MongoDB in principle, the current concrete implementation is tightly coupled to the Beanie ODM. Migrating away from Beanie, even while still using MongoDB, would require rewriting the repository implementations.</li> </ul> </li> <li>Neutral/Other:<ul> <li>The current pattern assumes a single primary database is configured via <code>settings.database</code> [source: 846]. Supporting connections to multiple databases simultaneously (of the same or different types) would require extending the configuration schema and the repository factory logic.</li> <li>The correct functioning relies on the successful initialization of the database connection and Beanie ODM during application startup (<code>init_database_connection</code> [source: 1038]). Failures during startup need to be handled appropriately.</li> </ul> </li> </ul>"},{"location":"adr/ADR-008-observability-stack-choice/","title":"ADR-008: Observability Stack Choice - Prometheus for Metrics, OpenTelemetry for Tracing","text":"<ul> <li>Status: Partially Accepted (Metrics implemented, Tracing in progress)</li> <li>Date: 2025-04-13 (Placeholder)</li> </ul>"},{"location":"adr/ADR-008-observability-stack-choice/#context","title":"Context","text":"<ul> <li>Problem: Operating complex, distributed systems like those built upon <code>athomic-docs</code> requires deep visibility into their runtime behavior. Without adequate observability, diagnosing performance issues, debugging errors across components, and understanding system health under load becomes extremely difficult. We need ways to answer questions like \"Why is this request slow?\", \"What component failed?\", \"Are we hitting resource limits?\", \"What's the error rate for service X?\". This requires collecting different kinds of telemetry data.</li> <li>Needs:<ul> <li>Metrics: Aggregate numerical data over time (e.g., request counts, latencies, error rates, queue sizes, cache hit/miss ratios [source: 722, 967-969]) for monitoring overall health, identifying trends, setting alerts, and capacity planning.</li> <li>Traces: Detailed, request-scoped information showing the path and timing of a request as it flows through different components or services (distributed tracing), essential for pinpointing bottlenecks and understanding complex interactions.</li> <li>Logs: Contextual, event-based information (often textual) for specific points in time, primarily used for detailed error diagnosis and understanding specific events. (Logging itself is covered by ADR-004, but its integration with tracing is relevant here).</li> </ul> </li> <li>Goal: Implement a comprehensive and standardized observability solution for <code>athomic-docs</code>, leveraging industry-standard, preferably open-source, tools and protocols. This ensures interoperability with common monitoring ecosystems (like Prometheus Operator, Grafana, Jaeger) and avoids vendor lock-in where possible. [source: 279]</li> <li>Alternatives Considered:<ul> <li>Proprietary APM Solutions (e.g., Datadog, New Relic, Dynatrace): These offer integrated platforms for metrics, tracing, and logging, often with easy setup via agents. Drawback: Can lead to vendor lock-in, potentially higher costs depending on scale, and less control over the data format and collection mechanisms.</li> <li>Metrics Only (Prometheus): Implementing only metrics collection using Prometheus. Drawback: While essential for monitoring and alerting, it lacks the detailed request-level visibility needed to diagnose complex latency issues or understand distributed workflows, which tracing provides.</li> <li>Tracing Only (e.g., Jaeger Client Libraries Directly): Implementing only distributed tracing. Drawback: Lacks the high-level aggregated view and alerting capabilities provided by metrics. Can be harder to grasp overall system health trends just from traces.</li> <li>Logging-Centric Stack (e.g., ELK - Elasticsearch, Logstash, Kibana): Primarily focused on log aggregation and analysis. Can be extended for metrics (Metricbeat) and tracing (Elastic APM). Drawback: Often more resource-intensive, setup can be complex, and Prometheus/OpenTelemetry are arguably more purpose-built and standard for metrics/tracing respectively in many cloud-native environments.</li> <li>Other Open Standards: Using older standards like OpenCensus (now merged into OpenTelemetry) or focusing solely on OpenMetrics (a Prometheus exposition format standard). Decision: OpenTelemetry emerged as the leading open standard for tracing, integrating aspects of OpenTracing and OpenCensus. Prometheus remains the de facto standard for metrics scraping.</li> </ul> </li> </ul>"},{"location":"adr/ADR-008-observability-stack-choice/#decision","title":"Decision","text":"<ul> <li>Adopt a best-of-breed approach using industry-standard open specifications and tools:<ul> <li>Metrics: Prometheus.<ul> <li>Utilize the official <code>prometheus-client</code> library [source: 5] for defining and exposing custom application metrics (Counters, Gauges, Histograms for requests, cache stats, secret access, auth events, etc.). [source: 967-969]</li> <li>Leverage <code>prometheus-fastapi-instrumentator</code> [source: 5] and/or custom middleware (<code>RequestMetricsMiddleware</code> [source: 1131]) to automatically instrument FastAPI HTTP requests for standard metrics (count, latency, in-progress).</li> <li>Expose all collected metrics via a standard <code>/metrics</code> HTTP endpoint [source: 1150] in the Prometheus exposition format, allowing a Prometheus server to scrape the data.</li> </ul> </li> <li>Tracing: OpenTelemetry (OTel). [source: 13, 187]<ul> <li>Standardize on OpenTelemetry as the vendor-neutral framework for generating, collecting, and exporting trace data. [source: 5]</li> <li>Utilize OTel instrumentation libraries (e.g., <code>opentelemetry-instrumentation-fastapi</code>, <code>opentelemetry-instrumentation-requests</code>, etc. [source: 5]) for automatic trace context propagation and span creation for supported libraries and frameworks. A basic setup exists in <code>src/nala/api/middleware/tracing.py</code>. [source: 1129]</li> <li>Configure an OTLP (OpenTelemetry Protocol) Exporter [source: 1129] (specifically <code>OTLPSpanExporter</code> initially) to send trace data to a compatible backend (e.g., Jaeger, Grafana Tempo, SigNoz, or commercial APM tools). The backend endpoint, protocol (gRPC/HTTP), headers, and sampling rate will be configurable via <code>ObservabilitySettings</code>. [source: 815, 816]</li> <li>Plan for manual instrumentation (creating spans explicitly) within critical sections of the <code>athomic</code> layer or complex business logic where auto-instrumentation doesn't provide sufficient detail (currently a TODO/roadmap item). [source: 187, 541]</li> </ul> </li> <li>Logging Integration: While handled by SafeLogger (ADR-004), ensure that trace context (Trace ID, Span ID) can be automatically injected into log records (a common OTel logging library integration pattern, not yet explicitly shown in the provided code but essential for correlating logs with traces).</li> </ul> </li> <li>Centralize all observability configurations (enable/disable flags, exporter details, sampling rates) within the <code>ObservabilitySettings</code> Pydantic model [source: 813] managed by the central configuration system (ADR-001).</li> </ul>"},{"location":"adr/ADR-008-observability-stack-choice/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Industry Standard &amp; Interoperability: Aligns <code>athomic-docs</code> with widely adopted open standards, ensuring compatibility with a vast ecosystem of monitoring, alerting, and analysis tools (Grafana, Prometheus Operator, Jaeger, various OTel backends).</li> <li>Vendor Neutrality (especially for Tracing): OpenTelemetry allows switching tracing backends without changing the application instrumentation code.</li> <li>Comprehensive Visibility: The combination of metrics (aggregate view, alerting) and traces (deep request-level diagnosis) provides powerful observability.</li> <li>Rich Ecosystem &amp; Community: Benefits from the extensive tooling, documentation, and community support surrounding Prometheus and OpenTelemetry.</li> <li>Automation Potential: OTel auto-instrumentation reduces the effort required for basic tracing coverage.</li> <li>Consistency: Establishes a standardized approach for instrumenting code within the <code>athomic-docs</code>.</li> </ul> </li> <li>Negative:<ul> <li>Operational Complexity: Requires setting up and managing a potentially complex external stack (Prometheus server, OTel Collector (recommended), tracing storage/backend like Jaeger or Tempo, visualization tool like Grafana).</li> <li>Performance Overhead: Collecting detailed metrics and traces inevitably introduces some performance overhead on the application, although tools are designed to minimize this. Trace sampling [source: 816] is often necessary in high-volume systems but means some requests won't be traced.</li> <li>Learning Curve: Developers need to understand the core concepts of metrics (types, labels, cardinality) and distributed tracing (spans, context, propagation) to instrument code effectively and interpret the data.</li> <li>Incomplete Implementation (Current State): While the foundation is laid and metrics are functional, the tracing implementation requires further work to be fully realized (completing OTel setup, adding manual instrumentation where needed, ensuring log correlation). [source: 13, 180, 194]</li> </ul> </li> <li>Neutral/Other:<ul> <li>Effective monitoring requires careful dashboard design (e.g., in Grafana) and well-configured alerts based on the collected metrics.</li> <li>The usefulness of tracing heavily depends on consistent context propagation across asynchronous tasks and service boundaries (if applicable), as well as sufficient instrumentation coverage (both automatic and manual).</li> <li>Managing metric cardinality (the number of unique label combinations) is important to avoid overloading the Prometheus server.</li> </ul> </li> </ul>"},{"location":"adr/ADR-009-resilience-partterns-by-decorators/","title":"ADR-009: Implementation of Resilience Patterns via Decorators","text":"<ul> <li>Status: Accepted (Implemented for Retry and Fallback; Circuit Breaker/Timeout Planned)</li> <li>Date: 2025-04-13 (Placeholder)</li> </ul>"},{"location":"adr/ADR-009-resilience-partterns-by-decorators/#context","title":"Context","text":"<ul> <li>Problem: Network requests, database calls, and interactions with external services are inherently unreliable in distributed systems. Transient failures (network glitches, temporary service unavailability, brief resource contention) and persistent failures can occur. Applications need strategies to handle these failures gracefully to avoid cascading failures, improve user experience, and maintain overall system stability. Implementing logic for retrying operations, falling back to alternative data sources or logic paths, or preventing repeated calls to failing services (circuit breaking) directly within the primary business logic flow makes the code verbose, repetitive, error-prone, and harder to read and test.</li> <li>Goal: Provide reusable, declarative, and minimally invasive mechanisms within <code>athomic-docs</code> to apply common resilience patterns (specifically Retry and Fallback initially, with plans for Circuit Breaker and Timeout) to function calls, particularly those involving I/O or external dependencies.</li> <li>Alternatives Considered:<ul> <li>Manual Implementation: Developers manually write <code>try...except</code> blocks with retry loops, sleep intervals, or conditional fallback logic within each function that needs resilience. Drawback: Highly repetitive, difficult to ensure consistency in retry strategies (e.g., backoff timing), error-prone, significantly clutters business logic.</li> <li>Utility Functions/Classes (Wrappers): Creating helper functions or context managers that accept the function to be executed and apply retry/fallback logic around it. Drawback: Can still be somewhat verbose to apply, requires passing callables around, less declarative than decorators.</li> <li>Aspect-Oriented Programming (AOP) Frameworks: Utilizing more heavyweight AOP frameworks (less common in the Python ecosystem compared to Java/C#). Drawback: Introduces significant complexity, potentially adds \"magic\" that obscures control flow, likely overkill for these specific patterns.</li> <li>Infrastructure-Level Resilience: Relying solely on retry/fallback mechanisms provided by service meshes (like Istio), API gateways, or load balancers. Drawback: Operates at a coarser granularity (HTTP requests), lacks application-specific context (e.g., cannot easily fall back to reading from a cache or using a default value computed by application logic), moves resilience logic outside the application's direct control and testing scope. Often better used as a complementary layer.</li> </ul> </li> </ul>"},{"location":"adr/ADR-009-resilience-partterns-by-decorators/#decision","title":"Decision","text":"<ul> <li>Implement core resilience patterns primarily as Python decorators, residing in <code>nala.athomic.utils.decorators</code>. [source: 942] This approach offers a declarative syntax that separates the resilience concern from the function's core logic.</li> <li>Retry Pattern:<ul> <li>Implement a <code>@retry_handler.with_retry()</code> decorator. [source: 944]</li> <li>Leverage the robust and well-maintained <code>tenacity</code> library [source: 4] internally to handle the retry logic (e.g., <code>stop_after_attempt</code>, <code>wait_exponential</code>, <code>retry_if_exception_type</code>). [source: 946]</li> <li>Make the decorator configurable regarding the number of attempts, wait strategy/timing, and the specific exceptions that should trigger a retry. [source: 943-946]</li> <li>Integrate basic logging to indicate when retries are occurring. [source: 946, 947]</li> </ul> </li> <li>Fallback Pattern:<ul> <li>Implement a <code>@fallback_handler()</code> decorator [source: 950] that accepts a list of one or more fallback functions as arguments. [source: 953]</li> <li>If the decorated function raises an exception, the decorator will iterate through the provided fallback functions, calling them sequentially until one executes successfully (without raising an exception). [source: 951, 955-958]</li> <li>The decorator must correctly handle both synchronous and asynchronous decorated functions and fallback functions, inspecting them to determine whether <code>await</code> is needed. [source: 954-959]</li> <li>If the original function and all provided fallback functions fail, the decorator will raise a custom <code>FallbackError</code> exception [source: 950] containing the list of all encountered errors for diagnostic purposes. [source: 146, 149]</li> </ul> </li> <li>Circuit Breaker &amp; Timeout Patterns: Recognize these as essential resilience patterns but defer their implementation for now. They are noted in the project backlog [source: 186, 219, 591]. The intention is to implement them using a similar decorator-based approach in the future when prioritized.</li> <li>Usage: Encourage the application of these decorators primarily to methods within the <code>athomic</code> layer that interact with external systems (e.g., database repository methods, secret provider <code>get_secret</code> calls [source: 871, 876, 888], cache provider interactions) and potentially in the <code>api</code> service layer for coordinating calls that might fail.</li> </ul>"},{"location":"adr/ADR-009-resilience-partterns-by-decorators/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Improved Code Readability &amp; Focus: Business logic within decorated functions remains clean and focused on its primary responsibility, as the resilience logic is handled declaratively by the decorator.</li> <li>Reduced Boilerplate: Significantly reduces the need for repetitive <code>try...except</code> blocks and manual retry/fallback logic throughout the codebase.</li> <li>Consistency: Promotes a standardized and consistent way of implementing retry and fallback mechanisms across the application.</li> <li>Reusability: Decorators are highly reusable components that can be applied to any compatible function (sync/async depending on the decorator).</li> <li>Configurability: The behavior of retries (attempts, backoff, specific exceptions) and the fallback chain are configurable per usage.</li> <li>Leverages Existing Library: The retry implementation benefits from the robustness and features of the <code>tenacity</code> library.</li> </ul> </li> <li>Negative:<ul> <li>Decorator Implementation Complexity: The decorators themselves, particularly <code>@fallback_handler</code> which needs to correctly manage both sync and async functions [source: 954-959], have non-trivial implementation logic.</li> <li>Potential Obscurity (\"Magic\"): For developers unfamiliar with decorators or the specific implementation, the added behavior might seem implicit or \"magical\". Clear documentation and naming are crucial.</li> <li>Debugging: Debugging issues within the decorator's execution flow can sometimes be less straightforward than debugging inline code.</li> <li>Incomplete Resilience Suite (Currently): The critical patterns of Circuit Breaker and Timeout are currently missing from the implemented set, requiring future work to achieve a more comprehensive resilience strategy. [source: 186]</li> </ul> </li> <li>Neutral/Other:<ul> <li>Requires developers to understand when and where it's appropriate to apply the <code>@retry_handler</code> versus the <code>@fallback_handler</code> (or potentially both, though careful consideration of interaction is needed).</li> <li>The effectiveness of the fallback pattern is entirely dependent on the logic provided within the fallback functions; they must provide a meaningful alternative or default.</li> </ul> </li> </ul>"},{"location":"adr/ADR-010-development-tooling-stack-choice/","title":"ADR-010: Development Tooling Stack Choice","text":"<ul> <li>Status: Accepted (Largely Implemented)</li> <li>Date: 2025-04-13 (Placeholder)</li> </ul>"},{"location":"adr/ADR-010-development-tooling-stack-choice/#context","title":"Context","text":"<ul> <li>Problem: Building and maintaining a high-quality software project like <code>athomic-docs</code> requires a robust ecosystem of development tools beyond just a text editor and the Python interpreter. Without a standardized and automated tooling stack, projects suffer from inconsistencies in code style, undetected bugs, cumbersome dependency management, insecure practices, slow release cycles, and a poor developer experience (DX). Relying on manual checks for quality and security is unreliable and doesn't scale.</li> <li>Goal: Define and implement a modern, integrated, and efficient development tooling stack for the <code>athomic-docs</code> project. This stack should automate quality checks, enforce consistency, manage dependencies reliably, enhance security posture, streamline the release process, and ultimately improve developer productivity and code maintainability.</li> <li>Alternatives Considered:<ul> <li>Minimal/Manual Tooling: Relying only on <code>pip</code> and manual execution of checks. Drawback: Inefficient, inconsistent, error-prone, poor DX.</li> <li>Traditional <code>requirements.txt</code>/<code>setup.py</code>: Standard Python packaging but less integrated than Poetry for dependency locking, environment management, and build lifecycle.</li> <li>Separate Linters/Formatters (Flake8, isort, pycodestyle, Pylint): The traditional stack for code quality. Drawback: Slower overall performance compared to Ruff, requires managing multiple configurations and dependencies. [source: 549]</li> <li>Standard <code>unittest</code> Framework: Python's built-in testing library. Drawback: Generally considered more verbose and less feature-rich (e.g., powerful fixtures, plugin ecosystem) compared to pytest.</li> <li>Alternative CI/CD Systems (Jenkins, GitLab CI, CircleCI): Viable alternatives to GitHub Actions. Decision: GitHub Actions [source: 557] provides tight integration with the GitHub repository, secrets management, and a large marketplace of reusable actions.</li> <li>Manual Versioning/Release Process: Manually updating version numbers, creating tags, and writing changelogs. Drawback: Tedious, error-prone, easy to forget steps.</li> </ul> </li> </ul>"},{"location":"adr/ADR-010-development-tooling-stack-choice/#decision","title":"Decision","text":"<ul> <li>Standardize the <code>athomic-docs</code> project on the following integrated tooling stack, primarily configured via <code>pyproject.toml</code> where possible:<ul> <li>Dependency Management &amp; Packaging: Poetry [source: 4]. Used for declaring, installing, and locking project dependencies (both main and development), managing virtual environments (<code>.venv</code> in project [source: 26]), and building/publishing the package. Configuration in <code>pyproject.toml</code>.</li> <li>Code Formatting: Black [source: 4, 551]. Adopted as the automatic, opinionated code formatter to ensure a consistent style across the entire codebase, eliminating style debates. Configuration in <code>pyproject.toml</code>.</li> <li>Linting: Ruff [source: 4, 548]. Chosen as the extremely fast linter and code quality tool, configured to enforce a wide range of checks (including those traditionally covered by Flake8, isort, pyupgrade, etc.). Configuration in <code>pyproject.toml</code>.</li> <li>Type Checking: mypy [source: 6, 553]. Used for static type analysis to catch type errors before runtime. Configured via <code>mypy.ini</code> [source: 17] (or <code>pyproject.toml</code>), aiming for strict checks where practical.</li> <li>Testing Framework: pytest [source: 5, 555]. Selected for its concise syntax, powerful fixture system, extensive plugin ecosystem, and good support for testing asynchronous code via pytest-asyncio [source: 5]. Configuration in <code>pytest.ini</code> [source: 1].</li> <li>Local Development Hooks: pre-commit [source: 6, 30]. Used to configure and run linters (Ruff), formatters (Black), type checkers (mypy), security scanners (Bandit, detect-secrets), and potentially other checks automatically before each commit, providing fast feedback to developers. Configured via <code>.pre-commit-config.yaml</code> (implied by setup script [source: 30]).</li> <li>Continuous Integration (CI): GitHub Actions [source: 23, 557]. Used to automatically run the full suite of quality checks (linting, formatting check, type checking, unit tests, integration tests, security scans) on every push and pull request to the main branches. Workflow(s) defined in <code>.github/workflows/</code>. [source: 558-561]</li> <li>Automated Versioning &amp; Release: python-semantic-release [source: 4, 199]. Adopted to automate version management according to Semantic Versioning, generate <code>CHANGELOG.md</code> [source: 2] entries, create Git tags, and draft GitHub releases based on Conventional Commit messages. Configured in <code>pyproject.toml</code>.</li> <li>Security Scanning:<ul> <li>Bandit [source: 6, 562]: For static analysis targeting common Python security vulnerabilities within the application code (<code>src/</code>).</li> <li>detect-secrets [source: 6, 611]: To scan the codebase for accidentally committed secrets, managed via an audited <code>.secrets.baseline</code> file [source: 614].</li> <li>(Potentially <code>Safety</code> [source: 562] or Poetry's built-in checks for dependency vulnerabilities).</li> </ul> </li> <li>Documentation Generation: MkDocs [source: 6] with the MkDocs Material theme [source: 6] and the mkdocstrings plugin [source: 6] for generating user-friendly project documentation from Markdown files in the <code>docs/</code> directory and from Python docstrings within the code.</li> <li>Helper Scripting Framework: Click [source: 645]. Used for building internal command-line interface (CLI) tools provided in the <code>helpers/</code> directory (e.g., <code>snapshot.py</code>, <code>mypy-fix</code>, <code>fix-callables</code>).</li> </ul> </li> </ul>"},{"location":"adr/ADR-010-development-tooling-stack-choice/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>High Code Quality &amp; Consistency: Automated enforcement of linting, formatting, and type rules leads to cleaner, more readable, and more maintainable code.</li> <li>Improved Developer Productivity (DX): Automation reduces manual effort (formatting, dependency management, releases). Fast local feedback via <code>pre-commit</code>. Poetry simplifies environment and dependency handling.</li> <li>Increased Reliability &amp; Confidence: Comprehensive automated testing via <code>pytest</code> and CI builds confidence in code changes and helps prevent regressions.</li> <li>Enhanced Security Posture: Automated security scanning (Bandit, detect-secrets) helps identify and prevent common vulnerabilities and accidental secret exposure early in the development cycle.</li> <li>Modern &amp; Standardized: Utilizes tools widely recognized as modern best practices in the Python ecosystem, facilitating onboarding for developers familiar with these tools.</li> <li>Streamlined Releases: Semantic Release automates the often tedious and error-prone process of versioning and releasing software.</li> </ul> </li> <li>Negative:<ul> <li>Initial Setup &amp; Configuration Overhead: Setting up and configuring all these tools requires initial effort (though much is consolidated in <code>pyproject.toml</code>).</li> <li>Learning Curve: Developers need to become familiar with the workflow and conventions imposed by the tools (e.g., Poetry commands, Conventional Commits for semantic-release, interpreting Ruff/mypy errors).</li> <li>Rigid Workflow: Automated checks, especially in <code>pre-commit</code> and CI, enforce standards strictly. This can initially feel restrictive or slow down developers unaccustomed to such checks.</li> <li>Tooling Maintenance: The tool configurations and versions need to be kept up-to-date. CI workflows might require occasional adjustments.</li> </ul> </li> <li>Neutral/Other:<ul> <li>The stack is heavily integrated with the Python ecosystem (Poetry, PyPI) and GitHub (Actions, Releases, Dependabot).</li> <li>Adopting Conventional Commits is necessary to fully leverage <code>python-semantic-release</code>.</li> </ul> </li> </ul>"},{"location":"adr/ADR-011-pybreaker-lib-for-cirtuit-breaker-pattern-implementatio/","title":"ADR-011: Use aiobreaker Library and Refactored Structure for Circuit Breaker","text":"<ul> <li>Status: Accepted (Implemented)</li> <li>Date: 2025-04-14</li> </ul>"},{"location":"adr/ADR-011-pybreaker-lib-for-cirtuit-breaker-pattern-implementatio/#context","title":"Context","text":"<ul> <li>Problem: The application requires protection against cascading failures caused by repeatedly calling dependencies that are experiencing persistent or systemic issues. While Retry (ADR-009) handles transient errors and Fallback (ADR-009) provides alternatives, a mechanism is needed to detect ongoing failures and temporarily block calls to the failing dependency (Circuit Breaker pattern). Initial attempts to use the <code>pybreaker</code> library encountered difficulties with its asynchronous function handling (<code>NameError: name 'gen' is not defined</code>) and inconsistencies in its constructor API across versions or environments (<code>TypeError</code> for arguments like <code>exclude_exception</code> and <code>throw_new_error_on_trip</code>), leading to significant debugging effort. Furthermore, the initial implementation combined the decorator, registry, and listener logic into a single file, reducing maintainability.</li> <li>Goal: Implement a robust, maintainable, and <code>asyncio</code>-native Circuit Breaker pattern within <code>athomic-docs</code>, consistent with existing resilience patterns (decorators) and project structure guidelines. Resolve the issues encountered with <code>pybreaker</code>.</li> <li>Alternatives Considered:<ol> <li>Manual Implementation: Rejected due to high complexity and risk of errors in state management and concurrency.</li> <li>Continue with <code>pybreaker</code> + Workarounds: Attempting workarounds like installing <code>tornado</code> or further debugging the constructor issues. Rejected due to introducing unnecessary dependencies (<code>tornado</code>) or uncertainty about library stability/API.</li> <li>Other Async Libraries (<code>async-circuitbreaker</code>, etc.): Considered, but <code>aiobreaker</code> seemed a well-known async-focused alternative to <code>pybreaker</code>.</li> <li>Infrastructure-Level Circuit Breaking: Viable as a complementary layer, but insufficient for application-level control and context.</li> </ol> </li> </ul>"},{"location":"adr/ADR-011-pybreaker-lib-for-cirtuit-breaker-pattern-implementatio/#decision","title":"Decision","text":"<ol> <li>Adopt <code>aiobreaker</code> Library: Switch from <code>pybreaker</code> to the <code>aiobreaker</code> library as the underlying implementation for the Circuit Breaker logic. <code>aiobreaker</code> is explicitly designed for <code>asyncio</code>, resolving the <code>NameError</code> related to <code>tornado.gen</code>.</li> <li>Refactor Module Structure: Move the circuit breaker implementation from <code>utils/decorators/</code> to a dedicated sub-package <code>nala.athomic.resilience.circuit_breaker/</code>. Divide the logic into separate files for better Separation of Concerns:<ul> <li><code>registry.py</code>: Manages the creation and storage (<code>_breakers</code> dict, <code>_registry_lock</code>) of named <code>aiobreaker.CircuitBreaker</code> instances using the <code>get_or_create_breaker</code> factory function.</li> <li><code>listeners.py</code>: Defines the <code>LoggingCircuitBreakerListener</code> class responsible for logging state changes, failures, and successes using the application's logger. Contains the <code>global_logging_listener</code> instance.</li> <li><code>exceptions.py</code>: Re-exports <code>aiobreaker.CircuitBreakerError</code> for consistent usage.</li> <li><code>decorator.py</code>: Contains the user-facing <code>@circuit_breaker</code> decorator factory and its internal <code>sync_wrapper</code> and <code>async_wrapper</code> functions, which utilize <code>get_or_create_breaker</code> and the correct <code>breaker.call</code>/<code>breaker.call_async</code> methods.</li> <li><code>__init__.py</code>: Exports the public symbols (<code>circuit_breaker</code>, <code>CircuitBreakerError</code>).</li> </ul> </li> <li>API Alignment: Update the implementation (decorator, registry, listeners, tests) to use the correct <code>aiobreaker</code> API, including:<ul> <li>Class name: <code>aiobreaker.CircuitBreaker</code>.</li> <li>Constructor parameters: <code>fail_max</code>, <code>timeout_duration</code> (expects <code>timedelta</code>), <code>exclude</code> (expects iterable), <code>name</code>, <code>listeners</code> (added via <code>add_listener</code>). Parameters like <code>throw_new_error_on_trip</code> are not used.</li> <li>Execution methods: <code>breaker.call()</code> for sync, <code>await breaker.call_async()</code> for async.</li> <li>State comparison: Use <code>type(breaker.state) is aiobreaker.CircuitBreakerState.XXX.value</code>.</li> <li>Exception: <code>aiobreaker.CircuitBreakerError</code>.</li> <li>Listener interface: Use simple classes with methods like <code>state_changed(cb, old_state, new_state)</code>, <code>failure(cb, exc)</code>, <code>success(cb)</code>.</li> </ul> </li> </ol>"},{"location":"adr/ADR-011-pybreaker-lib-for-cirtuit-breaker-pattern-implementatio/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Resolves <code>pybreaker</code> Issues: Eliminates the <code>NameError</code> related to <code>tornado.gen</code> and <code>TypeError</code>s from incorrect constructor parameters encountered previously. Works natively with <code>asyncio</code>.</li> <li>Improved Maintainability: Refactoring into smaller, focused files (<code>registry.py</code>, <code>listeners.py</code>, <code>decorator.py</code>) makes the code easier to understand, test, and modify, adhering to project guidelines.</li> <li>Robust Foundation: Leverages the tested state machine logic of the <code>aiobreaker</code> library.</li> <li>Consistency: Maintains the declarative decorator pattern used for other resilience features (<code>@retry_handler</code>, <code>@fallback_handler</code>).</li> <li>Clearer Structure: Places the Circuit Breaker logic within a more appropriate <code>resilience</code> package.</li> <li>Observability: Retains logging of important state changes and events via the listener.</li> </ul> </li> <li>Negative:<ul> <li>Refactoring Effort: Required significant changes to the decorator, registry, listener implementation, and associated unit tests.</li> <li>New Dependency: Replaced <code>pybreaker</code> dependency with <code>aiobreaker</code>.</li> <li>State Scope Limitation: The primary limitation remains: the default implementation uses in-memory state storage (<code>CircuitMemoryStorage</code>), meaning the circuit state is not shared across multiple horizontally scaled instances of the application. Distributed circuit breaking would require implementing or using a shared <code>aiobreaker.CircuitBreakerStorage</code> (e.g., Redis-based).</li> <li>Library Nuances: Still dependent on understanding the specific API and behavior of <code>aiobreaker</code> (e.g., how <code>exclude</code> works, when exactly listeners are called, how state transitions are triggered).</li> </ul> </li> <li>Neutral:<ul> <li>Still requires careful tuning of <code>fail_max</code> and <code>reset_timeout</code> per use case.</li> <li>Complements Retry and Fallback patterns as part of a larger resilience strategy.</li> </ul> </li> </ul>"},{"location":"adr/ADR-012-refact-loggin-to-solve-circular-dependence/","title":"ADR-012: Refactor Logging Initialization to Resolve Circular Dependencies","text":"<p>Status: Implemented</p> <p>Date: 2025-04-15</p>"},{"location":"adr/ADR-012-refact-loggin-to-solve-circular-dependence/#context","title":"Context","text":"<p>During test execution (<code>make test</code>), the application encountered an <code>ImportError</code> due to a circular dependency between the logging configuration modules and the observability setup/filter modules. Specifically, the main cycle involved:</p> <ol> <li><code>config/schemas/logging_config.py</code>: Needed access to information from the masker registry (<code>observability/log/registry.py</code>) or resolver functions (<code>log_masker_resolver.py</code>) to validate the <code>sensitive_patterns</code> configuration.</li> <li><code>observability/log/setup.py</code>: Needed to import the <code>LoggingSettings</code> model from <code>config/schemas/logging_config.py</code> to apply settings to the logger (Loguru).</li> <li><code>observability/log/filters/sensitive_data_filter.py</code>: Needed the registry (<code>registry.py</code>) and the resolver (<code>log_masker_resolver.py</code>), and was instantiated by <code>setup.py</code>.</li> <li>Additional imports in <code>observability/log/__init__.py</code> and <code>config/schemas/__init__.py</code> completed or exacerbated the cycle, leading to errors where names could not be imported from partially initialized modules.</li> <li>Redundant code for handling log levels (<code>LOG_LEVELS</code>, <code>get_log_level</code>) was also identified, duplicating functionality from the standard <code>logging</code> module.</li> </ol> <p>This issue prevented test execution and potentially the correct application startup in certain import scenarios.</p>"},{"location":"adr/ADR-012-refact-loggin-to-solve-circular-dependence/#decision","title":"Decision","text":"<p>To break the circular dependency and improve the separation of concerns, the following changes were implemented:</p> <ol> <li> <p>Decoupling of <code>setup.py</code> and <code>logging_config.py</code>:</p> <ul> <li>The <code>observability/log/setup.py::configure_logging</code> function was modified to receive the <code>LoggingSettings</code> instance as an argument instead of importing it directly.</li> <li>The responsibility of obtaining the settings (<code>get_settings()</code>) and passing the <code>settings.logging</code> section was moved to the call site of <code>configure_logging</code> (usually in the application startup).</li> </ul> </li> <li> <p>Centralization of Pattern Processing Logic in the Filter:</p> <ul> <li>The responsibility for processing the <code>sensitive_patterns</code> list (compiling regex, resolving masker names via <code>resolve_masker</code> or using callables, and ordering the patterns) was moved entirely into the constructor (<code>__init__</code>) of the <code>observability/log/filters/sensitive_data_filter.py::SensitiveDataFilter</code> class.</li> <li>The helper function <code>_parse_sensitive_patterns</code> was removed from <code>observability/log/setup.py</code>.</li> <li><code>configure_logging</code> now instantiates <code>SensitiveDataFilter</code> by passing the raw configuration list (<code>log_settings.sensitive_patterns</code>).</li> </ul> </li> <li> <p>Simplification of the Pydantic Validator:</p> <ul> <li>The <code>@field_validator(\"sensitive_patterns\")</code> in <code>config/schemas/logging_config.py</code> was simplified to validate only the structure and basic types of the list items (presence of keys, string/callable type, regex validity), without attempting to resolve or validate masker names against the registry. The call to <code>resolve_masker</code> was removed from this validator.</li> </ul> </li> <li> <p>Removal of Explicit Pattern Import:</p> <ul> <li>The explicit import of <code>default_patterns</code> within <code>observability/log/__init__.py</code> (which aimed to pre-populate the registry) was removed to simplify initialization and avoid side effects on import order. Registry population now occurs naturally when modules that use it (like <code>SensitiveDataFilter</code>) are imported.</li> </ul> </li> <li> <p>Usage of Native Logging Functions:</p> <ul> <li>The custom dictionary <code>LOG_LEVELS</code> and function <code>get_log_level</code> in <code>observability/log/utils/log_levels.py</code> were removed.</li> <li>Validators and functions needing to convert log level names to integers now directly use <code>logging._checkLevel()</code>, leveraging Python's native functionality.</li> </ul> </li> </ol>"},{"location":"adr/ADR-012-refact-loggin-to-solve-circular-dependence/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Complete elimination of <code>ImportError</code> related to circular dependencies in the logging system.</li> <li>Improved separation of concerns:<ul> <li>Pydantic Schemas (<code>logging_config.py</code>) focus on validating configuration structure.</li> <li>Logging Setup (<code>setup.py</code>) focuses on orchestrating the logger configuration (Loguru).</li> <li>Filter (<code>sensitive_data_filter.py</code>) encapsulates all the processing and application logic for masking.</li> </ul> </li> <li>Reduced coupling between configuration and observability modules.</li> <li>Cleaner and more maintainable code with the removal of redundant log level logic.</li> <li>Increased robustness of application startup.</li> </ul> </li> <li>Negative:<ul> <li>Validation that a masker name provided in the configuration corresponds to a registered masker now occurs slightly later (during <code>SensitiveDataFilter</code> instantiation within <code>configure_logging</code>), instead of during the initial Pydantic model validation. This is considered an acceptable trade-off for resolving the cycle. Configuration errors related to invalid names will now be detected (and logged) during filter initialization.</li> <li>A failure during <code>SensitiveDataFilter</code> initialization (e.g., an unrecoverable error resolving a masker) could result in logging without masking, although error handling was added in <code>configure_logging</code> to log this failure critically.</li> </ul> </li> </ul>"},{"location":"adr/ADR-012-refact-loggin-to-solve-circular-dependence/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>Keeping the masker name validation in the Pydantic validator and attempting to break the cycle in other ways (e.g., complex conditional imports), but this was considered more fragile and less clear than moving the responsibility to the filter.</li> <li>Moving the masker resolution/validation logic to <code>setup.py</code> instead of the filter. This was rejected because it mixed orchestration and filter logic responsibilities within the same module.</li> </ul>"},{"location":"adr/ADR-013-athomic-modular-architecture/","title":"ADR-013: Modular Architecture for the Nala Athomic Package","text":"<p>Status: Implemented</p> <p>Date: 2025-04-15</p>"},{"location":"adr/ADR-013-athomic-modular-architecture/#context","title":"Context","text":"<p>The <code>nala-athomic</code> package aims to provide a set of core, reusable functionalities (an internal \"framework\" or core library) for Nala applications, covering areas such as configuration, database access, observability, resilience, security, etc. As the complexity and number of features within <code>athomic</code> grow, a well-defined directory and module structure becomes crucial. Without clear modularization, the codebase can become difficult to navigate, maintain, test, and extend. Cross-cutting concerns could become entangled, increasing coupling and hindering reusability and collaboration. It was necessary to define a standard structure to organize the code within <code>src/nala/athomic</code>.</p>"},{"location":"adr/ADR-013-athomic-modular-architecture/#decision","title":"Decision","text":"<p>A modular architecture was adopted for the <code>nala-athomic</code> package, dividing the code into high-level Python packages based on distinct architectural concerns. The primary structure decided upon is:</p> <ol> <li> <p>Top-Level Packages by Concern: The code was organized into the following main directories/packages, each focused on a specific area:</p> <ul> <li><code>config</code>: Configuration management (reading files, environment variables, Pydantic schemas).</li> <li><code>database</code>: Database access (interfaces, NoSQL/SQL implementations, migrations).</li> <li><code>integration</code>: Communication with external services (queues, service discovery like Consul).</li> <li><code>observability</code>: Features related to monitoring and debugging (logging, metrics, tracing, health checks).</li> <li><code>performance</code>: Performance optimizations (caching, load balancing).</li> <li><code>resilience</code>: Resilience patterns (retry, circuit breaker, rate limiter, fallback).</li> <li><code>security</code>: Security features (authentication, authorization, secret management, cryptography).</li> <li><code>storage</code>: Access to file storage systems (if needed, e.g., <code>filestore</code>).</li> <li><code>utils</code>: Generic and cross-cutting utilities (date/string manipulation, custom exceptions, generic decorators).</li> </ul> </li> <li> <p>Internal Subdivision: Each top-level package is further subdivided into more specific modules or subpackages, reflecting the internal structure of that concern. Examples:</p> <ul> <li><code>observability</code> contains <code>log</code>, <code>metrics</code>, <code>tracing</code>, <code>health</code>.</li> <li><code>security</code> contains <code>auth</code>, <code>secrets</code>, <code>crypto</code>.</li> <li><code>config</code> contains <code>providers</code> and <code>schemas</code>.</li> </ul> </li> <li> <p>Organization of Implementations and Interfaces:</p> <ul> <li>Concrete implementations (Providers, Repositories) are generally nested within their respective modules (e.g., <code>config/providers</code>, <code>security/secrets/providers</code>, <code>database/nosql/mongo</code>).</li> <li>Interfaces (using <code>Protocol</code> or abstract base classes <code>ABC</code>) are often defined separately to promote low coupling and dependency inversion (e.g., <code>database/nosql/repository_interfaces.py</code>, <code>performance/cache/interfaces.py</code>).</li> </ul> </li> <li> <p>Utilities: Utility functions are grouped either within the relevant submodule (e.g., <code>observability/log/utils</code>) or in the top-level <code>utils</code> package if they are truly generic and used across multiple packages.</p> </li> </ol>"},{"location":"adr/ADR-013-athomic-modular-architecture/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Improved Maintainability: Easier to locate and modify code related to a specific feature (e.g., all rate limiting logic is under <code>resilience/rate_limiter</code>).</li> <li>Enhanced Readability &amp; Navigability: The clear structure makes the codebase easier to understand for existing and new developers.</li> <li>Better Testability: Modules with well-defined responsibilities are easier to test in isolation (unit testing).</li> <li>Reduced Coupling: Clear boundaries between packages minimize unnecessary dependencies between different functional areas.</li> <li>Increased Reusability: Individual modules (like <code>resilience</code> or specific <code>maskers</code>) could potentially be easier to extract or reuse in other contexts if needed.</li> <li>Facilitates Collaboration: Different developers or teams can work on distinct modules concurrently with fewer conflicts.</li> <li>Clear Separation of Concerns: Each package has a well-defined focus.</li> </ul> </li> <li>Negative:<ul> <li>Potential for Over-Granularity (Minor): In some very simple cases, creating many small modules might seem like overkill. However, for a library/framework like <code>athomic</code>, the organizational benefits usually outweigh this drawback.</li> <li>Requires Discipline: Maintaining the structure consistently requires ongoing discipline during development to avoid placing code in the wrong location.</li> </ul> </li> </ul>"},{"location":"adr/ADR-013-athomic-modular-architecture/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>Monolithic Structure: Placing all code directly under <code>athomic</code> without subpackages. Rejected for significantly hindering maintainability and navigation.</li> <li>Different Grouping: Grouping by component type (e.g., a <code>decorators</code> package, a <code>providers</code> package) instead of by architectural concern. Rejected as it could potentially increase coupling within features and make it harder to locate all code related to an area like \"security\" or \"observability.\" Grouping by architectural concern was deemed superior.</li> </ul>"},{"location":"adr/ADR-014-pytest-standarrize-asyncio-event-loop/","title":"ADR-014: Standardize asyncio Event Loop Management in Pytest","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-04-17</li> </ul>"},{"location":"adr/ADR-014-pytest-standarrize-asyncio-event-loop/#context","title":"Context","text":"<p>The project utilizes <code>pytest</code> and <code>pytest-asyncio</code> for running asynchronous tests, particularly within the messaging integration modules (<code>nala.athomic.integration.messaging</code>). Previously, a custom <code>event_loop</code> fixture with <code>session</code> scope was defined in <code>tests/conftest.py</code> in an attempt to manage the event loop.</p> <p>During test execution, especially when attempting to enable <code>pytest-asyncio</code>'s <code>strict</code> mode for better practices, several issues arose: 1.  <code>DeprecationWarning</code> warnings regarding the redefinition of the <code>event_loop</code> fixture managed by <code>pytest-asyncio</code>. 2.  <code>TypeError: loop must be an instance of AbstractEventLoop or None, not 'async_generator'</code> occurring during fixture setup, indicating a conflict in the loop policy or the type of loop being passed. 3.  <code>RuntimeError: Event loop is closed</code> in multiple asynchronous tests when using <code>asyncio_mode = strict</code>, suggesting problems with the loop's lifecycle and resource/task cleanup. 4.  Difficulties in consistently mocking async iterators (<code>TypeError: 'async for' received an object from __aiter__ ...</code>).</p> <p>Analysis indicated that the custom <code>session</code>-scoped <code>event_loop</code> fixture was incompatible with <code>pytest-asyncio</code>'s function-scoped loop management model (the default and recommended approach), causing the observed conflicts and errors. Attempting to maintain and fix the custom fixture proved complex and fragile.</p>"},{"location":"adr/ADR-014-pytest-standarrize-asyncio-event-loop/#decision","title":"Decision","text":"<ol> <li>Remove the custom <code>event_loop</code> fixture from <code>tests/conftest.py</code>.</li> <li>Explicitly configure <code>pytest-asyncio</code> via <code>pytest.ini</code>:     <code>ini     [pytest]     # ...     asyncio_mode = strict     asyncio_default_fixture_loop_scope = function     # ...</code></li> <li>Fully delegate to <code>pytest-asyncio</code> the responsibility for creating, providing, and closing the event loop for each test (function scope).</li> <li>Adopt as mandatory practice the explicit cleanup of asynchronous resources (e.g., Kafka clients, connections, created tasks) at the end of each test that uses them, preferably using <code>try...finally</code> blocks with <code>await resource.close()</code>.</li> </ol>"},{"location":"adr/ADR-014-pytest-standarrize-asyncio-event-loop/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Simplification: Removes custom and complex loop management code from <code>conftest.py</code>.</li> <li>Standardization: Aligns the project with the recommended practices and default behavior of <code>pytest-asyncio</code>.</li> <li>Error Resolution: Fixes the specific <code>TypeError</code>s and <code>RuntimeError</code>s encountered during the setup and execution of asynchronous tests.</li> <li>Robustness: The <code>strict</code> mode helps identify issues related to unclosed resources or pending asyncio tasks earlier, making tests more reliable.</li> <li>Clarity: The responsibility for the event loop lifecycle is clearly assigned to <code>pytest-asyncio</code>.</li> </ul> </li> <li>Negative:<ul> <li>Test Discipline Required: Requires greater attention and discipline from developers when writing asynchronous tests to ensure that all resources needing <code>await close()</code> (or similar cleanup) are properly closed in <code>finally</code> blocks. (Also considered a good practice).</li> </ul> </li> <li>Neutral/Other:<ul> <li>The explicit configuration of <code>pytest-asyncio</code>'s behavior is now centralized in <code>pytest.ini</code>.</li> <li>The mocking strategy for async iterators (specifically for <code>AIOKafkaConsumer</code>) was adjusted to work correctly with this approach (using <code>mock_instance.__aiter__.return_value = mock_instance</code> and <code>mock_instance.__anext__.side_effect = [...]</code>).</li> </ul> </li> </ul>"},{"location":"adr/ADR-015-locking-distribuido-cache/","title":"ADR-015: Mecanismo de Locking Distribu\u00eddo para Cache Decorator","text":"<p>Status: Aceito</p>"},{"location":"adr/ADR-015-locking-distribuido-cache/#contexto","title":"\ud83c\udfaf Contexto","text":"<p>O decorator <code>@cache_result</code> [source: 2346] possui a op\u00e7\u00e3o <code>use_lock=True</code> com o objetivo de prevenir o problema de cache stampede (m\u00faltiplas inst\u00e2ncias/requisi\u00e7\u00f5es recalculando o mesmo valor de cache simultaneamente quando ele expira). A implementa\u00e7\u00e3o inicial usava um <code>asyncio.Lock</code> [source: 2384], que s\u00f3 garante exclus\u00e3o m\u00fatua dentro de um \u00fanico processo/inst\u00e2ncia da aplica\u00e7\u00e3o.</p> <p>Para ambientes distribu\u00eddos com m\u00faltiplas inst\u00e2ncias da API rodando (ex: Kubernetes, m\u00faltiplos workers Gunicorn/Uvicorn), o <code>asyncio.Lock</code> \u00e9 insuficiente, pois cada inst\u00e2ncia teria seu pr\u00f3prio lock independente, n\u00e3o prevenindo o stampede entre inst\u00e2ncias.</p> <p>\u00c9 necess\u00e1rio um mecanismo de locking distribu\u00eddo, utilizando um recurso compartilhado como o Redis (que j\u00e1 \u00e9 uma depend\u00eancia e op\u00e7\u00e3o de backend para o cache [source: 2356]), e que seja compat\u00edvel com a natureza ass\u00edncrona (<code>asyncio</code>) do projeto.</p>"},{"location":"adr/ADR-015-locking-distribuido-cache/#decisao","title":"\ud83d\ude80 Decis\u00e3o","text":"<p>Adotar o mecanismo de locking distribu\u00eddo nativo fornecido pela biblioteca <code>redis-py</code> (vers\u00e3o &gt;= 4.2), especificamente atrav\u00e9s do m\u00e9todo <code>redis.asyncio.Redis.lock()</code>.</p> <p>A implementa\u00e7\u00e3o ser\u00e1 feita na fun\u00e7\u00e3o helper <code>acquire_lock</code> (localizada em <code>src/nala/athomic/performance/cache/looking/registry.py</code>), que encapsular\u00e1 a l\u00f3gica: 1.  Obter o cliente <code>redis.asyncio.Redis</code> configurado atrav\u00e9s do factory <code>get_redis_client_for_infra</code>. 2.  Se o cliente Redis n\u00e3o estiver dispon\u00edvel, usar como fallback o <code>asyncio.Lock</code> (mantendo a prote\u00e7\u00e3o intra-processo e evitando falhas). 3.  Se o cliente Redis estiver dispon\u00edvel, instanciar o lock nativo: <code>redis_client.lock(name=lock_key, timeout=lock_timeout_sec, blocking=True, blocking_timeout=blocking_timeout)</code>. 4.  Utilizar o objeto lock retornado como um context manager ass\u00edncrono (<code>async with lock_instance:</code>) para adquirir e liberar o lock automaticamente. 5.  Tratar exce\u00e7\u00f5es espec\u00edficas do <code>redis-py</code> (<code>LockError</code>, <code>LockNotOwnedError</code>) dentro do <code>acquire_lock</code> para fornecer um comportamento consistente e evitar propaga\u00e7\u00e3o de erros inesperados na libera\u00e7\u00e3o.</p>"},{"location":"adr/ADR-015-locking-distribuido-cache/#alternativas-consideradas","title":"\ud83e\udd14 Alternativas Consideradas","text":"<ol> <li> <p><code>redis-lock==0.2.0</code>:</p> <ul> <li>Foi a primeira biblioteca considerada e parcialmente implementada.</li> <li>Desvantagem: Verificou-se que esta vers\u00e3o \u00e9 s\u00edncrona. Sua utiliza\u00e7\u00e3o com o cliente <code>redis.asyncio</code> causou <code>TypeError</code>s (tentativa de tratar corrotinas como valores s\u00edncronos) e tornou os testes unit\u00e1rios complexos e a simula\u00e7\u00e3o de concorr\u00eancia problem\u00e1tica.</li> <li>Resultado: Abandonada devido \u00e0 incompatibilidade fundamental com o ambiente ass\u00edncrono do projeto.</li> </ul> </li> <li> <p><code>aioredlock</code>:</p> <ul> <li>Vantagem: Implementa o algoritmo Redlock completo, oferecendo maior robustez te\u00f3rica em cen\u00e1rios de alta disponibilidade com m\u00faltiplos mestres Redis independentes. \u00c9 compat\u00edvel com <code>asyncio</code>.</li> <li>Desvantagem: Adiciona uma depend\u00eancia externa nova ao projeto. Requer configura\u00e7\u00e3o mais complexa (m\u00faltiplos endpoints Redis) para obter os benef\u00edcios do Redlock. Considerado um exagero para os requisitos atuais e cen\u00e1rios de implanta\u00e7\u00e3o mais comuns (Redis \u00fanico, Cluster ou Sentinel gerenciados).</li> <li>Resultado: Descartada por enquanto, mas mantida como uma alternativa futura caso as garantias do Redlock se tornem um requisito expl\u00edcito.</li> </ul> </li> <li> <p><code>asyncio.Lock</code>:</p> <ul> <li>Implementa\u00e7\u00e3o inicial usada no helper <code>acquire_lock</code>.</li> <li>Desvantagem: Ineficaz em ambientes distribu\u00eddos/multi-inst\u00e2ncia.</li> <li>Resultado: Mantido apenas como mecanismo de fallback para quando o Redis n\u00e3o estiver dispon\u00edvel, garantindo que a aplica\u00e7\u00e3o n\u00e3o falhe nesses casos (embora a prote\u00e7\u00e3o distribu\u00edda seja perdida).</li> </ul> </li> </ol>"},{"location":"adr/ADR-015-locking-distribuido-cache/#consequencias","title":"\u2728 Consequ\u00eancias","text":"<p>Positivas:</p> <ul> <li>Resolve efetivamente o problema de cache stampede em implanta\u00e7\u00f5es multi-inst\u00e2ncia que utilizam Redis.</li> <li>Utiliza a biblioteca <code>redis-py</code>, que j\u00e1 \u00e9 uma depend\u00eancia do projeto para o cache Redis, evitando adicionar novas depend\u00eancias externas.</li> <li>Integra-se bem com a infraestrutura <code>asyncio</code> existente e com o factory do cliente Redis (<code>get_redis_client_for_infra</code>).</li> <li>A API do lock nativo (<code>client.lock()</code> usado com <code>async with</code>) \u00e9 relativamente clara e idiom\u00e1tica para Python ass\u00edncrono.</li> </ul> <p>Negativas:</p> <ul> <li>A funcionalidade de locking distribu\u00eddo agora depende de uma inst\u00e2ncia Redis configurada e dispon\u00edvel. O fallback para <code>asyncio.Lock</code> previne erros, mas n\u00e3o oferece a prote\u00e7\u00e3o distribu\u00edda.</li> <li>N\u00e3o implementa o algoritmo Redlock completo. Embora considerado suficiente para a maioria dos casos, n\u00e3o possui as mesmas garantias te\u00f3ricas do <code>aioredlock</code> contra falhas em cen\u00e1rios com m\u00faltiplos mestres Redis independentes.</li> <li>Testar o comportamento concorrente de forma 100% precisa ainda pode ser desafiador apenas com testes unit\u00e1rios e mocks; testes de integra\u00e7\u00e3o com um Redis real s\u00e3o recomendados para validar completamente a preven\u00e7\u00e3o de race conditions.</li> </ul>"},{"location":"adr/ADR-016-repository-multi-tenancy/","title":"ADR-016: Estrat\u00e9gia Multi-Tenancy Configur\u00e1vel para Reposit\u00f3rios","text":"<ul> <li>Status: Aceito</li> <li>Data: 2025-04-18</li> </ul>"},{"location":"adr/ADR-016-repository-multi-tenancy/#contexto","title":"Contexto","text":"<p>O <code>athomic-docs</code> precisa suportar implanta\u00e7\u00f5es no estilo SaaS (Software as a Service), onde uma \u00fanica inst\u00e2ncia da aplica\u00e7\u00e3o atende a m\u00faltiplos clientes (tenants). Isso exige um mecanismo robusto para garantir o isolamento estrito dos dados de cada tenant na camada de persist\u00eancia (<code>athomic/database</code>). A solu\u00e7\u00e3o precisa ser consistente, aplic\u00e1vel a diferentes backends de banco de dados (MongoDB inicialmente, potencialmente SQL no futuro), configur\u00e1vel para permitir flexibilidade (ex: rodar em modo single-tenant ou fazer rollout gradual) e integrada com o sistema de contexto j\u00e1 planejado (<code>tenant_id_ctx_var</code> populado por middleware).</p>"},{"location":"adr/ADR-016-repository-multi-tenancy/#alternativas-consideradas","title":"Alternativas Consideradas","text":"<ol> <li>Filtros Manuais nos Servi\u00e7os: Exigir que a camada de servi\u00e7o (acima do reposit\u00f3rio) sempre adicione filtros de <code>tenant_id</code>.<ul> <li>Rejeitado: Acopla a l\u00f3gica de isolamento aos servi\u00e7os, leva \u00e0 repeti\u00e7\u00e3o, aumenta a chance de erros/esquecimentos.</li> </ul> </li> <li>L\u00f3gica Fixa nos Reposit\u00f3rios: Implementar o filtro de tenant diretamente nos reposit\u00f3rios sem op\u00e7\u00e3o de desativa\u00e7\u00e3o.<ul> <li>Rejeitado: Menos flex\u00edvel para cen\u00e1rios single-tenant, testes ou rollout gradual.</li> </ul> </li> <li>L\u00f3gica Condicional nos Reposit\u00f3rios (com flag): Implementar a l\u00f3gica de filtro/inje\u00e7\u00e3o de <code>tenant_id</code> nos reposit\u00f3rios, mas ativ\u00e1-la/desativ\u00e1-la com uma flag de configura\u00e7\u00e3o.<ul> <li>Aceito: Oferece flexibilidade e encapsulamento.</li> </ul> </li> <li>Defini\u00e7\u00e3o da Coluna <code>tenant_id</code>:<ul> <li>a) Manualmente em cada modelo: Propenso a inconsist\u00eancias no nome, tipo ou indexa\u00e7\u00e3o.</li> <li>b) Via Model Mixin (<code>TenantMixin</code>): Garante consist\u00eancia na defini\u00e7\u00e3o do campo. (Recomendado).</li> </ul> </li> <li>Reutiliza\u00e7\u00e3o da L\u00f3gica de Isolamento:<ul> <li>a) Nenhuma (Repeti\u00e7\u00e3o): Implementar a l\u00f3gica condicional separadamente em cada classe concreta (<code>MongoRepo</code>, <code>SqlRepo</code>). (Rejeitado: Duplica\u00e7\u00e3o).</li> <li>b) Base/Mixin \u00danica para Tudo: Criar uma super classe base/mixin com flags para tenancy, soft delete, auditoria, etc. (Rejeitado: Alta complexidade, viola SRP).</li> <li>c) Base/Mixin Focada (Tenancy) + Mixins Futuras: Criar uma base/mixin para tenancy agora, e outras separadas (SoftDelete, Auditing) no futuro. (Aceito: Equil\u00edbrio entre reuso e complexidade).</li> </ul> </li> <li>Inje\u00e7\u00e3o Din\u00e2mica de Mixins via Config: Usar metaclasses ou <code>type()</code> para construir a classe do reposit\u00f3rio com mixins definidos em <code>settings.toml</code>.<ul> <li>Rejeitado: Aumenta drasticamente a complexidade (\"m\u00e1gica\"), dificulta an\u00e1lise est\u00e1tica, depura\u00e7\u00e3o e tipagem. A flexibilidade adicional n\u00e3o compensa os contras para este caso de uso.</li> </ul> </li> </ol>"},{"location":"adr/ADR-016-repository-multi-tenancy/#decisao","title":"Decis\u00e3o","text":"<p>Ser\u00e1 implementada uma estrat\u00e9gia de multi-tenancy configur\u00e1vel na camada de reposit\u00f3rio (<code>athomic.database</code>), seguindo estes pontos:</p> <ol> <li>Configurabilidade: Uma nova flag booleana <code>multi_tenancy_enabled</code> (default: <code>False</code>) ser\u00e1 adicionada ao schema <code>DatabaseSettings</code> (<code>src/nala/athomic/config/schemas/db_config.py</code>) e lida a partir dos arquivos de configura\u00e7\u00e3o (<code>settings.toml</code>, etc.).</li> <li>L\u00f3gica Condicional: Os m\u00e9todos das classes de reposit\u00f3rio (inicialmente <code>MongoBeanieRepository</code>) ler\u00e3o a flag <code>multi_tenancy_enabled</code>.<ul> <li>Se <code>True</code>: A l\u00f3gica de isolamento de tenant ser\u00e1 aplicada:<ul> <li>Contexto: O <code>tenant_id</code> ser\u00e1 obtido via <code>get_tenant_id()</code> (que l\u00ea o <code>tenant_id_ctx_var</code>). Uma falha em obter o <code>tenant_id</code> (retornar <code>None</code>) resultar\u00e1 em erro ou retorno vazio para opera\u00e7\u00f5es de leitura e erro para opera\u00e7\u00f5es de escrita.</li> <li>Leituras (<code>find</code>, <code>find_one</code>, <code>get_by_id</code>): O filtro <code>{\"tenant_id\": current_tenant_id}</code> (ou equivalente SQL) ser\u00e1 automaticamente adicionado \u00e0s queries.</li> <li>Escritas (<code>save</code>): Para novos documentos, o <code>tenant_id</code> do contexto ser\u00e1 injetado no campo <code>tenant_id</code> da entidade antes de salvar. Para atualiza\u00e7\u00f5es, ser\u00e1 validado se o <code>tenant_id</code> da entidade existente corresponde ao <code>tenant_id</code> do contexto.</li> <li>Dele\u00e7\u00f5es (<code>delete</code>): A entidade ser\u00e1 buscada usando <code>get_by_id</code> (que j\u00e1 inclui o filtro de tenant) antes de prosseguir com a exclus\u00e3o.</li> </ul> </li> <li>Se <code>False</code>: Os m\u00e9todos executar\u00e3o a l\u00f3gica padr\u00e3o do ORM/ODM sem aplicar filtros ou inje\u00e7\u00f5es/valida\u00e7\u00f5es de <code>tenant_id</code>.</li> </ul> </li> <li>Defini\u00e7\u00e3o do Campo <code>tenant_id</code>: Recomenda-se fortemente o uso de uma classe Model Mixin (ex: <code>TenantMixin</code>) para definir o campo <code>tenant_id</code> (com tipo e \u00edndice apropriados) de forma consistente nos modelos de dados (ex: Beanie <code>Document</code>, SQLAlchemy <code>DeclarativeBase</code>) que armazenam dados espec\u00edficos de tenants.</li> <li>Reutiliza\u00e7\u00e3o da L\u00f3gica: A l\u00f3gica condicional de tenancy ser\u00e1 implementada na(s) classe(s) base/concreta(s) atual(is) (ex: <code>MongoBeanieRepository</code>), preferencialmente usando m\u00e9todos auxiliares privados para facilitar a organiza\u00e7\u00e3o. Futuros comportamentos transversais de reposit\u00f3rio (ex: Soft Delete, Auditoria) devem ser implementados como Mixins separados e herdados adicionalmente pelas classes concretas, conforme necess\u00e1rio, em vez de adicionar mais flags e l\u00f3gicas \u00e0 base de tenancy.</li> </ol>"},{"location":"adr/ADR-016-repository-multi-tenancy/#consequencias","title":"Consequ\u00eancias","text":"<p>Positivas:</p> <ul> <li>Garante isolamento de dados entre tenants quando habilitado.</li> <li>Permite que a mesma base de c\u00f3digo opere em modo single-tenant ou multi-tenant via configura\u00e7\u00e3o.</li> <li>Facilita o rollout gradual da funcionalidade multi-tenancy.</li> <li>Desacopla a l\u00f3gica de isolamento da camada de servi\u00e7o.</li> <li>Centraliza a l\u00f3gica de isolamento na camada de reposit\u00f3rio.</li> <li>Promove consist\u00eancia na defini\u00e7\u00e3o do campo <code>tenant_id</code> via Model Mixin.</li> <li>Estabelece um padr\u00e3o arquitetural claro para futuros comportamentos transversais (Mixins).</li> </ul> <p>Negativas:</p> <ul> <li>Adiciona complexidade (l\u00f3gica condicional) aos m\u00e9todos do reposit\u00f3rio.</li> <li>Exige a presen\u00e7a do campo <code>tenant_id</code> nos modelos/tabelas relevantes.</li> <li>RISCO CR\u00cdTICO: A seguran\u00e7a do isolamento depende da configura\u00e7\u00e3o correta da flag <code>multi_tenancy_enabled</code> (deve ser <code>True</code>) em ambientes de produ\u00e7\u00e3o multi-tenant. Um erro de configura\u00e7\u00e3o aqui pode levar a vazamento de dados.</li> <li>N\u00e3o aborda diretamente o acesso a dados \"globais\" (n\u00e3o pertencentes a um tenant), que pode exigir estrat\u00e9gias adicionais (ex: roles especiais, reposit\u00f3rios espec\u00edficos).</li> </ul> <p>Neutras:</p> <ul> <li>Depende da correta implementa\u00e7\u00e3o e popula\u00e7\u00e3o do <code>tenant_id_ctx_var</code> pelo middleware.</li> <li>A performance das queries pode ser ligeiramente impactada pela adi\u00e7\u00e3o do filtro <code>tenant_id</code> (mitigado pela indexa\u00e7\u00e3o correta).</li> </ul>"},{"location":"adr/ADR-017-model-repository-decoupling/","title":"ADR-017: Desacoplamento de Modelos (Documents) dos Reposit\u00f3rios Athomic","text":"<ul> <li>Status: Aceito</li> <li>Data: 2025-04-18</li> </ul>"},{"location":"adr/ADR-017-model-repository-decoupling/#1-contexto","title":"1. Contexto","text":"<p>A camada <code>nala.athomic</code> visa fornecer componentes de infraestrutura reutiliz\u00e1veis e agn\u00f3sticos da aplica\u00e7\u00e3o, incluindo abstra\u00e7\u00f5es para acesso a dados atrav\u00e9s do Padr\u00e3o Reposit\u00f3rio (conforme ADR-007). Isso envolve interfaces gen\u00e9ricas (<code>IRepository</code>) e implementa\u00e7\u00f5es de base gen\u00e9ricas (ex: <code>MongoBeanieRepository</code> para MongoDB/Beanie) dentro do <code>athomic</code>.</p> <p>Por outro lado, a camada da aplica\u00e7\u00e3o (ex: <code>nala.api</code>) define os modelos de dados espec\u00edficos do dom\u00ednio (ex: <code>UserDocument</code>, <code>WorkspaceDocument</code>) que herdam de um ODM/ORM como <code>beanie.Document</code>.</p> <p>Surge a quest\u00e3o: como a implementa\u00e7\u00e3o gen\u00e9rica do reposit\u00f3rio em <code>athomic</code> (que precisa operar sobre esses modelos para executar queries, saves, etc.) pode acessar/conhecer os modelos espec\u00edficos definidos na <code>api</code> sem criar uma depend\u00eancia direta de <code>athomic</code> para <code>api</code>, o que quebraria o princ\u00edpio de <code>athomic</code> ser um core reutiliz\u00e1vel?</p>"},{"location":"adr/ADR-017-model-repository-decoupling/#2-alternativas-consideradas","title":"2. Alternativas Consideradas","text":"<ol> <li> <p>Reposit\u00f3rio Base Importa/Descobre Modelos da Aplica\u00e7\u00e3o: A classe base gen\u00e9rica em <code>athomic</code> (ex: <code>MongoBeanieRepository</code>) poderia tentar importar modelos de um local conhecido na <code>api</code> (ex: <code>nala.api.models</code>) ou usar mecanismos de descoberta.</p> <ul> <li>Contras: Cria uma depend\u00eancia direta <code>athomic</code> -&gt; <code>api</code>, tornando <code>athomic</code> acoplado \u00e0 aplica\u00e7\u00e3o espec\u00edfica e n\u00e3o reutiliz\u00e1vel em outros contextos. Viola a separa\u00e7\u00e3o de camadas.</li> </ul> </li> <li> <p>Definir Modelos (Exemplos/Gen\u00e9ricos) em <code>athomic</code>: <code>athomic</code> poderia definir suas pr\u00f3prias classes <code>Document</code> gen\u00e9ricas ou exemplos.</p> <ul> <li>Contras: Torna <code>athomic</code> espec\u00edfico do dom\u00ednio (precisaria de um <code>UserDocument</code> gen\u00e9rico?), ou as classes gen\u00e9ricas seriam muito limitadas para uso real pela aplica\u00e7\u00e3o. N\u00e3o resolve como operar nos modelos reais da aplica\u00e7\u00e3o.</li> </ul> </li> <li> <p>Inje\u00e7\u00e3o da Classe do Modelo via <code>__init__</code>: A classe base gen\u00e9rica do reposit\u00f3rio em <code>athomic</code> (ex: <code>MongoBeanieRepository</code>) recebe a classe do modelo Beanie (<code>Type[Document]</code>) como um argumento em seu construtor (<code>__init__</code>). A implementa\u00e7\u00e3o concreta do reposit\u00f3rio na camada <code>api</code> (ex: <code>MongoUserRepository</code>) \u00e9 respons\u00e1vel por importar o modelo espec\u00edfico (ex: <code>UserDocument</code>) e pass\u00e1-lo para o <code>super().__init__()</code> da classe base.</p> <ul> <li>Pr\u00f3s: Mant\u00e9m <code>athomic</code> completamente agn\u00f3stico aos modelos da aplica\u00e7\u00e3o. A depend\u00eancia flui corretamente (<code>api</code> -&gt; <code>athomic</code>). Permite que a base gen\u00e9rica opere sobre qualquer modelo compat\u00edvel fornecido. Usa inje\u00e7\u00e3o de depend\u00eancia (de tipo, neste caso) de forma clara.</li> </ul> </li> </ol>"},{"location":"adr/ADR-017-model-repository-decoupling/#3-decisao","title":"3. Decis\u00e3o","text":"<p>Fica decidido que a intera\u00e7\u00e3o entre os reposit\u00f3rios base gen\u00e9ricos em <code>athomic</code> e os modelos de dados espec\u00edficos da aplica\u00e7\u00e3o seguir\u00e1 a Alternativa 3 (Inje\u00e7\u00e3o da Classe do Modelo via <code>__init__</code>):</p> <ol> <li>Localiza\u00e7\u00e3o dos Modelos: Todas as classes de modelo de dados concretas (ex: <code>beanie.Document</code>) que representam entidades do dom\u00ednio da aplica\u00e7\u00e3o devem residir fora do pacote <code>nala.athomic</code>. Tipicamente, ficar\u00e3o na camada da aplica\u00e7\u00e3o (ex: <code>nala.api.models</code>).</li> <li>Construtor do Reposit\u00f3rio Base: As classes base de reposit\u00f3rio gen\u00e9ricas dentro de <code>athomic</code> (ex: <code>MongoBeanieRepository</code>) devem aceitar a classe do modelo de documento espec\u00edfica como um argumento obrigat\u00f3rio em seu m\u00e9todo <code>__init__</code>. Recomenda-se usar <code>Type[ModelDocType]</code> (onde <code>ModelDocType</code> \u00e9 um <code>TypeVar</code> vinculado a <code>Document</code>) para tipagem.     <code>python     # Exemplo em athomic/.../repositories.py     ModelDocType = TypeVar(\"ModelDocType\", bound=Document)     class MongoBeanieRepository(IRepository[ModelDocType]):         def __init__(self, document_model: Type[ModelDocType]):              self.model = document_model              # ...</code></li> <li> <p>Implementa\u00e7\u00e3o Concreta do Reposit\u00f3rio: As classes de reposit\u00f3rio concretas, que residem na camada da aplica\u00e7\u00e3o (ex: <code>nala.api.data.repositories</code>), s\u00e3o respons\u00e1veis por:</p> <ul> <li>Importar o modelo de dom\u00ednio espec\u00edfico (ex: <code>UserDocument</code> de <code>nala.api.models</code>).</li> <li>Importar a classe base gen\u00e9rica (ex: <code>MongoBeanieRepository</code> de <code>nala.athomic...</code>).</li> <li>Importar a interface espec\u00edfica que implementam (ex: <code>IUserRepository</code>).</li> <li>Chamar <code>super().__init__(document_model=UserDocument)</code> dentro do seu pr\u00f3prio <code>__init__</code> para fornecer o modelo \u00e0 classe base. ```python</li> </ul> </li> </ol>"},{"location":"adr/ADR-017-model-repository-decoupling/#exemplo-em-apidatarepositoriesmongo_user_repositorypy","title":"Exemplo em api/data/repositories/mongo_user_repository.py","text":"<p>from nala.athomic.database.nosql.mongo.repositories import MongoBeanieRepository from nala.athomic.database.nosql.repository_interfaces import IUserRepository from nala.api.models.user import UserDocument</p> <p>class MongoUserRepository(MongoBeanieRepository[UserDocument], IUserRepository):     def init(self):         super().init(document_model=UserDocument) # &lt;&lt;&lt; Liga\u00e7\u00e3o AQUI     # ... implementa\u00e7\u00e3o de m\u00e9todos de IUserRepository ... ```</p>"},{"location":"adr/ADR-017-model-repository-decoupling/#4-consequencias","title":"4. Consequ\u00eancias","text":"<p>Positivas:</p> <ul> <li>Desacoplamento Forte: Garante que <code>nala.athomic</code> permane\u00e7a completamente independente e agn\u00f3stico em rela\u00e7\u00e3o aos modelos de dom\u00ednio espec\u00edficos da aplica\u00e7\u00e3o <code>nala.api</code> (ou qualquer outra).</li> <li>Reutiliza\u00e7\u00e3o: Maximiza a capacidade de reutilizar o <code>nala.athomic</code> em diferentes projetos com diferentes modelos de dados.</li> <li>Clareza Arquitetural: Define uma separa\u00e7\u00e3o clara de responsabilidades entre a infraestrutura gen\u00e9rica e a l\u00f3gica/dados espec\u00edficos da aplica\u00e7\u00e3o.</li> <li>Seguran\u00e7a de Tipos: O uso de <code>TypeVar</code> e <code>Generic</code> (<code>IRepository[ModelType]</code>, <code>MongoBeanieRepository[ModelDocType]</code>) permite que a tipagem est\u00e1tica funcione corretamente atrav\u00e9s das camadas.</li> </ul> <p>Negativas:</p> <ul> <li>Boilerplate M\u00ednimo: Exige que cada implementa\u00e7\u00e3o concreta de reposit\u00f3rio na camada de aplica\u00e7\u00e3o tenha um <code>__init__</code> que chame <code>super().__init__(document_model=...)</code>.</li> </ul> <p>Neutras:</p> <ul> <li>A responsabilidade de fornecer o modelo correto para a base gen\u00e9rica recai sobre o desenvolvedor da implementa\u00e7\u00e3o concreta do reposit\u00f3rio.</li> <li>Esta decis\u00e3o complementa a estrat\u00e9gia de registro de reposit\u00f3rios (onde a <code>api</code> registra suas implementa\u00e7\u00f5es concretas na factory de <code>athomic</code>), refor\u00e7ando o fluxo de depend\u00eancia <code>api</code> -&gt; <code>athomic</code>.</li> </ul>"},{"location":"adr/ADR-018-rate-limiter-qos-factory/","title":"ADR-018: Rate Limiter Refactoring for QoS and Factory Pattern (Revised)","text":""},{"location":"adr/ADR-018-rate-limiter-qos-factory/#status","title":"Status","text":"<ul> <li>Status: Aceito</li> <li>Data: 2025-04-21 (Atualizado)</li> </ul>"},{"location":"adr/ADR-018-rate-limiter-qos-factory/#context","title":"Context","text":"<p>The previous rate limiting mechanism (ADR-006), while functional using the <code>limits</code> library via the <code>@rate_limited</code> decorator, lacked flexibility for applying different limits based on client profiles (Quality of Service - QoS) and didn't align with the Factory pattern used in other <code>athomic</code> modules (Cache, Secrets). Debugging also revealed challenges in provider instantiation during testing. This ADR addresses these points by introducing QoS capabilities and standardizing provider creation.</p>"},{"location":"adr/ADR-018-rate-limiter-qos-factory/#decision","title":"Decision","text":"<p>We will refactor the <code>nala.athomic.resilience.rate_limiter</code> module to: 1. Implement configurable Quality of Service (QoS) via policies. 2. Introduce a Factory pattern for provider instantiation. 3. Decouple provider-specific configuration validation from the generic configuration schema. 4. Refactor the <code>@rate_limited</code> decorator to support dynamic policy resolution.</p> <p>Detailed Decisions:</p> <ol> <li> <p>Factory Pattern Implementation:</p> <ul> <li>A new module <code>factory.py</code> contains the factory function <code>get_rate_limiter_provider() -&gt; AbstractRateLimiter</code>.</li> <li>The factory reads <code>settings.rate_limiter</code> from the global configuration (<code>get_settings</code>).</li> <li>It determines the provider implementation name (currently defaults to \"limits\", potentially configurable via settings in the future, e.g., <code>settings.rate_limiter.provider_backend</code>).</li> <li>It looks up the corresponding provider class in <code>registry.REGISTERED_PROVIDERS</code>.</li> <li>It instantiates the provider class, passing the entire <code>settings.rate_limiter</code> object as <code>config: RateLimiterSettings</code>.</li> <li>The factory function is decorated with <code>@lru_cache()</code> to ensure a singleton provider instance.</li> </ul> </li> <li> <p>Provider Constructor and Validation:</p> <ul> <li>All concrete rate limiter provider implementations (e.g., <code>LimitsRateLimiter</code>) must implement an <code>__init__</code> method that accepts a single argument <code>config: RateLimiterSettings</code>.</li> <li>Providers use this <code>config</code> object internally for their settings.</li> <li>Crucially, each provider's <code>__init__</code> is responsible for validating any provider-specific configuration formats. For example, <code>LimitsRateLimiter</code> validates the format of <code>config.default_policy_limit</code> and <code>config.policies</code> values using <code>limits.util.parse</code> during its initialization.</li> </ul> </li> <li> <p>QoS Configuration (<code>RateLimiterSettings</code>):</p> <ul> <li>The Pydantic model <code>RateLimiterSettings</code> (<code>config/schemas/rate_limiter_config.py</code>) is extended with:<ul> <li><code>default_policy_limit: Optional[str]</code>: The default rate limit string (e.g., \"100/hour\", following <code>limits</code> library format) applied when no specific policy matches or the resolved policy name is not found.</li> <li><code>policies: Optional[Dict[str, str]]</code>: A dictionary mapping policy names (e.g., \"free\", \"premium\") to rate limit strings (following <code>limits</code> library format, e.g., \"50/minute\", \"1000/minute\", \"nolimit\").</li> </ul> </li> <li>Provider-specific format validation (like checking if strings are valid for <code>limits.util.parse</code>) is removed from this Pydantic model to keep it provider-agnostic.</li> </ul> </li> <li> <p>Decorator Refactoring (<code>@rate_limited</code>):</p> <ul> <li>The decorator signature is changed to:     <code>python     def rate_limited(         policy_resolver: Callable[..., Awaitable[str]] = default_policy_resolver,         on_block: Optional[Callable[[], Awaitable[Any]]] = None,         provider: Optional[AbstractRateLimiter] = None,     ) -&gt; Callable[..., Awaitable[Any]]:</code><ul> <li><code>policy_resolver</code>: An async function that receives the decorated function's <code>*args</code> and <code>**kwargs</code> and returns a policy name string (e.g., \"premium\"). Defaults to <code>default_policy_resolver</code> (which returns \"default\"). The implementer of the resolver is responsible for accessing necessary context (e.g., request object, user data from <code>kwargs</code>).</li> <li><code>on_block</code>: An optional async handler called when the rate limit is exceeded. If not provided, exceeding the limit might result in a default behavior (e.g., returning <code>None</code> or raising a specific exception if <code>apply_rate_limit</code> were modified to do so).</li> <li><code>provider</code>: Allows injecting a specific provider instance, primarily for testing. If <code>None</code>, the factory <code>get_rate_limiter_provider()</code> is used.</li> </ul> </li> <li>The decorator's internal wrapper logic:<ul> <li>Determines the provider instance (injected or from factory).</li> <li>Calls <code>policy_name = await policy_resolver(*args, **kwargs)</code>. Handles exceptions during resolution by falling back to the policy name <code>\"default\"</code>.</li> <li>Determines a <code>resource_key</code> (e.g., based on the decorated function's name) used for identifying the resource being limited.</li> <li>Calls the core function <code>apply_rate_limit</code>, passing <code>policy_key</code>, <code>policy_name_resolved</code>, <code>provider</code>, the original <code>func</code>, <code>on_block</code>, and the original <code>args</code>/<code>kwargs</code>.</li> </ul> </li> </ul> </li> <li> <p>Core Logic (<code>core.apply_rate_limit</code>):</p> <ul> <li>This central async function (called by the decorator) encapsulates the main rate limiting flow:<ul> <li>Retrieves the <code>RateLimiterSettings</code> via <code>get_settings()</code>.</li> <li>Checks if rate limiting is enabled.</li> <li>Looks up the <code>policy_name_resolved</code> in <code>settings.rate_limiter.policies</code>.</li> <li>If found, uses that limit string; otherwise, uses <code>settings.rate_limiter.default_policy_limit</code>.</li> <li>Constructs the full storage key using <code>utils.build_rate_limit_key</code>.</li> <li>Calls <code>allowed = await provider.allow(full_key, resolved_limit_string)</code>.</li> <li>If <code>allowed</code>, calls and returns the result of the original <code>func(*args, **kwargs)</code>.</li> <li>If not <code>allowed</code>, calls and returns the result of <code>on_block()</code> if provided, otherwise returns <code>None</code>.</li> </ul> </li> </ul> </li> <li> <p>Standard Resolvers/Helpers:</p> <ul> <li>Common/example functions for policy resolution (e.g., based on request state, headers) can be provided in <code>resolvers.py</code> within the rate limiter module for reusability. <code>utils.py</code> remains for key generation helpers.</li> </ul> </li> </ol>"},{"location":"adr/ADR-018-rate-limiter-qos-factory/#consequences","title":"Consequences","text":"<p>Positive:</p> <ul> <li>Decoupling: Configuration schema is decoupled from specific library formats. Decorator is decoupled from provider implementation and core logic flow.</li> <li>Consistency: Aligns with Factory pattern usage in other modules.</li> <li>Configurability &amp; Flexibility: Enables rich, configuration-driven QoS policies. <code>policy_resolver</code> allows dynamic determination based on any request context.</li> <li>Testability: Factory, Provider (including its validation), Core, Decorator, and Resolvers can be tested independently.</li> <li>Clarity: Centralizes provider instantiation (Factory) and core rate limiting logic (<code>apply_rate_limit</code>). Provider-specific validation resides within the provider.</li> </ul> <p>Negative:</p> <ul> <li>Refactoring Effort: Required significant changes across the module (Provider, Factory, Core, Decorator, Config, Tests).</li> <li>Complexity in Use: Developers using <code>@rate_limited</code> for QoS need to implement appropriate <code>policy_resolver</code> functions that can access the necessary context from the decorated function's arguments/kwargs.</li> <li>Later Validation: Configuration format validation (for limit strings) happens during provider instantiation (runtime) rather than purely at Pydantic model parsing time, although this typically occurs early near application startup.</li> </ul>"},{"location":"adr/ADR-018-rate-limiter-qos-factory/#alternatives-considered","title":"Alternatives Considered","text":"<ol> <li>Validation in Pydantic Model: Keep the <code>limits.util.parse</code> validation within <code>RateLimiterSettings</code>.<ul> <li>Rejected because: Tightly couples the configuration schema to a specific library implementation, hindering the addition of providers with different limit formats.</li> </ul> </li> <li>Direct Instantiation in Decorator: Continue instantiating <code>LimitsRateLimiter</code> directly in <code>@rate_limited</code>.<ul> <li>Rejected because: Tight coupling, architecturally inconsistent, doesn't support different providers based on config easily.</li> </ul> </li> <li>Dependency Injection via Request State: Inject <code>RateLimiter</code> instance into <code>request.state</code>.<ul> <li>Rejected because: Adds complexity to startup/request lifecycle. Factory pattern provides sufficient decoupling and singleton management.</li> </ul> </li> <li>Embedding Policy Logic in Decorator: Hardcode logic in the decorator to inspect request context.<ul> <li>Rejected because: Tight coupling to application state structure. Passing <code>policy_resolver</code> is more flexible.</li> </ul> </li> </ol>"},{"location":"adr/ADR-018-rate-limiter-qos-factory/#references","title":"References","text":"<ul> <li>ADR-006: Rate Limiting (Initial decision to use <code>limits</code> library)</li> </ul>"},{"location":"adr/ADR-019-feature-flags/","title":"ADR-019: Creation of the Feature Flags (Toggles) Module","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-04-25</li> </ul>"},{"location":"adr/ADR-019-feature-flags/#context","title":"Context","text":"<ul> <li>Problem: Modern applications like <code>athomic-docs</code> often need to enable or disable features dynamically at runtime without requiring a new deployment. Use cases include gradual rollouts (Canary Releases), A/B testing, enabling features per tenant or user group, and the ability to quickly disable a problematic feature (kill switch). Implementing this logic directly in business code or in an ad-hoc manner makes the system less flexible, harder to manage, and prone to errors.</li> <li>Need: A centralized, configurable, and decoupled mechanism to manage and query the state of feature flags, integrated into the <code>athomic</code> architecture.</li> <li>Alternatives Considered:<ul> <li>Manual Conditional Logic: <code>if settings.get(\"feature_x_enabled\"): ...</code> scattered throughout the code. Rejected: High coupling, difficult centralized management, violates Open/Closed Principle.</li> <li>Include in Existing Modules:<ul> <li><code>athomic/config</code>: Feature flags are a form of configuration, but dynamic and queried at runtime, differing from static configuration loaded at initialization. Rejected: Would dilute the purpose of <code>config</code>.</li> <li><code>athomic/integrations</code>: Some providers (Consul, Redis, external services) involve integration, but the local provider does not, and the core functionality is behavior control, not communication. Rejected: Does not represent the primary responsibility.</li> <li><code>athomic/resilience</code>: Flags can be used for resilience (kill switch), but have other uses (A/B, rollouts) beyond the scope of resilience. Rejected: Limited scope.</li> </ul> </li> <li>Dedicated External Services (No Abstraction): Using SDKs from services like LaunchDarkly directly in the application code. Rejected: Would create direct coupling with the external service in business logic, hindering switching or unit testing.</li> </ul> </li> </ul>"},{"location":"adr/ADR-019-feature-flags/#decision","title":"Decision","text":"<ul> <li>Create a new first-class module <code>nala.athomic.feature_flags</code> within the <code>athomic</code> layer.</li> <li>Adopt the Provider/Registry/Factory pattern, already established in other <code>athomic</code> modules (Cache, Secrets, Rate Limiter, Repository):<ul> <li>Interface: Define a <code>FeatureFlagProvider</code> protocol with essential methods like <code>async def is_enabled(flag_key: str, default: bool = False, context: Optional[dict] = None) -&gt; bool</code> and <code>async def get_variant(flag_key: str, default: Optional[Any] = None, context: Optional[dict] = None) -&gt; Optional[Any]</code>.</li> <li>Providers: Implement concrete classes adhering to the <code>FeatureFlagProvider</code> interface:<ul> <li><code>LocalProvider</code>: Reads flags from a dictionary in the configuration (<code>settings.toml</code>) or a local file. Ideal for development and testing. (Implemented)</li> <li><code>ConsulProvider</code>: (Planned) Reads flags from Consul KV, potentially using caching (<code>aiocache</code>).</li> <li><code>RedisProvider</code>: (Planned) Reads flags from Redis, potentially using caching (<code>aiocache</code>).</li> <li>(Future) Providers for external services (LaunchDarkly, Unleash).</li> </ul> </li> <li>Registry: A simple dictionary mapping backend names (<code>\"local\"</code>, <code>\"consul\"</code>, <code>\"redis\"</code>) to the Provider classes. (Implemented)</li> <li>Factory: A <code>get_feature_flag_provider()</code> function with <code>@lru_cache</code> that reads <code>settings.feature_flags.backend</code>, queries the Registry, and returns the singleton instance of the configured provider. (Implemented)</li> </ul> </li> <li>Configuration: Manage settings (backend, prefixes, cache TTL for external providers, local flags) through a new Pydantic schema <code>FeatureFlagSettings</code> integrated into <code>AppSettings</code>. (Implemented)</li> <li>Usage: Expose the primary functionality through helper functions <code>async def is_feature_enabled(flag_key: str, **context)</code> and <code>async def get_feature_variant(flag_key: str, **context)</code> (which use the factory), and a decorator <code>@feature_enabled(...)</code>. (Implemented)</li> </ul>"},{"location":"adr/ADR-019-feature-flags/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Dynamic Control: Allows enabling/disabling features without deployment, increasing agility and operational safety (quick rollbacks).</li> <li>Decoupling: Business logic checking a flag (<code>is_feature_enabled(\"...\")</code>) doesn't need to know where the flag is stored (Consul, Redis, local config).</li> <li>Architectural Consistency: Reuses the Provider/Registry/Factory pattern, maintaining coherence with other <code>athomic</code> modules.</li> <li>Testability: Facilitates unit testing of business logic by mocking the helper functions or injecting a <code>LocalProvider</code> in tests.</li> <li>Extensibility: Adding support for a new backend (e.g., LaunchDarkly) only requires creating a new provider and registering it, without changing the code that consumes the flags.</li> </ul> </li> <li>Negative:<ul> <li>New Layer of Indirection: Adds the provider/factory abstraction layer.</li> <li>Infrastructure Complexity (if external): Using Consul or Redis as a backend requires those services to be available and configured.</li> <li>Flag Management: Requires a process for managing flags in the chosen backends (who creates, updates, removes flags?).</li> <li>Cache Consistency (if applicable): If external providers use caching (as suggested with <code>aiocache</code>), the invalidation/TTL strategy must be considered so that flag changes are reflected in the application in a timely manner.</li> </ul> </li> <li>Neutral/Other:<ul> <li>Effectiveness depends on the team's discipline in using the feature flags mechanism instead of hardcoded conditionals.</li> <li>The <code>context</code> logic (for tenant/user rollouts) needs to be well-defined and passed correctly to the usage functions/decorator.</li> </ul> </li> </ul>"},{"location":"adr/ADR-020-context-aware-architecture/","title":"ADR-020: Context-Aware Modular Architecture for Core Service Modules","text":"<p>Status: Accepted Date: 2025-04-27</p>"},{"location":"adr/ADR-020-context-aware-architecture/#context","title":"\ud83e\udde0 Context","text":"<p>As the <code>athomic-docs</code> evolves, we are introducing multiple service modules like <code>CacheModule</code>, <code>RateLimiter</code>, <code>FeatureFlags</code>, etc. These modules need to:</p> <ul> <li>Be multi-tenant aware (use <code>tenant_id</code> dynamically)</li> <li>Support lifecycle hooks (<code>init</code>, <code>shutdown</code>)</li> <li>Inject settings and logging automatically</li> <li>Allow key resolution with contextual prefixes (e.g., <code>\"nala:&lt;tenant&gt;:cache:&lt;key&gt;\"</code>)</li> <li>Remain pluggable and backend-agnostic, enabling providers to be swapped via configuration</li> </ul> <p>These requirements are cross-cutting and repeated across modules. Without a standardized base, implementation becomes inconsistent and error-prone.</p>"},{"location":"adr/ADR-020-context-aware-architecture/#decision","title":"\u2705 Decision","text":"<p>We introduce a standardized architecture for Athomic modules, built on the following components:</p> <ol> <li> <p><code>AthomicBaseComponent</code>     Base class for all Athomic modules. Provides:  </p> <ul> <li>Access to the current <code>ExecutionContext</code> via <code>self.context</code> (which reads context vars)  </li> <li>Typed <code>settings</code> injection  </li> <li><code>logger</code> instance aware of the component's name  </li> <li><code>init</code>, <code>close</code>, and <code>shutdown</code> lifecycle hooks</li> </ul> </li> <li> <p><code>ExecutionContext</code> + <code>context_vars</code>     Encapsulates request/session-level information (<code>tenant_id</code>, <code>user_id</code>, <code>request_id</code>, etc.) using thread-safe <code>contextvars</code>. The <code>ExecutionContext</code> class provides a convenient way to access these variables.</p> </li> <li> <p><code>KeyResolver</code> and <code>KeyResolvingKVClient</code> </p> <ul> <li><code>KeyResolver</code>: Ensures keys used in distributed systems are namespaced consistently based on configuration (prefix, namespace, use_tenant, use_user) and dynamically fetches the current <code>ExecutionContext</code> (reading from <code>context_vars</code>) during the <code>resolve()</code> call to incorporate relevant IDs (like tenant_id or user_id) into the final key. It no longer accepts <code>ExecutionContext</code> directly in its constructor.  </li> <li><code>KeyResolvingKVClient</code>: Wraps a <code>KVStoreProtocol</code> client, using <code>KeyResolver</code> internally to automatically prefix keys before delegating operations.</li> </ul> </li> <li> <p><code>KVStoreProtocol</code>     Common async interface for all key-value stores. Enables plug-and-play backends via:  </p> <ul> <li>Registry + Factory (local, redis, upstash...)</li> </ul> </li> <li> <p>Concrete Modules Follow This Pattern:     Each module (e.g., <code>CacheModule</code>) implements its domain logic but delegates:  </p> <ul> <li>Key formatting to <code>KeyResolver</code> (implicitly via <code>KeyResolvingKVClient</code> or used directly)  </li> <li>Persistence to a <code>KVStoreProtocol</code>-based client  </li> <li>Lifecycle and base utilities (context access, settings, logger) potentially by inheriting from <code>AthomicBaseComponent</code></li> </ul> </li> </ol>"},{"location":"adr/ADR-020-context-aware-architecture/#implementation-guidelines","title":"\ud83d\udee0\ufe0f Implementation Guidelines","text":"<pre><code>src/nala/athomic/performance/cache/cache_module.py\n\u2937 class CacheModule(AthomicBaseComponent, CacheProtocol)\n\nsrc/nala/athomic/database/kvstore/\n\u2937 registry.py, factory.py, wrappers/, clients/\n\nsrc/nala/athomic/context/\n\u2937 context_vars.py, execution.py, key_resolver.py\n\nsrc/nala/athomic/base/\n\u2937 base_component.py, core_module.py\n</code></pre>"},{"location":"adr/ADR-020-context-aware-architecture/#consequences","title":"\ud83d\udcc8 Consequences","text":""},{"location":"adr/ADR-020-context-aware-architecture/#positives","title":"\u2705 Positives","text":"<ul> <li>Consistency: All service modules behave similarly and respect multi-tenancy by default.</li> <li>Extensibility: New modules (e.g., <code>RateLimiter</code>) can be added by following the same pattern.</li> <li>Observability: Logs are enriched with tenant/request IDs.</li> <li>Reusability: Key logic (context reading via <code>context_vars</code>/<code>ExecutionContext</code>, key resolution via <code>KeyResolver</code>) is shared.</li> <li>Testability: Easy to mock <code>KVStoreProtocol</code>. Mocking context now involves patching <code>context_vars</code> getters (e.g., <code>get_tenant_id</code>) rather than injecting an <code>ExecutionContext</code> into <code>KeyResolver</code> or its users directly.</li> </ul>"},{"location":"adr/ADR-020-context-aware-architecture/#negatives","title":"\u26a0\ufe0f Negatives","text":"<ul> <li>Slight increase in boilerplate for each module inheriting <code>AthomicBaseComponent</code>.</li> <li>Requires discipline to always use <code>KeyResolver</code> (or wrappers like <code>KeyResolvingKVClient</code>) instead of hardcoded keys.</li> <li>Testing context-dependent logic now requires patching <code>context_vars</code> getters, which might be slightly less direct than injecting a context object in some cases.</li> </ul>"},{"location":"adr/ADR-020-context-aware-architecture/#neutralother","title":"\u2795 Neutral/Other","text":"<ul> <li>Enables future features like:</li> <li><code>SecretsModule</code>, <code>AuthModule</code>, <code>SessionStore</code>, etc.</li> <li>Automatic instrumentation and metric emission using the same context.</li> </ul>"},{"location":"adr/ADR-021-athomic-control/","title":"ADR-021: Introduce <code>athomic.control</code> as Unified Module for Dynamic Behavior","text":"<ul> <li>Status: Accepted  </li> <li>Date: 2025-05-02</li> </ul>"},{"location":"adr/ADR-021-athomic-control/#1-context","title":"1. Context","text":"<p>As the <code>athomic-docs</code> project grows, we are introducing more modules that influence runtime behavior without requiring redeployment. Examples include <code>feature_flags</code>, dynamic <code>config</code>, and service <code>discovery</code>. These modules share key architectural traits:</p> <ul> <li>Provider-based with pluggable backends (e.g., Consul, Redis, static)</li> <li>Context-sensitive (e.g., tenant-aware, environment-aware)</li> <li>Support fallback, dynamic control, and observability</li> <li>Govern service behavior dynamically and externally</li> </ul> <p>To promote cohesion and extensibility, we propose grouping these under a common namespace and architectural category.</p>"},{"location":"adr/ADR-021-athomic-control/#2-decision","title":"2. Decision","text":"<p>We introduce a new parent module:</p> <pre><code>athomic/control/\n</code></pre> <p>This directory will house all modules responsible for runtime behavior control, including but not limited to:</p> Submodule Description <code>feature_flags/</code> Dynamic feature toggles and variants <code>config/</code> Remote/dynamic config providers (e.g., Consul, Vault, etcd) <code>discovery/</code> Service discovery mechanisms (e.g., Consul) <code>kill_switches/</code> Emergency feature/service shutdowns (future) <code>experiments/</code> A/B testing, canary rollout, variant resolution (future) <code>routing/</code> Context-based dynamic routing (future) <code>policies/</code> Dynamic behavior policies and rules (retry, backoff, QoS, etc.) <p>Each module will follow <code>athomic</code> best practices: - Interface-first design (Protocol or ABC) - Registry + Factory pattern - Settings-driven configuration - Optional fallback/caching/tracing support</p>"},{"location":"adr/ADR-021-athomic-control/#3-justification","title":"3. Justification","text":"<ul> <li>Modular Cohesion: All submodules alter runtime behavior dynamically.</li> <li>Scalability: Adds clarity and room for controlled growth.</li> <li>Governance Support: Provides an architectural base for implementing runtime governance and dashboards.</li> <li>Reusability: Encourages shared utilities (context resolver, fallback decorators, metrics).</li> <li>Developer Experience: Clarifies purpose and expectations from modules in this group.</li> </ul>"},{"location":"adr/ADR-021-athomic-control/#4-consequences","title":"4. Consequences","text":""},{"location":"adr/ADR-021-athomic-control/#positive","title":"\u2705 Positive","text":"<ul> <li>Clear grouping of dynamic behavior modules.</li> <li>Easier to extend with new modules (experiments, routing, etc.).</li> <li>Improved documentation and architectural coherence.</li> <li>Aligns with modular, pluggable, observable infrastructure vision.</li> </ul>"},{"location":"adr/ADR-021-athomic-control/#neutralminor-migration","title":"\u26a0\ufe0f Neutral/Minor Migration","text":"<ul> <li>Existing modules (e.g., <code>feature_flags</code>, <code>discovery</code>, <code>config</code>) will be moved into <code>athomic/control/</code>.</li> <li>Internal imports need to be updated accordingly.</li> </ul>"},{"location":"adr/ADR-021-athomic-control/#risk","title":"\u2757 Risk","text":"<ul> <li>Adding a layer of indirection increases complexity slightly.</li> <li>Developers must understand this new layer's purpose and scope.</li> </ul>"},{"location":"adr/ADR-021-athomic-control/#5-action-items","title":"5. Action Items","text":"<ol> <li>Move <code>feature_flags/</code>, <code>discovery/</code>, and <code>config/</code> into <code>athomic/control/</code>.</li> <li>Update all internal imports and tests accordingly.</li> <li>Add new documentation under <code>docs/athomic/control.md</code> to explain this layer\u2019s role.</li> <li>Plan for future submodules like <code>kill_switches</code>, <code>experiments</code>, and <code>policies</code>.</li> </ol>"},{"location":"adr/ADR-022-background-task-abstraction/","title":"ADR-022: Background Task Abstraction Module","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-05-02</li> </ul>"},{"location":"adr/ADR-022-background-task-abstraction/#context","title":"Context","text":"<ul> <li>Problem: Modern APIs like <code>athomic-docs</code> frequently need to offload long-running, resource-intensive, or non-critical operations (e.g., sending emails/notifications, processing uploads, generating reports) to background workers. Implementing this requires integrating with task queue frameworks (like Celery, RQ, Dramatiq) which often depend on external message brokers (Redis, RabbitMQ). Directly using a specific task framework's API within business logic creates tight coupling, hindering framework changes, testing, and consistent application of cross-cutting concerns like context propagation and observability.</li> <li>Goal: Introduce a stable abstraction layer within <code>nala.athomic</code> for enqueuing background tasks. This layer should:<ul> <li>Decouple application code from the specific task queue framework implementation.</li> <li>Allow switching task queue backends via configuration.</li> <li>Integrate seamlessly with existing <code>athomic</code> modules (config, context, observability).</li> <li>Support reliable propagation of execution context (e.g., <code>tenant_id</code>, <code>trace_id</code>) from the web request to the background worker.</li> <li>Follow the established Provider/Registry/Factory pattern within <code>athomic</code>.</li> </ul> </li> </ul>"},{"location":"adr/ADR-022-background-task-abstraction/#decision","title":"Decision","text":"<ol> <li>Module Location: A new module, <code>nala.athomic.integration.tasks</code>, will be created.<ul> <li>Rationale: Since the primary interaction for frameworks like Celery and RQ involves external brokers (Redis, RabbitMQ), placing this abstraction within the <code>integration</code> package aligns with its purpose of managing external system communications, alongside <code>messaging</code> and <code>discovery</code>.</li> </ul> </li> <li>Architecture: Implement the Provider/Registry/Factory pattern:<ul> <li><code>TaskBrokerProtocol</code> (<code>integration/tasks/protocol.py</code>): Define the interface with <code>async enqueue_task(task_name: str, *args, **kwargs) -&gt; Optional[str]</code> (returns optional task ID). Methods like <code>get_task_result</code> or <code>is_available</code> can be added later if needed.</li> <li><code>TaskBrokerSettings</code> (<code>config/schemas/integration/tasks_config.py</code>): Pydantic schema (integrated into <code>AppSettings</code> likely under <code>integration</code>) to configure <code>enabled</code>, <code>backend</code> (e.g., \"local\", \"celery\", \"rq\"), and backend-specific settings (like <code>broker_url</code>, <code>result_backend_url</code>).</li> <li>Providers (<code>integration/tasks/providers/</code>):<ul> <li><code>LocalTaskBroker</code>: Simple in-memory/asyncio executor for development and testing.</li> <li><code>CeleryTaskBroker</code>: (Planned) Adapter for Celery's <code>send_task</code> API.</li> <li><code>RQTaskBroker</code>: (Planned) Adapter for RQ's <code>enqueue</code> API.</li> </ul> </li> <li>Registry (<code>integration/tasks/registry.py</code>): Dictionary mapping backend names to Provider classes. Registered <code>local</code> provider by default.</li> <li>Factory (<code>integration/tasks/factory.py</code>): <code>get_task_broker()</code> function (<code>@lru_cache</code>) reads settings, consults registry, instantiates, and returns the singleton provider.</li> </ul> </li> <li>Context Propagation:<ul> <li>A dedicated utility (<code>integration/tasks/context.py</code>) provides <code>capture_context_for_task()</code> to serialize relevant <code>context_vars</code> (tenant, trace, etc.).</li> <li>This captured context dictionary must be passed along with the task payload to the broker.</li> <li>A corresponding utility/context manager (<code>restore_context_from_task</code>) is provided to be used on the worker side to restore the context before task execution. (Worker setup is outside this ADR's scope).</li> </ul> </li> <li>Observability Integration: Providers should leverage <code>athomic/observability</code> for logging. Integration with tracing and metrics is planned for future phases.</li> </ol>"},{"location":"adr/ADR-022-background-task-abstraction/#alternatives-considered","title":"Alternatives Considered","text":"<ol> <li><code>athomic/tasks</code> (Top-Level Module): Considered but deferred as the initial focus is integration with external brokers, fitting better within <code>athomic/integration</code>. Can be revisited if scope expands significantly to internal scheduling/workflows.</li> <li><code>athomic/workflows/tasks</code>: Considered but deferred as it conflates single task enqueuing with higher-level workflow orchestration.</li> <li>Direct Library Usage: Rejected due to tight coupling and lack of consistency.</li> <li>Abstract via <code>athomic/messaging</code>: Rejected as it couples task semantics with generic messaging.</li> </ol>"},{"location":"adr/ADR-022-background-task-abstraction/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Decouples application services from specific task queue frameworks.</li> <li>Allows switching backends via configuration.</li> <li>Improves testability using <code>LocalTaskBroker</code> or mocks.</li> <li>Consistent architecture with other <code>athomic</code> modules.</li> <li>Provides a clear mechanism for context propagation.</li> </ul> </li> <li>Negative:<ul> <li>Adds a layer of abstraction and some boilerplate code.</li> <li>Context propagation requires careful implementation and testing on both the producer (API) and consumer (worker) sides.</li> <li>Requires separate setup and configuration for background workers.</li> </ul> </li> <li>Neutral/Other:<ul> <li>Relies on clear definition and discovery of task functions by the chosen backend framework.</li> </ul> </li> </ul>"},{"location":"adr/ADR-022-background-task-abstraction/#future-considerations","title":"Future Considerations","text":"<ul> <li>Implement providers for Celery and RQ.</li> <li>Integrate tracing and metrics for task enqueuing and execution.</li> <li>Refine context propagation mechanism if needed.</li> <li>Evaluate adding methods like <code>get_task_result</code>.</li> <li>Consider creating <code>athomic/scheduling</code> or <code>athomic/workflows</code> if requirements evolve significantly beyond broker integration.</li> </ul>"},{"location":"adr/ADR-023-advanced-message-patterns/","title":"ADR-023: Advanced Messaging Patterns within Athomic","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-05-04</li> </ul>"},{"location":"adr/ADR-023-advanced-message-patterns/#1-context","title":"1. Context","text":"<ul> <li>Problem: As we evolve <code>athomic-docs</code> to include advanced asynchronous messaging patterns (Internal Event Bus, Outbox Pattern, Dead Letter Queue (DLQ), Schema Registry Integration), it becomes necessary to define clear and consistent locations for these components within the <code>nala.athomic</code> modular architecture. Improper placement could violate the separation of concerns, increase coupling between modules, or hinder maintainability.</li> <li>Alternatives Considered:<ol> <li>Everything in <code>integration/messaging</code>: Grouping all new components under the existing messaging module. (Rejected: The internal event bus isn't strictly external integration; Outbox storage belongs to the data layer).</li> <li>New Top-Level <code>messaging</code> or <code>events</code> Module: Creating a new top-level module for all messaging/event logic. (Rejected: Communication with external brokers fits well within <code>integration</code>; the persistence part of Outbox belongs in <code>database</code>).</li> <li>Resilience Components in <code>resilience</code>: Placing the Outbox Publisher and DLQ Handler under <code>athomic/resilience</code>. (Rejected: Although they contribute to resilience, they are specific implementations within the messaging domain).</li> <li>Separation by Responsibility: Allocating each component to the <code>athomic</code> module that best represents its primary responsibility (internal communication vs. external integration vs. data persistence). (Accepted).</li> </ol> </li> </ul>"},{"location":"adr/ADR-023-advanced-message-patterns/#2-decision","title":"2. Decision","text":"<p>It is decided that the new advanced messaging features will be located as follows within the <code>nala.athomic</code> structure:</p> <ol> <li> <p>Internal Event Bus:</p> <ul> <li>Location: A new top-level module will be created: <code>athomic/events/</code>.</li> <li>Content: Will contain the <code>EventBusProtocol</code>, implementations (e.g., <code>MemoryEventBus</code>, <code>RedisEventBus</code>), registry, and factory (<code>get_event_bus</code>).</li> <li>Rationale: Clearly separates internal asynchronous communication (between Athomic components or the application) from external communication with brokers (which remains in <code>athomic/integration/messaging</code>).</li> </ul> </li> <li> <p>Outbox Pattern:</p> <ul> <li>Location (Split):<ul> <li>Storage: <code>athomic/database/outbox/</code> (or an appropriate submodule within <code>database</code>). Will contain <code>OutboxStorageProtocol</code>, the <code>OutboxEvent</code> model, and concrete implementations for supported databases (e.g., <code>MongoOutboxStorage</code>).</li> <li>Publisher: <code>athomic/integration/messaging/outbox/</code>. Will contain the <code>OutboxPublisher</code> logic that reads from the storage and publishes using the <code>ProducerProtocol</code> from <code>integration/messaging</code>.</li> </ul> </li> <li>Rationale: Storage is a data layer responsibility, while the act of publishing (even from a table) belongs to messaging integration.</li> </ul> </li> <li> <p>Dead Letter Queue (DLQ) Handling:</p> <ul> <li>Location: Logic will be added within the consumer implementations in <code>athomic/integration/messaging/providers/</code> (e.g., <code>KafkaConsumer</code>).</li> <li>Content: Modifications to consumption methods to include retry counting and redirection to the configured DLQ.</li> <li>Rationale: DLQ handling is intrinsic to how a consumer processes and deals with failures, making it most suitable to enhance the existing provider classes. Related configurations (<code>topic</code>, <code>dlq.max_attempts</code>) will belong to the specific consumer configuration schemas.</li> </ul> </li> <li> <p>Schema Registry Integration:</p> <ul> <li>Location: Serialization/deserialization and schema validation logic will primarily reside within the providers in <code>athomic/integration/messaging/providers/</code> (modifying <code>KafkaProducer</code>, <code>KafkaConsumer</code>, etc.).</li> <li>Content (Optional): If schema handling/serialization logic becomes complex, it could be extracted into a new submodule <code>athomic/integration/messaging/serializers/</code>.</li> <li>Rationale: Directly impacts how producers and consumers format and interpret message data during integration with the broker.</li> </ul> </li> </ol>"},{"location":"adr/ADR-023-advanced-message-patterns/#3-consequences","title":"3. Consequences","text":"<ul> <li>Positive:<ul> <li>Maintains clarity and separation of concerns within the <code>athomic</code> modular architecture.</li> <li>Allocates each component where its primary function resides (persistence in <code>database</code>, external communication in <code>integration</code>, internal communication in <code>events</code>).</li> <li>Facilitates independent evolution and maintenance of each pattern.</li> <li>Avoids overloading the <code>integration/messaging</code> module with logic not strictly related to external broker interaction (like the internal event bus or outbox storage).</li> </ul> </li> <li>Negative:<ul> <li>Introduces a new top-level module (<code>athomic/events</code>), slightly increasing the top-level structure.</li> <li>The Outbox Pattern implementation is split across two modules (<code>database</code> and <code>integration/messaging</code>), requiring clear documentation of their interaction.</li> </ul> </li> <li>Neutral/Other:<ul> <li>Reinforces the need for clear interfaces (<code>EventBusProtocol</code>, <code>OutboxStorageProtocol</code>) to enable collaboration between modules.</li> <li>Requires developers to understand the distinction between internal communication (<code>events</code>) and external communication (<code>integration/messaging</code>).</li> </ul> </li> </ul>"},{"location":"adr/ADR-024-transactional-outbox-parttern/","title":"ADR-024: Transactional Outbox Pattern for Reliable Messaging","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-05-05</li> </ul>"},{"location":"adr/ADR-024-transactional-outbox-parttern/#1-context","title":"1. Context","text":"<ul> <li>Problem: When publishing messages/events to an external broker (e.g., Kafka) after a database operation has been committed, there is an inherent risk of the publication failing (e.g., broker unavailable, network error, serialization error). This leads to an inconsistent state: the database change was persisted, but the corresponding event meant to notify other systems or trigger subsequent processes was not sent. This scenario violates delivery guarantees and can lead to desynchronized data or incomplete workflows.</li> <li>Need: To ensure more reliable delivery (\"at least once delivery\" with higher perceived atomicity) of messages/events that are logically coupled with database transactions. The failure to publish the event should not leave the system in a state where the primary action occurred, but the event was lost.</li> <li>Alternatives Considered:<ul> <li>Direct Synchronous Publishing: Calling <code>producer.publish()</code> directly after the DB transaction commits. Rejected: This is the status quo that exhibits the described consistency problem.</li> <li>Distributed Transactions (Two-Phase Commit - 2PC): Attempting to coordinate the database transaction and the broker publication as a single distributed transaction. Rejected: Extremely complex to implement correctly, poorly supported by most modern brokers and databases, significantly impacts performance and availability (CAP theorem).</li> <li>Direct Async Publishing with Robust Producer Retries: Relying solely on the internal retry mechanisms of the <code>ProducerProtocol</code> (e.g., <code>KafkaProducer</code> with <code>acks='all'</code> and configured retries). Rejected: While improving delivery chance, it still doesn't guarantee the message will be sent if the application crashes after the DB commit but before the publication is acknowledged by the broker. It doesn't solve the fundamental \"dual write\" problem.</li> </ul> </li> </ul>"},{"location":"adr/ADR-024-transactional-outbox-parttern/#2-decision","title":"2. Decision","text":"<p>Adopt the Transactional Outbox Pattern for sending critical messages/events coupled with database operations. The implementation will involve the following steps:</p> <ol> <li>Outbox Storage: Create a dedicated collection/table (<code>outbox_events</code>) within the same database as the primary business operations.</li> <li><code>OutboxEvent</code> Model: Define a model (Beanie <code>Document</code>) to represent events to be published, containing fields like <code>event_id</code>, <code>event_name</code>, <code>payload</code>, <code>status</code> (PENDING, PUBLISHED, FAILED), <code>destination_topic</code>, <code>message_key</code>, <code>attempts</code>, <code>last_error</code>, <code>created_at</code>.</li> <li><code>OutboxStorageProtocol</code> Interface: Define a protocol for interacting with the outbox storage, including methods <code>save_event</code>, <code>get_pending_events</code>, <code>mark_event_published</code>, <code>mark_event_failed</code>.</li> <li><code>MongoOutboxRepository</code> Implementation: Implement the <code>OutboxStorageProtocol</code> using Beanie/Motor for MongoDB. Crucially, the <code>save_event</code> method must accept and use the <code>ClientSession</code> object from the main transaction.</li> <li>Service Layer Modification: Business logic currently calling <code>producer.publish()</code> after a DB operation will be changed to:<ul> <li>Start an explicit database transaction (e.g., using <code>motor_client.start_session()</code> and <code>session.start_transaction()</code>).</li> <li>Perform the main business operation (e.g., <code>user_repo.save(user, session=session)</code>).</li> <li>Call <code>outbox_repo.save_event(..., db_session=session)</code> within the same transaction.</li> <li>Allow the transaction to commit (if both operations succeed) or abort (if either fails).</li> </ul> </li> <li>Dedicated Publisher (<code>OutboxPublisher</code>): Create a separate, continuously running process/worker that:<ul> <li>Periodically queries the <code>OutboxStorage</code> for events with <code>PENDING</code> status (<code>get_pending_events</code>).</li> <li>For each pending event, attempts to publish it to the actual message broker using the configured <code>ProducerProtocol</code> (<code>get_producer()</code>).</li> <li>If publishing succeeds, updates the event status to <code>PUBLISHED</code> in the Outbox (<code>mark_event_published</code>).</li> <li>If publishing fails (after producer retries), increments the attempt counter and records the error in the Outbox (<code>mark_event_failed</code>). If the attempt limit is reached, marks the event as permanently <code>FAILED</code> (potentially moving to a DLQ or just marking).</li> <li>(Optional) Implements logic to delete <code>PUBLISHED</code> or permanently <code>FAILED</code> events after a certain period (<code>delete_processed_event</code>).</li> </ul> </li> </ol>"},{"location":"adr/ADR-024-transactional-outbox-parttern/#3-consequences","title":"3. Consequences","text":"<ul> <li>Positive:<ul> <li>Increased Reliability: Significantly enhances the guarantee that an event associated with a DB transaction will eventually be published, even if temporary failures occur in the broker or the application after the commit. The outbox write is atomic with the main operation.</li> <li>Eventual Consistency: Ensures the external state (reflected by messages) eventually becomes consistent with the internal state (database).</li> <li>Temporal Decoupling: The main business logic no longer needs to wait for confirmation from the message broker, potentially making the primary operation faster.</li> <li>Resilience to Broker Downtime: Temporary broker outages do not prevent the main business operation from completing and being recorded for later publication.</li> </ul> </li> <li>Negative:<ul> <li>Increased Complexity: Introduces new components (outbox table, outbox repository, dedicated publisher) and increases the complexity of the publishing flow.</li> <li>Delivery Latency: Events are not published immediately but only after being picked up and processed by the <code>OutboxPublisher</code>. There's inherent latency introduced by the publisher's polling interval.</li> <li>Transaction Management: Requires explicit and careful management of database transactions in the services utilizing the Outbox pattern.</li> <li>Publisher as Potential Bottleneck: The <code>OutboxPublisher</code> becomes a critical component. Its failure or slowness can delay the delivery of all events. Requires monitoring and potentially high availability.</li> <li>Data Duplication (Temporary): The event payload is temporarily stored in the database.</li> <li>Event Ordering: Guarantees the order of saving to the outbox, but the order of publication depends on the publisher logic and the broker (does not guarantee strict FIFO across different events unless using partitioning keys and single consumers per partition).</li> </ul> </li> <li>Neutral/Other:<ul> <li>Requires a mechanism to run the <code>OutboxPublisher</code> (separate process, scheduled task, worker).</li> <li>Needs a strategy for handling permanently failed events (DLQ, manual alerting, cleanup).</li> </ul> </li> </ul>"},{"location":"adr/ADR-025-mongodb-replica-set-config/","title":"ADR-025: MongoDB ReplicaSet Configuration for Local Host-Based Development and Testing","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-05-12</li> </ul>"},{"location":"adr/ADR-025-mongodb-replica-set-config/#context","title":"Context","text":"<p>For the development and testing of features requiring MongoDB transactions (such as the Outbox Pattern), an environment with a MongoDB ReplicaSet is necessary. The goal is to allow integration tests to be executed directly on the developer's host machine, connecting to a multi-node ReplicaSet running in Docker containers.</p> <p>The primary challenge involves name resolution: 1.  MongoDB containers within the ReplicaSet need to communicate with each other using their Docker service names (e.g., <code>mongo_rs1</code>, <code>mongo_rs2</code>). 2.  The MongoDB driver, running in the test application on the host, after connecting to an initial replica set member (e.g., via <code>localhost:&lt;mapped_port&gt;</code>), receives the ReplicaSet topology which includes the Docker service names of the members. 3.  By default, the developer's host machine cannot resolve these Docker-internal service names, leading to connection failures (e.g., <code>ServerSelectionTimeoutError</code>, \"nodename nor servname provided, or not known\").</p>"},{"location":"adr/ADR-025-mongodb-replica-set-config/#decision","title":"Decision","text":"<p>To enable local integration testing from the host machine against a multi-node MongoDB ReplicaSet running in Docker, the following approach will be adopted:</p> <ol> <li> <p>Docker Compose Configuration:</p> <ul> <li>A <code>docker-compose.yml</code> file (located at <code>infra/database/document/mongo/docker-compose.yml</code>) will define three MongoDB services (<code>mongo1</code>, <code>mongo2</code>, <code>mongo3</code>).</li> <li>Each <code>mongod</code> service within its container will listen on its configured internal port (e.g., <code>mongo1</code> on <code>27017</code>, <code>mongo2</code> on <code>27018</code>, <code>mongo3</code> on <code>27019</code>).</li> <li>These internal container ports will be mapped 1:1 to ports on the host (e.g., host <code>localhost:27017</code> -&gt; <code>mongo1:27017</code>; host <code>localhost:27018</code> -&gt; <code>mongo2:27018</code>, etc.).</li> <li>Authentication and the use of a <code>keyFile</code> for ReplicaSet security will be enabled.</li> </ul> </li> <li> <p>ReplicaSet Initialization:</p> <ul> <li>The <code>rs.initiate()</code> command (executed by <code>mongo1</code>'s healthcheck or an initialization script like <code>devtools/mongo/init-replica-set.sh</code>) will configure the ReplicaSet members using their internal Docker service names and their respective internal container ports (e.g., <code>{_id:0, host:'mongo_rs1:27017'}, {_id:1, host:'mongo_rs2:27018'}, ...}</code>). This is crucial for correct inter-node communication within the ReplicaSet.</li> </ul> </li> <li> <p>Host File Modification on Developer's Machine:</p> <ul> <li>For the test application (running on the host) to resolve the ReplicaSet member names (<code>mongo_rs1</code>, <code>mongo_rs2</code>, <code>mongo_rs3</code>) as advertised by MongoDB, it will be necessary to add entries to the developer's machine <code>hosts</code> file (<code>/etc/hosts</code> on macOS/Linux, <code>C:\\Windows\\System32\\drivers\\etc\\hosts</code> on Windows).</li> <li>These entries will map the Docker service names to the loopback address (<code>127.0.0.1</code>):     <code>127.0.0.1 mongo_rs1     127.0.0.1 mongo_rs2     127.0.0.1 mongo_rs3</code></li> <li>A helper script (<code>devtools/mongo/manage_mongo_hosts.sh</code>) will be provided to assist in adding and removing these entries (requiring <code>sudo</code> privileges).</li> </ul> </li> <li> <p>Connection String in Tests:</p> <ul> <li>The MongoDB connection string in tests (executed on the host) should use the service names and their host-mapped ports:     <code>mongodb://nala_user:nala_password@mongo_rs1:27017,mongo_rs2:27018,mongo_rs3:27019/?replicaSet=rs0</code> <code># pragma: allowlist secret</code></li> </ul> </li> </ol>"},{"location":"adr/ADR-025-mongodb-replica-set-config/#alternatives-considered","title":"Alternatives Considered","text":"<ol> <li> <p>Using <code>host.docker.internal</code>:</p> <ul> <li>Pros: Docker Desktop standard for containers to access the host. Theoretically, if used in <code>rs.initiate</code> members AND in the host's connection string, it could work without manual <code>/etc/hosts</code> editing if the host resolves it.</li> <li>Cons: The user reported that <code>ping host.docker.internal</code> failed on their macOS host, indicating that host-side resolution of this name was not working as expected, making this approach unviable for the specific user environment.</li> <li>Status: Rejected due to resolution failure in the user's host environment.</li> </ul> </li> <li> <p>Single-Node Replica Set for Host-Based Tests:</p> <ul> <li>Pros: Much simpler network setup (connect via <code>localhost:&lt;port&gt;</code>), supports transactions, does not require <code>/etc/hosts</code> changes, and does not depend on <code>host.docker.internal</code> host-side resolution.</li> <li>Cons: Does not fully emulate a multi-node production environment; cannot test failover or multi-node specific behaviors.</li> <li>Status: Considered a strong alternative and fallback, especially if <code>/etc/hosts</code> modification is highly undesirable or problematic for some developers. This was the previous recommendation when <code>host.docker.internal</code> proved problematic.</li> </ul> </li> <li> <p>Running Tests Inside a Docker Container:</p> <ul> <li>Pros: Resolves all host-vs-container name resolution issues, as tests would run on the same Docker network as the MongoDB instances. Connection strings would use Docker service names directly.</li> <li>Cons: Alters the local development workflow; can complicate debugging and IDE integration for some developers.</li> <li>Status: Rejected for now, to maintain the simplicity of running tests directly on the host.</li> </ul> </li> <li> <p><code>network_mode: \"host\"</code> for MongoDB Containers:</p> <ul> <li>Pros: MongoDB containers would use the host's network stack, making them directly accessible via <code>localhost:&lt;port&gt;</code>.</li> <li>Cons: Loss of network isolation, potential for port conflicts, inconsistent behavior across platforms (Linux vs. Docker Desktop for Mac/Windows).</li> <li>Status: Rejected due to isolation and portability drawbacks.</li> </ul> </li> </ol>"},{"location":"adr/ADR-025-mongodb-replica-set-config/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-025-mongodb-replica-set-config/#positive","title":"Positive:","text":"<ul> <li>Enables robust local integration testing against a full multi-node MongoDB ReplicaSet from the developer's host machine.</li> <li>The connection string used in tests can utilize the canonical service names, which is clean and mirrors how services might refer to each other in a fully containerized deployment.</li> <li>The internal ReplicaSet configuration (for inter-node communication) remains correct using Docker service names.</li> <li>The approach is made explicit, and the need for host name resolution is addressed with a helper script.</li> </ul>"},{"location":"adr/ADR-025-mongodb-replica-set-config/#negative","title":"Negative:","text":"<ul> <li>Requires intervention on the developer's system: Modifying the <code>hosts</code> file is an additional setup step that must be performed (either manually or via a script requiring <code>sudo</code>).</li> <li>Potential for setup oversight: Developers might forget to add/remove the <code>hosts</code> file entries, leading to connection failures or, less commonly, conflicts.</li> <li>Dependency on <code>sudo</code>: The helper script to manage the <code>hosts</code> file requires administrator privileges.</li> <li>Possible conflicts: Although unlikely for these specific names, if <code>mongo_rs1</code>, etc., are used for other purposes in a developer's <code>hosts</code> file, conflicts could arise. The use of markers in the <code>manage_mongo_hosts.sh</code> script helps mitigate accidental removal of unrelated entries.</li> </ul>"},{"location":"adr/ADR-025-mongodb-replica-set-config/#neutralmitigation","title":"Neutral/Mitigation:","text":"<ul> <li>The complexity of <code>hosts</code> file management is mitigated by providing the <code>manage_mongo_hosts.sh</code> script.</li> <li>Clear documentation (this ADR and <code>infra/README.md</code>) is essential to guide developers through the setup.</li> <li>The single-node replica set remains a viable, simpler alternative for testing transactional features if full multi-node emulation from the host is not strictly necessary or if <code>hosts</code> file modification is to be avoided.</li> </ul>"},{"location":"adr/ADR-026-agnostic-outbox-architecture/","title":"ADR-026: Agnostic Outbox Architecture","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-17</li> <li>Supersedes: ADR-024</li> <li>Replaces: Outbox limited to messaging systems (Kafka, RabbitMQ)</li> </ul>"},{"location":"adr/ADR-026-agnostic-outbox-architecture/#1-context","title":"1. Context","text":"<p>The original ADR-024 defined a transactional Outbox pattern focused primarily on messaging systems like Kafka and RabbitMQ. However, as our platform evolves to support diverse asynchronous integrations (e.g., webhooks, internal async tasks), we need to generalize the Outbox into an agnostic dispatcher capable of supporting multiple delivery mechanisms beyond messaging.</p> <p>This decision emerged from the limitations and growing complexity when trying to route events to destinations that are not message brokers but still require delivery guarantees.</p>"},{"location":"adr/ADR-026-agnostic-outbox-architecture/#2-decision","title":"2. Decision","text":"<p>We adopted a generalized Outbox Architecture with the following characteristics:</p>"},{"location":"adr/ADR-026-agnostic-outbox-architecture/#generalized-event-model","title":"\u2705 Generalized Event Model","text":"<p>We extended the <code>OutboxEvent</code> model to include:</p> <ul> <li><code>event_type</code>: Defines the type of destination integration (<code>MESSAGING</code>, <code>WEBHOOK</code>, <code>TASK</code>, etc.).</li> <li><code>destination</code>: Target topic, URL, task name, etc.</li> <li>Backward-compatible fields like <code>destination_topic</code> remain supported.</li> </ul>"},{"location":"adr/ADR-026-agnostic-outbox-architecture/#outbox-router-executors","title":"\u2705 Outbox Router &amp; Executors","text":"<p>We introduced a pluggable routing mechanism:</p> <ul> <li><code>OutboxEventRouter</code>: Dynamically resolves the correct <code>OutboxExecutor</code> based on the <code>event_type</code>.</li> <li>Executors implement a common interface: <code>OutboxExecutorProtocol</code>.</li> </ul> <p>The current available executors include:</p> <ul> <li>\u2705 <code>MessagingExecutor</code> \u2192 Integrates with Kafka/RabbitMQ using existing protocols.</li> <li>[ ] <code>WebhookExecutor</code> \u2192 Sends events to HTTP endpoints. (Planned)</li> <li>[ ] <code>TaskExecutor</code> \u2192 Triggers internal async logic or workflows. (Planned)</li> </ul>"},{"location":"adr/ADR-026-agnostic-outbox-architecture/#refactor-of-publisher-logic","title":"\u2705 Refactor of Publisher Logic","text":"<p>The core publisher logic (<code>StandardOutboxPublisher</code>) is now:</p> <ul> <li>Protocol-based (agnostic to event destination).</li> <li>Uses <code>OutboxEventRouter</code> to resolve the correct executor.</li> <li>Maintains retry, failure marking, and delivery tracking.</li> </ul>"},{"location":"adr/ADR-026-agnostic-outbox-architecture/#decorator-support","title":"\u2705 Decorator Support","text":"<p>The <code>@publish_on_success</code> decorator now supports agnostic usage:</p> <pre><code>@publish_on_success(\n    event_name=\"user.created\",\n    event_type=OutboxEventType.WEBHOOK,\n    destination=\"https://hooks.myservice.com/events\"\n)\nasync def create_user(...):\n    ...\n</code></pre>"},{"location":"adr/ADR-026-agnostic-outbox-architecture/#backward-compatibility","title":"\u2705 Backward Compatibility","text":"<p>All legacy usages with Kafka/RabbitMQ remain supported with no change required.</p>"},{"location":"adr/ADR-026-agnostic-outbox-architecture/#3-consequences","title":"3. Consequences","text":""},{"location":"adr/ADR-026-agnostic-outbox-architecture/#positive","title":"\u2705 Positive","text":"<ul> <li>Supports more asynchronous integration patterns beyond messaging.</li> <li>Clean separation of publisher from the transport layer (via executor interface).</li> <li>Reusable executor logic and scalable extension points.</li> <li>Improved observability and testability.</li> </ul>"},{"location":"adr/ADR-026-agnostic-outbox-architecture/#negative","title":"\u26a0\ufe0f Negative","text":"<ul> <li>Increased complexity in routing and executor management.</li> <li>Executors need to be carefully designed to be idempotent and retry-safe.</li> </ul>"},{"location":"adr/ADR-026-agnostic-outbox-architecture/#4-related-tasks","title":"4. Related Tasks","text":"<ul> <li>\u2705 <code>OutboxEventRouter</code> implementation.</li> <li>\u2705 <code>MessagingExecutor</code> refactor.</li> <li>\u2705 <code>StandardOutboxPublisher</code> refactor.</li> <li>\u2705 Decorator enhancements.</li> <li>\u2705 Routing tests, publisher tests, integration coverage.</li> <li>[ ] Implement <code>WebhookExecutor</code>.</li> <li>[ ] Implement <code>TaskExecutor</code>.</li> <li>[ ] Update ADR-024 to reference this ADR as successor.</li> </ul>"},{"location":"adr/ADR-026-agnostic-outbox-architecture/#5-status","title":"5. Status","text":"<p>This ADR supersedes ADR-024, which remains as a historical reference for the messaging-only pattern.</p> <p>The architecture defined here is now the standard for any asynchronous reliable event dispatch in the platform.</p>"},{"location":"adr/ADR-027-messaging-dql-serializer/","title":"ADR-027: Messaging Module Architecture with Kafka, Serializer, and Retry (DLQ)","text":"<p>Status: Implemented Date: 2025-05-20 Author: Valter Hugo Guandaline  </p>"},{"location":"adr/ADR-027-messaging-dql-serializer/#context","title":"Context","text":"<p>The <code>athomic-docs</code> project required an extensible and decoupled messaging architecture with support for: - multiple backends (e.g., Kafka, RabbitMQ, GCP Pub/Sub), - customizable serialization for messages (value, key, and headers), - retry policies with support for DLQ (Dead Letter Queue), - both unit and integration tests validating the full publish-consume lifecycle.</p> <p>We decided to start the implementation with Kafka, leveraging its robust ecosystem and support via the <code>aiokafka</code> library.</p>"},{"location":"adr/ADR-027-messaging-dql-serializer/#decision","title":"Decision","text":"<p>We designed a messaging module based on the following pillars:</p>"},{"location":"adr/ADR-027-messaging-dql-serializer/#architecture","title":"\ud83e\uddf1 Architecture","text":"<ul> <li>Protocols:</li> <li><code>ProducerProtocol</code>, <code>ConsumerProtocol</code>, <code>MessageSerializerProtocol</code>, <code>RetryPolicyProtocol</code>.</li> <li>Factories:</li> <li><code>ProducerFactory</code>, <code>ConsumerFactory</code>, <code>MessageSerializerFactory</code>, <code>DLQHandlerFactory</code>.</li> <li>Registries:</li> <li>Allow dynamic registration of providers depending on the backend.</li> <li>Kafka Providers:</li> <li><code>KafkaProducer</code> and <code>KafkaConsumer</code> integrated with <code>aiokafka</code>.</li> </ul>"},{"location":"adr/ADR-027-messaging-dql-serializer/#serialization","title":"\ud83d\udd01 Serialization","text":"<ul> <li>Introduced <code>BaseMessageSerializer</code> with backend-specific overrides like <code>KafkaMessageSerializer</code>.</li> <li>The serializer is used by both producer and consumer, ensuring consistency across message transport.</li> <li>Full support for <code>value</code>, <code>key</code>, and <code>headers</code>.</li> </ul>"},{"location":"adr/ADR-027-messaging-dql-serializer/#testing","title":"\ud83e\uddea Testing","text":"<ul> <li>Unit tests for <code>DLQHandler</code>, <code>MaxAttemptsPolicy</code>, <code>MessageSerializer</code>.</li> <li>Integration test:</li> <li>Publishes a Kafka message using <code>KafkaProducer</code>,</li> <li>Consumes it via <code>KafkaConsumer</code> instantiated from <code>ConsumerFactory</code>,</li> <li>Asserts the round-trip serialization matches expectations.</li> </ul>"},{"location":"adr/ADR-027-messaging-dql-serializer/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-027-messaging-dql-serializer/#positive","title":"\u2705 Positive","text":"<ul> <li>Decoupled and extensible system for supporting additional backends.</li> <li>Safe and testable serialization logic.</li> <li>Fully supported DLQ and retry mechanism.</li> <li>Reliable end-to-end test coverage.</li> </ul>"},{"location":"adr/ADR-027-messaging-dql-serializer/#challenges","title":"\u26a0\ufe0f Challenges","text":"<ul> <li>The serializer behavior must be carefully tested when using pre-serialized messages (avoid double <code>json.dumps()</code>).</li> <li>It's important to align <code>consumer_settings.topics</code> with the <code>topic</code> used in integration tests.</li> <li>The consumer factory must correctly apply the <code>value_deserializer</code>.</li> </ul>"},{"location":"adr/ADR-027-messaging-dql-serializer/#next-steps","title":"Next Steps","text":"<ul> <li>Add native support for RabbitMQ using <code>aio-pika</code>.</li> <li>Implement message-level observability using OpenTelemetry.</li> <li>Enable fallback to <code>BaseMessageSerializer</code> for non-production environments.</li> <li>Optionally explore schema validation (e.g., Avro contracts).</li> </ul>"},{"location":"adr/ADR-027-messaging-dql-serializer/#attachments","title":"Attachments","text":"<ul> <li>Tests: <code>test_messaging_kafka_consumer.py</code>, <code>test_dlq.py</code></li> <li>Code: <code>KafkaProducer</code>, <code>KafkaConsumer</code>, <code>BaseMessageSerializer</code>, <code>MessagingFactory</code>, <code>DLQHandler</code></li> </ul>"},{"location":"adr/ADR-028-messaging-schema-registry/","title":"ADR-028: Messaging Schema Registry Integration","text":"<p>Date: 2025-06-11 Status: Accepted</p>"},{"location":"adr/ADR-028-messaging-schema-registry/#context","title":"Context","text":"<p>The Nala Athomic messaging module supports various data serialization formats, including binary formats like Protocol Buffers (Protobuf). A primary challenge with binary serialization is \"schema drift,\" where producers and consumers evolve their schemas independently. This can lead to runtime <code>DeserializationError</code> exceptions when a consumer receives a message payload it can no longer parse, compromising data integrity and system reliability.</p> <p>To mitigate this, we need a mechanism to validate and manage schemas centrally, ensuring that producers and consumers always operate with compatible schema versions. A Schema Registry is the standard solution for this problem.</p> <p>This ADR details the architecture for integrating a Schema Registry client into the Nala Athomic serialization layer, making it a configurable, extensible, and type-safe feature.</p>"},{"location":"adr/ADR-028-messaging-schema-registry/#decision","title":"Decision","text":"<p>We will implement a flexible, multi-layered, configuration-driven architecture to integrate schema registry functionality. This design is centered around dependency injection and clear separation of concerns, leveraging the existing Factory/Registry patterns within the Athomic framework.</p> <p>The architecture consists of the following key components:</p>"},{"location":"adr/ADR-028-messaging-schema-registry/#1-centralized-serializer-configuration","title":"1. Centralized Serializer Configuration","text":"<p>All settings for a specific, named serializer instance will be defined in a dedicated Pydantic model, <code>MessagingSerializerSettings</code>. This provides a single source of truth for its behavior.</p> <ul> <li>Path: <code>nala.athomic.config.schemas.serializer.messaging_serializer_config.py</code></li> <li>Key Fields:<ul> <li><code>backend: str</code>: The name of the concrete serializer class to be used (e.g., <code>\"protobuf\"</code>), which must be registered in the <code>MessageSerializerRegistry</code>.</li> <li><code>format: Literal[\"json\", \"protobuf\", \"avro\", ...]</code>: The fundamental data format family. This allows base classes to provide safe, format-specific default logic (e.g., the <code>BaseMessageSerializer</code> provides JSON logic only if <code>format</code> is <code>\"json\"</code>).</li> <li><code>handler: Optional[str]</code>: The name of a specialized schema handler to use (e.g., <code>\"confluent_protobuf\"</code>). If <code>None</code>, no handler is used.</li> <li><code>schema_validation_enabled: bool</code>: A flag to enable or disable schema registry integration for this serializer.</li> <li><code>schema_registry_url: Optional[SecretStr]</code>: The URL of the schema registry service.</li> <li>...and other schema-related settings (<code>schema_auto_register</code>, <code>schema_compatibility_level</code>, etc.).</li> </ul> </li> </ul> <p>These named configurations will be defined in a dictionary within the main <code>MessagingSettings</code> object, allowing for multiple, distinct serializer configurations in the application settings.</p> <p>Example <code>settings.toml</code>:</p> <pre><code>[NALA.INTEGRATION.MESSAGING]\n# ...\n  [NALA.INTEGRATION.MESSAGING.SERIALIZER]\n    # Define a named serializer configuration\n    [NALA.INTEGRATION.MESSAGING.SERIALIZER.PROTOBUF_WITH_REGISTRY]\n      backend = \"protobuf\"\n      format = \"protobuf\"\n      handler = \"confluent_protobuf\"\n      schema_validation_enabled = true\n      schema_registry_url = \"http://localhost:8081\"\n      schema_auto_register = true\n</code></pre>"},{"location":"adr/ADR-028-messaging-schema-registry/#2-abstraction-layer-for-handlers","title":"2. Abstraction Layer for Handlers","text":"<p>To keep the serializers decoupled from any specific schema registry library (e.g., <code>confluent-kafka</code>), we introduce a handler abstraction layer.</p> <ul> <li><code>SchemaHandlerProtocol</code>: An interface defining the public contract (<code>serialize</code>, <code>deserialize</code>) that any schema handler must implement.</li> <li><code>BaseSchemaHandler</code>: An abstract base class that provides common, reusable logic, such as a high-performance caching mechanism for underlying library objects (serializers/deserializers).</li> <li><code>ConfluentProtobufHandler</code>: The concrete implementation that inherits from <code>BaseSchemaHandler</code>, implements the <code>SchemaHandlerProtocol</code>, and contains all logic specific to the <code>confluent-kafka</code> library, including the use of <code>SerializationContext</code>.</li> </ul>"},{"location":"adr/ADR-028-messaging-schema-registry/#3-factories-and-registries","title":"3. Factories and Registries","text":"<p>The system relies on factories to read the configuration and construct objects with their dependencies injected.</p> <ul> <li><code>MessageSerializerFactory</code>: The primary factory for creating serializers. It reads the <code>serializer</code> configuration from <code>MessagingSettings</code>, looks up the appropriate <code>serializer_class</code> from the <code>MessageSerializerRegistry</code> based on the <code>backend</code> field, and injects the complete <code>MessagingSettings</code> object into the serializer's constructor.</li> <li><code>SchemaHandlerFactory</code>: A specialized factory responsible for creating schema handlers. It is called by serializers (like <code>ProtobufMessageSerializer</code>) that need a handler. It reads the <code>handler</code> field from the <code>MessagingSerializerSettings</code> to determine which concrete handler to build, instantiates the necessary client (e.g., <code>confluent_kafka.schema_registry.SchemaRegistryClient</code>), and injects it into the handler's constructor.</li> <li>Registries (<code>MessageSerializerRegistry</code>, <code>SchemaHandlerRegistry</code>): These singleton registries map string names to the corresponding Python classes, allowing the factories to be fully decoupled from concrete implementations.</li> </ul>"},{"location":"adr/ADR-028-messaging-schema-registry/#4-concrete-serializer-implementation","title":"4. Concrete Serializer Implementation","text":"<p>Concrete serializers will inherit from <code>BaseMessageSerializer</code> and override methods as needed.</p> <ul> <li><code>ProtobufMessageSerializer.__init__</code>:<ol> <li>Calls <code>super().__init__(...)</code> to set up the basic configuration.</li> <li>Checks <code>if self.settings.schema_validation_enabled</code>.</li> <li>If true, it calls <code>SchemaHandlerFactory.create(self.settings)</code> to obtain and store a fully configured handler instance in <code>self.handler</code>.</li> </ol> </li> <li><code>ProtobufMessageSerializer.serialize_value</code> / <code>deserialize_value</code>:<ol> <li>These methods now contain simple routing logic:</li> <li><code>if self.handler:</code> delegate the call to <code>self.handler.serialize(...)</code> or <code>self.handler.deserialize(...)</code>.</li> <li><code>else:</code> perform standard, no-registry Protobuf serialization/deserialization.</li> </ol> </li> </ul> <p>This design ensures that the <code>ProtobufMessageSerializer</code> itself remains clean and agnostic to the details of the schema registry interaction.</p>"},{"location":"adr/ADR-028-messaging-schema-registry/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-028-messaging-schema-registry/#positive","title":"Positive","text":"<ul> <li>Highly Decoupled Architecture: The <code>ProtobufMessageSerializer</code> depends on the <code>SchemaHandlerProtocol</code> abstraction, not on any concrete library. This makes the system more modular and easier to maintain.</li> <li>Extensible (Open/Closed Principle): Adding support for a new schema registry provider (e.g., Redpanda, AWS Glue) is straightforward:<ol> <li>Create a new handler class that implements the <code>SchemaHandlerProtocol</code>.</li> <li>Register the new handler with a unique name.</li> <li>Update the <code>SchemaHandlerFactory</code> with the logic to instantiate the new handler's client. No changes are needed in the serializers or other core components.</li> </ol> </li> <li>Configuration-Driven: The entire behavior is controlled via clear, explicit, and type-safe configuration (<code>MessagingSerializerSettings</code>), preventing hardcoded values.</li> <li>Architectural Consistency: This design follows the established <code>Protocol -&gt; Registry -&gt; Factory -&gt; Provider</code> pattern used throughout the Nala Athomic framework, making it predictable for developers.</li> <li>Improved Testability: Components can be tested in isolation by mocking their dependencies at the protocol level (e.g., mocking <code>SchemaHandlerProtocol</code> to test the <code>ProtobufMessageSerializer</code>).</li> </ul>"},{"location":"adr/ADR-028-messaging-schema-registry/#negative","title":"Negative","text":"<ul> <li>Increased Number of Classes: This architecture introduces more classes and files (protocols, base classes, factories, registries, handlers) compared to a monolithic approach. This adds a slight learning curve for new developers, which is a standard trade-off for a more robust and maintainable design.</li> </ul>"},{"location":"adr/ADR-029-agnostic-delayed-messaging-strategy/","title":"ADR-029: Agnostic Strategy for Delayed/Scheduled Messaging","text":"<p>Status: Accepted Date: 2025-06-13</p>"},{"location":"adr/ADR-029-agnostic-delayed-messaging-strategy/#context","title":"Context","text":"<p>The application requires the ability to publish messages that are to be consumed only after a specified delay. Key use cases include: - Sending notifications to users after a specific period (e.g., 24 hours). - Implementing scheduled retries with exponential backoff for failed operations. - Scheduling future jobs via the messaging system.</p> <p>A primary architectural challenge is that different message brokers handle delayed messages differently. Apache Kafka, our primary backend, has no native support for message delays, as it operates as an immutable log. In contrast, brokers like RabbitMQ offer native delayed-delivery mechanisms via plugins.</p> <p>A solution is needed that provides a consistent API for application developers (<code>producer.publish(..., delay_seconds=...)</code>) while remaining agnostic to the underlying broker's implementation, thus preventing implementation details from leaking into the business logic layer.</p>"},{"location":"adr/ADR-029-agnostic-delayed-messaging-strategy/#decision","title":"Decision","text":"<p>We will implement a Strategy Pattern to handle delayed message publishing. This decouples the application's intent (delaying a message) from the broker-specific implementation (how the delay is achieved).</p> <p>The key components of this decision are:</p> <ol> <li> <p><code>DelayedPublishingStrategyProtocol</code>: A new protocol will define the interface for any delay strategy. It will include a <code>publish_delayed(...)</code> method and a class attribute <code>requires_republisher: bool</code> to indicate if the strategy needs a background worker.</p> </li> <li> <p><code>KafkaDelayTopicStrategy</code>: The first implementation of the strategy will be for Kafka. It will:</p> <ul> <li>Reroute messages with a <code>delay_seconds</code> parameter to dedicated, time-bucketed \"delay topics\" (e.g., <code>__nala_delay_5m</code>, <code>__nala_delay_1h</code>).</li> <li>Wrap the original message in an \"envelope\" containing metadata like the final destination topic and the target processing timestamp.</li> <li>Set <code>requires_republisher = True</code>.</li> </ul> </li> <li> <p><code>DelayedMessageRePublisher</code> Service: A new, dedicated background service will be created. Its sole responsibility is to consume messages from the delay topics, wait (via <code>asyncio.sleep</code>) until the target timestamp is reached, and then re-publish the original message to its final destination topic.</p> </li> <li> <p><code>InfrastructureLifecycleManager</code>: The <code>DelayedMessageRePublisher</code> will be managed by a separate lifecycle manager. This separates the concerns of managing application-level business consumers from managing internal infrastructure services.</p> </li> <li> <p>Factories and Registries: A <code>DelayStrategyFactory</code> will be responsible for instantiating the correct strategy based on the application's settings. A <code>Registry</code> pattern will be used to map configuration strings (e.g., <code>\"kafka_topic_delay_strategy\"</code>) to their corresponding strategy classes, making the system extensible.</p> </li> <li> <p>Configuration: The strategy and its parameters will be configurable:</p> <ul> <li><code>settings.integration.messaging.delay_strategy_backend</code> will select the active strategy.</li> <li>Provider-specific settings (like Kafka's <code>delay_topics</code> map and <code>republisher</code> consumer settings) will be nested within their respective configuration schemas (e.g., <code>settings.integration.messaging.provider.</code>).</li> </ul> </li> </ol>"},{"location":"adr/ADR-029-agnostic-delayed-messaging-strategy/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-029-agnostic-delayed-messaging-strategy/#positive","title":"Positive:","text":"<ul> <li>Provider Agnostic: The application's business logic is completely decoupled from the underlying messaging infrastructure. Switching to a broker with native delay support would not require any changes to the application code.</li> <li>Extensible: Adding support for a new broker (e.g., RabbitMQ) is straightforward: create a new strategy class, register it, and no other core components need to be modified.</li> <li>Clean Architecture: Adheres to the Single Responsibility Principle (SRP). The logic for delaying messages is encapsulated within the strategies, and the lifecycle of different types of services (application vs. infrastructure) is managed separately.</li> <li>Highly Testable: The use of protocols, factories, and dependency injection makes all new components easy to unit test in isolation.</li> </ul>"},{"location":"adr/ADR-029-agnostic-delayed-messaging-strategy/#negative","title":"Negative:","text":"<ul> <li>Increased Complexity for Kafka: The workaround for Kafka introduces several new components (Delay Topics, <code>RePublisher</code> Service, <code>InfrastructureLifecycleManager</code>), increasing the overall complexity of the messaging system.</li> <li>Operational Overhead: The new delay topics must be created and managed in the Kafka cluster. The <code>DelayedMessageRePublisher</code> service must be deployed, monitored for health, and scaled if necessary.</li> <li>Delay Inaccuracy (Kafka Strategy): The delay achieved by the Kafka strategy is not millisecond-precise. It depends on consumer poll intervals and <code>asyncio.sleep</code> scheduling, making it suitable for business-level delays but not for real-time, high-precision timing.</li> </ul>"},{"location":"adr/ADR-029-agnostic-delayed-messaging-strategy/#considered-options","title":"Considered Options","text":""},{"location":"adr/ADR-029-agnostic-delayed-messaging-strategy/#1-consumer-side-delay-anti-pattern","title":"1. Consumer-Side Delay (Anti-Pattern)","text":"<ul> <li>Description: The final consumer receives the message, inspects a timestamp header, and calls <code>sleep()</code> if the message is premature.</li> <li>Rejected Because: This is highly inefficient. It blocks the consumer thread/task, destroys message throughput, and makes error handling, retries, and consumer scaling extremely difficult.</li> </ul>"},{"location":"adr/ADR-029-agnostic-delayed-messaging-strategy/#2-hardcoded-kafka-specific-logic","title":"2. Hardcoded Kafka-Specific Logic","text":"<ul> <li>Description: Implement the delay-topic logic directly inside the <code>KafkaProducer</code> without the Strategy pattern.</li> <li>Rejected Because: This would create a \"leaky abstraction,\" tightly coupling the application to Kafka's specific workaround and making it very difficult to support other brokers in the future.</li> </ul>"},{"location":"adr/ADR-029-agnostic-delayed-messaging-strategy/#3-use-a-dedicated-task-queue-eg-celery","title":"3. Use a Dedicated Task Queue (e.g., Celery)","text":"<ul> <li>Description: Introduce a separate system like Celery, which is purpose-built for scheduled and delayed tasks.</li> <li>Rejected Because: While powerful, this would add an entirely new, major piece of infrastructure (Celery workers, a new broker like Redis or RabbitMQ for Celery) to the stack. The decision was made to leverage the existing messaging abstraction to contain operational complexity, as the required delay precision was not high.</li> </ul>"},{"location":"adr/ADR-029-idempotent-consumers/","title":"ADR-027: Implementation of Idempotent Consumers","text":"<p>Status: Accepted</p> <p>Date: 2025-06-11</p>"},{"location":"adr/ADR-029-idempotent-consumers/#context-and-problem","title":"Context and Problem","text":"<p>In messaging systems with \"at-least-once\" delivery guarantees, such as Kafka, the same event may be delivered multiple times to a consumer. This can happen due to network failures, consumer rebalancing, or retries.</p> <p>Duplicate processing of the same message can cause undesirable side effects, such as creating duplicate records in the database, sending multiple notifications for the same event, or general state inconsistencies. The framework needs to provide a mechanism to transparently mitigate this risk.</p>"},{"location":"adr/ADR-029-idempotent-consumers/#decision","title":"Decision","text":"<p>We decided to implement an idempotency mechanism directly in the base consumer layer (<code>BaseConsumer</code>), making it available to all types of consumers (Kafka, RabbitMQ, etc.) in a standardized way.</p> <p>The chosen approach consists of:</p> <ol> <li> <p>Idempotency Key: Use a unique <code>message_id</code> that must be provided by the producer in the body of each message. The responsibility for generating this ID (e.g., a UUID) lies with the system originating the event.</p> </li> <li> <p>State Storage: Leverage the <code>athomic.kvstore</code> module (currently with a Redis backend) to store the <code>message_id</code>s of already processed messages. Redis is ideal for this task due to its high performance and support for atomic operations.</p> </li> <li> <p>Atomic Operation: Use Redis's <code>SETNX</code> (set if not exists) operation. For each new message, the consumer tries to insert the <code>message_id</code> into the KV store with a TTL (Time-To-Live).</p> <ul> <li>If the insertion succeeds (the key did not exist), the message is considered new and is processed.</li> <li>If the insertion fails (the key already existed), the message is considered a duplicate, is discarded, and its receipt is acknowledged (ack) without executing business logic.</li> </ul> </li> <li> <p>Configurability: The functionality will be controlled by settings in <code>MessagingSettings</code>, allowing it to be enabled/disabled globally and the idempotency key TTL to be adjusted:</p> <ul> <li><code>IDEMPOTENCY_CHECK_ENABLED</code> (bool)</li> <li><code>IDEMPOTENCY_KEY_TTL_SECONDS</code> (int)</li> </ul> </li> </ol>"},{"location":"adr/ADR-029-idempotent-consumers/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-029-idempotent-consumers/#positive","title":"Positive","text":"<ul> <li>Increased Resilience: The system becomes more robust and predictable, avoiding side effects from duplicate messages.</li> <li>Transparency for Developers: The idempotency logic is abstracted by the framework. Developers implementing a consumer do not need to worry about this control, as long as the <code>message_id</code> contract is followed.</li> <li>Reusability: The solution is implemented in <code>BaseConsumer</code>, and is automatically inherited by any future consumer implementation.</li> </ul>"},{"location":"adr/ADR-029-idempotent-consumers/#negative-or-to-consider","title":"Negative or To Consider","text":"<ul> <li>Dependency on KV Store: Message processing now has a direct dependency on the availability and performance of the KV store (Redis). A failure in Redis can impact message consumption.</li> <li>Latency Overhead: Each message incurs an extra network call to Redis for idempotency verification, adding a small latency to processing.</li> <li>Contract with Producers: Requires discipline from message producers, who are now required to include a unique and idempotent <code>message_id</code> in their payloads.</li> </ul>"},{"location":"adr/ADR-03-base-service/","title":"ADR-030: Adopt a BaseService Class for Stateful Components","text":"<p>Status: Accepted Date: 2025-06-26</p>"},{"location":"adr/ADR-03-base-service/#context","title":"Context","text":"<p>During the development of <code>nala.athomic</code>, it was observed that multiple components managing an external connection or internal state (e.g., <code>Producer</code>, <code>Consumer</code>, <code>CacheProvider</code>) needed to implement repetitive and complex logic for:</p> <ol> <li>Lifecycle Management: Orchestrating <code>connect()</code> and <code>close()</code> operations.</li> <li>Concurrency Control: Using <code>asyncio.Lock</code> to prevent <code>connect()</code> and <code>close()</code> from being called concurrently.</li> <li>State Management: Maintaining flags like <code>_connected</code>, <code>_is_closed</code> to track the service's current state.</li> <li>Observability: Emitting standardized Prometheus metrics and structured logs for connection attempts, failures, and status.</li> <li>Readiness Signaling: Informing the rest of the application when the service is fully initialized and ready to handle traffic.</li> </ol> <p>The absence of a centralized pattern was leading to code duplication, implementation inconsistencies, and increased complexity in maintenance and testing.</p>"},{"location":"adr/ADR-03-base-service/#decision","title":"Decision","text":"<p>We have decided to introduce a Base Service pattern for all stateful components within <code>nala.athomic</code>. This pattern consists of two main parts:</p> <ol> <li> <p><code>nala.athomic.base.ServiceProtocol</code>: An interface contract that defines the public signature of a manageable service (<code>connect</code>, <code>close</code>, <code>is_ready</code>, <code>is_enabled</code>, <code>health</code>). Code that consumes a service should depend on this protocol, not on a concrete implementation.</p> </li> <li> <p><code>nala.athomic.base.BaseService</code>: An abstract base class that implements the <code>ServiceProtocol</code> and provides all the common logic for orchestrating lifecycle, state, concurrency, and observability.</p> </li> </ol> <p>Subclasses (like <code>KafkaConsumer</code>) now inherit from <code>BaseService</code> and only need to implement the abstract <code>_connect()</code> and <code>_close()</code> methods with technology-specific logic, inheriting all the remaining robust behavior.</p>"},{"location":"adr/ADR-03-base-service/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-03-base-service/#positive","title":"Positive","text":"<ul> <li>Consistency and Standardization: All services will have an identical and predictable lifecycle and state behavior.</li> <li>Reduced Boilerplate: Eliminates the need to rewrite complex logic for locks, <code>try/except/finally</code> blocks, state flags, and metrics in every new component.</li> <li>Observability by Default: Any new service inheriting from <code>BaseService</code> automatically gains connection and readiness metrics, as well as structured logs.</li> <li>Improved Developer Experience (DX): Makes the creation of new, robust components significantly faster and less error-prone.</li> <li>Enhanced Reliability: The lifecycle logic, being centralized, can be rigorously tested in a single place.</li> </ul>"},{"location":"adr/ADR-03-base-service/#negative","title":"Negative","text":"<ul> <li>Coupling to the Base Class: Subclasses now have a direct coupling to <code>BaseService</code>. Although this is mitigated by using the <code>ServiceProtocol</code> in the service's consumers, the implementation itself is tied to inheritance.</li> <li>Learning Curve: New developers must first learn the <code>BaseService</code> pattern before they can contribute new stateful components, rather than creating them ad-hoc.</li> </ul>"},{"location":"adr/ADR-031-kvstore-refactoring/","title":"ADR-031: Refactoring the KVStore Module to Use <code>BaseService</code>","text":"<p>Status: Accepted Date: 2025-06-27</p>"},{"location":"adr/ADR-031-kvstore-refactoring/#context","title":"Context","text":"<p>The Key-Value Store module (<code>athomic/database/kvstore</code>) previously contained multiple provider implementations (e.g., <code>RedisKVClient</code>, <code>LocalMemoryKVClient</code>) that managed their own connection lifecycles and state independently.</p> <p>This resulted in several issues:</p> <ol> <li>Duplicated Logic: Each provider implemented its own connection, reconnection, and concurrency control logic (e.g., <code>_connection_lock</code>).</li> <li>Data Inconsistency: The way data was stored varied between providers. <code>RedisKVClient</code> serialized objects to JSON, while <code>LocalMemoryKVClient</code> stored Python objects directly, leading to different behaviors between production and test environments.</li> <li>Scattered Observability: Metrics and tracing (if present) had to be implemented in each provider, leading to inconsistencies.</li> <li>Implicit Lifecycle: \"Lazy connection\" made connection failures unpredictable, as they could occur during any operation (<code>get</code>, <code>set</code>, etc.) rather than explicitly at application startup.</li> </ol> <p>This approach violated the DRY (Don't Repeat Yourself) principle and made maintenance and the addition of new providers more complex and error-prone.</p>"},{"location":"adr/ADR-031-kvstore-refactoring/#decision","title":"Decision","text":"<p>Based on the pattern established in ADR-030 (Adoption of a <code>BaseService</code> Class), we decided to completely refactor the <code>kvstore</code> module to adopt a clear inheritance architecture with well-defined responsibilities.</p> <p>The new hierarchical structure is: <code>BaseService</code> -&gt; <code>BaseKVStoreProvider</code> -&gt; <code>ConcreteImplementation</code> (e.g., RedisKVClient)</p> <p>The main decisions implemented are:</p> <ol> <li> <p>Creation of <code>BaseKVStoreProvider</code>: A new abstract base class was created in <code>athomic/database/kvstore/providers/base.py</code>. It inherits from <code>BaseService</code> and is responsible for:</p> <ul> <li>Orchestrating the Lifecycle: Provides safe public methods like <code>get</code>, <code>set</code>, <code>delete</code>, etc.</li> <li>Centralizing Observability: Emits Prometheus metrics (<code>kvstore_operations_total</code>, <code>kvstore_operation_duration_seconds</code>) and creates tracing spans for each operation, ensuring uniform monitoring for any provider.</li> <li>Standardizing Serialization: Receives a <code>MessageSerializerProtocol</code> in its constructor, ensuring all data is serialized before being passed to the concrete provider and deserialized when read.</li> <li>Defining an Internal Contract: Defines internal abstract methods (<code>_get</code>, <code>_set</code>, etc.) that subclasses must implement.</li> </ul> </li> <li> <p>Simplification of Concrete Providers: Classes like <code>RedisKVClient</code> and <code>LocalMemoryKVClient</code> were refactored to inherit from <code>BaseKVStoreProvider</code>. Their responsibilities were reduced to:</p> <ul> <li>Implementing backend-specific connection and closing logic in the <code>_connect()</code> and <code>_close()</code> hooks.</li> <li>Implementing internal operation methods (<code>_get</code>, <code>_set</code>, etc.) dealing only with <code>bytes</code>, without worrying about serialization, metrics, or tracing.</li> </ul> </li> <li> <p>Abandonment of Lazy Connection: The KV clients' lifecycle is now explicit. The connection must be established via <code>await kv_client.connect()</code> during application initialization, making the system more predictable.</p> </li> </ol>"},{"location":"adr/ADR-031-kvstore-refactoring/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-031-kvstore-refactoring/#positive","title":"Positive","text":"<ul> <li>Architectural Consistency: The <code>kvstore</code> module now follows the same robust design pattern as other stateful components, such as those in <code>messaging</code>.</li> <li>Elimination of Duplicated Code: All observability, serialization, concurrency control, and state management logic has been moved to a single place.</li> <li>Robustness and Predictability: Explicit initialization and centralized state handling eliminate race conditions and make system behavior in case of connection failure much clearer.</li> <li>Simplified Extensibility: Adding a new provider (e.g., <code>MemcachedKVClient</code>) is now trivial, requiring only the implementation of internal methods with backend logic.</li> <li>Improved Testability: The clear separation of responsibilities makes it easier to create focused unit tests by mocking different layers of the abstraction.</li> </ul>"},{"location":"adr/ADR-031-kvstore-refactoring/#negative","title":"Negative","text":"<ul> <li>Slight Performance Overhead: The serialization layer and the extra call between the public and internal methods introduce a minimal theoretical latency. However, this cost is considered negligible compared to network latency and the immense gains in robustness and maintainability.</li> <li>Adherence to the Pattern: Creating new KV providers is now (correctly) coupled to the <code>BaseKVStoreProvider</code> pattern, requiring developers to follow the defined architecture.</li> </ul>"},{"location":"adr/ADR-031-promote-serializer-to-core/","title":"ADR-031: Promote Serializer to a Core Framework Component","text":"<p>Status: Accepted Date: 2025-06-27</p>"},{"location":"adr/ADR-031-promote-serializer-to-core/#context","title":"Context","text":"<p>The initial implementation of the <code>serializer</code> module was located within <code>nala.athomic.integration.messaging.serializer</code>. This was logical when its only consumer was the messaging system for handling message payloads.</p> <p>However, as the framework evolved, other components, specifically the <code>BaseKVStoreProvider</code> (as per ADR-031), required a standardized way to serialize and deserialize data to ensure consistency across different backends (e.g., Redis and in-memory).</p> <p>This created a problematic semantic dependency, where the <code>database</code> module would need to import from the <code>messaging</code> module (<code>from nala.athomic.integration.messaging.serializer import ...</code>). This coupling is misleading, as the database functionality has no true dependency on the messaging system. It violates the principles of high cohesion and low coupling, making the architecture harder to understand and maintain.</p>"},{"location":"adr/ADR-031-promote-serializer-to-core/#decision","title":"Decision","text":"<p>We have decided to refactor the <code>serializer</code> module, promoting it from a specific tool within the messaging system to a first-class, core component of the <code>athomic</code> framework.</p> <p>The entire module, including its protocol, factory, registry, and providers, will be moved from <code>nala.athomic.integration.messaging.serializer</code> to a new, top-level package: <code>nala.athomic.serializers</code>.</p> <p>The new dependency graph will be as follows: - <code>athomic.integration.messaging</code> -&gt; <code>athomic.serializers</code> - <code>athomic.database.kvstore</code> -&gt; <code>athomic.serializers</code></p> <p>This change makes it clear that serialization is a cross-cutting concern available to any module within the framework that requires it.</p>"},{"location":"adr/ADR-031-promote-serializer-to-core/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-031-promote-serializer-to-core/#positive","title":"Positive","text":"<ul> <li>Improved Decoupling and Cohesion: The <code>messaging</code> module is no longer responsible for a shared, framework-level capability. The <code>serializer</code> module now has a single, clear responsibility.</li> <li>Architectural Clarity: The location <code>athomic/serializers</code> explicitly communicates the module's role as a foundational utility. This makes the overall architecture easier to understand for current and future developers.</li> <li>Breaks Misleading Dependencies: Eliminates the confusing import path where the <code>database</code> module appeared to depend on the <code>messaging</code> module.</li> <li>Enhanced Scalability and Maintainability: Provides a clean, logical, and centralized location to add new serialization formats (e.g., Protobuf, Avro) in the future without affecting unrelated modules.</li> </ul>"},{"location":"adr/ADR-031-promote-serializer-to-core/#negative","title":"Negative","text":"<ul> <li>One-Time Refactoring Effort: Requires a project-wide search-and-replace operation to update all import paths that previously pointed to the old location. This is a minor, one-time cost.</li> </ul>"},{"location":"adr/ADR-032-looking-as-first-class-resilience-module/","title":"ADR-032: Locking as a First-Class Resilience Module","text":"<p>Status: Accepted Date: 2025-06-29</p>"},{"location":"adr/ADR-032-looking-as-first-class-resilience-module/#context","title":"Context","text":"<p>The initial implementation of distributed locking was a utility function (<code>acquire_lock</code>) located within the <code>performance.cache</code> module. This implementation was tightly coupled to a specific Redis client factory and had an implicit fallback to an in-memory <code>asyncio.Lock</code>.</p> <p>This design had several drawbacks: 1.  Tight Coupling: Locking, a core resilience pattern, was hidden inside the caching module. 2.  Lack of Extensibility: It was difficult to introduce new locking backends without modifying the core function. 3.  Implicit Fallback: The automatic fallback to an in-memory lock could mask configuration issues and behave incorrectly in a multi-instance production environment.</p>"},{"location":"adr/ADR-032-looking-as-first-class-resilience-module/#decision","title":"Decision","text":"<p>We have decided to elevate \"Locking\" to a first-class resilience pattern within the <code>athomic</code> framework. A new, dedicated module, <code>nala.athomic.resilience.locking</code>, has been created.</p> <p>This module follows the standard framework pattern: 1.  <code>LockingProtocol</code>: Defines a clear interface for any lock provider. 2.  Providers (<code>RedisLockProvider</code>, <code>InMemoryLockProvider</code>): Concrete implementations for different backends. The <code>RedisLockProvider</code> now depends on a <code>KVStoreProtocol</code> instance, making its dependency explicit. 3.  <code>LockingRegistry</code>: A class-based registry to map backend names to provider classes. 4.  <code>LockingFactory</code>: A factory responsible for creating the appropriate lock provider based on the application's global configuration, making the choice between <code>redis</code> and <code>in_memory</code> explicit.</p> <p>The old <code>acquire_lock</code> function is now deprecated and replaced by calls to <code>LockingFactory.create()</code>.</p>"},{"location":"adr/ADR-032-looking-as-first-class-resilience-module/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-032-looking-as-first-class-resilience-module/#positive","title":"Positive","text":"<ul> <li>Decoupling: Locking is now a standalone, reusable feature, available to any part of the application, not just the cache.</li> <li>Consistency: The module follows the established architectural pattern of the framework, making it predictable and easy to understand.</li> <li>Explicit Configuration: The choice of locking backend is now explicitly determined by the application's configuration, eliminating \"magic\" fallbacks.</li> <li>Improved Testability: The <code>RedisLockProvider</code> and <code>InMemoryLockProvider</code> can be tested in complete isolation.</li> </ul>"},{"location":"adr/ADR-032-looking-as-first-class-resilience-module/#negative","title":"Negative","text":"<ul> <li>A minor increase in the number of files, which is justified by the significant improvement in architectural clarity.</li> </ul>"},{"location":"adr/ADR-033-standardized-contextual-key-generation/","title":"ADR-033: Standardized Contextual Key Generation","text":"<p>Status: Accepted Date: 2025-06-29</p>"},{"location":"adr/ADR-033-standardized-contextual-key-generation/#context","title":"Context","text":"<p>Key generation was fragmented and inconsistent across the codebase. - The caching module used a utility function (<code>hash_key</code>) that attempted to handle multi-tenancy. - The <code>KeyResolvingKVClient</code> wrapper used a separate <code>KeyResolver</code> class for a similar purpose. - This led to code duplication and potential bugs, such as the <code>CacheKeyGenerator</code> initially missing the multi-tenancy logic. There was no single source of truth for how a key should be formatted based on context (tenant, user), namespace, and prefixes.</p>"},{"location":"adr/ADR-033-standardized-contextual-key-generation/#decision","title":"Decision","text":"<p>We have decided to unify all key generation logic into a single, powerful, and versatile class: <code>nala.athomic.context.ContextKeyGenerator</code>.</p> <p>This class is now the sole authority on creating keys. Its responsibilities include: 1.  Reading default behaviors (e.g., <code>use_tenant</code>, <code>static_prefix</code>) from the global application settings. 2.  Allowing these defaults to be overridden during instantiation for specific use cases. 3.  Dynamically fetching the current <code>ExecutionContext</code> (<code>tenant_id</code>, <code>user_id</code>) when a key is generated. 4.  Providing two main methods:     - <code>generate(*parts)</code>: For creating simple, namespaced keys (used by <code>KeyResolvingKVClient</code>).     - <code>generate_for_function(func, args, kwargs)</code>: For creating deterministic, hashed keys for function caching (used by the <code>@cache_result</code> decorator).</p> <p>The old <code>KeyResolver</code> class and <code>hash_key</code> utility function have been deprecated and removed in favor of this new, centralized service.</p>"},{"location":"adr/ADR-033-standardized-contextual-key-generation/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-033-standardized-contextual-key-generation/#positive","title":"Positive","text":"<ul> <li>Single Source of Truth: All key generation logic is now in one place, eliminating inconsistencies and making the system easier to reason about.</li> <li>Consistency (DRY): Both the KVStore wrappers and the caching decorators now use the exact same underlying mechanism, ensuring keys are always formatted identically.</li> <li>Bug Prevention: Critical logic, such as including the <code>tenant_id</code> in keys when multi-tenancy is enabled, is now handled centrally and reliably.</li> <li>Improved Cohesion: The <code>context</code> module is now properly responsible for all context-aware logic, including key generation.</li> </ul>"},{"location":"adr/ADR-033-standardized-contextual-key-generation/#negative","title":"Negative","text":"<ul> <li>None. This change represents a pure architectural improvement, simplifying the code and increasing robustness.</li> </ul>"},{"location":"adr/ADR-034-background-worker-base/","title":"ADR-034: Dedicated Abstraction for Background Services (Workers)","text":"<p>Date: 2025-06-30</p> <p>Status: Accepted</p>"},{"location":"adr/ADR-034-background-worker-base/#context","title":"Context","text":"<p>During the refactoring of <code>OutboxPublisherBase</code>, we identified a design issue in the <code>athomic</code> architecture. The <code>BaseService</code> class was designed to manage the lifecycle of \"connectable\" resources (like database or cache clients), with a public API of <code>connect()</code> and <code>close()</code>.</p> <p>We were attempting to force services that run a continuous background process (such as <code>OutboxPublisher</code> or <code>DelayedMessageRePublisher</code>) to inherit from <code>BaseService</code>. This resulted in a semantically confusing API, where <code>connect()</code> meant \"start a loop\" and <code>close()</code> meant \"stop a loop\". Furthermore, the \"connection\" metrics (<code>service_connection_status</code>, etc.) did not accurately describe the state of a background service, which should be \"running\" or \"stopped\".</p> <p>The complexity and lack of clarity indicated the need for a more suitable abstraction for this type of component.</p>"},{"location":"adr/ADR-034-background-worker-base/#decision","title":"Decision","text":"<p>We have decided to introduce a new, first-class abstraction dedicated to services that execute background tasks, which we will call Workers.</p> <ol> <li> <p>New Protocol (<code>RunnableServiceProtocol</code>):</p> <ul> <li>A new <code>RunnableServiceProtocol</code> interface will be created.</li> <li>This interface will define a clear and semantically correct public API for workers, with the following methods:<ul> <li><code>start()</code>: Starts the background process.</li> <li><code>stop()</code>: Gracefully stops the background process.</li> <li><code>is_running() -&gt; bool</code>: Checks if the worker's task is active.</li> <li><code>is_ready() -&gt; bool</code>: Checks if the worker is ready to process work (it might be running but still initializing).</li> <li><code>health() -&gt; dict</code>: Returns a dictionary with the detailed health status of the worker.</li> </ul> </li> </ul> </li> <li> <p>New Base Class (<code>RunnableServiceBase</code>):</p> <ul> <li>A new base class <code>RunnableServiceBase</code> will be created that implements the <code>RunnableServiceProtocol</code>.</li> <li>This class will not inherit from <code>BaseService</code> to maintain a clear separation of concepts.</li> <li>It will contain its own robust logic to manage the <code>asyncio.Task</code> lifecycle of the worker, including locks to ensure <code>start()</code> and <code>stop()</code> are idempotent, and an <code>asyncio.Event</code> to manage the <code>readiness</code> state.</li> </ul> </li> <li> <p>Dedicated Metrics:</p> <ul> <li>A new set of Prometheus metrics will be created, prefixed with <code>runnable_service_*</code> and using the label <code>worker_name</code>.</li> <li>The metrics will include:<ul> <li><code>runnable_service_running_status</code>: A <code>Gauge</code> with 3 states: <code>-1</code> (not started), <code>0</code> (stopped), <code>1</code> (running).</li> <li><code>runnable_service_readiness_status</code>: A <code>Gauge</code> for the readiness state (0 or 1).</li> <li><code>runnable_service_starts_total</code>: A <code>Counter</code> for <code>start</code> attempts.</li> <li><code>runnable_service_start_failures_total</code>: A <code>Counter</code> for <code>start</code> failures.</li> </ul> </li> <li>This avoids polluting the <code>service_*</code> metrics (which remain for connectable components) and makes observability more precise.</li> </ul> </li> <li> <p>Refactoring of <code>OutboxPublisherBase</code>:</p> <ul> <li>The <code>OutboxPublisherBase</code> will be refactored to inherit from the new <code>RunnableServiceBase</code>.</li> <li>Its manual management logic for <code>_run_task</code> and <code>_stop_event</code> will be completely removed.</li> <li>The class will now only need to implement the <code>_run_loop()</code> method, containing the specific business logic of the publisher.</li> </ul> </li> </ol>"},{"location":"adr/ADR-034-background-worker-base/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-034-background-worker-base/#positive","title":"Positive","text":"<ul> <li>Semantic Clarity: The architecture now clearly distinguishes between \"connectable\" services (<code>BaseService</code>) and \"runnable\" services (<code>RunnableServiceBase</code>), with public APIs (<code>connect/close</code> vs. <code>start/stop</code>) that match their function.</li> <li>Developer Simplicity: Developing new background workers (like <code>DelayedMessageRePublisher</code>) becomes much simpler. One only needs to inherit from <code>RunnableServiceBase</code> and implement the <code>_run_loop</code> logic.</li> <li>Precise Observability: The new dedicated metrics provide a much clearer and more actionable view of the state of background workers, without ambiguity with connection metrics.</li> <li>Maintainability: Centralizing the background task management logic into a robust base class reduces code duplication and the likelihood of bugs.</li> </ul>"},{"location":"adr/ADR-034-background-worker-base/#negative","title":"Negative","text":"<ul> <li>Increased Framework Complexity: The introduction of a new base class (<code>RunnableServiceBase</code>) and a new protocol slightly increases the conceptual surface area of the <code>athomic</code> framework. However, the gain in clarity and maintainability justifies this addition.</li> </ul>"},{"location":"adr/ADR-035-security-error-handle-policy/","title":"ADR-035: Standardize Error Handling in Secret Providers to Always Raise Exceptions","text":"<p>Status: Accepted</p> <p>Date: 2025-07-03</p>"},{"location":"adr/ADR-035-security-error-handle-policy/#context","title":"Context","text":"<p>The <code>nala.athomic.security.secrets</code> module uses a provider pattern (<code>SecretsBase</code>) to abstract how secrets are obtained (Vault, environment variables, files).</p> <p>It was identified that the error handling contract was inconsistent. The <code>get_secret</code> method could: 1.  Return <code>None</code> in case of connection failures or server errors (HTTP 5xx). 2.  Raise <code>SecretNotFoundError</code> if a resource was not found (HTTP 404).</p> <p>This dual error signaling (returning <code>None</code> and raising exceptions) makes consumer code more complex, requiring both a return value check and wrapping the call in a <code>try...except</code> block. This increases the chance of bugs, especially if the <code>None</code> case is not handled correctly.</p>"},{"location":"adr/ADR-035-security-error-handle-policy/#decision","title":"Decision","text":"<p>To create a more robust, explicit, and consistent API contract, we decided that all data-fetching methods in providers (<code>get_secret</code>, etc.) must adhere to the following error handling rule:</p> <ul> <li>On success, the method must return the requested value (e.g., <code>str</code>). The return value must never be <code>None</code> to indicate a failure.</li> <li>On any failure, the method MUST raise a specific exception that inherits from a common base class (e.g., <code>SecretProviderError</code>). This includes, but is not limited to:<ul> <li>Resource not found (<code>SecretNotFoundError</code>).</li> <li>Connection or network failure (<code>SecretConnectionError</code>).</li> <li>Permission or authentication error (<code>SecretAuthenticationError</code>).</li> <li>Unexpected provider errors.</li> </ul> </li> </ul> <p>Returning <code>None</code> as a way to signal an operational error is prohibited.</p>"},{"location":"adr/ADR-035-security-error-handle-policy/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-035-security-error-handle-policy/#positive","title":"Positive","text":"<ul> <li>Consistent API: All secret providers will have the same behavior in case of failure, simplifying their usage.</li> <li>Explicit Failures (Fail-Fast): Errors no longer silently pass as a <code>None</code> value. An unhandled exception interrupts the flow, making problems immediately visible during development and testing.</li> <li>Simplified Consumer Code: Code using the provider only needs a <code>try...except</code> block, without additional checks like <code>if secret is None:</code>.</li> <li>Improved Debugging: Exceptions carry more context about the nature of the failure (e.g., the original exception such as <code>httpx.ConnectError</code> as <code>__cause__</code>) than a simple <code>None</code>.</li> </ul>"},{"location":"adr/ADR-035-security-error-handle-policy/#negative","title":"Negative","text":"<ul> <li>More Verbose Consumer Code: Code consuming the providers is now required to use <code>try...except</code> blocks for any call, which may be seen as more verbose. However, for a critical component like secret management, this verbosity is justified by the security and robustness it provides.</li> </ul>"},{"location":"adr/ADR-036-standardized-integration-pattern/","title":"ADR-036: Standardized Integration Pattern for External Stateful Services via a Managed Client","text":"<p>Status: Accepted</p> <p>Date: 2025-07-03</p>"},{"location":"adr/ADR-036-standardized-integration-pattern/#context","title":"Context","text":"<p>The application needs to integrate with various external stateful services, starting with HashiCorp Consul. These services require managing a connection lifecycle (connect, health check, disconnect) and sharing a single, efficient connection pool across the application to avoid resource exhaustion.</p> <p>Without a standardized pattern, different modules might create their own client instances, leading to: -   Multiple unmanaged connection pools. -   Inconsistent configuration handling. -   Difficulty in managing the application's startup and shutdown sequences gracefully. -   Poor testability, as mocking direct client instantiations is complex and error-prone.</p>"},{"location":"adr/ADR-036-standardized-integration-pattern/#decision","title":"Decision","text":"<p>We have decided to establish a standardized, three-part pattern for integrating external stateful services, using Consul as the first implementation:</p> <ol> <li> <p>Managed Client Wrapper (<code>ConsulClient</code>): A dedicated class that encapsulates the third-party client (e.g., <code>consul.aio.Consul</code>). This class must inherit from our <code>BaseService</code> to integrate its lifecycle (<code>_connect</code>, <code>_close</code>) with the application's startup and shutdown events. It is responsible for all direct interaction logic with the external service.</p> </li> <li> <p>Singleton Factory (<code>ConsulClientFactory</code>): A singleton factory will be the sole entry point for accessing the managed client instance. Its <code>create()</code> method ensures that only one instance of the <code>ConsulClient</code> is created throughout the application's lifetime, enforcing a single connection pool. This factory reads its configuration directly from the global <code>get_settings()</code> function.</p> </li> <li> <p>Consumer Interaction: Any part of the application that needs to interact with the service must obtain the client instance via the factory (<code>ConsulClientFactory.create()</code>). It should then use <code>await instance.wait_ready()</code> before making calls to ensure the connection has been successfully established by the <code>BaseService</code> lifecycle manager.</p> </li> </ol>"},{"location":"adr/ADR-036-standardized-integration-pattern/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-036-standardized-integration-pattern/#pros","title":"Pros","text":"<ul> <li>Architectural Consistency: This pattern can be replicated for any future external service integration (e.g., StatsD, gRPC clients), making the codebase predictable and easier to navigate.</li> <li>Guaranteed Singleton: The factory pattern prevents resource leaks and performance issues by ensuring only one connection pool is used per service.</li> <li>Graceful Lifecycle Management: By inheriting from <code>BaseService</code>, all external connections are automatically established on application startup and gracefully closed on shutdown, improving reliability.</li> <li>High Testability: This pattern is extremely test-friendly.<ul> <li>Unit Tests: Can easily mock the factory (<code>@patch('...ConsulClientFactory.create')</code>) to inject a mock client into consumer services.</li> <li>Integration Tests: Can instantiate the client directly with explicit test configurations, bypassing the global settings and ensuring test isolation.</li> </ul> </li> <li>Centralized Configuration: All configuration logic is centralized within the factory, which reads from the global settings module. Consumers of the client do not need to be aware of configuration details.</li> </ul>"},{"location":"adr/ADR-036-standardized-integration-pattern/#cons","title":"Cons","text":"<ul> <li>Increased Boilerplate: This pattern is more verbose than direct instantiation. It requires creating two new classes (Wrapper and Factory) for each new service integration.</li> <li>Layer of Indirection: Developers must know to use the factory instead of directly instantiating the client library. This requires proper documentation and adherence to the established pattern.</li> </ul> <p>This trade-off is considered highly favorable, as the initial boilerplate provides significant long-term benefits in robustness, consistency, and maintainability.</p>"},{"location":"adr/ADR-037-standardized-architecture-for-the-feature-flag-module/","title":"ADR-037: Standardized Architecture for the Feature Flag Module","text":"<p>Status: Accepted</p> <p>Date: 2025-07-04</p>"},{"location":"adr/ADR-037-standardized-architecture-for-the-feature-flag-module/#context","title":"Context","text":"<p>The application requires a feature flag system to enable or disable features dynamically without deploying new code. This system needs to be flexible, supporting different backends (local configuration for development, Redis or Consul for production), while remaining performant, observable, and easy to use across the codebase.</p> <p>An initial implementation could lead to scattered logic, inconsistent error handling, and poor testability. A clear architectural pattern is needed to ensure the feature flag module is robust, extensible, and consistent with the rest of the <code>athomic-docs</code> architecture.</p>"},{"location":"adr/ADR-037-standardized-architecture-for-the-feature-flag-module/#decision","title":"Decision","text":"<p>We have decided to implement a comprehensive, multi-layered architecture for the feature flag module, based on the following key principles:</p> <ol> <li> <p>Abstract Protocol (<code>FeatureFlagProtocol</code>): A clear, abstract interface defines the core capabilities (<code>check</code>, <code>get_variant_details</code>). This ensures that any consumer of the feature flag system interacts with a consistent API, regardless of the underlying backend.</p> </li> <li> <p>Abstract Base Class (<code>FeatureFlagBase</code>): A shared base class was created to handle all common, cross-cutting concerns for the providers. This class is responsible for:</p> <ul> <li>Lifecycle Management: It inherits from <code>BaseService</code> to hook into the application's startup and shutdown lifecycle.</li> <li>Metrics Collection: It automatically records Prometheus metrics (<code>feature_flag_evaluations_total</code>, <code>feature_flag_evaluation_duration</code>) for every flag evaluation.</li> <li>Robust Error Handling: It wraps all data-fetching calls in a <code>try...except</code> block, ensuring that any exception from a provider (e.g., a network error) is caught, logged, and results in a safe default value being returned to the consumer.</li> </ul> </li> <li> <p>Concrete Providers (<code>FFLocalProvider</code>, <code>FFConsulProvider</code>, <code>FFRedisProvider</code>): Each backend is implemented as a specific class that inherits from <code>FeatureFlagBase</code>. The sole responsibility of these classes is to implement the <code>_fetch_*</code> methods, containing only the logic required to retrieve data from their specific source (a dictionary, Consul KV, or Redis).</p> </li> <li> <p>Singleton Factory (<code>FeatureFlagFactory</code>): A singleton factory is the single entry point for the entire application to get a feature flag provider.</p> <ul> <li>It reads the application's configuration (<code>get_settings()</code>).</li> <li>It uses a <code>_provider_map</code> (acting as a manual registry) to select and instantiate the correct provider based on the configured <code>backend</code>.</li> <li>It guarantees that only one instance of the provider is created, ensuring resource efficiency.</li> </ul> </li> <li> <p>Reusability of Existing Clients: The <code>FFConsulProvider</code> and <code>FFRedisProvider</code> do not create their own connections. They reuse the existing singleton clients provided by the <code>ConsulClientFactory</code> and <code>KVStoreFactory</code>, respectively. This enforces a single connection pool and centralizes connection management.</p> </li> </ol>"},{"location":"adr/ADR-037-standardized-architecture-for-the-feature-flag-module/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-037-standardized-architecture-for-the-feature-flag-module/#pros","title":"Pros","text":"<ul> <li>High Cohesion, Low Coupling: Each component has a single, well-defined responsibility. Providers only know how to fetch data; the base class knows how to handle metrics and errors; the factory knows how to build the provider.</li> <li>Extensibility (Open/Closed Principle): Adding a new backend (e.g., LaunchDarkly) is straightforward. It only requires creating a new provider class that implements the abstract methods and adding it to the <code>_provider_map</code> in the factory. No other part of the system needs to change.</li> <li>Guaranteed Observability: Because metrics are handled in the base class, it is impossible to add a new provider that forgets to be instrumented. All flag evaluations are automatically monitored.</li> <li>Increased Robustness: The centralized error handling in the base class ensures that a failure in a backend service (like Consul or Redis) will not crash the application. It will gracefully degrade by returning a default value.</li> <li>Consistency: The pattern aligns perfectly with other modules like <code>Secrets</code> and <code>ConsulClient</code>, making the entire codebase easier to understand and maintain.</li> </ul>"},{"location":"adr/ADR-037-standardized-architecture-for-the-feature-flag-module/#cons","title":"Cons","text":"<ul> <li>Increased Boilerplate: This pattern is more verbose than direct instantiation. It requires creating two new classes (Wrapper and Factory) for each new service integration.</li> <li>Indirection: Developers need to understand the pattern and know to use the <code>FeatureFlagFactory</code> instead of trying to instantiate the client library. This requires proper documentation and adherence to the established pattern.</li> </ul> <p>This trade-off is deemed highly favorable, as the initial setup effort provides a foundation that is significantly more robust, maintainable, and scalable for the long-term health of the project.</p>"},{"location":"adr/ADR-038-base-registry-refact/","title":"ADR-038: Refactoring BaseRegistry with <code>__init_subclass__</code> for State Isolation","text":"<p>Status: Accepted</p> <p>Date: 2025-07-05</p>"},{"location":"adr/ADR-038-base-registry-refact/#context-and-problem","title":"Context and Problem","text":"<p>The original implementation of <code>BaseRegistry</code> required each subclass (e.g., <code>EventBusRegistry</code>) to initialize its own registry dictionary (<code>_registry</code>). While functional, this pattern posed risks and inconsistencies:</p> <ol> <li>Risk of Shared State: If a subclass forgot to redefine <code>_registry</code>, it could accidentally share the parent class's dictionary, leading to state leakage between different types of registries.</li> <li>Lack of a Formal Contract: There was no mechanism enforcing that subclasses consistently register their default providers. Registering defaults was a convention, not an architectural contract.</li> <li>Code Duplication: Each new registry subclass needed similar initialization logic.</li> </ol> <p>This violated the <code>athomic</code> design principles, which aim for robustness, clarity, and low coupling.</p>"},{"location":"adr/ADR-038-base-registry-refact/#decision","title":"Decision","text":"<p>We decided to refactor <code>BaseRegistry</code> to use Python's <code>__init_subclass__</code> magic method and the Template Method Pattern. The new implementation adopts the following strategies:</p> <ol> <li> <p>Automatic Initialization via <code>__init_subclass__</code>: <code>BaseRegistry</code> now uses the <code>__init_subclass__</code> hook to create an empty, isolated <code>_registry</code> dictionary for each new subclass that inherits from it. This completely eliminates the risk of shared state.</p> </li> <li> <p>Initialization Contract with Abstract Method: A new abstract class method, <code>@abstractmethod def register_defaults(cls)</code>, was created. This forces any subclass of <code>BaseRegistry</code> to implement this method, explicitly defining its default providers.</p> </li> <li> <p>Guaranteed Execution: The base class's <code>__init_subclass__</code> calls <code>cls.register_defaults()</code> automatically, ensuring that defaults are registered at class definition time, consistently and predictably.</p> </li> <li> <p>Type Safety with <code>Generic</code>: <code>BaseRegistry</code> is now generic (<code>BaseRegistry[T]</code>), and <code>__init_subclass__</code> requires the expected protocol to be passed (e.g., <code>class EventBusRegistry(BaseRegistry, protocol=EventBusProtocol)</code>). This allows the <code>register</code> method to validate at runtime whether registered items conform to the protocol, increasing framework safety.</p> </li> </ol>"},{"location":"adr/ADR-038-base-registry-refact/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-038-base-registry-refact/#positive","title":"Positive","text":"<ul> <li>Guaranteed State Isolation: The state of each registry (<code>EventBusRegistry</code>, <code>MessagingRegistry</code>, etc.) is automatically isolated by the base class, making the creation of new registries safer and less error-prone.</li> <li>Declarative Design: Subclasses become much simpler and more declarative. Their only responsibility is to inherit from <code>BaseRegistry</code> and implement the <code>register_defaults</code> method, filling in the \"template\".</li> <li>Architectural Consistency: Establishes a single, robust, and reusable pattern for all modules that require a provider registration mechanism.</li> <li>Fail-Fast: Type checking against the protocol and the requirement of the abstract method ensure that implementation errors are caught during development, not in production.</li> </ul>"},{"location":"adr/ADR-038-base-registry-refact/#negative-or-impact","title":"Negative or Impact","text":"<ul> <li>Breaking Change: All existing registry implementations had to be refactored to fit the new pattern. This is an acceptable cost in exchange for the benefits of robustness and consistency.</li> <li>Learning Curve: Developers contributing to the project will need to be aware of this new pattern to create registries. This ADR serves as the main documentation to mitigate this.</li> </ul>"},{"location":"adr/ADR-039-documents-module-ddd/","title":"ADR-039: Domain Persistence Module Architecture","text":"<p>Status: Accepted</p> <p>Date: 2025-07-09</p>"},{"location":"adr/ADR-039-documents-module-ddd/#context","title":"Context","text":"<p>The application requires a robust, flexible, and testable architectural pattern for persisting business entities, starting with the \"Document\" entity. The initial implementation was tightly coupled to both business logic and the database technology (MongoDB), making unit testing, code evolution, and potential backend replacement difficult.</p> <p>The goal is to establish a pattern that promotes decoupling, following Domain-Driven Design (DDD) and Clean Architecture principles, and can be reused for all domain entities in the system (<code>Users</code>, <code>Orders</code>, etc.).</p>"},{"location":"adr/ADR-039-documents-module-ddd/#decision","title":"Decision","text":"<p>We will adopt a layered architecture for all domain modules that require persistence. This architecture clearly separates responsibilities and inverts dependencies, ensuring that business layers do not depend on infrastructure details.</p> <p>The structure will be divided into:</p> <ol> <li> <p>Domain Layer (<code>domain</code>):</p> <ul> <li>Entities: Pure Pydantic models encapsulating data and intrinsic business rules (e.g., <code>Document.archive()</code>).</li> <li>Repository Protocols: Abstract interfaces (e.g., <code>DocumentRepositoryProtocol</code>) defining the \"contract\" for persistence operations (<code>save</code>, <code>find_by_id</code>). These protocols will be generic (<code>Generic[T]</code>) to remain agnostic to the entity they handle.</li> </ul> </li> <li> <p>Application Layer (<code>application</code>):</p> <ul> <li>Application Services: Orchestrate system use cases (e.g., <code>DocumentService</code>). They contain workflow logic, but not core business logic.</li> <li>Factories: Application services will not instantiate infrastructure dependencies directly. They will use a <code>Factory</code> (e.g., <code>DocumentRepositoryFactory</code>) to obtain a concrete repository implementation, remaining decoupled.</li> </ul> </li> <li> <p>Infrastructure Layer (<code>infrastructure</code>):</p> <ul> <li>Concrete Repositories: Implement the domain layer protocols for a specific technology (e.g., <code>MongoDocumentRepository</code>, <code>LocalMemoryDocumentRepository</code>).</li> <li>Inheritance from <code>BaseService</code>: Implementations managing external connections (MongoDB, PostgreSQL) will inherit from <code>BaseService</code> to standardize lifecycle (<code>connect</code>, <code>close</code>), state (<code>is_ready</code>), and observability (metrics and tracing).</li> <li>Persistence Models: Database-specific models (e.g., Beanie models with <code>class Settings</code>) will reside in this layer, separate from domain entities.</li> <li>Mapping and Registry: A <code>Registry</code> (e.g., <code>DocumentRepositoryRegistry</code>) will register available \"drivers\" (e.g., \"mongodb\", \"local\"). A <code>Mapper</code> (<code>DomainToDbModelMapper</code>) will associate the domain entity with its corresponding persistence model.</li> </ul> </li> </ol>"},{"location":"adr/ADR-039-documents-module-ddd/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-039-documents-module-ddd/#positive","title":"Positive","text":"<ul> <li>High Flexibility: Switching a database backend becomes a matter of configuration and creating a new adapter, with no impact on business logic.</li> <li>Improved Testability:<ul> <li>Domain logic can be tested in complete isolation.</li> <li>Application services can be tested with in-memory repositories, resulting in fast and reliable unit tests.</li> <li>Integration tests can focus exclusively on validating the repository implementation against the real infrastructure.</li> </ul> </li> <li>Consistency and Reusability: The pattern can be consistently replicated for all new business entities, accelerating development and maintaining architectural quality.</li> <li>Standardized Observability: Inheriting from <code>BaseService</code> ensures all repository I/O operations have uniform metrics and tracing by default.</li> </ul>"},{"location":"adr/ADR-039-documents-module-ddd/#negative","title":"Negative","text":"<ul> <li>More Boilerplate: The structure requires more files (Protocol, Factory, Registry, etc.) compared to a simpler Active Record approach, which may seem excessive for very simple domains.</li> <li>Learning Curve: The team will need to understand Dependency Inversion flow and the role of each layer, instead of instantiating classes directly.</li> </ul> <p>Justification: The long-term benefits in maintainability, scalability, and testability significantly outweigh the initial cost of a more formal structure. This architecture is an investment in the longevity and quality of the software.</p>"},{"location":"adr/ADR-040-event-bus-resilience/","title":"ADR-017: Enhance Resilience of Event Bus Background Processing","text":"<p>Date: 2025-07-09</p> <p>Status: Accepted</p>"},{"location":"adr/ADR-040-event-bus-resilience/#context","title":"Context","text":"<p>The event bus system, built upon the <code>EventBusBase</code> and the <code>RunnableServiceBase</code> (as defined in ADR-008), is designed for reliable asynchronous communication. <code>RunnableServiceBase</code> provides a managed background task that executes a <code>_run_loop</code> method, which is intended to run continuously for the service's lifetime.</p> <p>The <code>RedisEventBus</code> implementation relies on this loop to poll for messages from Redis Pub/Sub channels. During integration and unit testing, we identified critical resilience issues that could cause the background processing task to terminate silently, leading to a non-functional event bus without clear error signals.</p>"},{"location":"adr/ADR-040-event-bus-resilience/#problem-statement","title":"Problem Statement","text":"<p>The initial implementation of the event processing loop was not robust enough to handle exceptions occurring during a single message processing cycle. Two primary issues were found:</p> <ol> <li> <p>Unhandled Exceptions in the Processing Loop: Any unexpected exception within the <code>_run_loop</code> of <code>EventBusBase</code> (e.g., a <code>TypeError</code> from attempting to decode a <code>string</code> instead of <code>bytes</code>, or a <code>JSONDecodeError</code> from a malformed message) would break out of the <code>while self.is_running():</code> block, causing the background task to exit permanently. This made the service fail silently.</p> </li> <li> <p>Race Condition on Subscription: The <code>RedisEventBus._process_events</code> method would immediately attempt to call <code>_pubsub_client.get_message()</code>. However, in tests and potentially in production, the background task could start before any <code>subscribe()</code> call was made. Calling <code>get_message()</code> on a <code>PubSub</code> client that is not subscribed to any channel raises a <code>redis.exceptions.RuntimeError</code>, which was another unhandled exception that killed the loop.</p> </li> </ol> <p>These issues resulted in flaky tests (<code>TimeoutError</code> while waiting for events that would never arrive) and a critical lack of resilience in the production code.</p>"},{"location":"adr/ADR-040-event-bus-resilience/#decision","title":"Decision","text":"<p>To address these problems, we decided to implement two key changes to bolster the resilience of the event bus system:</p> <ol> <li> <p>Isolate Failures within the <code>EventBusBase._run_loop</code>:     We will refactor the <code>_run_loop</code> method in <code>EventBusBase</code> to wrap the call to <code>_process_events()</code> and the subsequent <code>asyncio.sleep()</code> within a <code>try...except Exception:</code> block. This block is placed inside the main <code>while self.is_running():</code> loop. This change ensures that if an exception occurs during one processing cycle, it is logged, and the loop continues to the next iteration instead of terminating the entire service.</p> </li> <li> <p>Implement a Subscriber Guard in <code>RedisEventBus</code>:     We will add a guard condition at the beginning of the <code>RedisEventBus._process_events</code> method. The method will now check <code>if not self._subscribers:</code>. If there are no active subscribers, it will return early (with a small <code>asyncio.sleep</code> to prevent tight-looping). This completely prevents the race condition by ensuring <code>get_message()</code> is only called when the client is guaranteed to be subscribed to at least one channel.</p> </li> </ol>"},{"location":"adr/ADR-040-event-bus-resilience/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-040-event-bus-resilience/#positive","title":"Positive","text":"<ul> <li>Increased System Resilience: The event bus service is no longer susceptible to silent failures caused by transient errors or malformed individual messages. The background task will remain active and continue processing subsequent valid messages.</li> <li>Improved Test Reliability: Integration tests are now more stable as the event bus service remains operational, eliminating the <code>TimeoutError</code> issues caused by the background task dying prematurely.</li> <li>Enhanced Efficiency: The subscriber guard in <code>RedisEventBus</code> prevents unnecessary polling of the Redis connection when no events are being listened for, reducing needless network I/O and CPU usage.</li> <li>Clearer Error Logging: Failures within a processing cycle are now explicitly logged as non-fatal errors, providing better visibility for monitoring without halting the entire service.</li> </ul>"},{"location":"adr/ADR-040-event-bus-resilience/#negative","title":"Negative","text":"<ul> <li>Masking of Persistent Errors: While isolating failures is beneficial, it could potentially mask a persistent underlying issue (e.g., a recurring malformed message type) that now only appears as repeated log entries instead of a service crash. This requires diligent monitoring of error logs to detect such patterns.</li> <li>Slightly Increased Complexity: The logic in the base class loop is marginally more complex due to the nested exception handling.</li> </ul>"},{"location":"adr/ADR-041-route-level-dependency-security-policy/","title":"ADR 041 route level dependency security policy","text":""},{"location":"adr/ADR-041-route-level-dependency-security-policy/#adr-041-router-level-dependency-based-security-policy","title":"ADR-041: Router-Level, Dependency-Based Security Policy","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-11</li> </ul>"},{"location":"adr/ADR-041-route-level-dependency-security-policy/#context","title":"Context","text":"<p>The <code>athomic-docs</code> project requires a robust, scalable, and highly auditable security system to manage different access levels for its endpoints. As the number of endpoints grows, a clear and consistent method for applying security policies is needed to prevent accidental exposure of sensitive routes. A key requirement is to enforce a \"private by default\" philosophy, making it difficult for developers to accidentally create an unprotected endpoint.</p> <p>The primary requirements for this system are: 1.  Default to Secure: All new endpoints must be secure by default, requiring a valid authentication token. 2.  Multiple Security Levels: The system must support at least three distinct access policies:     * <code>public</code>: Open access, no authentication required (e.g., for <code>/health</code> checks).     * <code>internal</code>: Access restricted by the request's origin (e.g., IP Allowlist), without a user token. This is for machine-to-machine communication.     * <code>private</code>: Requires a valid user authentication token (e.g., JWT). 3.  Architectural Decoupling: The core security logic must reside in the <code>athomic</code> layer and remain completely agnostic of the web framework (<code>FastAPI</code>). The <code>api</code> layer should act as an adapter. 4.  Clarity and Auditability: The security policy for any given endpoint must be explicit and easy to determine.</p>"},{"location":"adr/ADR-041-route-level-dependency-security-policy/#decision","title":"Decision","text":"<p>We will implement a security architecture based on FastAPI's native Dependency Injection system, applied directly at the <code>APIRouter</code> level. This pattern enforces a default security policy for an entire group of routes based on which router they are defined in.</p> <p>The architecture is composed of two main layers:</p> <p>1. <code>athomic</code> Layer (Framework-Agnostic Core):</p> <ul> <li><code>AuthorizationService</code>: A central service that acts as a pure orchestrator. Its main method, <code>authorize(context, policy_name)</code>, dispatches the security check to the appropriate strategy.</li> <li>AuthExecutors (Strategy Pattern): A set of classes implementing the <code>AuthExecutor</code> protocol. Each class encapsulates the logic for a single security policy (e.g., <code>PrivateTokenExecutor</code>, <code>InternalIPExecutor</code>).</li> <li>ExecutorRegistry : A standardized registry that maps policy names (e.g., <code>\"private\"</code>) to instances of the corresponding <code>AuthExecutor</code> classes.</li> <li>Generic Models &amp; Exceptions: The service layer operates exclusively on framework-agnostic objects like <code>SecurityRequestContext</code>  and raises generic exceptions like <code>AthomicAuthError</code> and <code>AthomicForbiddenError</code>.</li> </ul> <p>2. <code>api</code> Layer (FastAPI-Specific Adapter):</p> <ul> <li>Security Dependencies (<code>api/dependencies/security.py</code>): A set of reusable FastAPI dependency functions (<code>require_token</code>, <code>require_internal_ip</code>).<ul> <li>These functions are the \"glue\" or \"adapter\" layer.</li> <li>They are responsible for:<ol> <li>Creating the generic <code>SecurityRequestContext</code> from the <code>fastapi.Request</code> object.</li> <li>Calling the <code>AuthorizationService</code> with the correct policy name (e.g., <code>\"private\"</code>).</li> <li>Catching <code>AthomicSecurityError</code> exceptions and translating them into standard <code>fastapi.HTTPException</code> responses with the appropriate status codes (401, 403).</li> </ol> </li> </ul> </li> <li> <p>Router-Level Policy Application:</p> <ul> <li>We will organize routes into distinct <code>APIRouter</code> instances based on their required security level (e.g., <code>private_router.py</code>, <code>public_router.py</code>, <code>internal_router.py</code>).</li> <li>The security policy is applied to all routes within a router by adding the corresponding dependency to the router's constructor. This makes the security policy for that group of routes explicit and non-negotiable.</li> </ul> <p>```python</p> </li> </ul> <p>This approach replaces any need for a complex, stateful authentication middleware, favoring FastAPI's explicit and powerful dependency injection system.</p>"},{"location":"adr/ADR-041-route-level-dependency-security-policy/#example-for-a-private-router-that-enforces-token-authentication-on-all-its-routes","title":"Example for a private router that enforces token authentication on all its routes","text":"<p>from nala.api.dependencies.security import require_token</p> <p>private_router = APIRouter(     prefix=\"/private\",     tags=[\"Private\"],     dependencies=[Depends(require_token)] )</p> <p>@private_router.get(\"/users/me\") async def get_user_data(request: Request):     # This endpoint is automatically protected by require_token.     # No need to add Depends() here.     ... ```</p>"},{"location":"adr/ADR-041-route-level-dependency-security-policy/#consequences","title":"Consequences","text":"<p>Positive: * Extremely Clear and Auditable: The security policy for any endpoint is immediately obvious from its file location and the <code>APIRouter</code> definition. There is no \"magic\" or hidden logic. * Secure by Default: Developers working on private features inside a <code>private_router</code> cannot accidentally create an unprotected endpoint. The security is enforced by the router they choose to use. This directly fulfills the \"private by default\" requirement. * DRY (Don't Repeat Yourself): Avoids the need to add <code>Depends(require_token)</code> to every single private route, reducing code duplication and the chance of error. * Leverages Native Framework Features: Uses FastAPI's Dependency Injection as intended, making the solution robust, performant, and likely to be compatible with future versions of the framework. * Maintains Decoupling: The <code>athomic</code> layer remains completely independent and reusable, fulfilling a key architectural requirement.</p> <p>Negative: * Requires Code Organization Discipline: This pattern relies on developers placing routes in the correct router module. This is also a strength, as it enforces a clean and predictable project structure. * Explicitness over Automation: Developers must explicitly choose which router to add a new endpoint to. This is a trade-off against a middleware that might automatically protect routes based on URL patterns. For security, explicitness is generally preferred.</p>"},{"location":"adr/ADR-042-decouplin-core-primitives/","title":"ADR 042 decouplin core primitives","text":""},{"location":"adr/ADR-042-decouplin-core-primitives/#adr-042-decoupling-core-primitives-for-acyclic-dependencies","title":"ADR-042: Decoupling Core Primitives for Acyclic Dependencies","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-11</li> </ul>"},{"location":"adr/ADR-042-decouplin-core-primitives/#context","title":"Context","text":"<p>As the <code>athomic</code> framework matured with the introduction of a sophisticated <code>LifecycleManager</code>, a critical architectural risk was identified: the potential for circular dependencies. The initial design placed core components like <code>BaseRegistry</code> and service skeletons (<code>BaseService</code>, <code>RunnableServiceBase</code>) within the same low-level package, <code>athomic.base</code>.</p> <p>The <code>LifecycleManager</code> and its <code>LifecycleRegistry</code> need to depend on the service protocols (from a <code>services</code> layer) and the registry primitives (from a <code>base</code> layer). However, the service base classes themselves might indirectly need components from the lifecycle manager in the future, creating a fragile, cyclical dependency graph (<code>lifecycle</code> -&gt; <code>services</code> -&gt; <code>base</code> -&gt; <code>lifecycle</code>). This would make the system difficult to maintain, test, and understand.</p> <p>A decision was needed to restructure these core primitives to enforce a strict, unidirectional dependency flow.</p>"},{"location":"adr/ADR-042-decouplin-core-primitives/#decision","title":"Decision","text":"<p>We will refactor the core <code>athomic</code> packages to establish a clear, layered hierarchy and ensure acyclic dependencies. The responsibilities will be segregated as follows:</p> <ol> <li> <p><code>athomic.base</code> (The Foundation):</p> <ul> <li>Responsibility: To provide the most fundamental, dependency-free building blocks of the framework.</li> <li>Contents: <code>BaseRegistry</code>, <code>BaseInstanceRegistry</code>, and other primitive components that have zero internal <code>athomic</code> dependencies.</li> </ul> </li> <li> <p><code>athomic.services</code> (The Service Layer):</p> <ul> <li>Responsibility: To define the \"what\" and \"how\" of a service within the framework. It defines the contracts and base implementations for services.</li> <li>Contents: <code>RunnableServiceProtocol</code> (the lifecycle contract), <code>BaseService</code> (for connection-based services), and <code>RunnableServiceBase</code> (for task-based services).</li> <li>Dependency Rule: This layer can depend on <code>athomic.base</code>, but not vice-versa.</li> </ul> </li> <li> <p><code>athomic.lifecycle</code> (The Lifecycle Management Layer):</p> <ul> <li>Responsibility: To orchestrate the startup and shutdown of all manageable services.</li> <li>Contents: <code>LifecycleRegistry</code> (which registers <code>RunnableServiceProtocol</code> instances) and the <code>LifespanManager</code>.</li> <li>Dependency Rule: This layer can depend on both <code>athomic.services</code> (for the protocol) and <code>athomic.base</code> (for the registry implementation), but neither of them can depend on <code>lifecycle</code>.</li> </ul> </li> </ol> <p>This structure guarantees a clear, one-way flow of dependencies: <code>lifecycle</code> -&gt; <code>services</code> -&gt; <code>base</code>.</p>"},{"location":"adr/ADR-042-decouplin-core-primitives/#consequences","title":"Consequences","text":"<p>Positive: * Circular Dependencies Eliminated: This is the primary benefit. The architecture now fundamentally prevents circular import errors, making the codebase more stable and predictable. * Clear Separation of Concerns: Each package has a distinct and well-defined responsibility, making the framework easier to navigate and understand for new developers. * Improved Modularity and Testability: Each layer (<code>base</code>, <code>services</code>, <code>lifecycle</code>) can be tested in greater isolation. * Enhanced Maintainability: Changes within one layer are less likely to have unintended side effects on others. For example, a change in the <code>LifespanManager</code> cannot break the <code>BaseRegistry</code>.</p> <p>Negative: * Increased Number of Packages: The refactoring introduces more directories and modules. This is a deliberate and accepted trade-off for architectural clarity and robustness. * Stricter Design Discipline: Developers must be mindful of the dependency flow and place new components in the correct layer. This is also a strength, as it enforces a clean architecture.</p>"},{"location":"adr/ADR-043-application-layered-architecture/","title":"ADR 043 application layered architecture","text":""},{"location":"adr/ADR-043-application-layered-architecture/#adr-043-application-layered-architecture-athomic-domain-api","title":"ADR-043: Application Layered Architecture (athomic, domain, api)","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-11</li> </ul>"},{"location":"adr/ADR-043-application-layered-architecture/#context","title":"Context","text":"<p>As the <code>athomic-docs</code> project evolved, a critical architectural challenge emerged: the initial design started to blend specific business logic (e.g., the <code>documents</code> module) with the generic, reusable core framework (<code>athomic</code>). This coupling presented several long-term risks:</p> <ol> <li>Limited Reusability: The <code>athomic</code> layer could not be easily extracted for use in other projects or be open-sourced, as it contained business-specific knowledge.</li> <li>Violation of Separation of Concerns: Changes in business requirements could force changes in the core framework, and vice-versa, leading to a brittle architecture.</li> <li>Decreased Testability: It was becoming difficult to test core framework features in isolation from the business logic, and to test the business logic without the overhead of the entire framework.</li> </ol> <p>A decision was needed to establish a clear, scalable, and maintainable architectural pattern that strictly separates the core framework, the business domain, and the application/delivery mechanism.</p>"},{"location":"adr/ADR-043-application-layered-architecture/#decision","title":"Decision","text":"<p>We will adopt a layered architecture based on the principles of Clean Architecture / Hexagonal Architecture. The project will be structured into three primary, high-level Python packages, each with a distinct responsibility and a strict dependency rule.</p> <p>The layers are defined as follows, from the inside out:</p> <p>1. <code>nala.domain</code> (The Business Core) * Responsibility: To contain the \"soul\" of the application. This layer encapsulates all the business-specific logic, entities, and rules. It is the most valuable and stable part of the project. * Contents: Domain entities (e.g., <code>Document</code>, <code>Order</code>) with rich behavior, repository protocols (interfaces), and domain services. * Core Principle: This layer has zero dependencies on any other layer within the <code>nala</code> project. It is pure business logic.</p> <p>2. <code>nala.athomic</code> (The Core Framework / Toolkit) * Responsibility: To provide generic, reusable, and business-agnostic technical infrastructure and tools. This is the foundation upon which applications are built. * Contents: Generic services (<code>BaseService</code>), shared infrastructure clients (<code>database.document.MongoProvider</code>), security logic (<code>AuthService</code>, <code>Executors</code>), and other cross-cutting concerns. * Core Principle: It knows nothing about the specific business rules of <code>nala.domain</code>. It provides tools, not business solutions.</p> <p>3. <code>nala.api</code> (The Application &amp; Infrastructure Layer) * Responsibility: To act as the adapter and orchestrator that connects the outside world (e.g., HTTP) to the business core. It also contains the concrete implementations of the infrastructure defined by the domain. * Contents:     * FastAPI routers, request/response models.     * Dependency Injection setup (<code>dependencies/security.py</code>).     * Concrete repository implementations (<code>infrastructure/repositories/mongo_document_repository.py</code>) that implement protocols from <code>nala.domain</code> using tools from <code>nala.athomic</code>.     * The main application entrypoint (<code>main.py</code>) and the lifecycle management (<code>lifespan.py</code>, <code>services.py</code>) that wires everything together.</p> <p>The Dependency Rule: Dependencies must only point inwards. * <code>nala.api</code> depends on <code>nala.domain</code> and <code>nala.athomic</code>. * <code>nala.athomic</code> has no dependencies on other <code>nala</code> layers. * <code>nala.domain</code> has no dependencies on other <code>nala</code> layers.</p>"},{"location":"adr/ADR-043-application-layered-architecture/#consequences","title":"Consequences","text":"<p>Positive: * True Separation of Concerns: The business logic (<code>domain</code>) is completely isolated from technological choices (<code>athomic</code>, <code>api</code>). We can change the database or web framework without touching the business rules. * High Cohesion, Low Coupling: Logic related to a specific business concept is highly cohesive within its domain module. The layers are loosely coupled through well-defined interfaces (protocols). * Enhanced Testability: Each layer can be tested in complete isolation. The <code>domain</code> can be tested with simple unit tests, while the <code>api</code> layer can be tested by mocking the domain interfaces. * Future-Proofing <code>athomic</code>: The <code>athomic</code> layer remains generic and can be extracted into its own library, potentially for open-sourcing or for use in other Nala projects, fulfilling a key strategic goal. * Clarity for Developers: The structure provides a clear mental model for where to place new code and how components interact.</p> <p>Negative: * Increased Initial Complexity: The architecture introduces more layers and files, which can have a steeper learning curve for new developers. * More Boilerplate: Requires a more disciplined approach and can lead to more \"ceremony\" (e.g., defining protocols, dependency injection wiring) for simple features. This is a deliberate trade-off in favor of long-term maintainability.</p>"},{"location":"adr/ADR-044-local-monitoring-stack-with-docker-composer/","title":"ADR 044 local monitoring stack with docker composer","text":""},{"location":"adr/ADR-044-local-monitoring-stack-with-docker-composer/#adr-044-local-monitoring-stack-with-docker-compose","title":"ADR-044: Local Monitoring Stack with Docker Compose","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-11</li> </ul>"},{"location":"adr/ADR-044-local-monitoring-stack-with-docker-composer/#context","title":"Context","text":"<p>To ensure the reliability and performance of <code>athomic-docs</code>, a robust monitoring solution is required. We need the ability to collect, store, and visualize key application metrics, such as request rates, error rates, and response times (RED metrics), as well as custom business and performance metrics (e.g., cache hit ratio).</p> <p>The solution must be easy to set up for local development, consistent across different developer machines, and architecturally decoupled from the main application's infrastructure. It should consume the metrics already being exposed by the API via the <code>/internal/metrics</code> endpoint, which is protected by an IP allowlist.</p>"},{"location":"adr/ADR-044-local-monitoring-stack-with-docker-composer/#decision","title":"Decision","text":"<p>We will implement a local monitoring stack using Prometheus for metrics collection and Grafana for visualization, both orchestrated by Docker Compose.</p> <ol> <li> <p>Dedicated Infrastructure: The entire monitoring stack will reside in a new <code>monitoring/</code> directory at the root of the project. This directory is separate from the application's <code>infra/</code> directory, maintaining a clear separation of concerns between the application's dependencies and its monitoring tools.</p> </li> <li> <p>Orchestration: A <code>docker-compose.yml</code> file within the <code>monitoring/</code> directory will define and manage the lifecycle of the Prometheus and Grafana containers. This ensures a consistent, one-command setup (<code>docker-compose up</code>) for any developer. <code>Makefile</code> shortcuts (<code>mup</code>, <code>mdown</code>) will be provided for convenience.</p> </li> <li> <p>Prometheus Configuration (<code>prometheus.yml</code>):</p> <ul> <li>Prometheus will be configured to \"scrape\" (collect) metrics from the <code>athomic-docs</code> application.</li> <li>The target will be <code>host.docker.internal:8000/internal/metrics</code> (or the appropriate host/port), allowing the Prometheus container to access the API running on the host machine.</li> </ul> </li> <li> <p>Grafana Provisioning:</p> <ul> <li>Grafana will be configured to automatically provision its resources on startup.</li> <li>Datasource: A <code>datasource.yml</code> will automatically configure Prometheus as the default data source for Grafana, pointing to the Prometheus container (<code>http://prometheus:9090</code>).</li> <li>Dashboard: A default dashboard (<code>api-overview.json</code>) will be automatically loaded, providing an immediate, out-of-the-box visualization of key RED metrics.</li> </ul> </li> <li> <p>Networking:</p> <ul> <li>A custom Docker bridge network (<code>monitoring-net</code>) will be created to allow stable communication between the containers and to assign a static, predictable IP address to the Prometheus container.</li> <li>This static IP is crucial for it to be added to the <code>IPAllowlist</code> in the <code>athomic-docs</code>'s configuration, allowing Prometheus to bypass the security checks on the <code>/internal/metrics</code> endpoint.</li> </ul> </li> </ol>"},{"location":"adr/ADR-044-local-monitoring-stack-with-docker-composer/#consequences","title":"Consequences","text":"<p>Positive: * Reproducibility: Any developer can spin up the entire monitoring stack with a single command, ensuring a consistent development environment. * Decoupling: The monitoring stack is completely decoupled from the application's core logic and infrastructure. It can be started, stopped, or modified without affecting the API. * Immediate Visibility: The provisioned dashboard provides instant insights into the application's health as soon as it's running, lowering the barrier to observability. * Production-Ready Pattern: This setup mirrors common production architectures, providing valuable experience and a clear path for deploying a similar stack in a production environment (e.g., in Kubernetes).</p> <p>Negative: * Increased Local Resource Usage: Running two additional containers (Prometheus, Grafana) will consume more local machine resources (CPU, RAM). * Configuration Management: The monitoring configuration (<code>prometheus.yml</code>, Grafana dashboards) now becomes part of the project's codebase and needs to be maintained.</p>"},{"location":"adr/ADR-045-integration-testingstrategy-for-database-dependent-components/","title":"ADR 045 integration testingstrategy for database dependent components","text":""},{"location":"adr/ADR-045-integration-testingstrategy-for-database-dependent-components/#adr-045-integration-testing-strategy-for-database-dependent-components","title":"ADR-045: Integration Testing Strategy for Database-Dependent Components","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-13</li> </ul>"},{"location":"adr/ADR-045-integration-testingstrategy-for-database-dependent-components/#context","title":"Context","text":"<p>As the application's architecture was refactored to follow Domain-Driven Design (DDD) and Clean Architecture principles, a clear need arose for a testing strategy that could validate the infrastructure layer, specifically the database repositories (e.g., <code>MongoDocumentRepository</code>).</p> <p>Unit tests using mocks are insufficient for this purpose as they cannot verify: 1.  The correctness of the interaction with the actual database. 2.  The behavior of the Object-Document Mapper (ODM), in our case, Beanie. 3.  The proper application of database-specific logic like multi-tenancy filters.</p> <p>We required a testing pattern that was reliable, isolated, and accurately reflected the application's dependency injection and lifecycle management architecture.</p>"},{"location":"adr/ADR-045-integration-testingstrategy-for-database-dependent-components/#decision","title":"Decision","text":"<p>We will adopt an integration testing pattern using <code>pytest</code> fixtures to manage the lifecycle of a real database connection for each test session. A central, reusable <code>pytest</code> fixture named <code>db_provider</code> will be the cornerstone of this strategy.</p> <p>The responsibilities of the <code>db_provider</code> fixture are as follows:</p> <ol> <li> <p>Setup Phase (Before each test):</p> <ul> <li>It reads the base database configuration from the application's settings.</li> <li>To ensure test isolation, it creates a new, temporary settings object, overriding the database name with a unique identifier (e.g., <code>test_db_&lt;uuid&gt;</code>).</li> <li>It instantiates the concrete database infrastructure service (e.g., <code>MongoProvider</code>) using these temporary settings.</li> <li>It calls the provider's <code>.start()</code> method to establish a real connection to the database service.</li> <li>It immediately calls <code>init_beanie</code>, passing the live database connection and the specific Beanie models required for the test.</li> <li>Finally, it <code>yields</code> the fully initialized and ready-to-use database provider instance.</li> </ul> </li> <li> <p>Teardown Phase (After each test):</p> <ul> <li>After the test function completes, the fixture resumes and performs a clean teardown.</li> <li>It drops the entire temporary test database to ensure no data leaks between tests.</li> <li>It calls the provider's <code>.stop()</code> method to gracefully close the database connection.</li> </ul> </li> </ol> <p>A second, simpler fixture (e.g., <code>repository</code>) will depend on <code>db_provider</code> and will be responsible for instantiating the <code>MongoDocumentRepository</code>, injecting the live provider into it.</p> <p>Example Fixture Implementation:</p> <pre><code>@pytest_asyncio.fixture\nasync def db_provider() -&gt; AsyncGenerator[MongoProvider, None]:\n    \"\"\"\n    Creates and manages the lifecycle of a real database connection for tests.\n    \"\"\"\n    base_mongo_settings = get_settings().database.document.provider\n    test_mongo_settings = base_mongo_settings.model_copy(\n        update={\"database_name\": f\"test_db_{uuid4().hex}\"}\n    )\n\n    provider = MongoProvider(test_mongo_settings)\n\n    await provider.start()\n    db = await provider.get_database()\n    await init_beanie(database=db, document_models=[SampleBeanieModel])\n\n    yield provider\n\n    # Teardown logic\n    if provider.client:\n        await provider.client.drop_database(test_mongo_settings.database_name)\n    await provider.stop()\n\n@pytest.fixture\ndef repository(db_provider: MongoProvider) -&gt; MongoDocumentRepository:\n    \"\"\"Injects the live db_provider into the repository.\"\"\"\n    return MongoDocumentRepository(\n        db_provider=db_provider,\n        # ... other required args\n    )\n</code></pre>"},{"location":"adr/ADR-045-integration-testingstrategy-for-database-dependent-components/#consequences","title":"Consequences","text":"<p>Positive:</p> <ul> <li>High Fidelity: Tests are run against a real database instance, providing high confidence that the repository logic, queries, and ODM mappings are correct.</li> <li>Complete Isolation: Each test (or test module, depending on fixture scope) operates on its own ephemeral database, which is created and destroyed for that test alone. This completely prevents state leakage between tests.</li> <li>DRY (Don't Repeat Yourself): The complex setup and teardown logic is centralized in the <code>db_provider</code> fixture, making the actual test functions clean and focused solely on testing business logic.</li> <li>Architectural Alignment: The testing pattern perfectly mirrors the application's real architecture (dependency injection of the provider into the repository), which increases the value and reliability of the tests.</li> </ul> <p>Negative:</p> <ul> <li>Slower Execution: As these are integration tests involving network I/O to a database service, they are inherently slower than unit tests. They should be run as part of a separate test suite from fast-running unit tests.</li> <li>Infrastructure Dependency: Running these tests requires a live MongoDB service to be available to the test runner (typically provided via Docker).</li> </ul>"},{"location":"adr/ADR-046-retry-refact/","title":"ADR-014: Standardization and Architecture of the Retry Module","text":"<ul> <li>Status: Accepted  </li> <li>Date: 2025-07-19</li> </ul>"},{"location":"adr/ADR-046-retry-refact/#context","title":"Context","text":"<p>Reliable retry mechanisms are essential in distributed systems, resilient APIs, and integrations with external resources that may experience transient failures. The Nala-Core-API requires a robust, extensible, and observable approach for retries, enabling:</p> <ul> <li>Configuration-driven and code-level customization (e.g., via RetrySettings, RetryPolicy, RetryFactory, and decorators).</li> <li>Support for callbacks on success, failure, circuit breaker hooks, and custom exception handling.</li> <li>Native integration with metrics, logging, and tracing (observability).</li> <li>Coverage for both synchronous and asynchronous functions.</li> </ul> <p>Alternatives considered included: - Direct use of libraries like Tenacity without an abstraction layer. - Hardcoded retry logic at usage points, without a global/configurable pattern. - Ad-hoc implementations detached from the rest of the framework.</p> <p>The decision was to create a standardized and decoupled module, centralizing logic, exposing a configurable decorator, and adapters for dynamic settings and runtime overrides.</p>"},{"location":"adr/ADR-046-retry-refact/#decision","title":"Decision","text":"<ul> <li> <p>The Retry module will consist of these main components:</p> <ul> <li><code>RetrySettings</code>: Configuration class (settings/config).</li> <li><code>RetryPolicy</code>: Class representing the business rules and retry policy.</li> <li><code>RetryHandler</code>: Orchestrator for execution, observability integration, callbacks, etc.</li> <li><code>retry</code>: Decorator for practical use, supporting both sync and async.</li> <li><code>RetryFactory</code>: Responsible for creating policies/decorators from application settings.</li> <li><code>create_policy_from_settings</code> (adapter): Decoupled function to convert settings to policy.</li> </ul> </li> <li> <p>Retry configuration should preferably be made via settings, but can be overridden in code via policy or decorator/factory parameters.</p> </li> <li>The adapter accepts exceptions as either strings (\"ValueError\") or types (ValueError), resolving them as needed for the project.</li> <li>The decorator supports both synchronous and asynchronous functions, forwarding all logic to the handler.</li> <li>The module does not require mandatory coupling between settings and policy; it is the caller\u2019s responsibility to ensure valid configuration.</li> <li>Callbacks such as <code>on_retry</code>, <code>on_fail</code>, and <code>circuit_breaker_hook</code> are passed directly to the handler/decorator.</li> </ul>"},{"location":"adr/ADR-046-retry-refact/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Centralization and standardization of retry throughout the system.</li> <li>Extensibility: easy to create custom policies and decorators for different contexts.</li> <li>Straightforward integration with observability (metrics, tracing, logging).</li> <li>Easier unit testing and behavior mocking.</li> <li>Transparent support for both sync and async.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Slight abstraction overhead (factory, adapter, policy vs. using Tenacity directly).</li> <li>Potential for configuration duplication if usage is not standardized.</li> <li>Need to avoid non-serializable functions in settings (e.g., lambda in retry_on_result).</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>Modular design allows for future evolution (native circuit breaker, fallback strategies, etc.).</li> <li>May require maintenance if the global configuration schema changes.</li> </ul> </li> </ul>"},{"location":"adr/ADR-047-fallback-fully-implemented/","title":"ADR-001: Implementation of a Configurable and Observable Fallback System","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-22</li> </ul>"},{"location":"adr/ADR-047-fallback-fully-implemented/#context","title":"Context","text":"<p>Our application requires a high degree of resilience against failures in its external dependencies, particularly caching services (like Redis). A simple inline <code>try/except</code> block for fetching data is insufficient as it leads to code duplication, lacks observability, and is hard to maintain or configure.</p> <p>Several alternatives were considered to address this:</p> <ol> <li> <p>Monolithic Provider with Flags: A single <code>FallbackKVProvider</code> class that uses boolean flags (e.g., <code>fallback_on_miss=True</code>) to control its internal logic with <code>if/else</code> statements. This was considered simpler for the initial implementation but was rejected because it violates the Open/Closed Principle, making it difficult to add new strategies without modifying the core class. It would also lead to high cyclomatic complexity.</p> </li> <li> <p>Simple Decorator: A basic decorator could encapsulate <code>try/except</code> logic. This was rejected as it offers limited configurability and is not well-suited for complex, stateful operations like multi-layered caching with self-healing.</p> </li> <li> <p>A Decoupled, Strategy-Based System: An approach where the core logic of how to perform fallback and write operations is encapsulated in separate \"Strategy\" objects. This was deemed the most robust and scalable solution for a core framework component.</p> </li> </ol> <p>The primary constraint was to build a solution that is not only resilient but also highly configurable and observable, fitting into our existing metrics and logging infrastructure. </p>"},{"location":"adr/ADR-047-fallback-fully-implemented/#decision","title":"Decision","text":"<p>We have decided to implement a comprehensive fallback system composed of two main parts: a specific <code>FallbackKVProvider</code> for data stores and a generic <code>@fallback_handler</code> for any function. The architecture is based on several key design patterns to ensure flexibility and maintainability.</p> <ol> <li> <p>Strategy Pattern: The core behaviors of the <code>FallbackKVProvider</code> are controlled by injectable strategies.</p> <ul> <li>Read Strategies (<code>ReadStrategyProtocol</code>): Dictate whether the fallback is triggered only on provider errors (<code>FallbackOnErrorStrategy</code>) or also on cache misses (<code>FallbackOnMissOrErrorStrategy</code>). </li> <li>Write Strategies (<code>WriteStrategyProtocol</code>): Control the write behavior, allowing for \"write-around\" (primary only)  or \"write-through\" (primary and then propagated to fallbacks)  operations.</li> </ul> </li> <li> <p>Factory Pattern: To decouple the <code>FallbackKVProvider</code> from the concrete creation of strategy objects, we use factories (<code>ReadStrategyFactory</code>, <code>WriteStrategyFactory</code>).  The provider requests a strategy from the factory based on an enum type (<code>FallbackReadStrategyType</code>, <code>FallbackWriteStrategyType</code>), and the factory is responsible for instantiating the correct class. </p> </li> <li> <p>Dependency Injection: To resolve the circular dependency between the provider and its strategies, the provider injects the necessary dependencies (<code>primary</code>, <code>fallbacks</code>, <code>heal_callable</code>) into the strategy methods at runtime.  The strategies are no longer aware of the <code>FallbackKVProvider</code> class, only of the protocols they need to operate on.</p> </li> <li> <p>Rich Observability: The system is instrumented with detailed Prometheus metrics that are specific to the fallback mechanism itself, separate from the metrics of the underlying data stores. This includes:</p> <ul> <li><code>fallback_operations_total</code>: A counter for the final success or failure of a fallback chain. </li> <li><code>fallback_operation_duration_seconds</code>: A histogram to measure the latency of fallback chain execution. </li> </ul> </li> <li> <p>Self-Healing: An optional self-healing mechanism is included. When a value is successfully retrieved from a lower-level fallback, it is written back to the primary provider and any other providers that failed along the way, improving cache consistency and future performance. </p> </li> <li> <p>Handler/Decorator Separation: The generic fallback logic was refactored into a <code>FallbackLogicHandler</code> class.  A simple <code>fallback_handler</code> decorator now acts as a convenient facade for this class, allowing the core logic to be used programmatically and tested more easily. </p> </li> </ol>"},{"location":"adr/ADR-047-fallback-fully-implemented/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>High Configurability: The system's behavior can be fundamentally changed via configuration (enums) without any code modification. </li> <li>Excellent Extensibility: Adding new read or write strategies is straightforward: create a new class and register it in the corresponding factory. The core components remain untouched.</li> <li>Decoupled and Highly Testable: Each component (Provider, Strategy, Factory) has a single responsibility and can be unit-tested in isolation.</li> <li>Deep Observability: Provides critical insights into how often the system relies on contingency plans and how well those plans perform. </li> <li>Increased System Resilience: The self-healing mechanism actively repairs cache state, reducing the impact of transient failures over time. </li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Initial Complexity: The number of classes and files is higher than a monolithic approach. This introduces a slight learning curve for developers new to the module.</li> <li>Indirection: The use of multiple patterns means a developer might need to navigate a few files to understand the full execution flow, as logic is delegated from the provider to the strategy.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This architecture strongly favors long-term maintainability and scalability over the speed of initial implementation.</li> <li>The performance of the fallback chain and the success rate of the <code>heal</code> operation become important metrics for operational monitoring.</li> </ul> </li> </ul>"},{"location":"adr/ADR-048-integration-configuration/","title":"ADR-048: Separation of Bootstrap and Dynamic Configuration","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-23</li> </ul>"},{"location":"adr/ADR-048-integration-configuration/#context","title":"Context","text":"<p>An application requires two fundamentally different types of configuration:</p> <ol> <li>Bootstrap Configuration: Static values needed for the application to start. This includes database connection strings, API keys for external services, and log levels. These are typically read once at startup from local files (<code>.toml</code>, <code>.env</code>) and are considered immutable for the application's lifecycle.</li> <li>Dynamic Configuration: Values that can change while the application is running and should be fetched from a remote source. This includes feature flags, rate-limiting thresholds, or A/B testing parameters. Fetching these values allows the application's behavior to be modified in real-time without a redeployment.</li> </ol> <p>The initial approach risked conflating these two concepts, potentially leading to a single, complex configuration mechanism that was ill-suited for both use cases. Using a file-based loader like Dynaconf for dynamic, runtime values is inefficient, and using a remote provider like Consul for static bootstrap values introduces an unnecessary dependency at startup.</p>"},{"location":"adr/ADR-048-integration-configuration/#decision","title":"Decision","text":"<p>To ensure architectural clarity and adhere to the Single Responsibility Principle, it was decided to formally separate these two concerns within the <code>athomic</code> framework.</p> <ol> <li>The existing <code>athomic/config</code> module will be solely responsible for Bootstrap Configuration. It will continue to use Dynaconf and Pydantic to load, validate, and provide immutable settings from local files at application startup.</li> <li>A new module, <code>athomic/configuration</code>, will be created to handle Dynamic Configuration. This module will provide a <code>ConfigurationProviderProtocol</code> and a factory to fetch values from external, remote sources (like Consul) during runtime. It will leverage <code>athomic</code>'s own caching layer to ensure high performance.</li> </ol>"},{"location":"adr/ADR-048-integration-configuration/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Architectural Clarity: The framework now has a clear, explicit distinction between static startup settings and dynamic runtime settings, making the system easier to reason about.</li> <li>Improved Testability: It's easier to mock bootstrap settings (via environment variables or file mocks) separately from mocking a remote configuration provider (like Consul) in tests.</li> <li>Enhanced Flexibility: The <code>athomic/configuration</code> module can be extended with new providers (e.g., for LaunchDarkly, Vault, etc.) without affecting the core bootstrap mechanism.</li> <li>Decoupling: The application's core services are not coupled to a specific remote source like Consul; they only depend on the <code>ConfigurationProviderProtocol</code>.</li> </ul> </li> <li>Negative:<ul> <li>Increased Surface Area: Introduces a new module and a new pattern (<code>ConfigurationProviderFactory</code>) that developers must learn, slightly increasing the framework's initial conceptual overhead.</li> </ul> </li> <li>Neutral/Other:<ul> <li>Developers must now consciously decide which configuration mechanism to use. Values needed at startup must be accessed via <code>get_settings()</code>, while values that can change at runtime must be accessed via the <code>ConfigurationProviderFactory</code>. This enforces a good design practice.</li> </ul> </li> </ul>"},{"location":"adr/ADR-049-consul-connectivity/","title":"ADR-049: Unification of Consul Connectivity","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-23</li> </ul>"},{"location":"adr/ADR-049-consul-connectivity/#context","title":"Context","text":"<p>It was identified that multiple modules within the <code>athomic</code> framework required a connection to Consul, including <code>feature_flags</code>, <code>configuration</code> (for remote configs), and <code>discovery</code> (for service discovery). The initial approach led to each module having its own logic for creating and managing a Consul client.</p> <p>This resulted in several problems: * Code Duplication: Redundant client implementation and connection logic across different modules. * Scattered Configuration: Connection settings for Consul were dispersed, making it difficult to manage and update. * Resource Inefficiency: Potentially multiple connection pools to the same Consul cluster, consuming unnecessary resources. * Inconsistency: Risk of different modules using different connection settings or client versions, leading to unpredictable behavior.</p>"},{"location":"adr/ADR-049-consul-connectivity/#decision","title":"Decision","text":"<p>To resolve these issues and create a more robust architecture, it was decided to centralize all Consul-related interactions.</p> <ol> <li>The <code>athomic/integration/consul</code> module is designated as the single source of truth for Consul connectivity.</li> <li>It provides a singleton <code>ConsulClientFactory</code> that manages the lifecycle (connection, disconnection) of a shared <code>ConsulClient</code> instance.</li> <li>All other modules (<code>feature_flags</code>, <code>configuration</code>, <code>discovery</code>, etc.) are refactored to be consumers of this shared client. They will no longer manage their own connections but will instead request the singleton instance from the <code>ConsulClientFactory</code>.</li> </ol>"},{"location":"adr/ADR-049-consul-connectivity/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>DRY Principle: Eliminates all redundant connection logic, making the codebase cleaner and easier to maintain.</li> <li>Centralized Configuration: All connection settings for Consul now reside in a single, well-defined section of the application's configuration, improving manageability.</li> <li>Resource Efficiency: A single, shared connection pool to Consul is used across the entire application, which is more efficient.</li> <li>Improved Reliability: Ensures that all parts of the application use the exact same, lifecycle-managed client, leading to more consistent and reliable behavior.</li> </ul> </li> <li>Negative:<ul> <li>Tighter Coupling to the Factory: Modules that consume the Consul client now have a direct dependency on the <code>ConsulClientFactory</code>. This is a deliberate architectural trade-off made to gain the significant benefits of centralization.</li> </ul> </li> <li>Neutral/Other:<ul> <li>This decision reinforces the \"shared infrastructure provider\" pattern within the <code>athomic</code> framework, where a central module provides a fundamental capability (like a database or broker connection) that other modules consume.</li> </ul> </li> </ul>"},{"location":"adr/ADR-050-performance-optimization/","title":"ADR-050: Multi-layered Performance Optimization Strategy","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-23</li> </ul>"},{"location":"adr/ADR-050-performance-optimization/#context","title":"Context","text":"<p>Application performance is not determined by a single factor but is rather the result of optimizations across different stages of a request's lifecycle. Relying solely on one technique (like caching) can leave significant bottlenecks unaddressed in other areas, such as network transfer, CPU-bound operations, or I/O event loop management.</p> <p>A holistic strategy was needed to ensure the <code>athomic</code> framework could provide comprehensive performance enhancements, making applications built with it fast by default.</p>"},{"location":"adr/ADR-050-performance-optimization/#decision","title":"Decision","text":"<p>It was decided that the <code>athomic/performance</code> module would be expanded to address performance optimization across three distinct layers. Instead of just being a caching module, it will now provide a suite of configurable performance tools.</p> <ol> <li> <p>Caching Layer: The existing advanced, multi-backend caching abstraction remains the core of data-level performance, with features like refresh-ahead and cache stampede prevention.</p> </li> <li> <p>Computation Layer: To optimize CPU-bound and I/O-bound operations, <code>athomic</code> will provide managed bootstraps for high-performance, third-party components. This includes:</p> <ul> <li>Event Loop: A bootstrap utility (<code>install_uvloop_if_available</code>) to replace Python's default asyncio event loop with <code>uvloop</code>.</li> <li>Serialization: A pluggable serializer architecture that supports high-speed libraries like <code>orjson</code>.</li> </ul> </li> <li> <p>Network Layer: To optimize data transfer between the server and the client, <code>athomic</code> will provide an abstraction for response compression. A <code>CompressionMiddlewareFactory</code> was created to support different algorithms (<code>gzip</code>, <code>brotli</code>), making the choice a configuration detail rather than an API-level implementation.</p> </li> </ol>"},{"location":"adr/ADR-050-performance-optimization/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Holistic Performance: The framework now offers a comprehensive performance toolkit, addressing bottlenecks from the event loop up to network delivery.</li> <li>Architectural Cohesion: All performance-related concerns are now logically grouped within the <code>athomic/performance</code> module, improving maintainability and discoverability.</li> <li>Flexibility &amp; Configurability: Developers can easily enable or switch between different performance optimizations (e.g., swapping <code>gzip</code> for <code>brotli</code>, enabling <code>uvloop</code>) through configuration, without changing application code.</li> <li>Improved Developer Experience: By providing managed bootstraps and factories, <code>athomic</code> simplifies the integration of these powerful but sometimes complex libraries.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Dependencies: Supporting these features adds new third-party dependencies (<code>uvloop</code>, <code>brotli-asgi</code>) to the framework, slightly increasing its installation footprint. This is mitigated by making the imports optional where possible.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This decision solidifies <code>athomic/performance</code> as a key pillar of the framework, on par with <code>resilience</code> and <code>observability</code>. It establishes a clear pattern for adding future performance-related enhancements.</li> </ul> </li> </ul>"},{"location":"adr/ADR-051-base-service-refact/","title":"ADR-051: Unify Service Lifecycle Management with a Single BaseService Class","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-24</li> </ul>"},{"location":"adr/ADR-051-base-service-refact/#context","title":"Context","text":"<p>The <code>athomic</code> framework initially had two separate base classes for managing service lifecycles: 1.  <code>BaseService</code>: Designed for services that manage a connection (e.g., database clients), providing <code>connect()</code> and <code>close()</code> methods. 2.  <code>RunnableServiceBase</code>: Designed for services that run a continuous background task (e.g., a message consumer or publisher), providing <code>start()</code>, <code>stop()</code>, and <code>_run_loop()</code> logic.</p> <p>This separation created a design conflict for services that are both connectable and runnable, such as the <code>VaultSecretsProvider</code>. The provider needs to connect to Vault and run a background task to renew its token.</p> <p>Attempting to solve this with multiple inheritance (<code>class VaultProvider(RunnableServiceBase, BaseService)</code>) led to technical issues, specifically a Method Resolution Order (MRO) conflict. The <code>set_ready()</code> method was defined with different signatures in both base classes (synchronous in <code>RunnableServiceBase</code>, asynchronous in <code>BaseService</code>), causing <code>TypeError</code> exceptions during testing.</p> <p>This indicated a flaw in the service model, creating complexity and forcing developers to choose between two conflicting abstractions or deal with fragile multiple inheritance.</p>"},{"location":"adr/ADR-051-base-service-refact/#decision","title":"Decision","text":"<p>To resolve the conflict and simplify the framework's service model, we decided to merge the functionality of <code>RunnableServiceBase</code> into <code>BaseService</code>.</p> <ol> <li>The <code>RunnableServiceBase</code> class is now deprecated and has been removed.</li> <li>The <code>BaseService</code> class is now the single, unified base class for all manageable services in the framework.</li> <li>The new <code>BaseService</code> intelligently manages both connection and background task lifecycles:<ul> <li>Subclasses can optionally implement <code>async def _connect()</code> and <code>async def _close()</code> for connection management.</li> <li>Subclasses can optionally implement <code>async def _run_loop()</code> to define a background task.</li> </ul> </li> <li>The public <code>start()</code> and <code>stop()</code> methods of <code>BaseService</code> now orchestrate the entire lifecycle. The <code>start()</code> method will call <code>_connect()</code> and, if <code>_run_loop()</code> is implemented, it will also start it in a background <code>asyncio.Task</code>.</li> <li>All existing services that previously inherited from <code>RunnableServiceBase</code> (e.g., <code>OutboxPublisherBase</code> ) and <code>BaseService</code> (e.g., <code>DocumentDatabaseRegistry</code> ) now inherit solely from the new <code>BaseService</code>.</li> <li>The corresponding protocols (<code>ConnectionServiceProtocol</code>, <code>RunnableServiceProtocol</code>) have also been merged into a single, unified <code>BaseServiceProtocol</code> to maintain consistency between implementation and interface.</li> </ol>"},{"location":"adr/ADR-051-base-service-refact/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-051-base-service-refact/#positive","title":"Positive:","text":"<ul> <li>Unified Service Model: Developers now only need to learn and inherit from a single <code>BaseService</code>, reducing cognitive load and simplifying the framework's API.</li> <li>Increased Flexibility &amp; Extensibility: Any service can easily be made \"runnable\" by simply implementing the <code>_run_loop</code> method, without needing to change its inheritance. This makes services like <code>VaultSecretsProvider</code> trivial to implement correctly.</li> <li>Conflict Resolution: The MRO conflict and the <code>TypeError</code> related to different method signatures are completely eliminated.</li> <li>Consistent Lifecycle Management: The application's <code>LifespanManager</code> can treat all services uniformly, calling <code>start()</code> and <code>stop()</code> on every registered component without needing to know its internal implementation details.</li> </ul>"},{"location":"adr/ADR-051-base-service-refact/#negative","title":"Negative:","text":"<ul> <li>Minor Violation of SRP/ISP: A service that is purely \"connectable\" will now inherit the logic and attributes for managing a background task (e.g., an empty <code>_run_loop</code> method and a <code>_run_task</code> attribute) that it does not use. This is a minor, pragmatic violation of the Single Responsibility and Interface Segregation principles.</li> <li>Slightly Larger Base Class: The <code>BaseService</code> class now has more responsibilities than before, though the added logic is minimal and intelligently applied only when needed.</li> </ul>"},{"location":"adr/ADR-051-base-service-refact/#neutralother","title":"Neutral/Other:","text":"<ul> <li>Refactoring Effort: A one-time effort was required to update all classes that inherited from the old <code>RunnableServiceBase</code> and <code>BaseServiceProtocol</code> to use the new unified versions.</li> </ul>"},{"location":"adr/ADR-053-vault-secrets-management/","title":"ADR-002: Adoption of HashiCorp Vault for Secrets Management","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-24</li> </ul>"},{"location":"adr/ADR-053-vault-secrets-management/#context","title":"Context","text":"<p>The project requires a secure, centralized, and auditable method for managing application secrets, such as database credentials, API keys, and certificates. The existing practice of using environment variables (<code>.env</code> files) is suitable for local development but poses significant security risks in production environments. It lacks audit trails, makes secret rotation difficult, and increases the risk of accidental exposure.</p> <p>We considered the following alternatives:</p> <ol> <li>Environment Variables: Simple to implement but insecure, hard to manage at scale, and not auditable. Considered the baseline to improve upon.</li> <li>Cloud-Native Secret Managers (e.g., AWS Secrets Manager, GCP Secret Manager): These are powerful, managed services that integrate well with their respective cloud ecosystems. However, they can lead to vendor lock-in for a critical security component.</li> <li>Encrypted Files in Version Control (e.g., git-crypt): An improvement over plain text, but still presents challenges with key management, access control, and dynamic rotation.</li> </ol> <p>The chosen solution needed to be abstractable within the <code>athomic</code> framework (via <code>SecretsProtocol</code>), support both local development and production environments, and offer strong security guarantees.</p>"},{"location":"adr/ADR-053-vault-secrets-management/#decision","title":"Decision","text":"<p>We have decided to adopt HashiCorp Vault as the standard secrets management solution for the project.</p> <p>The integration is implemented through a <code>VaultSecretsProvider</code> class, which conforms to the framework's generic <code>SecretsProtocol</code>. This decision encapsulates all Vault-specific logic into a single, reusable component.</p> <p>Key implementation details include: * Authentication: The provider supports multiple authentication methods, primarily direct <code>token</code> auth (for development and specific use cases) and AppRole, which is the recommended method for secure, automated machine-to-machine authentication in production. * Resilience: The provider features a built-in, resilient background task for automatic token renewal. This allows the use of short-lived tokens, which is a security best practice, without causing service interruptions. * Configuration: The connection to Vault (address, auth method, etc.) is managed through the application's central configuration (<code>settings.toml</code>), allowing for easy environment-specific setup. * Abstraction: Application code does not interact with Vault directly. It requests secrets through the <code>SecretsBase</code> or <code>SecretsProtocol</code> interface, making the system agnostic to the underlying secrets backend.</p>"},{"location":"adr/ADR-053-vault-secrets-management/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-053-vault-secrets-management/#positive","title":"Positive:","text":"<ul> <li>Centralized Security &amp; Auditability: All secrets are stored, encrypted, and managed in a single location. Vault provides detailed audit logs, showing who accessed which secret and when.</li> <li>Strong Access Control: Vault's policy-based access control allows for granular permissions, adhering to the principle of least privilege.</li> <li>Improved Developer Experience: Developers can run a local Vault instance via Docker for a consistent development environment that mirrors production.</li> <li>Cloud Agnostic: Vault is an open-source tool that can be deployed in any cloud or on-premises, avoiding vendor lock-in for this critical piece of infrastructure.</li> <li>Enables Dynamic Secrets: This decision opens the door to using Vault's more advanced features in the future, such as dynamic database credentials, which further reduces the attack surface.</li> </ul>"},{"location":"adr/ADR-053-vault-secrets-management/#negative","title":"Negative:","text":"<ul> <li>Increased Operational Overhead: We are now responsible for deploying, maintaining, monitoring, and backing up a new, stateful, and critical infrastructure component (the Vault server).</li> <li>Learning Curve: Developers and DevOps personnel will need to become familiar with Vault's concepts (e.g., policies, auth methods, leases, unsealing).</li> <li>Potential Single Point of Failure: Applications depend on Vault to retrieve secrets at startup. A Vault outage can prevent applications from starting. This necessitates a highly available Vault setup in production environments.</li> </ul>"},{"location":"adr/ADR-054-athomic-secrets-starting/","title":"ADR-003: Architectural Refactoring of Service Initialization and Dependency Management","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-24</li> </ul>"},{"location":"adr/ADR-054-athomic-secrets-starting/#context","title":"Context","text":"<p>The initial architecture of the <code>athomic</code> layer presented several design challenges that became apparent during the implementation of new features and tests:</p> <ol> <li> <p>Circular Imports: The attempt to create a cohesive system led to circular dependencies between modules. For example, the <code>services</code> module needed <code>security</code> (to resolve secrets), but the <code>security</code> module needed <code>services</code> (to inherit from <code>BaseService</code>), resulting in <code>ImportError</code> and blocking development.</p> </li> <li> <p>Tight Coupling and \"Magic\": Initialization logic was scattered. Services attempted to create their own dependencies (e.g., <code>RedisRateLimiter</code> calling <code>KVStoreFactory</code>) or relied on a global state (<code>get_settings()</code>) within their constructors. This created strong coupling, made the initialization flow implicit (\"magic\"), and made isolated unit testing extremely difficult.</p> </li> <li> <p>Code Repetition: Common logic, such as resolving credentials from different sources (<code>str</code>, <code>SecretStr</code>, <code>Callable</code>), was beginning to be repeated across multiple services (e.g., <code>MongoProvider</code>, <code>RedisKVClient</code>).</p> </li> <li> <p>Leaking Abstraction: The <code>api</code> layer (<code>lifespan.py</code>) was becoming responsible for orchestrating <code>athomic</code>'s startup sequence (e.g., calling <code>prepare</code>, then <code>start</code>), which violates the principle of separation of concerns. The <code>api</code> should use <code>athomic</code>, not manage it.</p> </li> </ol>"},{"location":"adr/ADR-054-athomic-secrets-starting/#decision","title":"Decision","text":"<p>To address these issues, we decided to refactor the <code>athomic</code> architecture to adopt a set of more robust and explicit software design patterns.</p> <ol> <li> <p>Inversion of Control (IoC) with Dependency Injection (DI): This is the core principle of the new architecture. Components no longer create their own dependencies. Instead, they receive their dependencies ready-made via their constructors (<code>__init__</code>). This breaks circular imports and makes dependencies explicit.</p> </li> <li> <p>Centralized Composition Root: The responsibility of creating and \"wiring\" all infrastructure services has been centralized into a single function: <code>register_athomic_infra_services</code>. This function acts as the \"composition root,\" where the entire object graph of <code>athomic</code> is constructed in the correct order (e.g., the <code>SecretsProvider</code> is created first and then injected into others).</p> </li> <li> <p>Facade Pattern (<code>AthomicFacade</code>): To hide the complexity of initialization and provide a single, simple entry point for the <code>api</code> layer, the <code>AthomicFacade</code> class was created. The <code>api</code> now interacts only with this facade, calling simple methods like <code>await athomic.startup()</code>. The facade encapsulates the <code>SecretsManager</code> and <code>LifespanManager</code>.</p> </li> <li> <p>Lazy Secret Resolution with Proxies: To support secret rotation and keep services decoupled from the security module, the <code>SecretsManager</code> was implemented. At startup, it traverses the configuration and replaces secret references (<code>SecretValue</code>) with <code>CredentialProxy</code> objects, which act as lazy, asynchronous getters.</p> </li> <li> <p>Logic Reusability with Specific Inheritance: To avoid repeating the credential \"unwrapping\" code, the <code>CredentialResolve</code> base class was created. Services that need this functionality (like <code>MongoProvider</code> and <code>RedisKVClient</code>) inherit from it, gaining the <code>_resolve_credential</code> method without polluting the main <code>BaseService</code>.</p> </li> </ol>"},{"location":"adr/ADR-054-athomic-secrets-starting/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Circular Imports Eliminated: The primary technical blocker has been resolved, unblocking tests and development.</li> <li>Exceptional Testability: Services can now be tested in complete isolation by injecting mocks of their dependencies into their constructors.</li> <li>Clarity and Explicit Code: The \"magic\" is gone. The initialization flow is now explicit, orchestrated by the <code>AthomicFacade</code>, and each service's dependencies are clearly visible in its <code>__init__</code> signature.</li> <li>Robust Separation of Concerns: The <code>api</code> layer is now a clean consumer of the <code>athomic</code> layer. All complex infrastructure logic is fully encapsulated within <code>athomic</code>.</li> <li>DRY Principle Adhered To: Common logic for credential resolution and service lifecycle is centralized in reusable base classes and orchestrators.</li> <li>Support for Secret Rotation: The lazy-loading proxy pattern ensures that services always fetch the latest version of a secret, making the system compatible with dynamic secrets.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Indirection: The new architecture introduces more classes and patterns (Facade, DI, Factories, Proxies). This adds a layer of indirection that may require a slightly steeper learning curve for new developers on the project.</li> <li>Centralized Complexity: The \"Composition Root\" (<code>register_athomic_infra_services</code>) and the <code>AthomicFacade</code> become critical, central points of the architecture. Changes in these files can have a wide-ranging impact.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>The framework is now more \"opinionated\" about how services are constructed and managed. This enforces consistency but requires developers to follow the established patterns.</li> </ul> </li> </ul>"},{"location":"adr/ADR-056-api-key-client-and-shared-auth/","title":"ADR-056: Policy-Based, Decoupled Authentication and Authorization Service","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-28</li> </ul>"},{"location":"adr/ADR-056-api-key-client-and-shared-auth/#context","title":"Context","text":"<p>The application requires a robust, flexible, and testable system for securing API endpoints. The primary needs include: 1.  Multiple Security Levels: Endpoints must support different access levels: fully public, internal (accessible only by trusted IPs), and private (requiring a valid user token). 2.  Decoupling: The core security logic should be independent of the web framework (FastAPI) to promote portability, reusability, and easier unit testing. It must integrate with the existing Athomic infrastructure layer. 3.  Extensibility: The architecture must be extensible to support future authentication methods (e.g., API Keys, OAuth2) and more complex authorization rules (e.g., role-based or scope-based access) without requiring significant refactoring.</p> <p>Several alternatives were considered: * Logic within FastAPI Dependencies: Placing all validation logic directly inside FastAPI dependency functions. This was rejected as it leads to code duplication and tightly couples the security logic to the web framework. * Third-Party Auth Libraries: Using a comprehensive library like <code>fastapi-users</code>. This was rejected in favor of a custom solution that integrates more deeply with the Athomic layer's patterns, such as configuration management, secret resolution, and observability. * A Monolithic Auth Service: Creating a single service to handle both IP validation and token validation. This was rejected because it violates the Single Responsibility Principle and makes the system less flexible.</p>"},{"location":"adr/ADR-056-api-key-client-and-shared-auth/#decision","title":"Decision","text":"<p>We have decided to implement a decoupled, policy-based authentication and authorization service within the Athomic layer. This architecture is composed of several distinct components with clear responsibilities:</p> <ol> <li> <p>Separation of Authentication and Authorization:</p> <ul> <li>Authentication (<code>AuthProviderProtocol</code>): Responsible for validating credentials and proving identity. The primary implementation is <code>JWTAuth</code>, which handles the creation and verification of JSON Web Tokens. It is an abstraction that proves who a user is.</li> <li>Authorization (<code>AuthService</code> and <code>AuthExecutorProtocol</code>): Responsible for determining if a request is permitted to access a resource based on a given policy. This service decides what a user (or system) is allowed to do.</li> </ul> </li> <li> <p>Policy-Based Execution:</p> <ul> <li>The <code>AuthService</code> acts as an orchestrator. It does not contain any validation logic itself. Instead, it accepts a <code>policy_name</code> (e.g., \"private\", \"internal\") and uses a registry to find the appropriate <code>AuthExecutor</code>.</li> <li>Executors (<code>AuthExecutorProtocol</code>): Each executor implements the logic for a specific security policy. We have implemented three default executors:<ul> <li><code>PublicExecutor</code>: A no-op for public endpoints.</li> <li><code>InternalIPExecutor</code>: Enforces IP allow-listing by delegating to the <code>IPAllowlist</code> service.</li> <li><code>PrivateTokenExecutor</code>: Enforces that a valid token is present. It uses the <code>AuthProviderProtocol</code> to validate the token and returns an <code>AuthenticatedUser</code> model on success.</li> </ul> </li> </ul> </li> <li> <p>Framework Decoupling:</p> <ul> <li>The core security logic resides entirely within the <code>athomic</code> layer and has no dependency on FastAPI.</li> <li>A generic <code>SecurityRequestContext</code> model is used to pass request data (headers, client IP) to the <code>AuthService</code>, making the core logic portable.</li> <li>Integration with FastAPI is done exclusively at the application's edge, within the <code>api/dependencies/security.py</code> module. Dependencies like <code>require_token</code> and <code>require_internal_ip</code> adapt the FastAPI <code>Request</code> object into a <code>SecurityRequestContext</code> before calling the <code>AuthService</code>.</li> </ul> </li> </ol>"},{"location":"adr/ADR-056-api-key-client-and-shared-auth/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>High Decoupling &amp; Testability: Each component (AuthProvider, AuthService, Executors) can be unit-tested in complete isolation by mocking its dependencies. The core logic can be tested without a web server.</li> <li>Excellent Extensibility:<ul> <li>Adding a new authentication method (e.g., API Key) only requires creating a new class that implements <code>AuthProviderProtocol</code> and updating the factory.</li> <li>Adding a new authorization policy (e.g., role-based checks) is as simple as creating a new <code>AuthExecutor</code> and registering it.</li> </ul> </li> <li>Declarative and Clean Routes: The security rules at the API route level are highly declarative and readable (e.g., <code>Depends(require_token)</code>), hiding the underlying complexity.</li> <li>Single Responsibility Principle: Each class has a single, well-defined purpose, making the system easier to understand and maintain.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Boilerplate: This decoupled approach introduces more classes and files (Protocols, Factories, Registries, Services, Executors) compared to a monolithic solution. This can present a slightly steeper learning curve for new developers.</li> <li>Potential for Over-Engineering: If the application's security needs remain extremely simple, this layered architecture could be considered more complex than necessary. However, it provides a solid foundation for future growth.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This architecture strongly enforces the distinction between Authentication (\"who you are\") and Authorization (\"what you can do\"), which is a core security principle.</li> </ul> </li> </ul>"},{"location":"adr/ADR-057-bulkhead-pattern/","title":"ADR-057: Bulkhead Pattern for Concurrency Control","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-28</li> </ul>"},{"location":"adr/ADR-057-bulkhead-pattern/#context","title":"Context","text":"<p>The Athomic layer provides several resilience patterns like Retry and Circuit Breaker. However, the application remains vulnerable to a specific type of cascading failure: resource exhaustion due to a slow or unresponsive dependency.</p> <p>If a downstream service becomes slow, all concurrent requests to it can hang, consuming finite resources in our application (e.g., database connections, memory, or simply overwhelming the asyncio event loop with pending tasks). This can starve other, unrelated parts of the application of resources, causing them to fail as well. The Circuit Breaker pattern is reactive and only trips after a certain number of failures, by which time resources may already be exhausted.</p> <p>We need a proactive mechanism to limit the number of concurrent executions for a given operation, thereby isolating the fault to one part of the system.</p> <p>Alternatives Considered:</p> <ul> <li>Relying on Timeouts and Circuit Breakers alone: This was rejected because it's a reactive approach. A surge of requests can still exhaust system resources before timeouts are hit or the circuit breaker trips.</li> <li>Using the existing Rate Limiter: This was rejected because Rate Limiting and Bulkheading solve different problems. A rate limiter restricts the rate of calls over time (e.g., 100 requests/minute), whereas a bulkhead restricts the number of concurrent calls at any single moment. They are complementary patterns.</li> </ul>"},{"location":"adr/ADR-057-bulkhead-pattern/#decision","title":"Decision","text":"<p>We will implement the Bulkhead pattern to isolate concurrent operations and prevent resource exhaustion. The implementation will be native to the Athomic resilience module and follow our established architectural patterns.</p> <ol> <li> <p>Core Implementation: The bulkhead will be implemented using <code>asyncio.Semaphore</code> for each defined policy. This is a lightweight and efficient mechanism for controlling concurrency in an <code>asyncio</code>-based application.</p> </li> <li> <p>Configuration: A new <code>BulkheadSettings</code> schema will be added to <code>resilience.resilience_config</code>. This will allow developers to:</p> <ul> <li>Enable or disable the pattern globally.</li> <li>Set a <code>default_limit</code> for concurrency.</li> <li>Define a dictionary of named <code>policies</code> with specific integer limits (e.g., <code>policies = {\"payment_api\": 5, \"user_service\": 15}</code>).</li> </ul> </li> <li> <p>Service and Factory:</p> <ul> <li>A singleton <code>BulkheadService</code> will be responsible for creating and managing the <code>asyncio.Semaphore</code> instances based on the loaded configuration.</li> <li>A <code>BulkheadFactory</code> will provide access to the singleton <code>BulkheadService</code> instance.</li> </ul> </li> <li> <p>Developer API (<code>@bulkhead</code> decorator):</p> <ul> <li>A simple decorator, <code>@bulkhead(policy=\"...\")</code>, will be provided. Developers can apply this to any <code>async def</code> function.</li> <li>The decorator will use the <code>BulkheadService</code> to acquire a semaphore slot before executing the function.</li> <li>If a slot cannot be acquired immediately (i.e., the bulkhead is full), a <code>BulkheadRejectedError</code> exception will be raised, which can be caught at the API layer and translated into an appropriate HTTP response (e.g., <code>503 Service Unavailable</code> or <code>429 Too Many Requests</code>).</li> </ul> </li> <li> <p>Observability:</p> <ul> <li>The <code>BulkheadService</code> will be instrumented with Prometheus metrics to track:<ul> <li><code>nala_bulkhead_concurrent_requests</code> (Gauge): The number of currently active requests for each policy.</li> <li><code>nala_bulkhead_calls_total</code> (Counter): The total number of calls that were <code>accepted</code> or <code>rejected</code> by each policy.</li> </ul> </li> </ul> </li> </ol>"},{"location":"adr/ADR-057-bulkhead-pattern/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Fault Isolation: Prevents a slow or failing dependency from causing a cascading failure that affects the entire application. The impact is contained within the bulkhead.</li> <li>Resource Protection: Proactively protects the application from resource exhaustion (memory, connections, event loop saturation) under high load or during partial system degradation.</li> <li>Declarative API: The <code>@bulkhead</code> decorator provides a clean, simple, and declarative way for developers to apply concurrency limits without needing to manage semaphores manually.</li> <li>Enhanced Observability: New metrics will provide direct insight into which parts of the application are under high concurrency pressure, helping to identify bottlenecks.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Configuration Overhead: Requires developers to analyze their services and define sensible concurrency limits. A limit that is set too low can unnecessarily throttle the application and reduce performance.</li> <li>Introduces a new failure mode: Requests can now be explicitly rejected with a <code>BulkheadRejectedError</code>, which must be handled gracefully by the calling code (e.g., at the API layer).</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This pattern is highly complementary to the existing Circuit Breaker and Retry patterns. A typical flow might be: <code>Retry</code> -&gt; <code>Circuit Breaker</code> -&gt; <code>Bulkhead</code> -&gt; <code>Function Execution</code>. The bulkhead ensures that even retries do not overwhelm a service.</li> </ul> </li> </ul>"},{"location":"adr/ADR-058-first-class-distributed-lock-decorator/","title":"ADR-058: First-Class Distributed Lock Decorator","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-29</li> </ul>"},{"location":"adr/ADR-058-first-class-distributed-lock-decorator/#context","title":"Context","text":"<p>The application needs to perform business-critical operations that are not idempotent (e.g., processing a payment, updating inventory). In a distributed environment with multiple running instances, there is a significant risk of race conditions, where two processes attempt to modify the same resource simultaneously, leading to data corruption or inconsistent states.</p> <p>While the <code>athomic</code> layer contained an underlying locking mechanism (<code>LockingFactory</code>, <code>RedisLockProvider</code>) used internally by the caching decorator , it was not exposed as a first-class, easy-to-use feature for developers to protect their business logic. The goal was to provide a simple, declarative way to enforce mutual exclusion for critical code sections.</p>"},{"location":"adr/ADR-058-first-class-distributed-lock-decorator/#decision","title":"Decision","text":"<p>We have decided to implement a high-level <code>@distributed_lock</code> decorator to expose the existing locking functionality as a primary resilience feature.</p> <ol> <li> <p>Decorator Implementation: A decorator named <code>@distributed_lock</code> was created in <code>athomic/resilience/locking/decorator.py</code>. It accepts two main arguments:</p> <ul> <li><code>key</code>: A string template that is formatted at runtime using the arguments of the decorated function. This allows for dynamic, resource-specific keys (e.g., <code>key=\"payment:{payment_id}\"</code>).</li> <li><code>timeout</code>: An integer specifying the maximum time in seconds to wait to acquire the lock.</li> </ul> </li> <li> <p>Core Logic: The decorator wraps the target <code>async</code> function. Before execution, it resolves the lock key, obtains a lock provider from the existing <code>LockingFactory</code> , and attempts to acquire the lock using <code>async with provider.acquire(...)</code>.</p> </li> <li> <p>Exception Handling: If the lock cannot be acquired within the specified timeout (the provider raises <code>asyncio.TimeoutError</code>), the decorator catches it and raises a new, specific <code>LockAcquisitionError</code> exception. This allows the application layer (e.g., an API route) to catch this specific error and return a meaningful response, such as <code>HTTP 409 Conflict</code>.</p> </li> <li> <p>Exporting: The decorator and its exception were made easily accessible by exporting them from the top-level <code>athomic/resilience/__init__.py</code> module.</p> </li> </ol>"},{"location":"adr/ADR-058-first-class-distributed-lock-decorator/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Developer Experience: Protecting a critical section from race conditions is now reduced to a single, declarative decorator line, significantly simplifying development.</li> <li>Safety and Correctness: Prevents data corruption and inconsistent states in distributed environments by ensuring mutual exclusion for critical operations.</li> <li>Reusability: Leverages the existing and tested <code>LockingFactory</code> and providers (<code>RedisLockProvider</code>, <code>LocalLockProvider</code>) .</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>New Failure Mode: Introduces a new, expected failure mode (<code>LockAcquisitionError</code>) that developers must handle. API endpoints using this decorator need <code>try...except</code> blocks to translate this error into a user-friendly response (e.g., HTTP 409).</li> <li>Performance Overhead: Each decorated call introduces a small overhead due to the network round-trip to acquire and release the lock from Redis.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>The reliability of this feature is now directly tied to the availability and performance of the underlying KVStore (Redis).</li> </ul> </li> </ul>"},{"location":"adr/ADR-059-high-Level-ackground-task-abstraction-decorator/","title":"ADR-059: High-Level Background Task Abstraction (@task)","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-29</li> </ul>"},{"location":"adr/ADR-059-high-Level-ackground-task-abstraction-decorator/#context","title":"Context","text":"<p>Many API operations need to trigger long-running or non-essential side effects (e.g., sending a welcome email, processing an image, generating a report). Executing these tasks synchronously within the request-response cycle increases API latency and couples the API's success to the success of the side effect.</p> <p>The <code>athomic</code> layer already had the low-level infrastructure to solve this, including a <code>TaskBrokerProtocol</code> , a <code>get_task_broker</code> factory , and robust context propagation utilities (<code>capture_context_for_task</code>, <code>restore_context_from_task</code>) . However, using these components directly was manual and required significant boilerplate from the developer. The goal was to create a high-level, developer-friendly API similar to established libraries like Celery.</p>"},{"location":"adr/ADR-059-high-Level-ackground-task-abstraction-decorator/#decision","title":"Decision","text":"<p>We have decided to implement a two-part abstraction layer to simplify the definition and execution of background tasks.</p> <ol> <li> <p>Client-Side Decorator (<code>@task</code>):</p> <ul> <li>A decorator named <code>@task</code> was created in <code>athomic/integration/tasks/decorator.py</code>.</li> <li>This decorator wraps an <code>async</code> function and returns a <code>TaskWrapper</code> object. This object retains the original callable behavior but adds a new <code>.delay()</code> method.</li> <li>The <code>.delay(*args, **kwargs)</code> method is the primary interface for enqueuing a job. It automatically performs the following steps:<ol> <li>Calls <code>get_task_broker()</code> to get the configured provider.</li> <li>Calls <code>capture_context_for_task()</code> to serialize the current execution context (trace ID, tenant ID, etc.).</li> <li>Bundles the captured context into the task's arguments under a reserved key (<code>_nala_context</code>).</li> <li>Calls the provider's <code>enqueue_task</code> method with the function's fully qualified name, arguments, and the propagated context.</li> </ol> </li> </ul> </li> <li> <p>Worker-Side Helper (<code>run_task_with_context</code>):</p> <ul> <li>A helper function/decorator named <code>run_task_with_context</code> was created in <code>athomic/integration/tasks/worker.py</code>.</li> <li>This wrapper is intended to be used in the background worker process.</li> <li>It inspects the incoming task's arguments, extracts the <code>_nala_context</code> dictionary, and uses the <code>restore_context_from_task</code> context manager to re-establish the execution context before calling the original task logic.</li> </ul> </li> </ol>"},{"location":"adr/ADR-059-high-Level-ackground-task-abstraction-decorator/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Drastically Simplified DX: Defining and dispatching background tasks is now a simple, declarative process (<code>@task</code> and <code>.delay()</code>), abstracting away all the complexity of brokers and context management.</li> <li>Improved API Performance: Allows API endpoints to offload work and respond to users more quickly.</li> <li>Guaranteed Observability: The automatic context propagation ensures that logs and distributed traces generated within a background task are correctly correlated with the original request that triggered them.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Infrastructure Complexity: The system now formally relies on an external message broker (like Redis/Kafka for Celery/RQ) and a separate, running worker process, which adds operational overhead.</li> <li>Layer of Indirection: The decorator adds a layer of \"magic\" that might require developers to understand the underlying mechanism if debugging is needed.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This decision formalizes the \"Background Task\" as a core pattern within the application's architecture.</li> </ul> </li> </ul>"},{"location":"adr/ADR-060-dynamic-configuration-hot-reload-system/","title":"ADR-060: Dynamic Configuration (Hot Reload) System","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-29</li> </ul>"},{"location":"adr/ADR-060-dynamic-configuration-hot-reload-system/#context","title":"Context","text":"<p>The application's configuration is loaded from static files (<code>.toml</code>) at startup using Dynaconf . This model is robust but rigid. Modifying any runtime behavior controlled by configuration (e.g., rate limits, circuit breaker thresholds, log levels, feature flags) requires a service restart or a new deployment. This introduces downtime and reduces operational agility, especially when responding to incidents.</p> <p>The goal was to create a system that could update its configuration at runtime by observing changes in a central, external source.</p>"},{"location":"adr/ADR-060-dynamic-configuration-hot-reload-system/#decision","title":"Decision","text":"<p>We have decided to implement a multi-component system for dynamic configuration that leverages existing <code>athomic</code> modules.</p> <ol> <li> <p>Provider Enhancement: The low-level <code>ConfigurationProviderProtocol</code> was enhanced with a <code>watch(key, index)</code> method to support long-polling. This was implemented in the <code>ConsulConfigProvider</code> using the native long-polling capabilities of the Consul client . Caching was explicitly removed from this provider to ensure it always fetches the latest state.</p> </li> <li> <p><code>ConfigWatcherService</code> (The Watcher):</p> <ul> <li>A new background service, inheriting from <code>BaseService</code>, was created in <code>athomic/control/live_config/service.py</code>.</li> <li>Its <code>_run_loop</code> starts a concurrent task for each key specified in the new <code>control.live_config.keys_to_watch</code> configuration section.</li> <li>Each task calls the provider's <code>watch</code> method in an infinite loop. When a change is detected, it publishes a <code>config.updated</code> event to the internal <code>EventBus</code>.</li> </ul> </li> <li> <p><code>DynamicConfigService</code> (The Manager):</p> <ul> <li>A singleton service was created, also in <code>service.py</code>.</li> <li>Upon initialization, it flattens the entire static <code>AppSettings</code> object into an in-memory key-value dictionary to serve as the initial state.</li> <li>It subscribes to the <code>config.updated</code> event on the <code>EventBus</code> . The event handler updates the corresponding key in its in-memory state.</li> <li>It exposes a simple <code>get(key, default)</code> method, which acts as the new, single source of truth for runtime configuration.</li> </ul> </li> <li> <p>Module Adaptation:</p> <ul> <li>To prove the concept and enable dynamic control, the <code>RateLimiterService</code> was refactored. Instead of reading its policy limits from its static settings object, its <code>check</code> method now calls <code>live_config_service.get(...)</code> on every execution to retrieve the most up-to-date limit string .</li> </ul> </li> </ol>"},{"location":"adr/ADR-060-dynamic-configuration-hot-reload-system/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>High Operability: Allows operators to change application behavior (e.g., tighten rate limits, adjust timeouts) in real-time without downtime, which is critical for incident response and performance tuning.</li> <li>Reduced Deployments: Simple configuration changes no longer require a full deployment cycle.</li> <li>Architecturally Sound: The solution is highly decoupled, cleanly separating the concerns of watching, managing state, and consuming the configuration. It leverages existing components like the <code>EventBus</code> and <code>ConsulClient</code>.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Complexity: The architecture now has more moving parts (a watcher service, event bus communication) which can be more complex to debug than a simple static configuration.</li> <li>Dependency on External System: The application's runtime stability becomes more tightly coupled to the availability and correctness of the external configuration store (Consul).</li> <li>Potential for Transient Inconsistency: In a distributed system, there can be a small delay between a config change and all service instances receiving the update.</li> </ul> </li> </ul>"},{"location":"adr/ADR-061-transparent-dynamic-configuration-system-with-live-config-model/","title":"ADR-061: Transparent Dynamic Configuration System with <code>LiveConfigModel</code>","text":"<p>Status: Accepted Date: 2025-07-29</p> <p>Context: ADR-060 introduced a dynamic configuration system that allowed real-time parameter updates. The initial implementation, while functional, required consuming services to explicitly know about <code>LiveConfigService</code> and use its <code>.get()</code> method to fetch values, resulting in more verbose and tightly coupled code. The goal evolved to find an architectural solution that would make dynamic configuration access completely transparent to consumer code, improving maintainability and developer experience.</p> <p>Decision: We decided to implement a transparent dynamic configuration pattern, where the logic for fetching updated values is encapsulated within the Pydantic configuration schemas themselves. The architecture consists of four pillars:</p> <ol> <li> <p><code>LiveConfigModel</code> (The Central Piece):</p> <ul> <li>A new base class, <code>LiveConfigModel</code>, was created, inheriting from <code>pydantic.BaseModel</code>.</li> <li>This class overrides the <code>__getattribute__</code> method to intercept access to all its fields.</li> <li>The internal logic checks if the accessed field is mapped as \"dynamic\". If so, it fetches the latest value from <code>LiveConfigService</code>; otherwise, it returns the static value loaded from the configuration file at startup.</li> <li>This makes dynamic behavior completely transparent to code using the settings object.</li> </ul> </li> <li> <p>Declarative Configuration via TOML:</p> <ul> <li>The \"wiring\" of which fields are dynamic was moved from Python code to the <code>settings.toml</code> file.</li> <li>Each configuration section can now contain an optional map called <code>_live_keys_config</code>, which associates a Pydantic schema attribute with its corresponding key in <code>LiveConfigService</code>.</li> <li>The <code>__init__</code> of <code>LiveConfigModel</code> reads this map during instantiation of the settings object.</li> </ul> <p>Example in <code>settings.toml</code>: ```toml [resilience.circuit_breaker]</p> </li> <li> <p><code>LiveConfigService</code> (The State Guardian):</p> <ul> <li>The former <code>DynamicConfigService</code> was renamed to <code>LiveConfigService</code> for clarity.</li> <li>It remains the singleton responsible for maintaining the state dictionary (<code>_config_state</code>) with the latest configuration values, updated by events from the <code>EventBus</code>.</li> </ul> </li> <li> <p><code>ConfigWatcher</code> (The Observer):</p> <ul> <li>The former <code>ConfigWatcherService</code> was renamed to <code>ConfigWatcher</code>.</li> <li>It continues as the background service that monitors an external source (e.g., Consul), reading the list of keys from <code>control.live_config.keys_to_watch</code> and publishing <code>config.updated</code> events when changes are detected.</li> </ul> </li> </ol> <p>Data Flow: 1.  An operator changes a value in Consul. 2.  <code>ConfigWatcher</code> detects the change and publishes a <code>config.updated</code> event on the <code>EventBus</code>. 3.  <code>LiveConfigService</code> (subscribed to the event) receives the payload and updates its internal state dictionary. 4.  Anywhere in the application, code accesses an attribute of a settings object (e.g., <code>settings.resilience.circuit_breaker.default_fail_max</code>). 5.  The <code>__getattribute__</code> of <code>LiveConfigModel</code> is triggered, queries <code>LiveConfigService</code>, and returns the latest value.</p> <p>Consequences:</p> <p>Positives: * Total Transparency: Code consuming configurations is extremely clean and simple (e.g., <code>settings.some_value</code>). It does not need to know whether the value is static or dynamic. * Maximum Decoupling: Services (<code>RateLimiterService</code>, <code>CircuitBreakerService</code>, etc.) no longer have any direct dependency on <code>LiveConfigService</code>. * Centralized Configuration: The decision to make a field dynamic is made entirely in the <code>.toml</code> file, which is the appropriate place for such \"wiring\". * Robustness: The implementation of <code>LiveConfigModel</code> was tested to avoid <code>RecursionError</code> with internal attributes, making it safe for general use. * High Maintainability: Adding new dynamic fields only requires a change in the <code>.toml</code> and ensuring the schema inherits from <code>LiveConfigModel</code>, with no need to alter service business logic.</p> <p>Negatives: * Internal Complexity: The implementation of <code>LiveConfigModel</code> uses metaprogramming (<code>__getattribute__</code>), which is an advanced part of Python and may be harder to understand for less experienced developers. * Small Overhead: There is a small but measurable performance cost for each access to a dynamic attribute due to the extra logic in <code>__getattribute__</code>. However, for configuration objects, this cost is considered negligible. * \"Magic\" Behavior: Because it is so transparent, it may not be immediately obvious to a new developer that the value of an attribute can change in real time. Good internal documentation is essential.</p>"},{"location":"adr/ADR-061-transparent-dynamic-configuration-system-with-live-config-model/#map-that-enables-dynamicity-for-fields-in-this-schema","title":"Map that enables \"dynamicity\" for fields in this schema","text":"<p>_live_keys_config = { default_fail_max = \"resilience.circuit_breaker.default_fail_max\" }</p>"},{"location":"adr/ADR-061-transparent-dynamic-configuration-system-with-live-config-model/#static-value-used-at-initialization-and-as-fallback","title":"Static value used at initialization and as fallback","text":"<p>DEFAULT_FAIL_MAX = 5 ```</p>"},{"location":"adr/ADR-062-resilient-and-configurable-http-client/","title":"ADR-062: Resilient and Configurable HTTP Client","text":"<p>Status: Accepted</p> <p>Date: 2025-07-31</p>"},{"location":"adr/ADR-062-resilient-and-configurable-http-client/#context","title":"Context","text":"<p>Application services frequently need to communicate with external or internal APIs via HTTP. Without a standardized approach, developers would implement their own clients, leading to inconsistent handling of critical cross-cutting concerns such as:</p> <ul> <li>Resilience: Ad-hoc implementations of retries and timeouts, with no circuit breaker pattern.</li> <li>Observability: Lack of standardized tracing, metrics, and structured logging for outbound requests.</li> <li>Authentication: Inconsistent and insecure handling of credentials like API keys and bearer tokens.</li> <li>Caching: No unified strategy for caching responses from external services.</li> <li>Configuration: Settings like base URLs and timeouts would be scattered and managed inconsistently.</li> </ul> <p>This would result in a codebase that is brittle, difficult to debug, and lacks visibility into the performance and reliability of its integrations. The <code>athomic</code> layer, which already standardizes other infrastructure concerns, was the logical place to provide a centralized solution.</p>"},{"location":"adr/ADR-062-resilient-and-configurable-http-client/#decision","title":"Decision","text":"<p>We decided to create a new, comprehensive <code>http</code> module within the <code>athomic</code> layer. This module provides a <code>HttpClientFactory</code> as a single entry point for creating named, fully-featured, and production-ready HTTP client instances.</p> <p>The architecture follows established patterns within the <code>athomic</code> framework to ensure consistency and extensibility:</p> <ol> <li> <p>Provider-Agnostic Design:</p> <ul> <li>The core logic is decoupled from the underlying HTTP library. A generic <code>HttpResponse</code> Pydantic model is used as the data transfer object between the provider and the base layer.</li> <li>A <code>HttpxProvider</code> is created as the concrete implementation, using the modern <code>httpx</code> library. The architecture allows for other providers (e.g., <code>AiohttpProvider</code>) to be added in the future.</li> </ul> </li> <li> <p>Centralized Orchestration in a Base Class:</p> <ul> <li>An abstract <code>HTTPClientBase</code> class encapsulates all shared logic, ensuring every client instance automatically benefits from:<ul> <li>Observability: Automatic creation of OpenTelemetry traces and Prometheus metrics for every request.</li> <li>Context Propagation: Automatic forwarding of context headers (<code>x-request-id</code>, <code>x-tenant-id</code>, etc.) on all outgoing requests.</li> <li>Lifecycle Management: Integration with <code>BaseService</code> for managed startup and shutdown.</li> </ul> </li> </ul> </li> <li> <p>Configurable, Injected Resilience:</p> <ul> <li>Instead of hardcoding resilience logic, we created a generic <code>ResilienceOrchestrator</code>.</li> <li>The <code>HttpClientFactory</code> reads the client's configuration and creates a <code>ResilienceOrchestrator</code> instance with the appropriate <code>RetryHandler</code> and <code>CircuitBreakerService</code> based on named policies defined in <code>.toml</code> files.</li> <li>This orchestrator is then injected into the <code>HTTPClientBase</code>, completely decoupling the client from the resilience implementation.</li> </ul> </li> <li> <p>Strategy Pattern for Authentication:</p> <ul> <li>An <code>AuthStrategyProtocol</code> defines the contract for authentication methods.</li> <li>The <code>HttpClientFactory</code> instantiates and injects the correct strategy (e.g., <code>BearerTokenAuth</code>, <code>APIKeyAuth</code>) based on the client's configuration.</li> <li>This pattern reuses the existing <code>CredentialResolve</code> logic for secure handling of secrets.</li> </ul> </li> <li> <p>Dynamic, Configurable Caching:</p> <ul> <li>The <code>HttpClientFactory</code> can dynamically apply the existing <code>@cache</code> decorator to a client instance's <code>get</code> method if caching is configured for it in the settings via a <code>CacheSettings</code> object.</li> </ul> </li> </ol>"},{"location":"adr/ADR-062-resilient-and-configurable-http-client/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-062-resilient-and-configurable-http-client/#positive","title":"Positive","text":"<ul> <li>Developer Experience: Developers can now obtain a fully resilient, observable, and authenticated HTTP client with a single line of code (<code>HttpClientFactory.create('my_api')</code>), drastically reducing boilerplate and letting them focus on business logic.</li> <li>Standardization &amp; Consistency: All outbound HTTP calls now behave uniformly, making the system more predictable and easier to maintain.</li> <li>Increased Robustness: The built-in retry and circuit breaker patterns make the application significantly more resilient to transient failures in downstream services.</li> <li>Deep Observability: Every HTTP call is now a \"white box\", with detailed metrics and traces available out-of-the-box, simplifying debugging and performance monitoring.</li> <li>High Flexibility: Every aspect of a client (timeout, auth, retry policy, cache TTL, provider) is configurable via <code>.toml</code> files on a per-client basis.</li> <li>Extensibility: The architecture makes it easy to add new providers, authentication methods, or resilience strategies in the future with minimal changes to existing code.</li> </ul>"},{"location":"adr/ADR-062-resilient-and-configurable-http-client/#negative","title":"Negative","text":"<ul> <li>Increased Abstraction: The new module introduces several layers of abstraction (<code>Factory</code>, <code>Base</code>, <code>Provider</code>, <code>Orchestrator</code>, <code>Strategy</code>). This adds a learning curve for new developers to understand the complete flow.</li> <li>New Test Dependency: The <code>pytest-httpx</code> library is now a mandatory development dependency for writing tests against any service that uses the new HTTP client.</li> </ul>"},{"location":"adr/ADR-063-dynamic-plugin-system/","title":"ADR-063: Dynamic Plugin System","text":"<p>Status: Proposed</p> <p>Context: The <code>athomic-docs</code> is evolving into a central platform that will serve multiple products and teams. There is a need to extend the core functionality in an isolated and scalable way, without having to modify the main source code for each new product-specific feature.</p> <p>Decision: We will implement a plugin system based on Python's <code>entry_points</code>. The core application (<code>athomic-docs</code>) will define an <code>entry_point</code> group (e.g., <code>\"nala.plugins\"</code>). Plugins will be independent, installable Python packages that register themselves in this <code>entry_point</code>.</p> <p>During initialization, the application will discover and load these plugins dynamically, allowing them to extend system functionality, such as registering new API routes, adding providers, etc.</p> <p>Consequences: - Positive:     - Decoupling: The core remains agnostic to product-specific features.     - Development Scalability: Teams can develop, test, and distribute plugins independently.     - Flexibility: Different instances of <code>athomic-docs</code> can have different sets of features enabled simply by installing the required plugins. - Negative:     - Initial Complexity: Introduces a layer of indirection and requires plugins to follow a clear interface contract.     - Dependency Management: Dependencies of plugins must be managed and compatibility with the core version ensured.</p>"},{"location":"adr/ADR-064-replace-celery-with-taskiq-for-asynchronous-task-processing/","title":"ADR-001: Replace Celery with Taskiq for Asynchronous Task Processing","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-06</li> </ul>"},{"location":"adr/ADR-064-replace-celery-with-taskiq-for-asynchronous-task-processing/#context","title":"Context","text":"<p>Our application's architecture is heavily based on <code>asyncio</code> for high-performance I/O-bound operations. The existing task queue system, Celery, while powerful, was not originally designed for an <code>asyncio</code>-native world. This has led to several challenges: * Asynchronous Code Complexity: Integrating <code>asyncio</code> code with Celery requires careful management of event loops, which can add complexity and potential performance overhead. * Legacy Tooling: Celery's dependency and configuration management feel disconnected from our modern stack, which relies on Pydantic for validation and FastAPI-style dependency injection for clear, testable code. * Verbosity: Setting up and configuring Celery tasks often involves more boilerplate code compared to more modern alternatives.</p> <p>We considered the following alternatives: 1.  Keep Celery: Continue with the current implementation. This was rejected due to the ongoing friction with our <code>asyncio</code>-first architecture and the desire for a more streamlined developer experience. 2.  RQ (Redis Queue): A simpler alternative to Celery. While robust for basic queuing, it lacks the sophisticated <code>asyncio</code> integration and built-in dependency injection system that Taskiq provides. 3.  Taskiq: A modern, <code>asyncio</code>-native task queue library. It is built from the ground up with <code>asyncio</code> support and integrates seamlessly with modern Python tooling like Pydantic and dependency injection.</p> <p>The primary driver for this change is to adopt a task queue system that aligns perfectly with our core architectural principles of being asynchronous, strongly typed, and leveraging dependency injection.</p>"},{"location":"adr/ADR-064-replace-celery-with-taskiq-for-asynchronous-task-processing/#decision","title":"Decision","text":"<p>We will officially adopt Taskiq as the standard library for all asynchronous background task processing.</p> <p>This transition will be implemented by: 1.  Integrating a new <code>TaskiqTaskBroker</code> provider into our \"Athomic\" infrastructure layer. This provider is configurable, allowing developers to select the underlying Redis transport mechanism (e.g., <code>RedisStreamBroker</code> or <code>ListQueueBroker</code>) via application settings. 2.  Writing all new asynchronous tasks using the Taskiq framework. 3.  Gradually migrating existing Celery tasks to Taskiq as modules are updated or refactored. Both systems may coexist during a transitional period.</p>"},{"location":"adr/ADR-064-replace-celery-with-taskiq-for-asynchronous-task-processing/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Native <code>asyncio</code> Integration: Code for tasks will be cleaner, more performant for I/O-bound operations, and will eliminate the complexity of managing separate event loops.</li> <li>Modern Developer Experience: Taskiq's use of Pydantic for parameter validation  and its FastAPI-style dependency injection system (<code>Depends</code>)  will make tasks easier to write, read, and test.</li> <li>Architectural Consistency: The new provider fits perfectly within our existing <code>TaskBroker</code> abstraction, requiring no changes to the application-level code that dispatches tasks.</li> <li>Increased Flexibility: The implementation allows for easy configuration of different backend brokers (e.g., Streams, Lists) without code changes, adapting to different reliability requirements.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Migration Effort: There is a non-trivial effort required to rewrite all existing Celery tasks into the Taskiq format. This will need to be planned and executed over time.</li> <li>Smaller Ecosystem: Celery has a very mature and large ecosystem of plugins and a larger community knowledge base. Taskiq, being newer, has a smaller community and fewer third-party integrations. This might require us to build custom solutions for niche problems that Celery plugins might have already solved.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>Team Learning: The development team will need to familiarize themselves with the Taskiq API and best practices. However, given its similarity to FastAPI, the learning curve is expected to be low.</li> <li>Monitoring and Observability: Existing monitoring tools for Celery (like Flower) will need to be replaced. We will leverage Taskiq's built-in Prometheus middleware  to integrate with our existing observability stack.</li> </ul> </li> </ul>"},{"location":"adr/ADR-065-context-drive-timeout-management/","title":"ADR-065: Context-Driven Timeout Management for Services","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-09</li> </ul>"},{"location":"adr/ADR-065-context-drive-timeout-management/#context","title":"Context","text":"<p>The framework's core services, inheriting from <code>BaseService</code>, initially managed their own timeouts through a <code>timeout</code> parameter in lifecycle methods like <code>start()</code>, <code>stop()</code>, and <code>wait_ready()</code>. The implementation within <code>BaseService</code> would use <code>asyncio.wait_for</code> to enforce this limit.</p> <p>This pattern presented several architectural challenges: 1.  Lack of Overall Request Budget: Each service call had its own isolated timeout. A sequence of calls (e.g., connect to DB, call external API, publish to Kafka) could cumulatively exceed the total time a user or system was willing to wait, leading to wasted server resources on requests that would ultimately time out at a higher level (e.g., at the load balancer). 2.  Coupled Concerns: The service methods were responsible for both their primary function (e.g., starting) and for managing their own timeout logic. This violates the Single Responsibility Principle. The responsibility for how long to wait should belong to the caller (orchestrator), not the callee. 3.  Inconsistent Application: While core services had this parameter, business logic functions and other I/O operations required a separate <code>@timeout</code> decorator, leading to two different ways of handling timeouts.</p>"},{"location":"adr/ADR-065-context-drive-timeout-management/#decision","title":"Decision","text":"<p>We decided to refactor the timeout mechanism to be caller-driven and context-aware, removing the <code>timeout</code> parameter from all service method signatures.</p> <ol> <li> <p>Caller-Driven Timeouts: The responsibility of enforcing a timeout is moved to the component that calls the service method. For the application's main lifecycle, the <code>LifecycleManager</code> now wraps each <code>service.start()</code> and <code>service.stop()</code> call within an <code>async with asyncio.timeout()</code> block.</p> </li> <li> <p>Context-Aware Timeouts for Operations: For granular control within a request's lifecycle, we will leverage <code>context_vars</code>.</p> <ul> <li>A <code>timeout_deadline</code> is set in the context at the beginning of a request, typically in a middleware. This deadline represents the absolute time when the entire operation must be completed.</li> <li>A new decorator, <code>@cancellable_operation</code>, was created. It reads the <code>deadline</code> from the context, calculates the remaining time, and applies it as a dynamic timeout to the decorated function using <code>asyncio.wait_for</code>.</li> <li>If the deadline has already passed when the function is called, it raises a <code>CancelledException</code> immediately (fail-fast).</li> </ul> </li> </ol> <p>This unified approach is applied to all potentially long-running, asynchronous I/O operations, such as database queries, external API calls, and interactions with infrastructure like Vault.</p>"},{"location":"adr/ADR-065-context-drive-timeout-management/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Centralized Timeout Control: The overall timeout policy is now managed by orchestrators (like the <code>LifecycleManager</code> or API middleware) instead of being scattered across dozens of service implementations.</li> <li>Resource Savings (Fail-Fast): The system now respects the entire request's \"timeout budget\". Operations late in a call chain will fail immediately if the budget is exceeded, preventing wasted CPU and I/O on doomed requests.</li> <li>Cleaner Code and API: Service method signatures are simpler (<code>start()</code> instead of <code>start(timeout=...)</code>). The internal logic of <code>BaseService</code> is also simplified, as it no longer needs to catch <code>asyncio.TimeoutError</code> itself.</li> <li>Consistency: We now have a single, consistent pattern for handling timeouts on any <code>async</code> operation, whether it's a service lifecycle method or a business logic function.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Caller Responsibility: Developers must remember to wrap service calls in a timeout context where appropriate. This is mitigated by centralizing most lifecycle calls within the already-refactored <code>LifecycleManager</code>.</li> <li>Test Refactoring: All unit and integration tests that previously passed a <code>timeout</code> parameter to service methods had to be updated to either remove the parameter or wrap the call in <code>asyncio.wait_for</code> to test the timeout behavior correctly.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>The framework now relies more heavily on standard <code>asyncio</code> patterns (<code>asyncio.timeout</code>). This increases consistency but requires developers working on the core framework to be familiar with them.</li> </ul> </li> </ul>"},{"location":"adr/ADR-066-multi-backend-task-scheduling-system/","title":"ADR-066: Multi-Backend Task Scheduling System","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-13</li> </ul>"},{"location":"adr/ADR-066-multi-backend-task-scheduling-system/#context","title":"Context","text":"<p>The application required a reliable and flexible system to execute tasks at a future time or after a specific delay. This is essential for features like sending follow-up notifications, generating periodic reports, and handling other asynchronous time-based operations. The initial challenge was to design a system that was not tightly coupled to a single technology (like Redis or a specific message broker), allowing for future extensibility without refactoring the application code that schedules the tasks. Alternatives considered were implementing scheduling logic directly within specific services, which would lead to code duplication, or relying on a single third-party library, which would limit future architectural choices.</p>"},{"location":"adr/ADR-066-multi-backend-task-scheduling-system/#decision","title":"Decision","text":"<p>We decided to create a new, abstracted <code>scheduler</code> module within the <code>athomic</code> framework. The architecture is built on these key principles:</p> <ol> <li>SchedulerProtocol: A clear interface defining the core capabilities (<code>schedule</code>, <code>schedule_at</code>, <code>cancel</code>).</li> <li>Factory and Creator Patterns: A <code>SchedulerFactory</code> is used to create the scheduler instance. It delegates the actual instantiation logic to dedicated <code>Creator</code> classes (<code>KVSchedulerCreator</code>, <code>DramatiqSchedulerCreator</code>) found in a <code>scheduler_creator_registry</code>. This adheres to the Open/Closed Principle, as adding new providers only requires adding a new Creator, not modifying the factory.</li> <li>Multi-Backend Support: Two initial backend providers were implemented:<ul> <li><code>KVSchedulerProvider</code>: A custom solution using a KVStore (Redis) with Sorted Sets to persist tasks. This provides fine-grained control and visibility.</li> <li><code>DramatiqProvider</code>: An adapter for the <code>dramatiq</code> library, leveraging its robust message broker capabilities.</li> </ul> </li> <li>Dedicated Worker: A <code>SchedulerWorkerService</code> was created to poll the KVStore for due tasks scheduled by the <code>KVSchedulerProvider</code> and dispatch them to the application's main task broker.</li> </ol>"},{"location":"adr/ADR-066-multi-backend-task-scheduling-system/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>The system is highly extensible and decoupled. New backend providers (e.g., Celery, RQ) can be added with minimal friction by implementing the protocol and creating a corresponding creator class.</li> <li>The application's business logic is completely isolated from the scheduling implementation. A service simply calls <code>scheduler.schedule(...)</code> without needing to know which backend is active.</li> <li>The <code>custom_kv</code> provider offers excellent control and observability over the scheduled task queue.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>The initial architectural setup is more complex than a single-library solution due to the number of components involved (Protocol, Base, Providers, Worker, Factory, Creators, Registry).</li> <li>The custom cancellation logic for <code>dramatiq</code> (using a denylist in Redis) introduces a dependency on a KV store even when using a broker-based scheduler.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This decision establishes a clear pattern for creating other multi-backend systems within the <code>athomic</code> framework in the future.</li> </ul> </li> </ul>"},{"location":"adr/ADR-067-generic-resilience-module-for-exponential-backoff/","title":"ADR-067: Generic Resilience Module for Exponential Backoff","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-13</li> </ul>"},{"location":"adr/ADR-067-generic-resilience-module-for-exponential-backoff/#context","title":"Context","text":"<p>Background services that operate on a polling mechanism, such as the <code>SchedulerWorkerService</code>, can create unnecessary load on their dependencies (e.g., databases, KV stores) when they are idle. A fixed-interval polling strategy is inefficient, leading to wasted CPU cycles and network traffic. The logic for a smarter, adaptive polling interval (exponential backoff) was needed, but implementing it directly within the worker would lead to code duplication if other polling services were created in the future.</p>"},{"location":"adr/ADR-067-generic-resilience-module-for-exponential-backoff/#decision","title":"Decision","text":"<p>Instead of embedding the backoff logic within a single service, we decided to create a generic, reusable component. A new <code>backoff</code> submodule was created within the existing <code>athomic/resilience</code> package, elevating it to a first-class resilience pattern alongside <code>Retry</code> and <code>Circuit Breaker</code>.</p> <p>The implementation consists of: 1.  <code>BackoffSettings</code>: A Pydantic schema allowing for the configuration of multiple, named backoff policies directly in <code>settings.toml</code> files. 2.  <code>BackoffPolicy</code>: A simple data class holding the parameters for a single backoff strategy (min/max delay, factor). 3.  <code>BackoffHandler</code>: A stateful class that manages the backoff logic. It provides <code>wait()</code> and <code>reset()</code> methods, encapsulating the complexity of calculating the sleep interval. It also includes an <code>on_error</code> callback hook for enhanced observability. 4.  <code>BackoffFactory</code>: Creates configured instances of the <code>BackoffHandler</code> based on the application's settings.</p>"},{"location":"adr/ADR-067-generic-resilience-module-for-exponential-backoff/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>The backoff logic is now a standardized and reusable component available to any part of the application, promoting DRY (Don't Repeat Yourself) principles.</li> <li>The code for services using this component, like <code>SchedulerWorkerService</code>, is significantly cleaner and more focused on its primary responsibility.</li> <li>Polling behavior is now highly configurable through settings files without requiring code changes, allowing for fine-tuning in different environments.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Introduces a minor layer of abstraction that developers need to be aware of when building new polling services.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This solidifies the design pattern for resilience modules within the <code>athomic</code> framework, making future additions (like Hedging) more predictable to implement.</li> </ul> </li> </ul>"},{"location":"adr/ADR-069-refactoring-base-consumer-with-the-strategy-pattern-for-message-processing/","title":"ADR-069: Refactoring BaseConsumer with the Strategy Pattern for Message Processing","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-13</li> </ul>"},{"location":"adr/ADR-069-refactoring-base-consumer-with-the-strategy-pattern-for-message-processing/#context","title":"Context","text":"<p>The <code>BaseConsumer</code> class was responsible for a wide range of tasks: managing the connection to the message broker, running the consumption loop, deserializing messages, checking for idempotency, executing the business logic callback, and handling failures (retry/DLQ). This concentration of responsibilities made the class complex, difficult to unit test in isolation, and violated the Single Responsibility Principle (SRP). Furthermore, the logic to handle different types of messages (e.g., messages needing a delay versus immediate processing) was poised to become a series of conditional <code>if/else</code> statements within the consumer, making it hard to extend in the future.</p>"},{"location":"adr/ADR-069-refactoring-base-consumer-with-the-strategy-pattern-for-message-processing/#decision","title":"Decision","text":"<p>We decided to refactor the consumer architecture by introducing two new patterns:</p> <ol> <li> <p><code>MessageProcessor</code> Class: A new class, <code>MessageProcessor</code>, will be created to encapsulate the entire processing pipeline for a single message. The <code>BaseConsumer</code>'s responsibility will be reduced to only managing the broker connection and fetching raw messages, delegating all processing to an injected <code>MessageProcessor</code> instance.</p> </li> <li> <p>Strategy Pattern: Inside the <code>MessageProcessor</code>, the decision of how to execute the business logic will be delegated to a <code>ConsumerStrategyProtocol</code>. We will provide two initial concrete implementations:</p> <ul> <li><code>ImmediateExecutionStrategy</code>: Calls the business logic callback directly.</li> <li><code>DelayedExecutionStrategy</code>: Does not call the callback, but instead uses the <code>Scheduler</code> service to schedule the message for later processing.</li> </ul> </li> </ol> <p>The <code>MessageProcessor</code> will contain a resolver method that selects the appropriate strategy based on message attributes (e.g., the presence of a <code>x-delay-seconds</code> header).</p>"},{"location":"adr/ADR-069-refactoring-base-consumer-with-the-strategy-pattern-for-message-processing/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Improved Single Responsibility Principle (SRP): The <code>BaseConsumer</code> is now solely responsible for consumption, and the <code>MessageProcessor</code> is solely responsible for processing. The different execution strategies are also isolated in their own classes.</li> <li>Enhanced Testability: The entire message processing pipeline can be unit tested via the <code>MessageProcessor</code> without needing a live message broker connection. Each strategy can also be tested independently.</li> <li>Extensibility (Open/Closed Principle): The system is now open for extension. New ways of handling messages (e.g., batching, priority queuing) can be added by creating new strategy classes without modifying the <code>MessageProcessor</code> or <code>BaseConsumer</code>.</li> <li>Increased Clarity: The flow of a message through the system is now more explicit and easier to follow.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Complexity: The refactoring introduces more classes and files (<code>MessageProcessor</code>, <code>ConsumerStrategyProtocol</code>, two strategy providers) compared to a single monolithic <code>BaseConsumer</code>. This adds a level of indirection that new developers will need to learn.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This decision solidifies the use of Dependency Injection within the messaging module, as the <code>MessageProcessor</code> and its dependencies are now constructed by the <code>ConsumerFactory</code>.</li> </ul> </li> </ul>"},{"location":"adr/ADR-070-persistent-message-delay-strategy-using-the-scheduler/","title":"ADR-070: Persistent Message Delay Strategy using the Scheduler","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-13</li> </ul>"},{"location":"adr/ADR-070-persistent-message-delay-strategy-using-the-scheduler/#context","title":"Context","text":"<p>The existing message delay mechanism (<code>KafkaDelayTopicStrategy</code>) relied on a background service (<code>DelayedMessageRePublisher</code>) that consumed from dedicated delay topics and used an in-memory <code>asyncio.sleep</code>. This approach had a significant drawback: if the service was restarted while \"sleeping,\" the message's delay state would be lost. The message would only be reprocessed after the Kafka consumer offset expired, leading to unpredictable and potentially long delays, which is not acceptable for a resilient framework. We needed a delay mechanism that was persistent and could survive application restarts.</p>"},{"location":"adr/ADR-070-persistent-message-delay-strategy-using-the-scheduler/#decision","title":"Decision","text":"<p>We decided to implement a new, persistent delay strategy that leverages existing <code>athomic</code> components: the <code>Scheduler</code> (<code>control.scheduler</code>) and the <code>Task Broker</code> (<code>integration.tasks</code>).</p> <ol> <li> <p><code>SchedulerDelayStrategy</code>: A new provider for the <code>DelayStrategyProtocol</code> was created. When a producer calls <code>publish</code> with <code>delay_seconds</code>, this strategy does not send the message to a different Kafka topic. Instead, it uses the <code>SchedulerFactory</code> to schedule a task for future execution.</p> </li> <li> <p>Generic Republish Task: A new, generic, framework-level background task (<code>generic_republish_message_task</code>) was created within the messaging module. The <code>Scheduler</code> schedules this task.</p> </li> <li> <p>Flow:</p> <ul> <li>A producer requests a delay.</li> <li>The <code>SchedulerDelayStrategy</code> creates a <code>GenericTask</code> containing all the original message details (payload, topic, headers) and tells the <code>Scheduler</code> to execute it in <code>N</code> seconds.</li> <li>The <code>Scheduler</code> persists this request (e.g., in a Redis Sorted Set).</li> <li>When the time comes, the <code>SchedulerWorkerService</code> picks up the request and enqueues the <code>generic_republish_message_task</code> into the <code>Task Broker</code> (Taskiq).</li> <li>A <code>Taskiq</code> worker executes the task, which uses the <code>ProducerFactory</code> to publish the original message to its final topic.</li> </ul> </li> </ol> <p>This entire mechanism is made available to applications by simply changing a configuration key: <code>NALA.INTEGRATION.MESSAGING.REPUBLISHER.delay_strategy_backend = \"scheduler\"</code>.</p>"},{"location":"adr/ADR-070-persistent-message-delay-strategy-using-the-scheduler/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Resilience and Persistence: The delay mechanism is now fully persistent. If the application restarts at any point, the scheduled task remains safely in Redis and will be processed when the system is back online.</li> <li>Reusability and Abstraction: The solution is highly reusable and well-abstracted. It leverages existing robust components (<code>Scheduler</code>, <code>Task Broker</code>) and is exposed to the user via the same <code>DelayStrategyProtocol</code>, requiring only a configuration change to activate.</li> <li>Zero Boilerplate: Applications that use <code>athomic</code> can get persistent delays without writing any new logic; they just need to configure and use it.</li> <li>Backend Agnostic: The solution is agnostic to the messaging backend, as the final step uses the abstract <code>ProducerFactory</code>.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Dependencies: This strategy introduces a dependency on a running <code>SchedulerWorkerService</code> (within the main app) and a separate <code>Taskiq</code> worker process, in addition to the application itself. This increases operational complexity compared to the simpler (but less reliable) in-memory approach.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This solidifies the <code>Scheduler</code> and <code>Task Broker</code> as core, interconnected components of the <code>athomic</code> framework.</li> </ul> </li> </ul>"},{"location":"adr/ADR-071-dedicated-service-for-dead-letter-queue-processing/","title":"ADR-071: Dedicated Service for Dead Letter Queue Processing","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-16</li> </ul>"},{"location":"adr/ADR-071-dedicated-service-for-dead-letter-queue-processing/#context","title":"Context","text":"<p>The existing Dead Letter Queue (DLQ) mechanism was resilient in routing failed messages, but the logic for processing those messages was not well-defined. Initial attempts to integrate a DLQ consumer into the standard <code>ConsumerManager</code> created several architectural issues. This approach tightly coupled application consumer logic with infrastructure-level failure processing, violating the Single Responsibility Principle. It made the DLQ consumer difficult to configure, disable, or test in isolation without instantiating the entire application's consumer stack. Furthermore, the logic for handling a terminally failed message (e.g., logging, alerting, archiving) was not flexible or easily extensible. We needed a robust, decoupled, and configurable architecture for handling messages that have permanently failed.</p>"},{"location":"adr/ADR-071-dedicated-service-for-dead-letter-queue-processing/#decision","title":"Decision","text":"<p>We decided to refactor the DLQ processing into a dedicated, standalone background service named <code>DeadLetterProcessorService</code>, which inherits from our <code>BaseService</code> class.</p> <p>This service is not auto-discovered by the <code>ConsumerManager</code>. Instead, it is explicitly instantiated and started at the application's entry point, running in parallel with other core infrastructure services.</p> <p>To ensure flexibility, the internal logic of the service implements the Strategy Pattern: 1.  A <code>DeadLetterStrategyProtocol</code> defines the interface for handling a failed message. 2.  Concrete implementations like <code>LoggingOnlyStrategy</code> and <code>RepublishToParkingLotStrategy</code> provide specific behaviors. 3.  A <code>DeadLetterStrategyFactory</code> reads the application's configuration from the settings files and injects the appropriate strategy into the <code>DeadLetterProcessorService</code> upon its creation.</p> <p>This design completely decouples the act of consuming from the DLQ (the service's responsibility) from the logic of what to do with the failed message (the strategy's responsibility).</p>"},{"location":"adr/ADR-071-dedicated-service-for-dead-letter-queue-processing/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>High Decoupling: The <code>ConsumerManager</code> is now solely responsible for application consumers. The DLQ processing logic is fully encapsulated within its own dedicated service and can be maintained independently.</li> <li>Improved Testability: The <code>DeadLetterProcessorService</code> can be tested as an integrated component, while individual strategies can be unit-tested in isolation.</li> <li>Extensible by Design: Adding new failure-handling behaviors (e.g., creating a Jira ticket, archiving to S3) is now as simple as creating a new strategy class and registering it, requiring no changes to the core service.</li> <li>Configuration-Driven Behavior: The entire DLQ handling process can be altered via settings files (e.g., switching from logging to a parking-lot topic) without a code deployment.</li> <li>Architectural Clarity: The application's startup sequence now explicitly shows all running infrastructure services, making the system's topology easier to understand.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Component Count: This solution introduces several new classes (a service, a factory, a registry, multiple strategies). This is a deliberate trade-off for a much cleaner and more maintainable design.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>The application's top-level startup logic is now explicitly responsible for initializing and managing the lifecycle of one additional service.</li> </ul> </li> </ul>"},{"location":"adr/ADR-072-unify-delayed-message-handling-with-standard-consumer-architecture/","title":"ADR-072: Unify Delayed Message Handling with Standard Consumer Architecture","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-16</li> </ul>"},{"location":"adr/ADR-072-unify-delayed-message-handling-with-standard-consumer-architecture/#context","title":"Context","text":"<p>The current delayed messaging mechanism is implemented via the <code>KafkaDelayTopicStrategy</code>, which publishes messages to dedicated delay topics (e.g., <code>__nala_delay_5m</code>). A custom background service, <code>DelayedMessageRepublisher</code>, consumes from these topics.</p> <p>This service implements its own consumption loop, manually handles deserialization, calculates any remaining delay with <code>asyncio.sleep</code>, and then re-serializes and re-publishes the message to its final destination.</p> <p>This implementation has several architectural drawbacks: 1.  Lack of Resilience: If the final republish action fails, the service logs an exception, but the message is effectively lost. It does not benefit from the robust retry and DLQ mechanisms available to application consumers. 2.  Inconsistent Observability: The service bypasses the standard <code>MessageProcessor</code>, meaning it lacks the automatic distributed tracing, metrics, and structured logging that are standard for all other consumers. 3.  Code Duplication: It contains bespoke logic for creating a producer, and for serializing/deserializing messages, which is already managed by our core messaging factories and processors.</p>"},{"location":"adr/ADR-072-unify-delayed-message-handling-with-standard-consumer-architecture/#decision","title":"Decision","text":"<p>We have decided to deprecate the custom <code>DelayedMessageRepublisher</code> service and replace it with a standard messaging consumer that leverages the existing <code>MessageProcessor</code>.</p> <p>The new implementation is as follows: 1.  A new, callable class, named <code>DelayedMessageHandler</code>, was created. Its sole responsibility is to extract the original message payload and metadata from the delay envelope and use a standard, injected <code>MessagingProducer</code> to republish it to its final destination topic. Using a class improves testability through dependency injection. 2.  A new <code>setup</code> function (<code>register_delay_republisher_from_settings</code>) was created. At application startup, this function reads the configured delay topics from the settings and programmatically registers a single, shared infrastructure consumer using the <code>consumer_registry</code>. 3.  This consumer is configured to listen to all delay topics and uses an instance of the <code>DelayedMessageHandler</code> as its callback.</p> <p>This change reframes the republisher from a special-purpose worker into a standard, albeit infrastructure-level, consumer.</p>"},{"location":"adr/ADR-072-unify-delayed-message-handling-with-standard-consumer-architecture/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Massive Resilience Improvement: Delayed message handling will now fully benefit from the <code>DLQHandler</code>. If a message fails to be republished, the action will be retried, and if it continues to fail, the message will be safely routed to the DLQ instead of being lost.</li> <li>Standardized Observability: The entire process is automatically instrumented with the same high-quality tracing, metrics, and logs as any other application consumer.</li> <li>Code Simplification &amp; Consistency: The complex, custom <code>_run_loop</code> of the <code>DelayedMessageRepublisher</code> is eliminated and replaced by a small, clean, and testable handler class. This reduces code duplication and aligns all message consumption to a single architectural pattern.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Slight Overhead: Introducing the <code>MessageProcessor</code> adds a few layers of abstraction. This overhead is negligible and a worthwhile trade-off for the gains in resilience and observability.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This decision solidifies the pattern of using explicitly registered, infrastructure-level consumers for background messaging tasks, creating a clear distinction from auto-discovered application consumers.</li> </ul> </li> </ul>"},{"location":"adr/ADR-073-decouple-consumption-logic-via-strategy-pattern/","title":"ADR-074: Introduce Batch Message Processing for High-Throughput Consumers","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-16</li> </ul>"},{"location":"adr/ADR-073-decouple-consumption-logic-via-strategy-pattern/#context","title":"Context","text":"<p>The current consumer architecture processes one message at a time. The <code>KafkaConsumer</code> iterates over messages using <code>async for msg in self._consumer</code>, passing each individual message to the <code>MessageProcessor</code>. While robust for many use cases, this one-by-one approach can become a performance bottleneck for high-volume topics where throughput is critical. The overhead of the processing pipeline (deserialization, idempotency checks, context creation) for each single message can limit the overall consumption rate. We need a mechanism to process messages in batches to maximize efficiency in these scenarios, enabling use cases like bulk database inserts.</p>"},{"location":"adr/ADR-073-decouple-consumption-logic-via-strategy-pattern/#decision","title":"Decision","text":"<p>We will introduce an optional batch processing mode for consumers. This will be an opt-in feature, configured directly at the consumer level, ensuring that existing consumers continue to function without any changes.</p> <p>The implementation will involve the following key changes:</p> <ol> <li> <p>Decorator Enhancement: The <code>@subscribe_to</code> decorator will be enhanced with a new optional parameter: <code>batch_size: int</code>. If this parameter is provided, the consumer will be configured to run in batch mode.</p> </li> <li> <p>Configuration Update: The <code>ConsumerHandlerConfig</code> will be updated to store the <code>batch_size</code> and a new <code>batch_mode</code> flag.</p> </li> <li> <p>Consumer Logic Modification (Strategy Pattern):</p> <ul> <li>The consumption logic will be extracted from the <code>KafkaConsumer</code> into dedicated strategy classes.</li> <li>A <code>SingleMessageConsumptionStrategy</code> will encapsulate the current <code>async for</code> loop.</li> <li>A new <code>BatchMessageConsumptionStrategy</code> will use <code>await self._consumer.getmany(max_records=batch_size)</code> to fetch batches of messages.</li> <li>The <code>ConsumerFactory</code> will be responsible for choosing and injecting the correct strategy into the consumer instance based on the <code>handler_config</code>.</li> </ul> </li> <li> <p>New Processing Path: A new method, <code>process_batch</code>, will be added to the <code>MessageProcessor</code>. This method will receive the list of raw messages, deserialize them, and then pass the entire list of deserialized messages to the user's callback.</p> </li> <li> <p>Resilience for Batches: Error handling for batches will be \"all-or-nothing\". If the user's callback fails for the entire batch (by returning <code>ProcessingOutcome.RETRY</code> or raising an exception), the entire batch of original messages will be sent to the <code>DLQHandler</code> for the standard retry/DLQ process.</p> </li> <li> <p>Callback Contract Evolution: The contract for batch-enabled handlers will change. They will be expected to receive a <code>List</code> of messages, e.g., <code>async def my_batch_handler(messages: List[MyModel], ...)</code>.</p> </li> </ol>"},{"location":"adr/ADR-073-decouple-consumption-logic-via-strategy-pattern/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Significant Performance Gains: For high-throughput scenarios, batch processing can dramatically increase the number of messages processed per second by reducing per-message overhead.</li> <li>Opt-In and Backwards Compatible: The change is entirely opt-in via a new decorator argument. Existing consumers are unaffected.</li> <li>Enables New Use Cases: Allows for efficient implementation of patterns like batch database inserts or bulk API calls triggered by messages.</li> <li>Clean Architecture: Using the Strategy Pattern for consumption logic keeps the consumer providers (like <code>KafkaConsumer</code>) clean and adheres to the Open/Closed Principle.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Atomic Batch Error Handling: The initial implementation will treat the batch as an atomic unit. A failure in processing a single message will cause the entire batch to be retried. More granular error handling is a possible future enhancement but adds significant complexity.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This introduces a second processing path within the <code>MessageProcessor</code> and a new abstraction layer (Consumption Strategies), which will need to be maintained. However, the logic is self-contained and activated by a clear configuration flag.</li> </ul> </li> </ul>"},{"location":"adr/ADR-074-refine-messaging-consumer-responsibilities-between-strategy-and-rocessor/","title":"ADR-074: Refine Messaging Consumer Responsibilities between Strategy and Processor","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-20</li> </ul>"},{"location":"adr/ADR-074-refine-messaging-consumer-responsibilities-between-strategy-and-rocessor/#context","title":"Context","text":"<p>During a recent refactoring, the <code>MessageProcessor</code> was simplified to handle only single messages. The goal was to delegate batch iteration logic to the newly created <code>BatchMessageConsumptionStrategy</code>. However, this led to two critical issues: 1.  Incorrect Callback Invocation: The <code>MessageProcessor</code>'s <code>process</code> method was designed to call single-message handlers (expecting <code>message</code>, <code>message_key</code>, etc.). When the <code>BatchMessageConsumptionStrategy</code> iterated and called <code>process</code>, it incorrectly attempted to pass single-message arguments to a batch handler (which expects a single <code>messages: List</code> argument), causing a <code>TypeError</code>. 2.  Logic Duplication &amp; Unclear Responsibility: The <code>BatchMessageConsumptionStrategy</code> had to contain logic for iterating over a batch and calling the processor repeatedly. This blurred its primary responsibility, which should be exclusively about how messages are fetched and grouped from the broker, not how they are processed.</p> <p>The core problem was that the <code>MessageProcessor</code> had lost its capability to act as a cohesive \"Unit of Work\" processor for batches, violating the Single Responsibility Principle (SRP) at a higher level.</p>"},{"location":"adr/ADR-074-refine-messaging-consumer-responsibilities-between-strategy-and-rocessor/#decision","title":"Decision","text":"<p>We have decided to re-introduce a dedicated batch processing method to the <code>MessageProcessor</code> and clarify the responsibilities between the components:</p> <ol> <li> <p><code>MessageProcessor</code> Responsibilities:</p> <ul> <li>It will now have two distinct public methods: <code>process()</code> for single messages and <code>process_batch()</code> for lists of messages.</li> <li><code>process_batch()</code> is responsible for the entire batch lifecycle: iterating the raw messages, deserializing each one, handling any deserialization failures for the entire batch as a single transaction, and, if all successful, invoking the user's batch callback once with the complete list of deserialized objects. It also centralizes error handling (DLQ) for the entire batch.</li> </ul> </li> <li> <p><code>ConsumptionStrategy</code> Responsibilities:</p> <ul> <li>Its sole responsibility is to fetch messages from the broker and group them.</li> <li>The <code>SingleMessageConsumptionStrategy</code> fetches one message and delegates it to <code>processor.process()</code>.</li> <li>The <code>BatchMessageConsumptionStrategy</code> accumulates a batch and delegates the entire raw batch to <code>processor.process_batch()</code>. It does not iterate or process individual messages.</li> </ul> </li> </ol> <p>This decision restores the <code>MessageProcessor</code>'s role as the central \"guardian\" that prepares data securely and resiliently before handing it off to the business logic, whether as a single item or a complete batch.</p>"},{"location":"adr/ADR-074-refine-messaging-consumer-responsibilities-between-strategy-and-rocessor/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Corrects Batch Functionality: Fixes the <code>TypeError</code> and <code>TimeoutError</code> in batch processing tests by ensuring the correct handler signature is used.</li> <li>Strengthens SRP: The <code>ConsumptionStrategy</code> is now purely concerned with I/O and message grouping (the \"how\"), while the <code>MessageProcessor</code> is concerned with the processing pipeline of a unit of work (the \"what\").</li> <li>Centralized Batch Error Handling: Logic for handling a failure of a single message within a batch (which should fail the whole batch) is now correctly centralized in the <code>MessageProcessor</code>, rather than being scattered.</li> <li>Cleaner Business Logic: The user's decorated handler remains simple, receiving either a single Pydantic object or a clean <code>List[PydanticObject]</code>, without any knowledge of the underlying consumption strategy.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased <code>MessageProcessor</code> Complexity: The <code>MessageProcessor</code> is slightly larger as it now contains logic for two distinct processing paths (single vs. batch). However, this is considered a necessary trade-off for creating a clearer and more robust abstraction.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This reinforces the architectural pattern of separating I/O-bound strategies from CPU-bound (or logic-bound) processing pipelines.</li> </ul> </li> </ul>"},{"location":"adr/ADR-075-introduce-messaging-orchestrator-for-separation-of-concerns/","title":"ADR-075: Introduce Messaging Orchestrator for Clearer Separation of Concerns","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-21</li> </ul>"},{"location":"adr/ADR-075-introduce-messaging-orchestrator-for-separation-of-concerns/#context","title":"Context","text":"<p>The <code>ConsumerManager</code> was previously responsible for both building the entire messaging stack and managing its runtime lifecycle (<code>start</code>/<code>stop</code>). This included discovering decorated consumers, calling various factories (<code>ConsumerFactory</code>, <code>ProducerFactory</code>, <code>DLQHandlerFactory</code>), creating shared dependencies, and assembling the final consumer objects.</p> <p>This approach violated the Single Responsibility Principle (SRP), making the <code>LifecycleManager</code> overly complex and tightly coupled to the implementation details of its managed components. Its responsibility was diluted from being a pure lifecycle operator to also being a master builder.</p>"},{"location":"adr/ADR-075-introduce-messaging-orchestrator-for-separation-of-concerns/#decision","title":"Decision","text":"<p>We decided to introduce a new class, the <code>MessagingOrchestrator</code>, to cleanly separate these responsibilities.</p> <ol> <li> <p><code>MessagingOrchestrator</code>: This class is now solely responsible for the construction of the entire messaging stack. It creates shared dependencies like the <code>Producer</code> for the DLQ, discovers all consumer handlers, and uses the appropriate factories (<code>ConsumerFactory</code>, <code>DLQHandlerFactory</code>, etc.) to build a complete list of ready-to-use consumer services.</p> </li> <li> <p><code>ConsumerManager</code>: This class has been simplified to be a pure lifecycle operator. It now receives a pre-configured <code>MessagingOrchestrator</code> instance during its initialization. Its only responsibilities are to get the list of fully-built services from the orchestrator and then call <code>start()</code> on them during application startup and <code>stop()</code> during shutdown.</p> </li> </ol>"},{"location":"adr/ADR-075-introduce-messaging-orchestrator-for-separation-of-concerns/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Strong SRP Adherence: The responsibilities are now atomic and clear: the <code>Orchestrator</code> builds, and the <code>LifecycleManager</code> runs.</li> <li>Improved Decoupling: The <code>LifecycleManager</code> no longer has direct dependencies on <code>ConsumerFactory</code>, <code>ProducerFactory</code>, or other builder components. It only depends on the <code>Orchestrator</code> abstraction.</li> <li>Enhanced Testability: Both components can be unit-tested in isolation more effectively. The <code>LifecycleManager</code> can be tested with a mock orchestrator that returns a simple list of mock services.</li> <li>Centralized Construction Logic: All logic for assembling the messaging components is now located in a single, dedicated place, making it easier to understand and maintain.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Indirection: This change adds one more layer of abstraction (<code>Orchestrator</code>) to the messaging startup flow. This can slightly increase the cognitive load for developers new to this part of the codebase.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>The application's main startup sequence (e.g., in <code>register_services.py</code>) is now responsible for the two-step process of first creating the <code>Orchestrator</code> and then injecting it into the <code>ConsumerManager</code>.</li> </ul> </li> </ul>"},{"location":"adr/ADR-076-use-adapter-pattern-for-consistent-failure-handling/","title":"ADR-076: Use Adapter Pattern (FailureContext) for Consistent Failure Handling","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-21</li> </ul>"},{"location":"adr/ADR-076-use-adapter-pattern-for-consistent-failure-handling/#context","title":"Context","text":"<p>The <code>DLQHandler.handle_failure</code> method previously accepted a long list of individual parameters (<code>original_message</code>, <code>original_key</code>, <code>original_headers</code>, etc.). A significant problem was that the structure of these parameters, particularly <code>original_headers</code>, was different for single-message failures (<code>List[Tuple]</code>) versus batch failures (<code>List[List[Tuple]]</code>).</p> <p>This forced the <code>DLQHandler</code> to contain conditional logic (<code>if isinstance...</code>) to inspect and determine the shape of the data it received. This made the handler's logic complex, brittle, and violated the principle of a clean, predictable interface.</p>"},{"location":"adr/ADR-076-use-adapter-pattern-for-consistent-failure-handling/#decision","title":"Decision","text":"<p>To resolve this, we implemented the Adapter Pattern by creating a dedicated data class, <code>FailureContext</code>.</p> <ol> <li> <p><code>FailureContext</code> Class: This <code>dataclass</code> encapsulates all information about a failure event into a single, consistent structure. It includes fields for messages, keys, headers, the exception, the topic, and an explicit <code>consumption_type</code> enum (<code>SINGLE</code> or <code>BATCH</code>).</p> </li> <li> <p><code>MessageProcessor</code> as the Adapter Creator: The responsibility of creating the <code>FailureContext</code> object was given to the <code>MessageProcessor</code>. It uses named factory methods (<code>FailureContext.for_single_message</code> and <code>FailureContext.for_batch</code>) to adapt the raw failure data into this consistent object. For single messages, it wraps the message, key, and headers in lists to match the structure of a batch.</p> </li> <li> <p>Simplified <code>DLQHandler</code>: The <code>DLQHandler.handle_failure</code> signature was simplified to accept only one argument: <code>context: FailureContext</code>. It can now operate on this object with full confidence in its data structure, without needing any conditional type or shape checking.</p> </li> </ol>"},{"location":"adr/ADR-076-use-adapter-pattern-for-consistent-failure-handling/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Simplified <code>DLQHandler</code>: All conditional logic for parsing different data shapes was removed from the handler, making it dramatically simpler and more robust. The code is now declarative instead of interrogative.</li> <li>Clear and Explicit Contract: The <code>FailureContext</code> provides a strong, predictable contract between the <code>MessageProcessor</code> and <code>DLQHandler</code>. The intent of the failure is explicit via <code>consumption_type</code>.</li> <li>Improved Encapsulation and SRP: The responsibility of adapting data is now correctly placed in the <code>MessageProcessor</code>, while the <code>DLQHandler</code> is only responsible for acting on the pre-adapted, consistent data.</li> <li>Enhanced Extensibility: If more data is needed for failure handling in the future (e.g., consumer metadata), only the <code>FailureContext</code> class needs to be changed, not every method signature in the call chain.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Introduces a New Class: Adds a new data class (<code>FailureContext</code>) and an enum (<code>ConsumptionType</code>) to the codebase. This is a very minor trade-off for the significant gain in clarity and robustness.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This pattern establishes a clear and reusable way to handle complex error information throughout the messaging module.</li> </ul> </li> </ul>"},{"location":"adr/ADR-077-refine-messaging-consumer-responsibilities-between-strategy-and-processor/","title":"ADR-077: Refine Messaging Consumer Responsibilities Between Strategy and Processor","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-21</li> </ul>"},{"location":"adr/ADR-077-refine-messaging-consumer-responsibilities-between-strategy-and-processor/#context","title":"Context","text":"<p>The <code>nala/athomic</code> messaging system requires a robust and extensible way to consume messages from different backends (like Kafka) and using different patterns (single message, batch). The initial consumer logic needed a clearer separation of concerns to enhance testability, adhere to the Single Responsibility Principle (SRP), and allow for future expansion without significant refactoring.</p> <p>Alternatives considered included: 1.  A monolithic consumer class: A single class would handle fetching, batching, deserializing, and processing. This was rejected as it violates SRP, mixes infrastructure concerns with processing logic, and makes isolated unit testing nearly impossible. 2.  Processor handles batching: Placing batch accumulation logic within the <code>MessageProcessor</code>. This was rejected because it would make the processor stateful and responsible for both how messages are acquired and what to do with them, again violating SRP.</p> <p>The primary architectural driver is to create a clean separation between the infrastructure-aware logic of message acquisition and the broker-agnostic logic of message processing.</p>"},{"location":"adr/ADR-077-refine-messaging-consumer-responsibilities-between-strategy-and-processor/#decision","title":"Decision","text":"<p>We will enforce a strict separation of responsibilities between two main components, connected by a protocol:</p> <ol> <li> <p><code>ConsumptionStrategyProtocol</code>: Implementations of this protocol are solely responsible for interacting with the message broker client. Their single responsibility is to fetch messages and group them according to the chosen strategy (e.g., one by one, or accumulating a batch based on size, time, or byte count). The strategy is the \"how\" and \"when\" of message acquisition.</p> </li> <li> <p><code>MessageProcessor</code>: This class is responsible for the entire processing pipeline of a message or a batch of messages delivered to it by a strategy. Its responsibilities include:</p> <ul> <li>Orchestrating telemetry (tracing and metrics).</li> <li>Deserializing message components.</li> <li>Performing idempotency checks.</li> <li>Invoking the specific business logic handler (<code>callback</code>).</li> <li>Orchestrating error handling (forwarding to the <code>DLQHandler</code>).</li> </ul> </li> </ol> <p>The <code>MessageProcessor</code> is completely agnostic of the message broker. The <code>ConsumptionStrategy</code>'s <code>consume</code> method receives the appropriate <code>MessageProcessor</code> at runtime to execute the processing logic.</p>"},{"location":"adr/ADR-077-refine-messaging-consumer-responsibilities-between-strategy-and-processor/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Strong SRP Adherence: The Strategy's concern is message acquisition; the Processor's concern is the processing pipeline. This creates highly cohesive, loosely coupled components.</li> <li>Enhanced Testability: The <code>MessageProcessor</code> can be unit-tested in complete isolation by simply passing it mock message data. The strategies can be tested by mocking only the broker client.</li> <li>Improved Extensibility (Open/Closed Principle): We can introduce new consumption strategies (e.g., for a different broker like RabbitMQ, or a transactional batch strategy) without any changes to the <code>MessageProcessor</code>.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Atomic Batch Error Handling: The initial implementation will treat the batch as an atomic unit. A failure in processing a single message will cause the entire batch to be retried. More granular error handling is a possible future enhancement but adds significant complexity.</li> <li>Increased Abstraction: This pattern introduces an additional layer of abstraction, which can slightly increase the initial cognitive load for developers new to this module. This is a deliberate trade-off for long-term robustness and flexibility.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This design solidifies the pattern where broker-agnostic components (like the Processor) are driven by broker-aware components (like the Strategy).</li> </ul> </li> </ul>"},{"location":"adr/ADR-078-saga-pattern-for-distributed-transactions/","title":"ADR-078: Saga Pattern for Distributed Transactions","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-21</li> </ul>"},{"location":"adr/ADR-078-saga-pattern-for-distributed-transactions/#context","title":"Context","text":"<p>Managing data consistency across multiple microservices is a primary challenge in distributed architectures. Traditional distributed transactions using two-phase commit (2PC) do not scale well and introduce tight coupling and blocking, making them unsuitable for high-performance systems. Business processes like order fulfillment or user registration often span several services (e.g., Orders, Payments, Stock, Notifications) and require a mechanism to ensure they either complete fully or are safely rolled back to a consistent state.</p>"},{"location":"adr/ADR-078-saga-pattern-for-distributed-transactions/#decision","title":"Decision","text":"<p>We will implement first-class support for the Saga pattern within <code>nala/athomic</code>. This will be located in a new module, <code>nala/athomic/resilience/sagas</code>.</p> <p>The implementation will separate the Saga Definition from the Saga Execution Strategy: 1.  Definition Layer: A <code>SagaBuilder</code> will provide a fluent, declarative API for developers to define the sequence of saga steps. Each step will consist of an <code>action</code> (the business logic to execute) and a corresponding <code>compensation</code> action (the logic to undo the action). 2.  Execution Layer: A <code>SagaExecutorProtocol</code> will define the contract for running a saga. We will provide two initial implementations, selectable via configuration:     * Choreography Executor: Uses the <code>ProducerProtocol</code> from the messaging module to publish events upon step completion, allowing services to react and trigger the next step in a decoupled manner.     * Orchestration Executor: Acts as a central coordinator, making direct requests (e.g., via an <code>HttpClientProtocol</code>) to participant services to execute steps. 3.  State Management: A <code>SagaStateRepositoryProtocol</code>, implemented using the existing <code>KVStoreProtocol</code>, will be used to persist the state of long-running saga instances.</p>"},{"location":"adr/ADR-078-saga-pattern-for-distributed-transactions/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Enables Robust Distributed Transactions: Provides a standardized, resilient way to manage complex, multi-service business processes.</li> <li>Improves Data Consistency: Prevents the system from being left in a partially complete, inconsistent state after a failure.</li> <li>Flexible Coordination: Developers can choose between decentralized choreography (for decoupling) and centralized orchestration (for visibility) via a simple configuration change, without altering the business logic.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Complexity: Writing business logic in terms of actions and compensations introduces a higher cognitive load compared to traditional monolithic transactions.</li> <li>Debugging Challenges: Tracing a distributed saga across multiple services can be complex and will rely heavily on the existing observability (distributed tracing) module.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This establishes a formal framework for designing and implementing long-running business workflows within the platform.</li> </ul> </li> </ul>"},{"location":"adr/ADR-079-claim-check-pattern-for-large-message-payloads/","title":"ADR-079: Claim Check Pattern for Large Message Payloads","text":"<ul> <li>Status: Proposed</li> <li>Date: 2025-08-21</li> </ul>"},{"location":"adr/ADR-079-claim-check-pattern-for-large-message-payloads/#context","title":"Context","text":"<p>Message brokers, including Kafka, are optimized for high-throughput, low-latency transport of relatively small messages. Sending large payloads (e.g., images, documents, detailed reports exceeding 1MB) directly through the broker can lead to several problems: * Degraded broker performance. * Increased network bandwidth and storage costs. * Hitting hard limits on message size configured on the broker (<code>max.request.size</code>).</p>"},{"location":"adr/ADR-079-claim-check-pattern-for-large-message-payloads/#decision","title":"Decision","text":"<p>We will implement the Claim Check pattern as an opt-in feature within the messaging module. The implementation will be integrated into the serialization/publishing layer.</p> <ol> <li>A <code>LargeMessageHandlingStrategy</code> will be introduced, configurable at the <code>Producer</code> level.</li> <li>When publishing a message, this strategy will check the serialized message size against a configurable threshold (e.g., <code>large_message_threshold_bytes</code>).</li> <li>If the message exceeds the threshold, the strategy will:<ul> <li>Use the existing <code>StorageProtocol</code> to upload the large payload to an external blob store (e.g., S3, GCS).</li> <li>Replace the original message payload with a \"claim check\" \u2013 a small JSON object containing the reference or URI to the stored payload.</li> <li>Add a specific header (e.g., <code>x-claim-check: true</code>) to the message.</li> </ul> </li> <li>On the consumer side, the <code>MessageProcessor</code> will be enhanced to detect this header. If present, it will use the <code>StorageProtocol</code> to retrieve the full payload from the blob store before passing it to the business logic handler.</li> </ol>"},{"location":"adr/ADR-079-claim-check-pattern-for-large-message-payloads/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Efficient Broker Usage: Keeps the message bus lean and fast, using it only for small notification/command messages.</li> <li>Scalable Large Data Handling: Allows the system to handle arbitrarily large payloads by leveraging scalable blob storage.</li> <li>Cost Optimization: Can reduce network and broker storage costs associated with large messages.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Latency: The end-to-end processing time for large messages will increase due to the two extra network calls to the blob store (one write, one read).</li> <li>Additional Point of Failure: The system now depends on the availability and reliability of the external blob store for the flow to complete.</li> <li>Orphaned Data Risk: A robust lifecycle/garbage-collection policy for the stored objects will be required to prevent orphaned data if a consumer fails to process a claim check.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>Decouples the message transport lifecycle from the large payload storage lifecycle.</li> </ul> </li> </ul>"},{"location":"adr/ADR-080-message-lineage-for-data-governance-and-traceability/","title":"ADR-080: Data Lineage and Message Tracing using OpenLineage","text":"<ul> <li>Status: Proposed</li> <li>Date: 2025-09-03</li> </ul>"},{"location":"adr/ADR-080-message-lineage-for-data-governance-and-traceability/#context","title":"Context","text":"<p>As the system scales, understanding the flow of data across different services becomes increasingly complex. It is difficult to answer critical questions such as: \"Which services consume events of type X?\" or \"What was the originating service for the data that caused this error?\". This lack of visibility complicates debugging, impact analysis for changes, and data governance efforts. We need a standardized way to trace message consumption across the entire platform.</p>"},{"location":"adr/ADR-080-message-lineage-for-data-governance-and-traceability/#decision","title":"Decision","text":"<p>We will implement a data lineage system within the <code>athomic</code> layer, based on the OpenLineage open standard. This system will automatically capture an event whenever a message consumer successfully processes a message.</p> <p>The architecture will consist of the following core components:</p> <ol> <li><code>LineageProcessor</code>: A central service responsible for constructing a canonical <code>LineageEvent</code> model and translating it into the OpenLineage JSON format.</li> <li><code>LineageStorageProtocol</code>: An interface that decouples the <code>LineageProcessor</code> from the storage backend, allowing lineage events to be sent to different destinations (e.g., logs, a message topic, or a dedicated OpenLineage collector).</li> <li>Header Injection: The base message publishing strategy (<code>BasePublishingStrategy</code>) will be modified to inject standardized headers (<code>x-nala-source-service</code>, <code>x-nala-event-type</code>, <code>message_id</code>) into all outgoing messages.</li> <li>Consumption Hook: The core message consumer logic (<code>MessageProcessor</code>) will call the <code>LineageProcessor</code> after a message has been successfully processed by a handler.</li> <li>Initial Storage Backends: We will provide an initial <code>LoggingLineageStore</code> for easy debugging and development. The architecture also supports a <code>MessagingLineageStore</code> to publish lineage events to a dedicated Kafka topic.</li> </ol>"},{"location":"adr/ADR-080-message-lineage-for-data-governance-and-traceability/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Enhanced Observability: Provides a clear, event-driven audit trail of data flow between services.</li> <li>Standardization: Adopting the OpenLineage standard makes our platform compatible with a growing ecosystem of data governance and observability tools (e.g., Marquez).</li> <li>Decoupling &amp; Extensibility: The storage protocol allows us to easily switch or add new storage backends in the future (e.g., a direct HTTP collector) without changing the core lineage logic (Open/Closed Principle).</li> <li>Simplified Debugging: Makes it easier to trace the path of a specific message through multiple consumer services.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Minor Performance Overhead: Adds a small overhead to every message publication (header injection) and consumption (lineage event creation and storage). This is considered an acceptable trade-off for the increased visibility.</li> <li>Dependency on Collector: To realize the full benefits of OpenLineage, a collector service (like Marquez) must eventually be deployed and maintained. However, the <code>LoggingLineageStore</code> provides immediate value without this dependency.</li> </ul> </li> </ul>"},{"location":"adr/ADR-081-field-level-encryption-for-message-payloads/","title":"ADR-081: Field-Level Encryption for Message Payloads","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-21</li> </ul>"},{"location":"adr/ADR-081-field-level-encryption-for-message-payloads/#context","title":"Context","text":"<p>While TLS encrypts data in transit between services and the message broker, the data itself often sits at rest on the broker's disk in plain text. For systems handling highly sensitive or regulated information (PII, financial data, health records), this represents a significant security risk. If the broker is compromised or its storage is accessed improperly, the sensitive data is exposed.</p>"},{"location":"adr/ADR-081-field-level-encryption-for-message-payloads/#decision","title":"Decision","text":"<p>We will implement end-to-end, field-level encryption as part of the serialization layer to provide \"zero-trust\" security for data at rest.</p> <ol> <li>A new serializer, <code>EncryptedJsonSerializer</code>, will be created that wraps an existing serializer (like <code>JsonPydanticSerializer</code>).</li> <li>Developers will mark sensitive fields in their Pydantic message models using the <code>SecretStr</code> type from Pydantic.</li> <li>On Serialization (Producer side): The <code>EncryptedJsonSerializer</code> will inspect the Pydantic model. For any field of type <code>SecretStr</code>, it will encrypt the field's value before the full message is serialized to JSON. The encrypted value could be stored in a structured way, e.g., <code>{\"_encrypted\": \"ciphertext\", \"key_id\": \"v1\"}</code>.</li> <li>On Deserialization (Consumer side): The serializer will detect the <code>_encrypted</code> structure, use the <code>key_id</code> to retrieve the appropriate decryption key, decrypt the value, and place it back into the Pydantic model as a <code>SecretStr</code>.</li> <li>Key management will be delegated to the existing <code>SecretsProtocol</code>, allowing keys to be fetched securely from providers like HashiCorp Vault.</li> </ol>"},{"location":"adr/ADR-081-field-level-encryption-for-message-payloads/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Zero-Trust Security: Sensitive data is protected end-to-end and remains encrypted at rest on the broker, drastically reducing the attack surface.</li> <li>Seamless Developer Experience: Encryption is handled transparently by the framework. Developers only need to mark fields as <code>SecretStr</code> in their Pydantic models, requiring no cryptographic knowledge.</li> <li>Granular Control: Only specified fields are encrypted, allowing non-sensitive data to remain in plain text for potential filtering or routing by broker-side tools.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Computational Overhead: Encryption and decryption add CPU overhead to both the producer and consumer, which will impact end-to-end latency and maximum throughput.</li> <li>Key Management Complexity: Key rotation and management become critical operational security tasks. An outage or misconfiguration of the secret management system (e.g., Vault) would halt message processing.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>The message payload is no longer a simple, human-readable JSON for sensitive fields, which may require specialized tooling for debugging.</li> </ul> </li> </ul>"},{"location":"adr/ADR-082-fifo-ordering-guarantee-for-the-transactional-outbox-pattern/","title":"ADR-082: FIFO Ordering Guarantee for the Transactional Outbox Pattern","text":"<ul> <li>Status: Proposed</li> <li>Date: 2025-08-22</li> </ul>"},{"location":"adr/ADR-082-fifo-ordering-guarantee-for-the-transactional-outbox-pattern/#context","title":"Context","text":"<p>The existing <code>Outbox</code> implementation successfully guarantees at-least-once delivery of events atomically with business data transactions. It achieves this by persisting events in a local <code>outbox</code> collection within the same database transaction and having a background service (<code>OutboxPublishingService</code>) relay these messages to the message broker.</p> <p>However, the current relay service processes pending messages in parallel without respect to their original order. For business processes that span multiple events against the same entity (e.g., <code>OrderCreated</code>, <code>ItemAddedToOrder</code>), this can lead to consumers processing events out of sequence, causing state inconsistencies and processing errors. The challenge is to introduce FIFO (First-In, First-Out) ordering per business aggregate without sacrificing the scalability and resilience of the pattern.</p>"},{"location":"adr/ADR-082-fifo-ordering-guarantee-for-the-transactional-outbox-pattern/#decision","title":"Decision","text":"<p>We will evolve the <code>Outbox</code> pattern to enforce strict FIFO ordering for messages belonging to the same business aggregate. This will be achieved through the following architectural changes:</p> <ol> <li> <p>Aggregate Key in Data Model: The <code>OutboxMessage</code> model will be extended with an <code>aggregate_key: str</code> field. This key will be mandatory and will contain the identifier of the business aggregate (e.g., <code>order_id</code>, <code>customer_id</code>).</p> </li> <li> <p>Lock-Based Sequential Processing: The <code>OutboxPublishingService</code> will be refactored to a more sophisticated, lock-based workflow:     a.  Discovery: The service will first query the outbox collection to find distinct <code>aggregate_key</code>s that have pending messages.     b.  Serialized Processing per Aggregate: The service will iterate through these keys. For each <code>aggregate_key</code>, it will attempt to acquire a distributed lock (e.g., <code>lock:outbox:order-123</code>) using the existing <code>LockingProtocol</code>.     c.  Ordered Publication: If the lock is acquired, the service will fetch all pending messages for that specific <code>aggregate_key</code>, sort them chronologically by their creation timestamp, and publish them to the message broker in that strict sequence.     d.  State Update: Only after a message is successfully published will it be marked as <code>processed</code>. If a publication fails, the relay will stop processing for that aggregate, leaving the lock to be released, and will retry the entire sequence for that aggregate in the next cycle.</p> </li> <li> <p>Leverage Existing <code>athomic</code> Components: This solution will be built on top of our robust, existing <code>athomic</code> modules. The <code>LockingProtocol</code> is critical for preventing multiple publisher instances from processing the same aggregate concurrently, which would violate the ordering guarantee.</p> </li> </ol>"},{"location":"adr/ADR-082-fifo-ordering-guarantee-for-the-transactional-outbox-pattern/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Guaranteed Ordering: Provides a strong guarantee that events for a specific business entity are processed in the correct order, which is critical for system consistency.</li> <li>Increased Robustness: Makes the entire event-driven architecture more predictable and resilient to common distributed systems problems.</li> <li>High Cohesion: The solution is self-contained within the <code>outbox</code> module and reuses existing framework components (<code>LockingProtocol</code>, <code>ProducerProtocol</code>), strengthening the <code>athomic</code> ecosystem.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Reduced Parallelism (Per Aggregate): Throughput for a single, high-traffic aggregate is now limited to a single publisher at a time. This is a deliberate trade-off in favor of correctness, and the overall system throughput remains high as different aggregates are processed in parallel.</li> <li>\"Poison Pill\" Scenario: If a single message for an aggregate is \"stuck\" (e.g., consistently fails to publish due to a non-transient error), it will block all subsequent messages for that same aggregate. This will require robust monitoring and DLQ (Dead Letter Queue) strategies, which are already part of our messaging module.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This change makes the contract with the developer more explicit: when saving an <code>OutboxMessage</code>, they are now required to provide a meaningful <code>aggregate_key</code>.</li> </ul> </li> </ul>"},{"location":"adr/ADR-083-optimistic-event-ordering-with-consumer-side-resolution/","title":"ADR-083: Optimistic Event Ordering with Consumer-Side Resolution","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-23 (Updated 2025-09-23)</li> </ul>"},{"location":"adr/ADR-083-optimistic-event-ordering-with-consumer-side-resolution/#context","title":"Context","text":"<p>The lock-based FIFO ordering for the Transactional Outbox pattern (<code>ADR-078</code>) provides strong consistency guarantees. However, it introduces a performance trade-off: the throughput for a single, high-traffic business aggregate is serialized, as only one publisher worker can process its messages at a time. This can become a bottleneck for \"hot\" aggregates (e.g., a popular product's inventory).</p> <p>The challenge is to explore an alternative approach that can provide the same strict ordering guarantee while allowing for higher parallelism and throughput on the publisher side, thereby improving the computational cost in the common \"happy path\" scenario where events arrive in order.</p>"},{"location":"adr/ADR-083-optimistic-event-ordering-with-consumer-side-resolution/#decision","title":"Decision","text":"<p>We will implement a novel, \"optimistic ordering\" mechanism. This approach decouples the publisher from the ordering responsibility and delegates the re-sequencing logic to an intelligent consumer-side framework component within <code>athomic</code>.</p> <p>The implementation will be based on the following principles:</p> <ol> <li> <p>Sequenced-at-Source (Publisher):</p> <ul> <li>When an event is written to the <code>Outbox</code> collection, it will be atomically assigned a <code>sequence_id</code> that is strictly and gap-lessly incrementing per <code>aggregate_key</code>.</li> <li>The <code>OutboxPublishingService</code> is then free to publish these messages in parallel, with each message carrying its <code>aggregate_key</code> and <code>sequence_id</code> as metadata.</li> </ul> </li> <li> <p>Consumer-Side Resolution (<code>@ordered_consumer</code>):</p> <ul> <li>A new decorator, <code>@ordered_consumer</code>, will wrap the business logic of a message handler.</li> <li>State Tracking: It will use the <code>KVStoreProtocol</code> to maintain the <code>last_processed_sequence</code> for each <code>aggregate_key</code>.</li> <li>The \"Waiting Room\": When a message arrives out of order, it is placed into a \"waiting room\" buffer (a Redis Sorted Set).</li> <li>Backlog Processing: After successfully processing an in-order message, the handler immediately checks the waiting room for the next message in the sequence.</li> </ul> </li> </ol>"},{"location":"adr/ADR-083-optimistic-event-ordering-with-consumer-side-resolution/#refined-implementation-details-the-three-layer-strategy-architecture","title":"Refined Implementation Details: The Three-Layer Strategy Architecture","text":"<p>To ensure the implementation is modular, extensible, and adheres to the Single Responsibility Principle (SRP), the logic within the consumer pipeline is structured using three distinct layers of strategies:</p> <ol> <li> <p>Consumption Strategy: Its sole responsibility is how messages are fetched from the broker. It is unaware of business logic. Implementations include <code>SingleMessageConsumptionStrategy</code> and <code>BatchMessageConsumptionStrategy</code>.</p> </li> <li> <p>Orchestration Strategy: This is the core workflow layer. Its responsibility is to manage what happens before the user's business logic is called. The <code>OrderedOrchestrationStrategy</code> implements the state tracking and \"waiting room\" logic described above. A simpler <code>DirectOrchestrationStrategy</code> exists for standard consumers.</p> </li> <li> <p>Execution Strategy: Its sole responsibility is how the user's callback is executed. For now, an <code>ImmediateExecutionStrategy</code> is used, but this layer allows for future extensions like delayed or scheduled execution.</p> </li> </ol> <p>This entire pipeline is assembled by the <code>ConsumerFactory</code>. It reads the strategy name (e.g., <code>\"ordered\"</code>) from the handler configuration, uses a <code>Registry</code> and <code>Creator</code> pattern to build the appropriate <code>OrchestrationStrategy</code>, and injects it into a central <code>MessageProcessor</code>, which coordinates the entire flow for a given message.</p>"},{"location":"adr/ADR-083-optimistic-event-ordering-with-consumer-side-resolution/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Higher Publisher Throughput: Removes the distributed lock bottleneck from the <code>OutboxPublishingService</code>.</li> <li>Increased Resilience: The system can gracefully handle and re-order out-of-sequence messages.</li> <li>State-of-the-Art Feature: Provides a highly advanced feature for the <code>athomic</code> framework.</li> <li>Highly Modular and Testable Consumer: The final implementation using the Creator and Strategy patterns makes the consumer pipeline extremely modular, consistent, and easy to test and extend.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Consumer Complexity: The internal logic is significantly more complex, though this is encapsulated by the framework.</li> <li>Increased Latency for Out-of-Order Messages: Buffered messages will have a higher end-to-end processing time.</li> <li>Higher Redis Usage: The consumer will perform more read/write operations against the <code>KVStore</code>.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This represents a philosophical shift from pessimistic locking to optimistic processing.</li> </ul> </li> </ul>"},{"location":"adr/ADR-084-payload-processing-pipeline/","title":"ADR-084: Payload Processing Pipeline","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-31</li> </ul>"},{"location":"adr/ADR-084-payload-processing-pipeline/#context","title":"Context","text":"<p>The existing messaging system handles message payloads as opaque blocks of data. As the system evolves, there is a growing need to apply systematic, ordered transformations to payloads before they are published and after they are consumed. Such transformations include compression to reduce message size and cost, and encryption to secure sensitive data in transit. Implementing these as ad-hoc steps within the business logic of each publisher or consumer would lead to code duplication, inconsistent application of transformations, and a tightly coupled architecture.</p>"},{"location":"adr/ADR-084-payload-processing-pipeline/#decision","title":"Decision","text":"<p>We will introduce a formal payload processing pipeline, managed by a <code>PayloadProcessor</code>. This pipeline will consist of a sequence of well-defined processing steps, each conforming to a <code>ProcessingStepProtocol</code>.</p> <ol> <li><code>ProcessingStepProtocol</code>: A protocol will be defined for a single processing step. It will have two methods: <code>apply</code> for outbound messages (e.g., compress, encrypt) and <code>revert</code> for inbound messages (e.g., decrypt, decompress).</li> <li><code>PayloadProcessor</code>: This class will be responsible for managing and executing a list of processing steps in the correct order. For an outbound message, it will iterate through the steps and call <code>apply</code> on each. For an inbound message, it will iterate in reverse order and call <code>revert</code>.</li> <li>Integration: The <code>MessageFactory</code> and <code>MessageProcessor</code> will be updated to use the <code>PayloadProcessor</code>. This ensures that all messages passing through the system are consistently processed according to the configured pipeline.</li> </ol>"},{"location":"adr/ADR-084-payload-processing-pipeline/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Extensibility: New processing steps (e.g., signing, validation, different compression algorithms) can be added easily without changing the core messaging logic.</li> <li>Decoupling: The logic for each transformation (compression, encryption) is isolated into its own component, respecting the Single Responsibility Principle.</li> <li>Consistency: Guarantees that all message payloads undergo the same sequence of transformations, improving reliability and security.</li> </ul> </li> <li>Negative:<ul> <li>Increased Complexity: Introduces new layers of abstraction (<code>PayloadProcessor</code>, <code>ProcessingStepProtocol</code>) that must be understood by developers.</li> <li>Performance Overhead: Each step in the pipeline adds a small amount of processing overhead. For very high-throughput, low-latency scenarios, this could be a factor.</li> </ul> </li> <li>Neutral/Other:<ul> <li>This decision establishes a foundational pattern for handling message transformations that can be built upon in the future.</li> </ul> </li> </ul>"},{"location":"adr/ADR-085-payload-compression/","title":"ADR-085: Payload Compression","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-31</li> </ul>"},{"location":"adr/ADR-085-payload-compression/#context","title":"Context","text":"<p>Message payloads, especially those containing verbose data formats like JSON, can be large. Transmitting large messages over the message bus can lead to increased network bandwidth usage, higher storage costs for brokers that persist messages, and potentially slower performance. A mechanism to reduce the size of the message payloads before transmission is needed to mitigate these issues. Several compression algorithms exist, with different trade-offs between compression ratio and CPU usage.</p>"},{"location":"adr/ADR-085-payload-compression/#decision","title":"Decision","text":"<p>We will add payload compression as a step in the new Payload Processing Pipeline (ADR-084).</p> <ol> <li><code>CompressionProtocol</code>: A protocol will be created to define the interface for any compression provider, with <code>compress</code> and <code>decompress</code> methods.</li> <li>Initial Provider (Gzip): The first implementation of this protocol will be <code>GzipProvider</code>. Gzip is chosen for its widespread availability, good compression ratio for text-based data, and reasonable performance, making it a balanced default choice.</li> <li>Integration via Adapter: A <code>CompressionStepAdapter</code> will be created to adapt the <code>CompressionProtocol</code> to the <code>ProcessingStepProtocol</code>, allowing it to be seamlessly inserted into the <code>PayloadProcessor</code>'s pipeline.</li> </ol>"},{"location":"adr/ADR-085-payload-compression/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Reduced Costs: Significantly lowers network and message broker storage costs, especially for high-volume, text-heavy payloads.</li> <li>Improved Throughput: Smaller message sizes can lead to higher message throughput.</li> <li>Flexibility: The protocol-based approach allows for easily swapping or adding other compression algorithms (like Zstd, Brotli) in the future.</li> </ul> </li> <li>Negative:<ul> <li>CPU Overhead: Compression and decompression consume CPU cycles on both the publisher and consumer sides, which could impact latency.</li> <li>Debugging Difficulty: Compressed message payloads are not human-readable, making debugging directly from the message broker more difficult without proper tooling.</li> </ul> </li> <li>Neutral/Other:<ul> <li>This feature makes the system more efficient for a broad range of use cases where payload size is a concern.</li> </ul> </li> </ul>"},{"location":"adr/ADR-086-payload-encryption/","title":"ADR-086: Payload Encryption","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-31</li> </ul>"},{"location":"adr/ADR-086-payload-encryption/#context","title":"Context","text":"<p>Some messages may carry sensitive user data or system information. While transport-level security (TLS) encrypts data in transit between the service and the message broker, the data resides unencrypted (\"at rest\") within the broker's queues. This poses a security risk, as anyone with access to the broker could potentially inspect message contents. A mechanism for end-to-end encryption of the payload itself is required to ensure data confidentiality.</p>"},{"location":"adr/ADR-086-payload-encryption/#decision","title":"Decision","text":"<p>We will introduce payload encryption as a step in the Payload Processing Pipeline (ADR-084).</p> <ol> <li><code>EncryptionProtocol</code>: A protocol will define the standard interface for encryption providers, featuring <code>encrypt</code> and <code>decrypt</code> methods.</li> <li>Initial Provider (Fernet): The initial implementation will be <code>FernetProvider</code>, which uses the Fernet specification (AES-128 in CBC mode with PKCS7 padding, signed with HMAC using SHA256). Fernet is part of the Python <code>cryptography</code> library and provides symmetric authenticated encryption, which is secure and straightforward to use.</li> <li>Key Management: The encryption key will be managed by the <code>SecretsManager</code>, ensuring it is stored securely and retrieved at runtime.</li> <li>Integration via Adapter: An <code>EncryptionStepAdapter</code> will adapt the <code>EncryptionProtocol</code> to the <code>ProcessingStepProtocol</code> for use in the pipeline.</li> </ol>"},{"location":"adr/ADR-086-payload-encryption/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Enhanced Security: Provides end-to-end encryption for message payloads, ensuring data confidentiality even if the message broker is compromised.</li> <li>Compliance: Helps meet regulatory and compliance requirements (like GDPR, HIPAA) for handling sensitive data.</li> <li>Centralized Logic: Encryption logic is centralized and applied consistently, avoiding scattered and potentially insecure implementations.</li> </ul> </li> <li>Negative:<ul> <li>Key Management Complexity: Introduces the operational burden of managing encryption keys. Key rotation and access control policies must be carefully handled.</li> <li>Performance Overhead: Encryption and decryption are computationally intensive operations that will add latency to message processing.</li> <li>Data Recovery: If the encryption key is lost, all data encrypted with that key becomes irrecoverable.</li> </ul> </li> <li>Neutral/Other:<ul> <li>This positions security as a first-class citizen in the messaging architecture.</li> </ul> </li> </ul>"},{"location":"adr/ADR-087-distributed-leasing-mechanism-for-concurrency-control/","title":"ADR-087: Distributed Leasing Mechanism for Concurrency Control","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-09-06</li> </ul>"},{"location":"adr/ADR-087-distributed-leasing-mechanism-for-concurrency-control/#context","title":"Context","text":"<p>The implementation of FIFO ordering in the Outbox Pattern (ADR-078) requires an exclusive lock per <code>aggregate_key</code> to prevent multiple publisher instances from processing messages for the same aggregate concurrently. A simple distributed lock (e.g., <code>SETNX</code> with a TTL) is vulnerable to \"stuck lock\" scenarios where a worker acquires a lock and crashes, forcing other workers to wait until the TTL expires. We need a more resilient, fault-tolerant concurrency mechanism that ensures a lock is automatically released if the holding process dies.</p>"},{"location":"adr/ADR-087-distributed-leasing-mechanism-for-concurrency-control/#decision","title":"Decision","text":"<p>We will implement a distributed leasing mechanism within the <code>athomic.resilience</code> module. This system treats a lock as a \"lease\" that must be actively maintained by the holder.</p> <ol> <li> <p>Core Components:</p> <ul> <li><code>LeaseProtocol</code>: An interface defining the contract for lease providers (<code>acquire</code>, <code>renew</code>, <code>release</code>).</li> <li><code>RedisLeaseProvider</code>: A concrete implementation using Redis. It will use atomic operations:<ul> <li><code>acquire</code>: <code>SET key value NX EX seconds</code> for atomic acquisition.</li> <li><code>renew</code> / <code>release</code>: Atomic Lua scripts that first verify the lease <code>owner_id</code> before modifying or deleting the key, preventing race conditions.</li> </ul> </li> <li><code>LeaseManager</code> &amp; <code>LeaseHolder</code>: Client-side components providing a clean <code>async with</code> interface.</li> </ul> </li> <li> <p>Heartbeat Mechanism:</p> <ul> <li>When a lease is successfully acquired, the <code>LeaseManager</code> returns a <code>LeaseHolder</code> context manager.</li> <li>Upon entering the <code>with</code> block, the <code>LeaseHolder</code> starts a background <code>asyncio.Task</code> that periodically calls the provider's <code>renew</code> method. This acts as a \"heartbeat,\" continuously extending the lease's TTL.</li> <li>If the worker process crashes, the heartbeat stops. The lease in Redis will naturally expire after its TTL, allowing another worker to safely acquire it.</li> <li>Upon exiting the <code>with</code> block (normally or via exception), the <code>LeaseHolder</code> cancels the heartbeat task and explicitly releases the lease.</li> </ul> </li> </ol>"},{"location":"adr/ADR-087-distributed-leasing-mechanism-for-concurrency-control/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>High Fault Tolerance: Eliminates \"stuck lock\" scenarios. If a worker crashes, the system automatically recovers after the lease TTL expires.</li> <li>Developer Experience: The <code>async with manager.acquire(...)</code> pattern provides a clean, safe, and easy-to-use interface for developers, abstracting away the complexity of heartbeats and renewals.</li> <li>Centralized &amp; Reusable: Creates a robust, reusable concurrency primitive within the <code>Athomic</code> framework that can be used by any component, not just the Outbox.</li> </ul> </li> <li>Negative:<ul> <li>Increased Complexity: The implementation (especially the client-side heartbeat task and Lua scripts) is more complex than a simple distributed lock.</li> <li>Dependency: Relies on the target backend (Redis) supporting Lua scripting for atomic operations.</li> </ul> </li> <li>Neutral/Other:<ul> <li>The system's reliability is tied to the lease duration and renewal interval configurations. These must be balanced to provide quick failure recovery without generating excessive network traffic from heartbeats.</li> </ul> </li> </ul>"},{"location":"adr/ADR-088-generic-sharding-service/","title":"ADR-088: Generic Sharding Service for Distributed Workloads","text":"<ul> <li>Status: Proposed</li> <li>Date: 2025-09-06</li> </ul>"},{"location":"adr/ADR-088-generic-sharding-service/#context","title":"Context","text":"<p>The current implementation of the <code>OutboxPublisherService</code> identifies a potential scalability bottleneck. While multiple instances of the service can run concurrently, they all fetch the complete list of pending aggregates and compete for leases on them. This leads to high contention, wasted work on failed lease acquisitions, and does not provide true horizontal scaling, as adding more workers primarily increases contention rather than throughput.</p> <p>This pattern of distributing a list of work items (e.g., aggregate keys, cache keys to invalidate, etc.) across a pool of workers is a recurring architectural challenge.</p>"},{"location":"adr/ADR-088-generic-sharding-service/#alternatives-considered","title":"Alternatives Considered","text":"<ol> <li>Leasing-Only (Current State): Rely solely on the existing leasing mechanism for coordination. This was rejected due to the high contention and inefficient resource usage described above.</li> <li>Broker-Side Partitioning: Utilize the native partitioning capabilities of the message broker (e.g., Kafka partitions). While powerful, this approach would tightly couple the sharding logic to a specific broker technology and would not be a generic solution for workloads that do not originate from a message queue.</li> <li>Outbox-Specific Sharding Logic: Implement the sharding logic directly within the <code>OutboxPublisherBase</code>. This was rejected as it violates the Single Responsibility Principle (SRP) and misses the opportunity to create a reusable component for other potential use cases within the Athomic framework.</li> </ol>"},{"location":"adr/ADR-088-generic-sharding-service/#decision","title":"Decision","text":"<p>We will create a new, generic, and reusable Sharding Service as a first-class component within the Nala Athomic framework, located at <code>nala/athomic/resilience/sharding/</code>.</p> <p>This service will be responsible for distributing a list of string-based items across a dynamic group of workers. Its implementation will be based on the following principles:</p> <ol> <li>Service Discovery Integration: The <code>ShardingService</code> will be initialized with a <code>group_name</code> (e.g., \"outbox-publishers\"). It will use the existing <code>ServiceDiscoveryProtocol</code> within Athomic to fetch a list of all healthy, active service instances belonging to that group.</li> <li>Consistent Hashing: It will use a consistent hashing algorithm (via a library like <code>uhashring</code>) to build an in-memory hash ring from the list of active worker IDs. This ensures minimal item re-shuffling when workers are added or removed.</li> <li>Core Functionality: The service will expose a primary method, <code>filter_owned_items(items: List[str]) -&gt; List[str]</code>, which accepts a complete list of items and returns only the subset that is mapped to the current worker's unique instance ID.</li> <li>Worker Lifecycle: Service instances intended to participate in a sharded group (like the <code>OutboxPublisherService</code>) will be responsible for registering themselves with the service discovery on startup and deregistering on shutdown. The new <code>ShardingService</code> will provide helper methods (<code>register_self</code>, <code>deregister_self</code>) to facilitate this.</li> </ol> <p>The <code>OutboxPublisherService</code> will be the first client of this new service. It will be refactored to: * Instantiate the <code>ShardingService</code> with a specific group name. * Register/Deregister itself upon startup/shutdown. * In its processing loop, fetch all pending aggregate keys and then use <code>sharding_service.filter_owned_items()</code> to determine its specific workload for that cycle.</p>"},{"location":"adr/ADR-088-generic-sharding-service/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Horizontal Scalability: The workload for processing outbox aggregates can now be distributed across multiple service instances, directly improving throughput.</li> <li>Reduced Contention: Eliminates race conditions and failed lease acquisitions, as each worker will only attempt to lease aggregates it already owns according to the hash ring.</li> <li>Reusability: The <code>ShardingService</code> is generic and can be immediately reused for other distributed workloads within the application (e.g., batch processing, cache invalidation, etc.).</li> <li>Decoupling &amp; SRP: The <code>OutboxPublisherService</code> is simplified, focusing on its core logic of processing events, while the complex logic of workload distribution is encapsulated in the <code>ShardingService</code>.</li> <li>Resilience: The system automatically rebalances the workload as workers are added or removed from the service discovery pool.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>New Dependency: The sharding mechanism introduces a hard dependency on a functioning Service Discovery system (e.g., Consul). If the discovery service is down, workers cannot determine the sharding topology. A fallback strategy (e.g., a single worker processing all items) must be considered.</li> <li>Potential for Uneven Load: Consistent hashing algorithms provide good, but not perfectly uniform, distribution. Some workers may receive a slightly higher or lower number of items, especially with a small number of workers. This is an acceptable trade-off for the scalability gains.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>The state of the hash ring is ephemeral and recalculated on each cycle based on the \"live\" state from service discovery. This is simple and resilient but introduces a small overhead for discovery queries and ring reconstruction on each processing cycle.</li> <li>Observability will become more critical. We will need to monitor the number of active workers per sharding group and the number of items processed by each to detect imbalances.</li> </ul> </li> </ul>"},{"location":"adr/ADR-089-centralized-connection-management/","title":"ADR-089: Centralized Connection Management","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-09-18</li> </ul>"},{"location":"adr/ADR-089-centralized-connection-management/#context","title":"Context","text":"<p>The Athomic framework relies on various external data stores (document databases, KV stores) for its modules, including Caching, Leasing, Sagas, Feature Flags, and Outbox. Historically, the configuration was monolithic, allowing for only a single document database and a single KV store. Components would create their clients through specific factories (<code>KVStoreFactory</code>, <code>DocumentsDatabaseFactory</code>), leading to scattered connection logic and making it difficult to support scenarios requiring multiple connections to different databases of the same type (e.g., a primary DB and an audit DB). This approach violated the Single Responsibility Principle, as multiple components were concerned with connection lifecycle, and limited the framework's flexibility.</p> <p>Alternatives considered: 1.  Keep decentralized factories: Rejected due to increasing complexity and poor scalability. 2.  Introduce a full DI container library: Rejected as it would add a significant external dependency for a problem that could be solved natively within the framework's existing service lifecycle pattern.</p>"},{"location":"adr/ADR-089-centralized-connection-management/#decision","title":"Decision","text":"<p>We will introduce a <code>ConnectionManager</code>, a central <code>BaseService</code> responsible for the entire lifecycle of all database connections within the Athomic application.</p> <ol> <li>Centralized Management: The <code>ConnectionManager</code> will be the single source of truth for creating, starting, stopping, and providing access to database clients.</li> <li>Named Connections: The configuration in <code>settings.toml</code> will be refactored to support a dictionary of named connections under <code>database.documents</code> and <code>database.kvstore</code>. This allows defining multiple, distinct connections (e.g., <code>default_mongo</code>, <code>audit_mongo</code>, <code>default_redis</code>, <code>rate_limiter_redis</code>).</li> <li>Singleton Factory: A <code>ConnectionManagerFactory</code> will be created to provide singleton access to the <code>ConnectionManager</code> instance throughout the application.</li> <li>Refactoring Consumers: All modules and factories that previously created their own database clients (e.g., <code>CacheFallbackFactory</code>, <code>SagaStateRepositoryFactory</code>, <code>APIKeyDBProvider</code>) will be refactored to request a named connection instance from the <code>ConnectionManagerFactory</code>.</li> </ol>"},{"location":"adr/ADR-089-centralized-connection-management/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Increased Flexibility: The framework now natively supports connecting to multiple databases of the same type, a critical feature for complex microservice architectures.</li> <li>Improved SRP: The responsibility of connection lifecycle is now encapsulated entirely within the <code>ConnectionManager</code>, making other components simpler as they only need to request a connection by name.</li> <li>Enhanced Consistency: All parts of the framework now acquire database connections through a single, consistent mechanism.</li> <li>Clearer Configuration: The settings are more explicit, as components now reference connections by a logical name (e.g., <code>kv_store_connection_name = \"default_redis\"</code>).</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Indirection: Components have an additional layer of indirection to get a connection (Component -&gt; <code>ConnectionManagerFactory</code> -&gt; <code>ConnectionManager</code> -&gt; Connection Client).</li> <li>Initial Refactoring Effort: This decision required a significant refactoring effort across multiple modules to adapt to the new connection acquisition pattern.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>The <code>ConnectionManager</code> becomes a critical core service, essential for the startup of most other data-dependent services.</li> </ul> </li> </ul>"},{"location":"adr/ADR-090-agnostic-database-migration-system/","title":"ADR-090: Agnostic Database Migration System","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-09-18</li> </ul>"},{"location":"adr/ADR-090-agnostic-database-migration-system/#context","title":"Context","text":"<p>As the application schema evolves, there is no automated, version-controlled, or reproducible process for applying changes to the database (e.g., creating collections, adding indexes, backfilling data). This relies on manual, error-prone scripts and processes, which is unsustainable and risky, especially across different environments (development, staging, production). The framework needs a robust migration tool that is integrated with its core principles.</p> <p>Alternatives considered: 1.  Rely on manual scripts: Rejected as it is not safe, reproducible, or scalable. 2.  Use a third-party library like <code>yoyo-migrations</code>: This was heavily considered, and the final design is inspired by its script-based approach. However, a custom solution was chosen to achieve deeper integration with Athomic's async-first nature, Pydantic-based configuration, observability stack (tracing/metrics), and the new <code>ConnectionManager</code>.</p>"},{"location":"adr/ADR-090-agnostic-database-migration-system/#decision","title":"Decision","text":"<p>We will implement a new, database-agnostic migration module within Athomic (<code>athomic.database.migrations</code>).</p> <ol> <li>Protocol-Driven Design: The system will be built around a <code>MigrationBackendProtocol</code>, defining a contract for database-specific operations (connect, apply, record, etc.). This ensures extensibility.</li> <li>Initial MongoDB Backend: The first implementation will be <code>MongoDbBackend</code>, which uses the <code>ConnectionManager</code> to get its database connection and <code>beanie</code> to manage a <code>MigrationStatus</code> collection for tracking applied migrations.</li> <li>Python-Based Scripts: Migrations will be plain Python files containing <code>upgrade</code> and <code>downgrade</code> async functions. This allows migrations to be powerful, leveraging any other part of the Athomic framework if needed.</li> <li>Orchestration via <code>MigrationRunner</code>: A <code>MigrationRunner</code> class will orchestrate the entire process. It discovers scripts from configured paths, compares them against the <code>MigrationStatus</code> collection, and executes pending migrations via the appropriate backend.</li> <li>CLI Integration: The migration runner will be primarily invoked through a new CLI command, <code>athomic db upgrade</code>.</li> </ol>"},{"location":"adr/ADR-090-agnostic-database-migration-system/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Traceability &amp; Reproducibility: Database schema changes are now versioned, auditable, and can be applied automatically and consistently across all environments.</li> <li>Deep Framework Integration: Being a native Athomic component, the migration system seamlessly integrates with the framework's lifecycle, configuration, and observability, providing tracing and metrics for migration operations out-of-the-box.</li> <li>Extensibility: The protocol-based design makes it straightforward to add support for other databases (e.g., SQL via an adapter) in the future.</li> <li>Improved Developer Experience: Provides a clear and standardized workflow for managing database evolution.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Internal Maintenance: As a custom solution, its maintenance and future enhancements are our responsibility, unlike a third-party open-source tool.</li> <li>MongoDB Transactions: The framework does not provide automatic transaction management for NoSQL migrations. Developers writing migration scripts for MongoDB must manually handle atomicity using client sessions if a migration involves multiple write operations.</li> </ul> </li> </ul>"},{"location":"adr/ADR-091-unified-command-line-interface/","title":"ADR-091: Unified Command-Line Interface (CLI)","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-09-18</li> </ul>"},{"location":"adr/ADR-091-unified-command-line-interface/#context","title":"Context","text":"<p>The Athomic framework lacked a standardized, user-friendly entry point for developer and operational tasks, such as running database migrations, generating security keys, or managing services. Such tasks were either not possible or would require custom, disconnected Python scripts, leading to an inconsistent developer experience and making CI/CD automation more complex.</p> <p>Alternatives considered: 1.  Use standard <code>argparse</code>: Rejected as it is more verbose and less modern compared to newer alternatives. 2.  Use <code>Click</code>: A strong candidate. However, <code>Typer</code> was chosen because it is built on top of Click and its type-hint-based approach aligns perfectly with the modern Python practices used throughout the rest of the Athomic codebase.</p>"},{"location":"adr/ADR-091-unified-command-line-interface/#decision","title":"Decision","text":"<p>We will implement a new, unified Command-Line Interface (CLI) for the Athomic framework, accessible via a single <code>nala</code> (or <code>athomic</code>) entry point.</p> <ol> <li>Technology: The CLI will be built using the <code>Typer</code> library.</li> <li>Extensible Structure: The main CLI application (<code>nala.cli.main</code>) will dynamically discover and register subcommands from the <code>nala.cli.commands</code> package. This allows new commands to be added as simple modules (e.g., <code>database.py</code>, <code>keys.py</code>) without modifying the main entry point, adhering to the Open/Closed Principle.</li> <li>Initial Commands: The initial implementation will include <code>nala db upgrade</code> to run database migrations and <code>nala keys generate</code> as a security utility.</li> <li>Project Integration: The CLI will be registered as a script entry point in <code>pyproject.toml</code>, making it available directly after installation.</li> </ol>"},{"location":"adr/ADR-091-unified-command-line-interface/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Improved Developer Experience: Provides a single, powerful, and self-documenting interface for interacting with the framework from the command line.</li> <li>Automation-Friendly: The CLI offers a stable contract for integration into CI/CD pipelines for tasks like automated migrations.</li> <li>High Extensibility: The modular, discovery-based design makes it trivial to add new commands and functionalities as the framework grows.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>New Dependency: Adds <code>typer</code> and its dependencies to the project.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>Establishes a formal pattern for creating and managing operational tasks within the project.</li> </ul> </li> </ul>"},{"location":"adr/ADR-092-implementation-of-a-backpressure-mechanism-for-resilient-services/","title":"ADR-092: Implementation of a Backpressure Mechanism for Resilient Services","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-09-18</li> </ul>"},{"location":"adr/ADR-092-implementation-of-a-backpressure-mechanism-for-resilient-services/#context","title":"Context","text":"<p>Distributed components, such as the <code>OutboxPublisher</code> running across multiple workers, are susceptible to transient downstream failures (e.g., an unavailable message broker) or high contention on specific resources (i.e., \"hot aggregates\"). In these scenarios, retrying operations immediately and aggressively exacerbates the problem, leading to wasted resources (CPU, network) and increasing the risk of cascading failures across the system. A mechanism is required to allow services to detect these conditions and gracefully \"back off\" to allow the struggling components to recover.</p>"},{"location":"adr/ADR-092-implementation-of-a-backpressure-mechanism-for-resilient-services/#decision","title":"Decision","text":"<p>We will implement a distributed, time-based backpressure mechanism. The implementation consists of:</p> <ol> <li>A generic <code>BackpressureManager</code> that leverages a shared <code>KVStoreProtocol</code> (e.g., Redis) to manage the state of embargoed resources.</li> <li>The embargo is created by setting a key in the KV store with a specific Time-To-Live (TTL). This ensures the backpressure is temporary and the system automatically attempts to resume processing after the cooldown period.</li> <li>The <code>OutboxPublisherBase</code> will be the first component to utilize this pattern. It will apply backpressure on a specific <code>aggregate_key</code> when it detects a <code>PublishError</code> or a <code>LeaseAcquisitionError</code>.</li> <li>Before processing a batch of aggregates, the publisher will query the <code>BackpressureManager</code> to filter out and skip any aggregates that are currently under an embargo.</li> </ol>"},{"location":"adr/ADR-092-implementation-of-a-backpressure-mechanism-for-resilient-services/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Increased System Stability: Prevents downstream failures or contention from overwhelming the publishers and causing cascading failures.</li> <li>Reduced Resource Waste: Avoids pointless, rapid-fire retries against a component that is known to be temporarily unavailable, saving CPU and network resources.</li> <li>Automatic Recovery: The TTL-based approach ensures that processing for an affected resource resumes automatically without manual intervention.</li> <li>Reusable Pattern: The <code>BackpressureManager</code> is a generic, decoupled component that can be reused in other parts of the system (e.g., API calls, task processing) to manage load.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Introduces Intentional Latency: By design, the processing for a specific aggregate will be delayed when backpressure is applied. This is the trade-off for overall system health.</li> <li>Adds a Dependency: The mechanism relies on a fast, distributed Key-Value store to share state across multiple workers. The performance of this store is critical to the effectiveness of the backpressure system.</li> </ul> </li> </ul>"},{"location":"adr/ADR-093-provider-specific-consumption-strategies/","title":"ADR-093: Abstract Consumer Interface for Provider Consistency","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-09-24</li> </ul>"},{"location":"adr/ADR-093-provider-specific-consumption-strategies/#context","title":"Context","text":"<p>During the consumer architecture refactoring (<code>ADR-083</code>), a key design decision was the interface between the <code>ConsumptionStrategy</code> and the concrete <code>Consumer</code> providers (like <code>KafkaConsumer</code>). Two primary approaches were debated:</p> <ol> <li> <p>Provider-Specific Strategies: Make the <code>BaseConsumer</code> minimal, only managing the client lifecycle. The <code>ConsumptionStrategy</code> would be specialized for each provider (e.g., <code>KafkaBatchConsumptionStrategy</code>), interacting directly with the native client. This approach simplifies initial provider creation.</p> </li> <li> <p>Abstract Interface on <code>BaseConsumer</code>: Define a generic, abstract interface for fetching messages (e.g., <code>get_message_iterator()</code>, <code>get_batch()</code>) on the <code>BaseConsumer</code> class. All providers would be required to implement this interface. This allows <code>ConsumptionStrategy</code> implementations to be generic and reusable.</p> </li> </ol> <p>While Approach 1 offers ease of provider implementation, it was determined that it could lead to inconsistencies between providers and a less clear contract for the strategies. A formal, abstract interface was deemed necessary to ensure long-term maintainability, predictability, and consistency across the framework.</p>"},{"location":"adr/ADR-093-provider-specific-consumption-strategies/#decision","title":"Decision","text":"<p>We will adopt Approach 2: an Abstract Interface on <code>BaseConsumer</code>.</p> <p>The <code>BaseConsumer</code> abstract class will define two abstract methods: * <code>get_message_iterator() -&gt; AsyncGenerator[RawMessage, None]</code> * <code>get_batch(max_records: int, timeout_ms: int) -&gt; List[RawMessage]</code></p> <p>All concrete provider implementations (<code>KafkaConsumer</code>, <code>LocalConsumer</code>, etc.) must implement this interface. This moves the responsibility of adapting the specific client library's output into a standardized <code>List[RawMessage]</code> into the provider itself (using the Adapter Pattern where necessary, like <code>AdaptedKafkaMessage</code>).</p> <p>As a result, the <code>ConsumptionStrategy</code> implementations (<code>SingleMessageConsumptionStrategy</code>, <code>BatchMessageConsumptionStrategy</code>) become generic, provider-agnostic, and reusable across the framework.</p>"},{"location":"adr/ADR-093-provider-specific-consumption-strategies/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Enforces Consistency: Guarantees that all consumer providers behave in a predictable and consistent manner.</li> <li>Improves Encapsulation: Strategies interact with a clean, public interface on the <code>BaseConsumer</code>, not its internal implementation details.</li> <li>Enables Generic Strategies: Allows for the creation of simple, reusable <code>ConsumptionStrategy</code> classes that are not tied to any specific broker technology.</li> <li>Clear Contract: Provides an explicit, formal contract for developers creating new consumer providers, improving long-term maintainability.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increases Provider Implementation Workload: The initial effort to create a new provider is higher, as the developer must implement both abstract methods, potentially having to simulate one pattern (e.g., batching) if it's not natively supported by the underlying client library.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This decision prioritizes long-term architectural integrity and consistency over the short-term speed of implementing new providers.</li> </ul> </li> </ul>"},{"location":"adr/ADR-094-explicit-dependency-injection/","title":"ADR-094: Explicit Dependency Injection via Composition Root","text":"<ul> <li>Status: Proposed</li> <li>Date: 2025-10-05</li> </ul>"},{"location":"adr/ADR-094-explicit-dependency-injection/#context","title":"Context","text":"<p>The <code>athomic</code> framework currently employs a Service Locator pattern through various singleton factories (e.g., <code>ConnectionManagerFactory</code>, <code>ProducerFactory</code>). While this provides easy access to shared services, it creates an implicit coupling between components and the global factory state. This makes unit testing more complex, requiring explicit calls to <code>.clear()</code> methods to ensure isolation, and obscures the true dependencies of a class, which are resolved internally rather than being declared in its constructor. The goal is to move towards a more explicit, robust, and testable dependency management strategy.</p>"},{"location":"adr/ADR-094-explicit-dependency-injection/#decision","title":"Decision","text":"<p>We will refactor the framework to adopt an explicit Dependency Injection (DI) pattern, with the <code>Athomic.startup</code> method acting as the Composition Root.</p> <ol> <li>Centralized Instantiation: Core singleton services (<code>ConnectionManager</code>, <code>Producer</code>, <code>SecretsFactory</code>, etc.) will be instantiated once within the <code>Athomic.startup</code> flow.</li> <li>Constructor Injection: These shared instances will be passed down through the constructors of the services and factories that depend on them. Classes will no longer call static <code>.create()</code> methods internally to resolve their dependencies.</li> <li>Factory Role: Factories will be simplified into pure object builders, receiving their dependencies (like a <code>ConnectionManager</code> instance) rather than locating them globally.</li> </ol>"},{"location":"adr/ADR-094-explicit-dependency-injection/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Improved Testability: Dependencies can be easily replaced with mocks or fakes during testing by simply passing a different object to the constructor. This eliminates the need for managing global singleton state in tests.</li> <li>Explicit Dependencies: The <code>__init__</code> signature of a class now clearly declares all its required dependencies, improving code readability and maintainability.</li> <li>Clearer Object Graph: The application's object graph is constructed explicitly in one place (the Composition Root), making it easier to understand and debug.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Initial Refactoring Effort: This requires a significant upfront refactoring effort to modify the constructors of many classes throughout the framework.</li> <li>Increased Boilerplate at Startup: The <code>Athomic.startup</code> method will become more complex as it will be responsible for wiring the entire object graph.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This decision aligns the framework with patterns commonly used by mature DI containers, paving the way for potentially adopting a library like <code>dependency-injector</code> in the future if needed.</li> </ul> </li> </ul>"},{"location":"adr/ADR-095-stateful-stream-processing-with-tumbling-windows/","title":"ADR-095: Stateful Stream Processing with Tumbling Windows","text":"<ul> <li>Status: Proposed</li> <li>Date: 2025-10-05</li> </ul>"},{"location":"adr/ADR-095-stateful-stream-processing-with-tumbling-windows/#context","title":"Context","text":"<p>The current messaging infrastructure in <code>athomic</code> is primarily designed for stateless, individual message processing. While it excels at reliable delivery and ordered consumption within an aggregate, it lacks the capability to perform stateful operations over streams of events, such as aggregations within time windows. This prevents the implementation of advanced, real-time use cases like fraud detection (\"3 failed logins from an IP in 10 seconds\"), live analytics dashboards, or dynamic recommendations.</p>"},{"location":"adr/ADR-095-stateful-stream-processing-with-tumbling-windows/#decision","title":"Decision","text":"<p>We will introduce a Stateful Stream Processing capability into the <code>athomic</code> framework. The initial implementation will focus on Tumbling Windows (fixed, non-overlapping time windows).</p> <ol> <li>New Decorator: A new decorator, <code>@StreamProcessor</code>, will be created. It will allow developers to define a window (e.g., <code>Window.tumbling(seconds=30)</code>) and a grouping key (<code>group_by=\"customer_id\"</code>).</li> <li>State Management: A new <code>WindowStateManager</code> service will be responsible for managing the state of each window. It will use the existing <code>KVStoreProtocol</code> abstraction, requiring a backend that supports atomic operations (like Redis) for distributed consistency.</li> <li>Window Triggering: A new background service, <code>WindowTriggerService</code> (implementing <code>BaseService</code> ), will be introduced. This service will act as a \"ticker,\" periodically checking the <code>WindowStateManager</code> for windows that have closed.</li> <li>Handler Execution: When the <code>WindowTriggerService</code> identifies a closed window, it will dispatch a task via the existing <code>TaskBrokerProtocol</code> to execute the user's decorated handler function. The handler will receive the window's final state (e.g., a list of events or the final aggregation).</li> <li>Integration: The <code>MessageProcessor</code> will be updated to identify messages destined for a stream processor. Instead of calling the handler directly, it will pass the event to the <code>WindowStateManager</code> to update the corresponding window's state.</li> </ol>"},{"location":"adr/ADR-095-stateful-stream-processing-with-tumbling-windows/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Enables High-Value Features: Unlocks a new category of real-time, event-driven features within the framework.</li> <li>Developer Experience: The decorator-based approach abstracts away the immense complexity of distributed state management, windowing, and triggering, providing a simple API for developers.</li> <li>Architectural Consistency: Reuses existing, robust components like <code>KVStoreProtocol</code>, <code>TaskBrokerProtocol</code>, and <code>BaseService</code>, ensuring the new feature integrates cleanly with the framework's core principles.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Complexity: Introduces significant new complexity into the framework's core, including a new background service and sophisticated distributed state management logic.</li> <li>New Dependency: For this feature to be effective in a distributed environment, a performant KV store with atomic operations (like Redis) becomes a hard dependency.</li> <li>Latency: The time-based trigger mechanism of the <code>WindowTriggerService</code> introduces a small delay between the window's actual end time and the execution of the handler.</li> </ul> </li> </ul>"},{"location":"adr/ADR-096-causal-consistency-with-vector-clocks/","title":"ADR-096: Causal Consistency with Vector Clocks","text":"<ul> <li>Status: Proposed</li> <li>Date: 2025-10-05</li> </ul>"},{"location":"adr/ADR-096-causal-consistency-with-vector-clocks/#context","title":"Context","text":"<p>The framework's <code>ordered_consumer</code> successfully guarantees Total Order for events within a single aggregate (sharing the same <code>aggregate_key</code>). However, it does not address Causal Ordering between events from different services or aggregates. For example, an <code>OrderShipped</code> event (from a Shipping Service) that was caused by a <code>PaymentApproved</code> event (from a Payment Service) could be consumed by a downstream service (e.g., Analytics) before the <code>PaymentApproved</code> event, leading to an inconsistent state representation. This is a common and subtle problem in choreographed microservice architectures.</p>"},{"location":"adr/ADR-096-causal-consistency-with-vector-clocks/#decision","title":"Decision","text":"<p>We will implement a mechanism to enforce Causal Consistency using Vector Clocks.</p> <ol> <li>Vector Clock Model: A <code>VectorClock</code> data structure will be created to track the logical time of each service in the system (<code>Dict[str, int]</code>).</li> <li>Context Propagation: The current <code>VectorClock</code> will be added to the <code>ExecutionContext</code> and managed via a <code>contextvar</code>. It will be marked for automatic propagation.</li> <li>Producer Middleware: The <code>BaseProducer</code>'s <code>publish</code> method will be enhanced. Before sending a message, it will:     a. Retrieve the current <code>VectorClock</code> from the context.     b. Increment the logical time for the current service.     c. Inject the serialized <code>VectorClock</code> into the <code>MessageHeaders</code> of the outgoing message.</li> <li>Consumer Middleware &amp; Buffering: The <code>BaseMessageProcessor</code> will be enhanced. Before processing a message, it will:     a. Extract the message's <code>VectorClock</code> from its headers.     b. Compare it to the consumer service's local <code>VectorClock</code>.     c. If the message's clock is \"ahead\" of the local clock (indicating a missing causal event), the message will be temporarily stored in a \"Causal Waiting Room\" (backed by the <code>KVStoreProtocol</code> ).     d. If the message is the next expected event, it will be processed. After successful processing, the consumer's local clock will be merged with the message's clock.     e. After any successful processing, the \"Causal Waiting Room\" will be checked for messages that can now be processed.</li> </ol>"},{"location":"adr/ADR-096-causal-consistency-with-vector-clocks/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Guarantees Causal Consistency: Eliminates a class of subtle and hard-to-debug race conditions in event-driven architectures, leading to more robust and correct systems.</li> <li>Enhanced Debuggability: Vector Clocks can also detect concurrent events (which are not causally related), which is valuable information when debugging distributed flows.</li> <li>Integrates Cleanly: The proposed implementation leverages existing components like <code>ExecutionContext</code>, <code>MessageHeaders</code>, and <code>KVStoreProtocol</code>, maintaining architectural consistency.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Overhead: Adds a small amount of data to every message header and computational overhead to both the producer and consumer paths for clock management.</li> <li>Increased Latency: The consumer-side buffering for out-of-order causal messages introduces latency. Messages may be intentionally delayed while waiting for their causal dependencies to be met.</li> <li>Implementation Complexity: The logic for comparing clocks and managing the \"Causal Waiting Room\" is highly complex and must be implemented carefully to be performant and correct.</li> </ul> </li> </ul>"},{"location":"adr/ADR-097-standardized-management-of-multiple-connections/","title":"ADR-097: Standardized Management of Multiple Connections","text":"<ul> <li>Status: Accepted (Revised)</li> <li>Date: 2025-11-07</li> </ul>"},{"location":"adr/ADR-097-standardized-management-of-multiple-connections/#context","title":"Context","text":"<p>The Athomic framework initially managed configurations for different types of connections (e.g., document databases, key-value stores, messaging brokers) in distinct and inconsistent ways. Each manager (<code>ConnectionManager</code>, <code>ProducerManager</code>, <code>RegularConsumerOrchestrator</code>) implemented its own logic for discovering, parsing, and managing the lifecycle of its connections. This led to several architectural challenges:</p> <ul> <li>Violation of DRY (Don't Repeat Yourself): Boilerplate code for configuration parsing and lifecycle management (<code>_connect</code>, <code>_run_loop</code>, <code>_close</code>) was duplicated across different managers.</li> <li>Inconsistent Configuration: The structure of settings (<code>.toml</code> models) varied between modules, increasing the cognitive load.</li> <li>Violation of SRP (Single Responsibility Principle): Managers were responsible for both how to build their clients (discovery/factory logic) and how to manage their lifecycle (connect/run/close).</li> <li>Reduced Extensibility: Adding a new type of managed connection group (e.g., a cluster of HTTP clients) required writing a significant amount of new, repetitive code.</li> </ul> <p>The primary architectural force was the need to create a unified, scalable, and developer-friendly pattern for managing groups of named connections, adhering strictly to SOLID principles.</p>"},{"location":"adr/ADR-097-standardized-management-of-multiple-connections/#decision","title":"Decision","text":"<p>We have implemented a standardized, multi-layered architecture for connection management that separates lifecycle orchestration from client creation.</p> <ol> <li> <p>Standardized Configuration Model (<code>ConnectionGroupSettings</code>):</p> <ul> <li>A generic Pydantic <code>BaseModel</code> named <code>ConnectionGroupSettings</code> has been introduced. It enforces a consistent structure for all connection groups, containing <code>connections</code> (a <code>Dict</code>) and a <code>default_connection_name</code>.</li> <li>All relevant settings models (e.g., <code>DatabaseSettings</code>, <code>IntegrationSettings</code>) now use this generic model, ensuring a predictable structure.</li> </ul> </li> <li> <p>Abstract Lifecycle Manager (<code>BaseManager</code>):</p> <ul> <li>An abstract <code>BaseManager</code> class has been created to encapsulate all common lifecycle logic.</li> <li>Its sole responsibility is to orchestrate the lifecycle (<code>_connect</code>, <code>_run_loop</code>, <code>_close</code>) of a pool of managed services.</li> <li>It is not responsible for how those services are created.</li> </ul> </li> <li> <p>Standardized Client Creation (in <code>BaseManager</code>):</p> <ul> <li><code>BaseManager</code> provides a concrete, default implementation of the <code>before_start</code> method.</li> <li>This default method handles the most common use case: iterating over the <code>ConnectionGroupSettings</code>, resolving active configs, and creating one client instance per config using a <code>self.factory</code> (which must be set by the subclass).</li> </ul> </li> <li> <p>Specialized Managers (Inheritance and Overriding):</p> <ul> <li>All specialized managers now inherit from <code>BaseManager</code>.</li> <li>Simple Managers (e.g., <code>DocumentDBManager</code>, <code>KVStoreManager</code>, <code>ProducerManager</code>): These managers have simple 1:1 creation logic. They simply set <code>self.factory</code> in their <code>__init__</code> and inherit the default <code>before_start</code> implementation from <code>BaseManager</code>, requiring no custom build logic.</li> <li>Complex Managers/Orchestrators (e.g., <code>RegularConsumerOrchestrator</code>, <code>RepublisherManager</code>): These managers have unique, conditional build logic. They override the <code>before_start</code> method to implement their specific logic (e.g., checking <code>auto_start_consumers</code> or <code>republisher.enabled</code>) before creating their <code>ConsumerManager</code> instances.</li> </ul> </li> <li> <p>Facade Managers (Composition):</p> <ul> <li>High-level \"meta-managers\" like <code>ConnectionManager</code> and <code>MessagingManager</code> act as pure Facades.</li> <li>They are composed of one or more specialized managers (e.g., <code>ConnectionManager</code> has a <code>DocumentDBManager</code> and a <code>KVStoreManager</code>).</li> <li>Their only responsibility is to delegate the lifecycle calls (<code>_connect</code>, <code>_run_loop</code>, <code>_close</code>) to their child managers and expose their <code>get_...()</code> methods as a unified entry point.</li> </ul> </li> </ol>"},{"location":"adr/ADR-097-standardized-management-of-multiple-connections/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Consistency &amp; Predictability: All connection management and service pools now follow an identical, easy-to-understand pattern.</li> <li>Adherence to SOLID:<ul> <li>SRP: Each class has a single, clear responsibility:<ul> <li><code>BaseManager</code>: Manages the lifecycle of a service pool.</li> <li><code>SimpleManager</code> (e.g., <code>KVStoreManager</code>): Provides a factory to the <code>BaseManager</code>.</li> <li><code>ComplexManager</code> (e.g., <code>RepublisherManager</code>): Provides custom build logic for its pool.</li> <li><code>FacadeManager</code> (e.g., <code>ConnectionManager</code>): Orchestrates other managers.</li> </ul> </li> <li>Open/Closed Principle: Adding a new group of manageable clients is trivial. One only needs to create a new lightweight specialized manager inheriting from <code>BaseManager</code> and (if necessary) override <code>before_start</code>.</li> </ul> </li> <li>DRY Principle: All boilerplate for lifecycle management (<code>_connect</code>, <code>_run_loop</code>, <code>_close</code>) and simple (1:1) creation logic is centralized in <code>BaseManager</code>.</li> <li>Improved Testability: Components can be tested in greater isolation. The <code>BaseManager</code> lifecycle can be tested generically, and unit tests for components can easily mock the relevant <code>get_...()</code> method or <code>Factory.create</code> call.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Abstraction: The architecture introduces more layers and relies on both inheritance (for specialization) and composition (for facades). This is a justified trade-off for the gains in consistency and strict adherence to SOLID.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This establishes a clear and enforceable architectural pattern for any future feature that requires managing multiple, named client instances or services.</li> </ul> </li> </ul>"},{"location":"adr/ADR-098-event-sourcing-core-implementation/","title":"ADR-098: Event Sourcing Core Implementation","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-11-07</li> </ul>"},{"location":"adr/ADR-098-event-sourcing-core-implementation/#context","title":"Context","text":"<p>The <code>athomic</code> framework lacked a native pattern for services requiring a complete, immutable audit log, the ability to reconstruct state at any point in time, and a clear separation of write (commands) and read (projections) models. The initial proposal was to build an <code>EventSourcedRepository</code> that would append events directly to a Kafka topic and use the existing <code>KVStoreProtocol</code> for snapshots.</p> <p>The primary architectural constraint was adhering to <code>athomic</code>'s strict design principles (SOLID, SRP, DIP). Any solution must integrate with existing modules (<code>database</code>, <code>messaging</code>, <code>observability</code>) via their exposed protocols, without creating tight coupling to any specific provider (like <code>aiokafka</code>).</p> <p>A major challenge was identified: the <code>athomic.integration.messaging</code> module was designed for long-running streaming services (<code>ConsumerProtocol</code>) and did not expose a public protocol for on-demand client queries (i.e., \"read all messages for key X\"), which is essential for the <code>load</code> method of an event sourcing repository.</p>"},{"location":"adr/ADR-098-event-sourcing-core-implementation/#decision","title":"Decision","text":"<p>We decided to implement a new, dedicated module, <code>nala.athomic.event_sourcing</code>, built on a foundation of decoupled protocols:</p> <ol> <li> <p>Core Abstractions:</p> <ul> <li><code>Event</code>: A Pydantic <code>BaseModel</code> for all domain events.</li> <li><code>EventSourcedAggregate</code>: A base class managing internal state application via events.</li> <li><code>EventStoreProtocol</code>: An interface for the write-log (source of truth).</li> <li><code>SnapshotStoreProtocol</code>: An interface for the read-optimization cache.</li> </ul> </li> <li> <p>Orchestrator:</p> <ul> <li><code>EventSourcedRepository</code>: A generic orchestrator that depends only on the protocols above.</li> <li><code>SnapshotPolicyProtocol</code>: We introduced the Strategy Pattern to make the logic for when to save a snapshot (e.g., <code>FrequencyPolicy</code>) extensible.</li> </ul> </li> <li> <p>Solving the Reading Constraint:</p> <ul> <li>Instead of coupling the <code>event_sourcing</code> module to <code>aiokafka</code>, we expanded the <code>athomic.integration.messaging</code> module itself.</li> <li>We introduced a new <code>readers</code> module, defining a <code>TopicReaderProtocol</code> for on-demand client queries.</li> <li>We implemented a concrete <code>KafkaTopicReader</code> (<code>providers/kafka/reader.py</code>) that encapsulates the <code>AIOKafkaConsumer</code> logic for <code>seek_to_beginning</code> and <code>getmany</code> loops.</li> <li>This new protocol was then properly registered in <code>providers/register.py</code>.</li> </ul> </li> <li> <p>Concrete Drivers:</p> <ul> <li><code>MessagingEventStore</code>: Implements <code>EventStoreProtocol</code>. It now correctly depends on <code>ProducerProtocol</code> (for writing) and <code>TopicReaderFactory</code> (for reading), remaining 100% decoupled from Kafka. It continues to use <code>KVStoreProtocol</code> for distributed concurrency locking.</li> <li><code>KVStoreSnapshotStore</code>: Implements <code>SnapshotStoreProtocol</code> using the existing <code>athomic.database.kvstore</code> module.</li> <li><code>InMemory...Store</code>: Implementations for both protocols were created for testing.</li> </ul> </li> <li> <p>Integration:</p> <ul> <li>A new configuration schema (<code>config/schemas/event_sourcing.py</code>) was created and added to <code>AppSettings</code>.</li> <li>A central <code>EventSourcingFactory</code> (<code>event_sourcing/factory.py</code>) was created to read these settings and assemble the <code>EventSourcedRepository</code> with all its dependencies (<code>drivers</code>, <code>policies</code>) correctly injected.</li> <li>Full observability (tracing and metrics) was added to all new components.</li> </ul> </li> </ol>"},{"location":"adr/ADR-098-event-sourcing-core-implementation/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>The framework now has a robust, production-ready Event Sourcing capability.</li> <li>The solution fully adheres to <code>athomic</code>'s design principles (SRP, DIP), remaining decoupled and extensible.</li> <li>The <code>athomic.integration.messaging</code> module is now more capable, offering a new <code>TopicReaderProtocol</code> for on-demand client queries, which can benefit future framework features beyond event sourcing.</li> <li>The implementation is fully integrated with configuration (<code>AppSettings</code>), DI (factories), and observability (metrics, tracing).</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increases the overall complexity of the <code>athomic</code> framework by adding one new module (<code>event_sourcing</code>) and significantly expanding another (<code>messaging.readers</code>).</li> <li>The <code>MessagingEventStore</code> implementation has a production dependency on a <code>KVStore</code> (like Redis) to handle concurrency locking, in addition to the message broker.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>The <code>KafkaTopicReader</code> implementation (full scan-and-filter) is a standard pattern but may encounter performance hotspots on topics with millions of aggregates. This is an inherent trade-off of the \"Kafka-as-Event-Store\" pattern, not a flaw in the implementation itself.</li> </ul> </li> </ul>"},{"location":"adr/ADR-099-causal-consistency-vector-clocks/","title":"ADR-099: Implementing Causal Consistency via Vector Clocks","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-11-12</li> </ul>"},{"location":"adr/ADR-099-causal-consistency-vector-clocks/#context","title":"Context","text":"<p>In our distributed system, services communicate asynchronously via message brokers (e.g., Kafka). While Kafka guarantees order within a single partition, it cannot guarantee causal order across different topics or partitions. This leads to race conditions where an \"Effect\" event might be processed before its \"Cause\" event, resulting in subtle, hard-to-reproduce data consistency bugs.</p> <p>We needed a mechanism to guarantee that if Event A causes Event B, every consumer sees A before B, regardless of arrival order.</p> <p>Alternatives Considered: 1.  Total Order (Sequencer): Using a central service to stamp a global sequence number on every message. Rejected due to becoming a single point of failure and a massive performance bottleneck. 2.  Lamport Timestamps: Simpler to implement but cannot distinguish between concurrent events and causally related events, potentially forcing an artificial order on independent operations. Rejected in favor of Vector Clocks. 3.  Kafka-native Ordering: Relying solely on careful partitioning (hashing by <code>aggregate_key</code>). Rejected because it does not solve causality across different topics or when logic spans multiple services.</p> <p>Constraints: * The solution must be infrastructure-agnostic (work with Kafka, Local, etc.). * It must support high-throughput scenarios. * It must align with the existing <code>athomic</code> architecture (SRP, DI).</p>"},{"location":"adr/ADR-099-causal-consistency-vector-clocks/#decision","title":"Decision","text":"<p>We decided to implement Causal Consistency using Vector Clocks managed by the application layer (<code>nala.athomic</code>).</p> <p>Core Components: 1.  Vector Clocks: We implemented a <code>VectorClock</code> logic unit (<code>nala.athomic.causality</code>) to track and compare causal histories. 2.  Producer Middleware: The <code>BasePublishingStrategy</code> was modified to automatically inject the current service's Vector Clock into the message headers (<code>x-athomic-vector-clock</code>) before publishing. 3.  Causal Orchestration Strategy: We created a new <code>CausalOrchestrationStrategy</code> for consumers. It intercepts incoming messages and uses the Vector Clock to decide whether to:     * Process immediately: If the message is the direct causal successor.     * Buffer (Waiting Room): If the message arrives from the \"future\" (a gap in history is detected). It is stored in a <code>KVStore</code> (Redis Hash) until the missing dependencies arrive.     * Discard: If the message is a duplicate or from the \"past\". 4.  KVStore Evolution: We upgraded the <code>KVStoreProtocol</code> to officially support Hash operations (<code>hset</code>, <code>hgetall</code>, <code>hdel</code>) to efficiently manage the \"Waiting Room\" state, implementing this in both Redis and Local providers. 5.  Decoupled Fetch vs. Processing: To support high throughput without sacrificing causal correctness, we refactored the <code>ConsumerFactory</code>.     * Renamed <code>batch_size</code> to <code>fetch_size</code> in configurations to clarify intent.     * Separated the \"Fetch Strategy\" (I/O optimization) from the \"Processing Strategy\" (Business Logic).     * Pseudo-Batching: Causal consumers use batch fetching for I/O efficiency but process messages one-by-one (<code>processing_mode=\"single\"</code>) to apply causal checks and state updates atomically per message.</p> <p>API Changes: * Introduced <code>@causal_consumer</code> decorator. * Updated <code>@subscribe_to</code> to map <code>batch_size</code> to <code>fetch_size</code> + <code>processing_mode=\"batch\"</code> (preserving legacy \"True Batch\" behavior). * Updated <code>@ordered_consumer</code> to use <code>fetch_size</code> + <code>processing_mode=\"single\"</code>.</p>"},{"location":"adr/ADR-099-causal-consistency-vector-clocks/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Strong Consistency: Guarantees <code>Cause -&gt; Effect</code> processing order across the entire distributed system, eliminating a class of race-condition bugs.</li> <li>Infrastructure Agnostic: The ordering logic lives in the framework, not the broker, making the system resilient to re-partitioning or broker changes.</li> <li>I/O Efficiency: The \"Pseudo-Batch\" architecture allows <code>CausalConsumer</code> and <code>OrderedConsumer</code> to fetch messages in bulk (reducing network RTT) while still processing them sequentially for correctness.</li> <li>Flexibility: Developers can choose between \"True Batch\" (bulk processing, no ordering) and \"Causal/Ordered\" (sequential processing, ordering guarantees) via explicit decorators.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Complexity: The consumer state management (Waiting Room) adds significant complexity. Consumers are now stateful and depend on a persistence layer (Redis).</li> <li>Message Overhead: Every message now carries a Vector Clock header, which grows linearly with the number of participating services (though typically small).</li> <li>Latency: Messages arriving out-of-order are buffered, introducing latency for those specific messages until the causal gap is filled.</li> <li>No Bulk Inserts for Causal: Since causal correctness requires processing messages sequentially to update the local clock, <code>bulk_insert</code> optimizations in the database are not possible for <code>@causal_consumer</code>.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>Redis Dependency: The Causal and Ordered strategies heavily rely on Redis for the State Store and Waiting Room. High availability of Redis becomes critical for message processing.</li> </ul> </li> </ul>"},{"location":"adr/ADR-100-batch-and-ordered-consumer-idempotency/","title":"ADR-100: Batch and Ordered Consumer Idempotency Strategies","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-11-16</li> </ul>"},{"location":"adr/ADR-100-batch-and-ordered-consumer-idempotency/#context","title":"Context","text":"<p>The Athomic framework required robust mechanisms to handle message consumption guarantees (\"at-least-once\") provided by brokers like Kafka. Processing duplicate messages could corrupt system state. We identified two distinct processing patterns with conflicting requirements:</p> <ol> <li>High Throughput (Batch): Consumers processing large volumes of data need to handle messages in batches to optimize I/O. Applying distributed locking sequentially (one by one) negates the performance benefits of batching.</li> <li>Strict Ordering: Business domains require processing events in the exact order they arrive for a specific entity. Batching logic that processes messages in parallel or filters them aggressively could break causal chains.</li> </ol> <p>Additionally, key generation logic across the framework was fragmented, leading to inconsistent namespace usage and potential key collisions between caching, locking, and rate limiting modules.</p>"},{"location":"adr/ADR-100-batch-and-ordered-consumer-idempotency/#decision","title":"Decision","text":"<p>We decided to implement a unified Idempotency architecture supported by extended KV Store capabilities and standardized key generation.</p>"},{"location":"adr/ADR-100-batch-and-ordered-consumer-idempotency/#1-kv-store-batch-extensions","title":"1. KV Store Batch Extensions","text":"<p>To support high-performance locking, we extended the <code>KVStoreProtocol</code> and its implementations (<code>RedisKVClient</code>, <code>LocalKVClient</code>) to support atomic batch operations: * <code>set_many(mapping, ttl, nx=True)</code>: Uses Redis Pipelines to attempt setting multiple keys in a single network round-trip. The <code>nx=True</code> flag implements \"Set if Not Exists\", effectively functioning as a Batch Distributed Lock. * <code>delete_many(keys)</code>: Allows releasing multiple locks simultaneously.</p>"},{"location":"adr/ADR-100-batch-and-ordered-consumer-idempotency/#2-idempotency-handler-modes","title":"2. Idempotency Handler Modes","text":"<p>The <code>IdempotencyHandler</code> was refactored to support two distinct modes of operation:</p> <ul> <li> <p>Batch Mode (<code>acquire_batch</code>):</p> <ul> <li>Used by <code>@subscribe_to</code> when <code>batch_size &gt; 0</code>.</li> <li>Extracts keys from all messages using <code>ContextKeyResolvers</code>.</li> <li>Performs a single <code>acquire_batch</code> call to the store.</li> <li>Filtering: Messages that fail to acquire a lock (duplicates) are filtered out before the user handler is invoked.</li> <li>Transactionality: If the handler succeeds, locks are confirmed (TTL extended). If it fails, all locks for that batch are released immediately to allow retries.</li> </ul> </li> <li> <p>Single-Flight Mode (<code>execute</code>):</p> <ul> <li>Used by <code>@ordered_consumer</code> and standard RPC calls.</li> <li>Follows the Cache-Aside pattern: Check Cache -&gt; Acquire Lock -&gt; Compute -&gt; Store Result.</li> <li>Ensures strict sequential execution required for ordered processing.</li> </ul> </li> </ul>"},{"location":"adr/ADR-100-batch-and-ordered-consumer-idempotency/#3-standardized-key-generation","title":"3. Standardized Key Generation","text":"<p>We unified all key creation logic into two components: * <code>ContextKeyGenerator</code> (Renamed from <code>ContextualKeyGenerator</code>): Responsible for Construction. It combines static prefixes, namespaces, tenant IDs, and logical keys into a consistent format (e.g., <code>nala:tenant_1:idempotency:order_123</code>). * <code>ContextKeyResolvers</code>: Responsible for Extraction. A strategy pattern to reliably extract business keys (e.g., <code>transaction_id</code>, <code>message_id</code>) from various object types (Pydantic models, Dicts, Message Headers).</p>"},{"location":"adr/ADR-100-batch-and-ordered-consumer-idempotency/#4-ordered-consumer-constraints","title":"4. Ordered Consumer Constraints","text":"<p>The <code>@ordered_consumer</code> decorator now strictly enforces <code>processing_mode=\"single\"</code>. While it supports <code>fetch_size</code> for broker I/O optimization, it does not expose a batch interface to the application developer, preserving the guarantee that messages are processed one by one in order.</p>"},{"location":"adr/ADR-100-batch-and-ordered-consumer-idempotency/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-100-batch-and-ordered-consumer-idempotency/#positive","title":"Positive","text":"<ul> <li>Performance: Batch idempotency reduces Redis interactions from $O(N)$ to $O(1)$ network calls per batch, significantly lowering latency for high-throughput topics.</li> <li>Consistency: The <code>ContextKeyGenerator</code> eliminates classes of bugs related to key collision or malformed keys across different modules (Rate Limiter, Cache, Lock).</li> <li>Flexibility: Developers can switch between Batch and Ordered processing simply by changing decorators (<code>@subscribe_to</code> vs <code>@ordered_consumer</code>), with the framework handling the complex locking logic transparently.</li> <li>Fail-Safety: The \"Rollback on Failure\" mechanism in batch mode prevents deadlocks where keys remain locked after a consumer crash, ensuring messages can be retried by the broker.</li> </ul>"},{"location":"adr/ADR-100-batch-and-ordered-consumer-idempotency/#negative","title":"Negative","text":"<ul> <li>Atomic Batch Error Handling: In batch mode, the unit of work is the batch. If processing fails for valid messages after locks are acquired, the entire batch is rolled back. This might lead to reprocessing of successful messages within that failed batch during the retry (though idempotency will catch them on the next run).</li> <li>Complexity: The internal logic of <code>_apply_batch_idempotency</code> involves careful state management (filtering, tracking acquired keys) which increases the maintenance burden of the framework core.</li> </ul>"},{"location":"adr/ADR-100-batch-and-ordered-consumer-idempotency/#neutral","title":"Neutral","text":"<ul> <li>Redis Dependency: The performance gains rely heavily on Redis Pipelining. The implementation assumes a low-latency connection to the KV Store.</li> </ul>"},{"location":"adr/ADR-101-agnostic-workflow-orchestration/","title":"ADR-101: Agnostic Durable Workflow Orchestration","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-11-25</li> </ul>"},{"location":"adr/ADR-101-agnostic-workflow-orchestration/#context","title":"Context","text":"<p>The Nala framework requires a robust mechanism to orchestrate long-running, complex, and mission-critical business processes (e.g., Saga patterns for distributed transactions, payment processing, or multi-step provisioning). While traditional event-driven architectures (using Kafka consumers) handle high throughput well, they struggle with maintaining complex state over long periods and managing compensations (rollbacks) effectively without significant boilerplate code.</p> <p>We selected Temporal.io as the industry-standard engine for Durable Execution to solve these state management problems. However, integrating Temporal directly presents several challenges:</p> <ol> <li>Testability &amp; Developer Experience: Testing Temporal workflows typically requires a running Temporal server (often via Docker), which introduces significant latency and infrastructure overhead to the unit testing loop (TDD).</li> <li>Data Privacy &amp; Efficiency: Nala has strict requirements for payload handling, including transparent encryption, compression, and the \"Claim Check\" pattern for large payloads. Relying solely on Temporal's default Data Converter would bypass our established security and efficiency pipelines located in <code>nala.athomic.payload</code>.</li> <li>Vendor Coupling: While unlikely to change soon, tightly coupling domain logic to a specific infrastructure SDK violates our Clean Architecture principles.</li> </ol>"},{"location":"adr/ADR-101-agnostic-workflow-orchestration/#decision","title":"Decision","text":"<p>We decided to implement an Agnostic Workflow Orchestration Module (<code>nala.athomic.workflows</code>) that abstracts the underlying execution engine.</p> <p>This architecture relies on three key pillars:</p> <ol> <li> <p>Protocol Abstraction:     We defined a <code>WorkflowClientProtocol</code> that enforces a unified API for starting, signaling, querying, and terminating workflows. This allows the application code to remain unaware of whether it is running against a real Temporal cluster or a local simulation.</p> </li> <li> <p>Polymorphic Provider Strategy:     We implemented the Strategy Pattern via a Factory to switch backends based on configuration (<code>settings.workflows.backend</code>):</p> <ul> <li><code>TemporalWorkflowClient</code> (Production/Staging): Wraps the official <code>temporalio</code> SDK. It manages connection pooling, mTLS authentication, and maps Nala's exceptions to Temporal's error handling mechanisms.</li> <li><code>LocalWorkflowClient</code> (Development/Testing): An in-memory implementation using Python's <code>asyncio.Task</code>. It simulates the lifecycle of a workflow within the current process, allowing for millisecond-speed unit tests without external infrastructure.</li> </ul> </li> <li> <p>Payload Pipeline Adapter:     We integrated the Nala <code>PayloadProcessor</code> directly into the workflow engine's serialization layer.</p> <ul> <li>We created a custom <code>AthomicDataConverter</code> and <code>AthomicPayloadConverter</code> that act as adapters.</li> <li>These adapters intercept data ingress/egress in Temporal and delegate the actual processing to Nala's async <code>PayloadProcessor</code>.</li> <li>Since Temporal's Data Converter interface is synchronous, we utilized a concurrency bridge (<code>run_sync</code>) to offload the asynchronous encryption/compression tasks to a thread pool, ensuring non-blocking compatibility with the Temporal worker loop.</li> </ul> </li> </ol>"},{"location":"adr/ADR-101-agnostic-workflow-orchestration/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Zero-Infra Unit Tests: Developers can write and run tests for complex workflows instantly using the <code>local</code> backend, significantly accelerating the feedback loop.</li> <li>Transparent Security: All data entering or leaving a workflow is automatically encrypted and compressed according to the global <code>PayloadSettings</code>, ensuring compliance without burdening the workflow developer.</li> <li>Consistency: The API for starting a workflow is identical in all environments, reducing the cognitive load when switching context.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Feature Parity Limitations: The <code>LocalWorkflowClient</code> is a simulation. It does not support advanced Temporal features such as \"Child Workflows,\" complex Cron schedules, or \"Signal with Start\" with 100% fidelity. Tests relying on these specific behaviors still require integration tests against a real Temporal server.</li> <li>Performance Overhead: The <code>run_sync</code> bridge introduces a thread context switch for every payload serialization/deserialization. While necessary for the async-sync integration, it adds a slight CPU and latency overhead compared to raw JSON handling.</li> <li>Serialization Boundaries: The <code>LocalWorkflowClient</code> must explicitly simulate serialization (encoding/decoding to bytes) to catch errors where developers might pass non-serializable objects (like raw <code>datetime</code> without encoders) that would otherwise work in memory but fail in production.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>Async/Sync Impedance: The implementation highlights the complexity of mixing synchronous SDKs (Temporal's serialization layer) with asynchronous frameworks (Nala). Future SDK updates from Temporal might offer async hooks, allowing us to remove the threading bridge.</li> </ul> </li> </ul>"},{"location":"adr/ADR-102-service-mesh-readiness/","title":"ADR-102: Service Mesh Readiness (Cloud-Native Mode)","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-11-27</li> </ul>"},{"location":"adr/ADR-102-service-mesh-readiness/#context","title":"Context","text":"<p>The Nala framework was originally designed as a \"Smart Client\" chassis, embedding sophisticated logic for Service Discovery (via Consul), Resilience (Retries, Circuit Breakers), and internal load balancing.</p> <p>However, as we migrate towards Cloud-Native environments using Kubernetes and Service Meshes (e.g., Istio, Linkerd, AWS App Mesh), these \"Smart Client\" features introduce conflict and redundancy: 1.  Retry Storms: If both the application and the Service Mesh sidecar retry a failed request, the traffic amplification can destabilize the system ($N_{app} \\times M_{mesh}$). 2.  Discovery Conflicts: The Mesh handles service location via Envoy proxies and K8s DNS. Nala's active polling of Consul is unnecessary overhead and bypasses the Mesh's traffic shifting capabilities. 3.  Observability Gaps: Without explicit W3C context propagation, traces generated by the application are disconnected from traces generated by the Mesh sidecars.</p> <p>We needed a way to allow the framework to operate in a \"bimodal\" capacity: fully featured for standalone deployments, and lightweight (\"Dumb Client\") for Mesh deployments, without code duplication.</p>"},{"location":"adr/ADR-102-service-mesh-readiness/#decision","title":"Decision","text":"<p>We decided to implement a global Deployment Strategy pattern controlled by a single configuration flag: <code>DEPLOYMENT_MODE</code>.</p> <ol> <li> <p>Configuration Authority:</p> <ul> <li>Introduced <code>DEPLOYMENT_MODE</code> (Enum: <code>standalone</code> | <code>mesh</code>) in <code>AppSettings</code>.</li> <li>This acts as the master switch for framework behavior.</li> </ul> </li> <li> <p>Strategy Injection via Decorators:</p> <ul> <li>We implemented a <code>@apply_mesh_strategy</code> decorator.</li> <li>This decorator intercepts <code>Factory.create()</code> methods. When <code>DEPLOYMENT_MODE=\"mesh\"</code>, it bypasses the standard logic and injects specialized \"Cloud-Native\" implementations (e.g., <code>K8sDnsDiscoveryProvider</code>, <code>NoOpRetryPolicy</code>).</li> <li>This keeps the core domain logic clean and adheres to the Open/Closed Principle.</li> </ul> </li> <li> <p>Resilience Offloading:</p> <ul> <li>Retries: In Mesh mode, the internal retry logic is replaced by a <code>NoOpRetryPolicy</code> (single attempt), delegating retry logic to the Sidecar proxy (VirtualService).</li> <li>Circuit Breakers: Replaced by <code>NoOpCircuitBreakerService</code> (pass-through), delegating outlier detection to the Mesh.</li> </ul> </li> <li> <p>Security &amp; Discovery Offloading:</p> <ul> <li>mTLS: <code>HttpClientFactory</code> automatically disables internal SSL verification (<code>verify=False</code>) in Mesh mode, trusting the local Sidecar to handle mTLS tunnels.</li> <li>Discovery: Uses native Kubernetes DNS (FQDN) instead of Consul lookups.</li> </ul> </li> <li> <p>Observability Continuity:</p> <ul> <li>Enforced explicit injection of W3C <code>traceparent</code> headers using <code>opentelemetry.propagate</code> within the <code>HttpClientBase</code>. This ensures application spans are correctly parented to Mesh spans.</li> </ul> </li> </ol>"},{"location":"adr/ADR-102-service-mesh-readiness/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Infrastructure alignment: Eliminates conflict with Istio/Envoy features, preventing \"Retry Storms\" and routing loops.</li> <li>Clean Architecture: The use of decorators keeps the existing standalone logic untouched and verifiable.</li> <li>Operational Simplicity: Switching modes requires only an environment variable change (<code>NALA_DEPLOYMENT_MODE=mesh</code>), enabling smooth migration paths.</li> <li>Full Observability: Distributed traces are now seamless between the Application code and the Infrastructure network layer.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Configuration Opacity: Developers might be confused why their <code>settings.toml</code> retry configurations are ignored when running in K8s. This requires strict documentation.</li> <li>Debugging Complexity: Issues related to networking (retries, timeouts) now require checking both the Application logs and the Envoy/Istio configuration, increasing the cognitive load during troubleshooting.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>Future Scope: Currently, this only covers Synchronous (HTTP) communication. Asynchronous patterns (Kafka) and Database drivers will require similar treatment in future ADRs to ensure full trace propagation and mTLS support.</li> </ul> </li> </ul>"},{"location":"adr/ADR-103-graph-database-infrastructure-neo4j/","title":"ADR-103: Graph Database Infrastructure (Neo4j)","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-11-28</li> </ul>"},{"location":"adr/ADR-103-graph-database-infrastructure-neo4j/#context","title":"Context","text":"<p>As the Athomic evolves to support advanced Data Lineage features and complex relationship mapping, our existing storage solutions (Document-based MongoDB and Key-Value Redis) have proven insufficient for efficiently querying deep hierarchical structures and network topologies.</p> <p>We required a dedicated Graph Database solution to model entities like Services, Topics, and Events as nodes and relationships.</p> <p>The integration needed to adhere to the Athomic framework principles: 1.  Asynchronous Operation: Full support for <code>async/await</code> IO to match the FastAPI architecture. 2.  Abstraction: Decoupling the application logic from the specific database driver via Protocols. 3.  Observability: Native integration with tracing (OpenTelemetry) and metrics (Prometheus). 4.  Lifecycle Management: Consistent startup/shutdown handling via Managers. 5.  Configuration: Support for connection groups (multi-tenancy) via Dynaconf.</p>"},{"location":"adr/ADR-103-graph-database-infrastructure-neo4j/#decision","title":"Decision","text":"<p>We decided to implement the Graph Database Module within <code>nala.athomic.database.graph</code>, adopting Neo4j as the primary backend.</p> <p>The implementation strategy includes:</p> <ol> <li>Protocol Definition: Created <code>GraphDatabaseProtocol</code> to define the contract for graph operations (e.g., <code>execute_query</code>), ensuring Dependency Inversion.</li> <li>Neo4j Provider: Implemented <code>Neo4jProvider</code> using the official <code>neo4j</code> Python driver (version 5.x+) which supports asynchronous sessions.</li> <li>Observability Wrapper: Introduced <code>BaseGraphStore</code> to wrap the provider calls, automatically injecting OpenTelemetry spans and Prometheus metrics (latency/count) for every query.</li> <li>Lifecycle Management: Implemented <code>GraphManager</code> (inheriting from <code>BaseManager</code>) to handle the connection lifecycle (connect/disconnect) and support multiple named connections (Connection Groups) via <code>GraphSettings</code>.</li> <li>Factory Pattern: Utilized <code>GraphDatabaseFactory</code> to instantiate providers dynamically based on the configuration (<code>settings.toml</code>), ensuring Singleton behavior where appropriate.</li> <li>Integration: Updated the main <code>ConnectionManager</code> to orchestrate the <code>GraphManager</code> alongside Document and KVStore managers.</li> </ol>"},{"location":"adr/ADR-103-graph-database-infrastructure-neo4j/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-103-graph-database-infrastructure-neo4j/#positive","title":"Positive","text":"<ul> <li>Native Graph capabilities: We can now execute efficient Cypher queries for Data Lineage and Impact Analysis, which would be computationally expensive or impossible in MongoDB.</li> <li>Architectural Consistency: The module follows the exact same patterns (Factory, Manager, Protocol) as the existing Document and KVStore modules, reducing cognitive load for developers.</li> <li>Observability: All graph interactions are automatically traced and metered, providing immediate visibility into database performance.</li> <li>Flexibility: The <code>GraphDatabaseProtocol</code> allows us to swap Neo4j for another graph database (e.g., Amazon Neptune) in the future with minimal changes to domain logic.</li> </ul>"},{"location":"adr/ADR-103-graph-database-infrastructure-neo4j/#negative","title":"Negative","text":"<ul> <li>Infrastructure Complexity: Introducing Neo4j adds another piece of infrastructure to maintain, monitor, and backup in production environments.</li> <li>Learning Curve: Developers working on Lineage features must learn Cypher (Neo4j's query language), which is distinct from SQL or Mongo Query Language.</li> <li>Driver Limitations: While the official Neo4j driver supports async, some edge-case error handling (like specific transaction errors) requires careful mapping to our internal exceptions to avoid leaking driver details.</li> </ul>"},{"location":"adr/ADR-103-graph-database-infrastructure-neo4j/#neutralother","title":"Neutral/Other","text":"<ul> <li>Async-First: The implementation is strictly asynchronous. Synchronous contexts (if any exist in legacy code) will need to use bridges (<code>asyncio.to_thread</code> or <code>run_async_from_sync</code>) to interact with the graph database.</li> </ul>"},{"location":"adr/ADR-104-refactoring-lineage-architecture-provider-collector-pattern/","title":"ADR-104: Refactoring Lineage Architecture to Provider-Collector Pattern and Composite Storage","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-12-01</li> </ul>"},{"location":"adr/ADR-104-refactoring-lineage-architecture-provider-collector-pattern/#context","title":"Context","text":"<p>The initial implementation of Data Lineage in the Athomic framework relied on a monolithic <code>LineageProcessor</code> singleton. This component was responsible for both constructing OpenLineage events and persisting them directly to a configured backend (e.g., Logging or Messaging).</p> <p>This approach presented several limitations: 1.  Coupling: The application producing the lineage was tightly coupled to the storage implementation details. 2.  Single Destination: It was difficult to configure multiple storage backends simultaneously (e.g., storing in a Document DB for auditing while projecting to a Graph DB for impact analysis). 3.  Performance Risk: Heavy persistence logic (like writing to Neo4j) executed within the application's request/response cycle or consumer loop could impact throughput. 4.  Rigid Configuration: The configuration mixed \"identity\" settings (who is producing) with \"storage\" settings (where it goes).</p> <p>We needed a solution that decouples event emission from persistence, allows for multiple simultaneous storage backends, and ensures the application core remains lightweight.</p>"},{"location":"adr/ADR-104-refactoring-lineage-architecture-provider-collector-pattern/#decision","title":"Decision","text":"<p>We decided to refactor the Lineage subsystem into a Provider-Collector Architecture with Composite Storage.</p> <ol> <li> <p>Separation of Concerns (Producer vs. Collector):</p> <ul> <li>LineageProducer: A lightweight service responsible solely for constructing <code>LineageEvent</code> objects and publishing them to a messaging topic (<code>platform.lineage.events</code>). It handles the \"Identity\" aspect (Namespace, Producer URI).</li> <li>LineageCollectorService: A dedicated consumer service that subscribes to the lineage topic. It is responsible for hydration, context restoration, and delegating persistence to the storage layer.</li> </ul> </li> <li> <p>Dynamic Storage Management (<code>LineageManager</code>):</p> <ul> <li>We introduced a <code>LineageManager</code> that acts as a factory/orchestrator. It dynamically initializes storage backends based on the configuration provided in <code>settings.toml</code>.</li> </ul> </li> <li> <p>Composite Storage Pattern:</p> <ul> <li>We implemented <code>CompositeLineageStore</code>. When multiple backends are configured (e.g., <code>document</code> AND <code>graph</code>), the manager wraps them in a composite.</li> <li>The <code>save()</code> method broadcasts the event to all active stores concurrently.</li> <li>Fail-Fast Strategy: If writing to any store fails, the exception is propagated to trigger the messaging retry/DLQ mechanism, ensuring data consistency across all stores.</li> </ul> </li> <li> <p>Specific Store Implementations:</p> <ul> <li>DocumentLineageStore (MongoDB): Stores the raw OpenLineage event payload for audit trails and historical retrieval.</li> <li>Neo4jLineageStore (Graph): Projects the event into a property graph (Job -&gt; Run -&gt; Dataset) for visualization and impact analysis.</li> </ul> </li> <li> <p>Configuration Refactoring:</p> <ul> <li>Split <code>LineageSettings</code> into <code>LineageProducerSettings</code> (Identity) and <code>LineageStoreSettings</code> (Storage).</li> <li>Removed the monolithic <code>LineageProcessor</code> entirely.</li> </ul> </li> </ol>"},{"location":"adr/ADR-104-refactoring-lineage-architecture-provider-collector-pattern/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Decoupling: The application only needs to know how to emit events to a topic. It is unaware of whether the data goes to Mongo, Neo4j, or logs.</li> <li>Scalability: Persistence load is offloaded to the <code>LineageCollectorService</code>, which can be scaled independently of the main application.</li> <li>Observability Depth: We can now support both Audit (Document) and Analysis (Graph) use cases simultaneously without code changes.</li> <li>Resilience: Failures in the Graph DB (e.g., connection timeout) do not block the main application flow, only the Collector consumer (which has retries and DLQ).</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Complexity: The architecture introduces more moving parts (Producer, Collector, Manager, Stores) compared to the previous single class.</li> <li>Eventual Consistency: Data lineage information is no longer available immediately after execution; there is a small latency due to the async messaging transport.</li> <li>Configuration Overhead: Requires more detailed configuration in <code>settings.toml</code> to define multiple backends.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>The system relies heavily on the reliability of the underlying Messaging Broker (Kafka) to ensure lineage events are not lost between the Producer and the Collector.</li> </ul> </li> </ul>"},{"location":"adr/ADR-105-modular-vector-store-architecture/","title":"ADR-105: Modular Vector Store Architecture","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-12-04</li> </ul>"},{"location":"adr/ADR-105-modular-vector-store-architecture/#context","title":"Context","text":"<p>The application requires vector database capabilities to support semantic search and Retrieval-Augmented Generation (RAG) features.</p> <p>We identified several constraints and requirements: 1.  Vendor Agnosticism: We must not couple the core application logic to a specific vector database vendor (e.g., Qdrant or Weaviate) to avoid vendor lock-in and allow for easy switching based on infrastructure needs or costs. 2.  Observability: Every interaction with the vector database (search, upsert, delete) must be traced (OpenTelemetry) and measured (Prometheus metrics) consistently, regardless of the underlying backend. 3.  Multi-Tenancy: The system requires strict tenant isolation. The logic for injecting tenant context should be uniform and not scattered across business logic. 4.  Lifecycle Management: Database connections must integrate with the application's startup/shutdown lifecycle to ensure connection pooling and graceful termination.</p>"},{"location":"adr/ADR-105-modular-vector-store-architecture/#decision","title":"Decision","text":"<p>We decided to implement a modular, provider-based architecture for the Vector Store module using the following patterns:</p> <ol> <li> <p>Protocol-Based Abstraction:     We defined a <code>VectorStoreProtocol</code> that enforces a standard API for essential operations: <code>create_collection</code>, <code>upsert</code>, <code>search</code>, and <code>delete</code>. All interactions within the domain layer must rely on this protocol, not concrete implementations.</p> </li> <li> <p>Unified Data Transfer Objects (DTOs):     We created database-agnostic Pydantic models, <code>VectorRecord</code> and <code>SearchResult</code>, to standardize data exchange. This decouples the internal data representation from specific vendor SDK objects (e.g., Qdrant <code>PointStruct</code> or Weaviate objects).</p> </li> <li> <p>Template Method Pattern for Observability:     We implemented an abstract base class, <code>BaseVectorStore</code>.</p> <ul> <li>Public Methods: The public methods (<code>search</code>, <code>upsert</code>) are concrete and handle cross-cutting concerns such as readiness checks, tracing (<code>vectordb.{operation}</code>), and metrics recording.</li> <li>Protected Abstract Methods: These methods delegate the actual execution to provider-specific implementations (e.g., <code>_search</code>, <code>_upsert</code>).</li> <li>This ensures that observability is enforced uniformly without code duplication in every provider.</li> </ul> </li> <li> <p>Wrapper Pattern for Cross-Cutting Concerns:     Functionality that modifies behavior without changing the core provider logic is implemented via Wrappers (Decorators).</p> <ul> <li>ContextAwareVectorStore: This wrapper intercepts calls to inject the current <code>tenant_id</code> from the application context into the request, ensuring seamless multi-tenancy isolation.</li> </ul> </li> <li> <p>Factory-Based Instantiation:     A <code>VectorStoreFactory</code> is responsible for instantiating the correct provider based on configuration (<code>qdrant</code> or <code>weaviate</code>) and dynamically applying the configured wrappers.</p> </li> </ol>"},{"location":"adr/ADR-105-modular-vector-store-architecture/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Vendor Independence: Switching between Qdrant and Weaviate is a configuration change. The business logic remains untouched.</li> <li>Consistent Observability: Metrics and Traces are guaranteed for all vector operations, aiding significantly in debugging and performance monitoring.</li> <li>Simplified Multi-Tenancy: Developers do not need to manually pass tenant IDs in every service call; the wrapper handles context injection automatically.</li> <li>Testability: The <code>VectorStoreProtocol</code> allows for easy mocking of the vector layer in unit tests without spinning up actual database containers.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>\"Lowest Common Denominator\" API: To maintain a generic protocol, we may lose access to niche, vendor-specific features (e.g., specific quantization settings or complex hybrid search parameters) unless we extend the protocol or allow \"pass-through\" kwargs.</li> <li>Serialization Overhead: Converting between internal Pydantic models and vendor-specific SDK objects adds a slight serialization/deserialization overhead to every operation.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>Lifecycle Management: The <code>VectorManager</code> acts as the bridge between the <code>BaseVectorStore</code> and the application lifecycle, handling connection locks and state transitions.</li> </ul> </li> </ul>"},{"location":"adr/ADR-106-ai-provider-abstraction/","title":"ADR-106: AI Provider Abstraction Strategy","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-12-04</li> </ul>"},{"location":"adr/ADR-106-ai-provider-abstraction/#context","title":"Context","text":"<p>The application needs to integrate with multiple Generative AI providers (e.g., OpenAI, Google Vertex AI) for text generation and vector embeddings. Directly coupling the domain logic to specific vendor SDKs (like <code>openai</code> or <code>google-cloud-aiplatform</code>) creates vendor lock-in, makes testing difficult, and complicates the configuration management when switching providers based on cost or availability. Furthermore, different providers handle \"Structured Output\" (JSON extraction) differently, requiring a unified interface to ensure reliability across models.</p>"},{"location":"adr/ADR-106-ai-provider-abstraction/#decision","title":"Decision","text":"<p>We decided to implement a Protocol-based abstraction layer with a Registry pattern for both LLMs and Embeddings.</p>"},{"location":"adr/ADR-106-ai-provider-abstraction/#1-unified-protocols","title":"1. Unified Protocols","text":"<p>We defined <code>LLMProviderProtocol</code> and <code>EmbeddingModelProtocol</code> to enforce a standard API contract. * LLM Protocol: Enforces <code>generate_content</code> for unstructured text and <code>generate_structured</code> for strict Pydantic schema extraction. * Embedding Protocol: Explicitly separates <code>embed_documents</code> (for storage) and <code>embed_query</code> (for retrieval) to support asymmetric embedding models (like Google Vertex/Gecko).</p> <pre><code>class LLMProviderProtocol(Protocol):\n    async def generate_content(self, prompt: str, ...) -&gt; str: ...\n    async def generate_structured(self, prompt: str, response_model: Type[T], ...) -&gt; T: ...\n</code></pre>"},{"location":"adr/ADR-106-ai-provider-abstraction/#2-registry-pattern","title":"2. Registry Pattern","text":"<p>We implemented <code>LLMProviderRegistry</code> and <code>EmbeddingRegistry</code>. These registries map simple backend strings (e.g., \"openai\", \"vertex\") to their concrete provider classes. This allows the application to select the implementation at runtime based on the <code>settings.toml</code> configuration.</p>"},{"location":"adr/ADR-106-ai-provider-abstraction/#3-standardized-exception-hierarchy","title":"3. Standardized Exception Hierarchy","text":"<p>All vendor-specific errors are caught within the provider implementation and re-raised as standardized Athomic exceptions. This ensures that upstream consumers do not need to handle <code>openai.RateLimitError</code> or <code>google.api_core.exceptions.ResourceExhausted</code> separately. * <code>ProviderError</code>: Generic 5xx API errors. * <code>AuthenticationError</code>: Invalid keys or permissions. * <code>ContextWindowExceededError</code>: Token limit violations.</p>"},{"location":"adr/ADR-106-ai-provider-abstraction/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Vendor Agnosticism: Switching from GPT-4 to Gemini requires only a configuration change; no code changes are needed in the business logic.</li> <li>Structured Data Reliability: The <code>generate_structured</code> method standardizes how JSON is extracted and validated against Pydantic models, abstracting the differences between OpenAI Tools and Vertex Function Calling.</li> <li>Resilience: Unified error handling allows for generic retry policies and circuit breakers to work effectively across different providers.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Feature Lag: New, vendor-specific features (e.g., OpenAI Assistants API, specific caching parameters) are not immediately available via the generic protocol unless specific extensions are added.</li> <li>Abstraction Leakage: Prompt engineering often varies by model family. A prompt optimized for GPT-4 might perform poorly on Gemini, requiring model-specific prompt management despite the code abstraction.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>Lazy Loading: Dependencies for providers are imported locally within the registry to avoid installing heavy SDKs (like <code>vertexai</code>) if they are not used in a particular environment.</li> </ul> </li> </ul>"},{"location":"adr/ADR-107-ai-governance-and-observability/","title":"ADR-107: AI Governance and Observability Architecture","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-12-04</li> </ul>"},{"location":"adr/ADR-107-ai-governance-and-observability/#context","title":"Context","text":"<p>Interacting with Large Language Models (LLMs) introduces specific challenges compared to standard external API calls: 1.  Non-determinism &amp; Latency: LLM calls are slow and variable. We need deep visibility into duration, token usage, and model parameters to optimize performance and costs. 2.  Security &amp; Compliance: We must ensure that sensitive data (PII) is not sent to third-party providers and that usage complies with internal quotas and budgets. 3.  Provider Diversity: We use multiple providers (OpenAI, Vertex AI). Implementing logging, metrics, and safety checks individually for each provider leads to code duplication and inconsistent enforcement.</p> <p>We needed a pattern that guarantees these cross-cutting concerns are addressed for every call, regardless of the underlying provider implementation.</p>"},{"location":"adr/ADR-107-ai-governance-and-observability/#decision","title":"Decision","text":"<p>We decided to implement the Template Method Design Pattern combined with an Interceptor (Guard) Pattern to enforce governance and observability at the base abstraction level.</p>"},{"location":"adr/ADR-107-ai-governance-and-observability/#1-template-method-in-base-classes","title":"1. Template Method in Base Classes","text":"<p>We implemented abstract base classes (<code>BaseLLM</code>, <code>BaseEmbedding</code>) that define the public-facing methods (e.g., <code>generate_content</code>) as \"templates\".</p> <p>These template methods are responsible for the standard execution flow: 1.  Readiness Check: Ensures the service is initialized. 2.  Tracing Initialization: Starts an OpenTelemetry span with standard semantic attributes (<code>llm.model</code>, <code>llm.provider</code>, <code>llm.system</code>). 3.  Guard Execution: Iterates through configured <code>AIGuardProtocol</code> instances (Interceptors) to validate the request before sending it. 4.  Delegation: Calls the abstract protected method (e.g., <code>_generate_content</code>) which must be implemented by concrete provider classes to handle the specific API logic. 5.  Metrics Recording: Records duration and operation counts (<code>llm_operations_total</code>) in a <code>finally</code> block to capture both successes and failures.</p>"},{"location":"adr/ADR-107-ai-governance-and-observability/#2-governance-guards","title":"2. Governance Guards","text":"<p>We defined the <code>AIGuardProtocol</code> to standardize safety checks. Guards act as middleware that can inspect the prompt and metadata. * RateLimitGuard: Adapts the centralized <code>RateLimiterService</code> to the AI context, allowing limits to be scoped by user, tenant, or model family.</p> <pre><code># Example of the Template Method implementation\nasync def generate_content(self, prompt: str, **kwargs):\n    # 1. Observability Start\n    with self.tracer.start_as_current_span(\"llm.generate\") as span:\n        try:\n            # 2. Governance\n            await self._run_guards(prompt, **kwargs)\n\n            # 3. Provider Execution (Abstract)\n            result = await self._generate_content(prompt, **kwargs)\n\n            return result\n        finally:\n             # 4. Metrics\n             self._record_metrics(...)\n</code></pre>"},{"location":"adr/ADR-107-ai-governance-and-observability/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Guaranteed Consistency: It is impossible to implement a new LLM provider in the system without inheriting the base class, which automatically enforces logging, tracing, and metrics standards.</li> <li>Centralized Governance: Security policies (like PII filtering or rate limiting) are configured globally and applied to all providers. Changing a policy requires updating only the Guard configuration, not every provider integration.</li> <li>Vendor-Agnostic Telemetry: Metrics are normalized (e.g., standard token counting attributes), making it easier to build unified dashboards for cost and performance monitoring across different vendors.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Latency Overhead: Executing a chain of guards adds a small latency penalty to every request. While usually negligible compared to LLM inference time, extensive text analysis guards (e.g., PII scanning) could become a bottleneck.</li> <li>Implementation Rigidity: Concrete providers are forced to adhere strictly to the base class structure. If a provider has a radically different interaction model that doesn't fit the <code>generate_content</code> template, the abstraction might become leaky or restrictive.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>Token Usage Dependency: Accurate cost tracking relies on the underlying provider returning usage metadata (prompt/completion token counts). If a provider API changes or omits this data, the base class metrics will be incomplete.</li> </ul> </li> </ul>"},{"location":"adr/ADR-108-semantic-memory-service/","title":"ADR-108: Semantic Memory Service Architecture","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-12-04</li> </ul>"},{"location":"adr/ADR-108-semantic-memory-service/#context","title":"Context","text":"<p>Building Retrieval-Augmented Generation (RAG) features requires coordinating two distinct infrastructure components: an Embedding Model (to convert text to vectors) and a Vector Store (to persist and search these vectors).</p> <p>Developers were previously manually wiring these components in various parts of the application, leading to: 1.  Inconsistent Implementations: Different modules handled vectorization and storage differently. 2.  Incorrect Embedding Usage: Modern embedding models (e.g., Google Vertex <code>text-embedding-gecko</code>) require specific task types for \"retrieval query\" vs \"retrieval document\". Manual implementation often missed this distinction, degrading search quality. 3.  Tight Coupling: Business logic became coupled to low-level vector DB operations and specific AI provider SDKs. 4.  Metadata Fragmentation: There was no standard way to associate metadata (authors, timestamps, sources) with the stored vectors.</p>"},{"location":"adr/ADR-108-semantic-memory-service/#decision","title":"Decision","text":"<p>We decided to implement a high-level Semantic Memory Service that acts as an orchestrator facade over the Embedding and Vector Database modules.</p>"},{"location":"adr/ADR-108-semantic-memory-service/#1-orchestration-pattern","title":"1. Orchestration Pattern","text":"<p>The <code>SemanticMemoryService</code> encapsulates the entire RAG workflow, abstracting the complexity of the underlying components: * Add Memory: Orchestrates <code>Text -&gt; Embedding (Document Mode) -&gt; VectorRecord -&gt; VectorStore Upsert</code>. * Recall Memory: Orchestrates <code>Query -&gt; Embedding (Query Mode) -&gt; VectorStore Search -&gt; Domain Object Mapping</code>.</p>"},{"location":"adr/ADR-108-semantic-memory-service/#2-asymmetric-embedding-support","title":"2. Asymmetric Embedding Support","text":"<p>The service explicitly distinguishes between embedding a document for storage (<code>embed_documents</code>) and embedding a query for search (<code>embed_query</code>). This ensures compatibility with asymmetric embedding models that optimize vectors differently based on the task type.</p>"},{"location":"adr/ADR-108-semantic-memory-service/#3-unified-data-model","title":"3. Unified Data Model","text":"<p>We introduced domain-specific Pydantic models (<code>Memory</code>, <code>MemorySearchResult</code>) to represent semantic data. * <code>Memory</code>: Represents the stored unit, including content, metadata, and a unique ID. * <code>MemorySearchResult</code>: Wraps a <code>Memory</code> with a relevance score, decoupling the consumer from the raw vector DB response format (e.g., <code>ScoredPoint</code>).</p>"},{"location":"adr/ADR-108-semantic-memory-service/#4-factory-based-dependency-injection","title":"4. Factory-Based Dependency Injection","text":"<p>The service is instantiated via a <code>SemanticMemoryFactory</code>. This factory automatically resolves the active <code>VectorStore</code> from the global <code>ConnectionManager</code> and creates the appropriate <code>EmbeddingModel</code> based on the AI settings, ensuring the service always uses the correctly configured infrastructure.</p> <pre><code># Simplified workflow implemented by the service\nasync def recall(self, query: str):\n    # 1. Trace and Measure\n    with tracer.start_as_current_span(\"memory.recall\"):\n        # 2. Embed Query (Task: Retrieval Query)\n        query_vector = await self._embedding_model.embed_query(query)\n\n        # 3. Search Vector DB\n        results = await self._store.search(..., query_vector=query_vector)\n\n        # 4. Map to Domain Models\n        return [MemorySearchResult(memory=..., score=res.score) for res in results]\n</code></pre>"},{"location":"adr/ADR-108-semantic-memory-service/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Simplified Developer Experience (DX): Developers interact with a high-level API (<code>add</code>, <code>recall</code>, <code>forget</code>) without needing to understand vector dimensions, distance metrics, or DB-specific upsert syntax.</li> <li>Correctness by Design: Enforces the correct embedding task types (asymmetric embeddings), preventing silent performance degradation in semantic search.</li> <li>Portability: The underlying Vector Store (Qdrant, Weaviate) or Embedding Model (OpenAI, Vertex) can be swapped via configuration without changing the memory service contract or client code.</li> <li>Standardized Metadata: Automatically handles timestamping (<code>created_at</code>) and metadata persistence, ensuring a consistent schema for stored memories.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Consistency Latency: Vector databases are often eventually consistent. The <code>add</code> operation returns immediately, but <code>recall</code> might not find the item instantly (indexing delay). The abstraction does not strictly mask this behavior.</li> <li>Generic Metadata Schema: While flexible, the use of <code>Dict[str, Any]</code> for metadata prevents strict schema validation for the context stored alongside the text, potentially leading to data quality issues if not managed carefully by the caller.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>Observability: The service adds its own tracing span (<code>memory.add</code>, <code>memory.recall</code>), providing a clear view of the orchestration overhead separate from the raw database or AI provider latency.</li> </ul> </li> </ul>"},{"location":"adr/ADR-109-decoupled-prompt-management-system/","title":"ADR-109: Decoupled Prompt Management System","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-12-08</li> </ul>"},{"location":"adr/ADR-109-decoupled-prompt-management-system/#context","title":"Context","text":"<p>The application requires a robust mechanism to manage the text prompts sent to Large Language Models (LLMs). </p> <p>Previously, prompts might have been hardcoded strings or managed in an ad-hoc manner. This approach presented several issues: 1.  Lack of Versioning: Modifying a prompt for testing could break production flows. 2.  Tight Coupling: Logic for reading files and formatting text was mixed with business logic. 3.  Limited Logic: Simple f-strings do not support complex conditions (e.g., \"if user has history, include summary\") required for advanced Agentic patterns. 4.  Testing Difficulty: Hard to mock file reads or complex template logic in unit tests.</p> <p>We needed a system that separates the storage of prompts (File, DB) from the rendering of prompts (Jinja2, f-string) and orchestrates them via a unified Service.</p>"},{"location":"adr/ADR-109-decoupled-prompt-management-system/#decision","title":"Decision","text":"<p>We have decided to implement a Decoupled Prompt Management System based on a layered architecture comprising Infrastructure (IO), Logic (Render), and Service layers.</p>"},{"location":"adr/ADR-109-decoupled-prompt-management-system/#1-architectural-components","title":"1. Architectural Components","text":"<ul> <li> <p>IO Layer (Loaders): Responsible solely for retrieving the prompt definition (metadata + raw template) from a storage backend.</p> <ul> <li>We defined a <code>PromptLoaderProtocol</code> to enforce consistency.</li> <li>The initial implementation is <code>FileSystemPromptLoader</code>, which reads versioned YAML files from a directory structure (e.g., <code>prompts/sentiment/v1.0.0.yaml</code>).</li> <li>It implements Semantic Versioning Resolution: If no version is requested, the loader automatically resolves the highest semantic version (e.g., <code>v1.10.0</code> &gt; <code>v1.9.0</code>).</li> </ul> </li> <li> <p>Render Layer (Engines): Responsible solely for interpolating variables into the raw template string.</p> <ul> <li>We defined a <code>PromptRendererProtocol</code>.</li> <li>The chosen default engine is Jinja2 (<code>JinjaPromptRenderer</code>). It allows for loops, conditionals, and filters within prompts.</li> <li>We enforce <code>StrictUndefined</code>, meaning the system throws a <code>RenderError</code> immediately if a required variable is missing, preventing \"hallucinated\" empty values sent to the LLM.</li> </ul> </li> <li> <p>Service Layer (Orchestrator): The <code>PromptService</code> acts as a Facade.</p> <ul> <li>It initializes the specific Loader and Renderer via Factories based on configuration.</li> <li>It exposes a clean API: <code>render(name, variables)</code>.</li> </ul> </li> </ul>"},{"location":"adr/ADR-109-decoupled-prompt-management-system/#2-configuration-factories","title":"2. Configuration &amp; Factories","text":"<ul> <li>We implemented the Abstract Factory Pattern. <code>PromptLoaderFactory</code> and <code>PromptRendererFactory</code> instantiate the correct classes based on <code>settings.backend</code> (e.g., \"filesystem\") and <code>settings.renderer</code> (e.g., \"jinja2\").</li> <li>Simplification: We explicitly decided not to use <code>ConnectionGroupSettings</code> for this module initially. The <code>PromptService</code> will operate as a single-tenant/single-connection service to reduce complexity. The configuration is a direct <code>PromptSettings</code> object.</li> </ul>"},{"location":"adr/ADR-109-decoupled-prompt-management-system/#3-data-structure","title":"3. Data Structure","text":"<ul> <li>We utilize Pydantic models (<code>PromptTemplate</code>) to strictly validate the schema of loaded prompts (ensuring fields like <code>input_variables</code> and <code>template</code> exist) before any processing occurs.</li> </ul>"},{"location":"adr/ADR-109-decoupled-prompt-management-system/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-109-decoupled-prompt-management-system/#positive","title":"Positive","text":"<ul> <li>High Extensibility: We can switch from FileSystem to MongoDB or Redis storage in the future by simply adding a new Loader class and changing one line in <code>settings.toml</code>. The Service code remains untouched.</li> <li>Robustness: The Jinja2 integration with strict validation ensures that incomplete data contexts are caught at the application layer, not at the API level of the LLM provider.</li> <li>Safe Experimentation: The versioning system allows developers to deploy <code>v2.0.0</code> of a prompt alongside <code>v1.0.0</code>, enabling A/B testing or Canary deployments by simply passing a version string to the service.</li> <li>Testability: The modular design allows unit testing the Renderer (logic) separately from the Loader (I/O), and integration testing the Service with mocked components.</li> </ul>"},{"location":"adr/ADR-109-decoupled-prompt-management-system/#negative","title":"Negative","text":"<ul> <li>Filesystem Latency: The current <code>FileSystemPromptLoader</code> reads from the disk on every request (unless the OS caches it). High-throughput scenarios might require implementing an in-memory caching decorator over the Loader.</li> <li>Complexity: The architecture introduces more boilerplate (factories, registries, protocols) compared to a simple function that reads a file.</li> <li>File Management: Developers must adhere to a strict directory and naming convention (<code>prompt_name/vX.Y.Z.yaml</code>) for the loader to function correctly.</li> </ul>"},{"location":"adr/ADR-109-decoupled-prompt-management-system/#neutral","title":"Neutral","text":"<ul> <li>Jinja2 Overhead: While powerful, Jinja2 is slower than Python f-strings. Given the latency of LLM calls (seconds), the rendering time (microseconds) is negligible, but it is a dependency addition.</li> </ul>"},{"location":"adr/ADR-110-modular-ai-agent-architecture/","title":"ADR-110: Implementation of Modular AI Agent Architecture","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-12-09</li> </ul>"},{"location":"adr/ADR-110-modular-ai-agent-architecture/#context","title":"Context","text":"<p>As the Nala application evolves from simple text completion tasks to complex, multi-step workflows, a need has emerged to orchestrate interactions between Large Language Models (LLMs) and external tools. The previous implementation of LLM providers allowed for direct generation but lacked the state management, tool execution logic, and reasoning loops required for autonomous agents.</p> <p>We faced the following requirements and constraints: * Provider Agnosticism: The system must support multiple LLM backends (OpenAI, Google Vertex AI, Ollama) without changing the core agent logic. * Tool Execution Safety: Executing tools (functions) generated by LLMs requires a robust layer to handle errors, async/sync execution, and lifecycle management without crashing the main application loop. * Configuration-Driven: Agents should be definable via configuration profiles (system prompts, model selection, tool access) rather than hardcoded classes. * Observability: The reasoning loop (Think-Act-Observe) must be traceable for debugging and monitoring.</p>"},{"location":"adr/ADR-110-modular-ai-agent-architecture/#decision","title":"Decision","text":"<p>We have decided to implement a modular Agentic Architecture centered around the ReAct (Reasoning + Acting) pattern. The architecture consists of the following key components:</p> <ol> <li>AgentService: The core orchestrator that manages the conversation history and implements the ReAct loop. It sends history to the LLM, parses the response, decides whether to call tools, and feeds the results back into the context.</li> <li>Tool Execution Layer:<ul> <li>We introduced <code>BaseToolExecutor</code> and <code>SyncToolExecutor</code> to decouple tool execution from the agent logic.</li> <li>The executor handles tool lookup, lifecycle (connect/stop), and safe execution (catching exceptions and formatting them as standard error outputs).</li> </ul> </li> <li>LLM Manager: A <code>LLMManager</code> class was introduced to manage the lifecycle of multiple LLM connections, serving as a factory and registry for model providers.</li> <li>Standardized Schemas: We defined domain-specific Pydantic models (<code>ChatMessage</code>, <code>ToolCall</code>, <code>ToolOutput</code>) in <code>nala.athomic.ai.schemas</code>. This acts as an anti-corruption layer, translating provider-specific formats (e.g., OpenAI JSON) into a unified internal format.</li> <li>Profile-Based Configuration: Agents are instantiated via <code>AgentFactory</code> using <code>AgentProfileSettings</code>. This allows defining agent behaviors (e.g., <code>max_iterations</code>, <code>system_prompt</code>, <code>tools</code> list) entirely through configuration files.</li> </ol>"},{"location":"adr/ADR-110-modular-ai-agent-architecture/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Decoupling: The agent logic is now completely independent of the specific LLM provider. Switching from OpenAI to Google GenAI is a configuration change.</li> <li>Robustness: The <code>SyncToolExecutor</code> wraps tool execution in try/except blocks, ensuring that a single tool failure does not crash the agent, but instead returns a standardized error message to the LLM.</li> <li>Flexibility: New agents can be created by defining a new profile in the settings without writing new Python code, provided the necessary tools are registered.</li> <li>Observability: The <code>AgentService</code> integrates with the tracing system, creating spans for the main run and individual steps, making it easier to debug reasoning loops.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Complexity: The architecture introduces several layers of abstraction (Service, Executor, Manager, Factory), which increases the cognitive load compared to a simple script calling an API.</li> <li>Latency: The ReAct loop implies sequential round-trips to the LLM (Reason -&gt; Tool -&gt; Reason), which inherently increases the total request duration compared to a single-shot completion.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>State Management: Currently, conversation history is held in memory during the <code>run</code> execution. Future iterations may need to integrate with the <code>nala.athomic.ai.memory</code> module for persistent long-term memory.</li> </ul> </li> </ul>"},{"location":"adr/ADR-111-modular-document-ingestion-pipeline/","title":"ADR-111: Modular Document Ingestion Pipeline","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-12-10</li> </ul>"},{"location":"adr/ADR-111-modular-document-ingestion-pipeline/#context","title":"Context","text":"<p>To enable Retrieval-Augmented Generation (RAG) capabilities within the Nala platform, the system requires a robust mechanism to transform unstructured data (files like PDF, Markdown, Text) into searchable vector embeddings.</p> <p>The previous <code>SemanticMemoryService</code> provided the low-level capability to store and retrieve vectors but lacked the higher-level logic to: 1.  Load content from various file formats while preserving metadata (e.g., page numbers). 2.  Split long texts into semantically meaningful chunks suitable for context windows. 3.  Orchestrate the end-to-end flow from a raw file to a persisted memory.</p> <p>We needed a solution that adheres to the Open/Closed Principle (OCP), allowing the addition of new file formats (e.g., DOCX, HTML) without modifying the core ingestion logic. Additionally, configuration for Embedding models needed to be aligned with the existing LLM infrastructure.</p>"},{"location":"adr/ADR-111-modular-document-ingestion-pipeline/#decision","title":"Decision","text":"<p>We have decided to implement a Modular Document Ingestion Pipeline based on the Strategy and Facade patterns.</p> <p>Key architectural decisions include:</p> <ol> <li> <p>Protocol-Based Abstractions:</p> <ul> <li>Defined <code>DocumentLoaderProtocol</code> for file parsing strategies.</li> <li>Defined <code>TextSplitterProtocol</code> for chunking strategies.</li> <li>Implemented <code>RecursiveCharacterTextSplitter</code> as the default splitting strategy to preserve semantic context.</li> </ul> </li> <li> <p>Loader Registry:</p> <ul> <li>Created <code>DocumentLoaderRegistry</code> to dynamically map file extensions (e.g., <code>.pdf</code>, <code>.txt</code>) to specific loader implementations.</li> <li>Adopted <code>pypdf</code> as the standard library for PDF extraction due to its reliability and page-level extraction capabilities.</li> </ul> </li> <li> <p>Service Orchestration:</p> <ul> <li>Implemented <code>DocumentIngestionService</code> as a Facade. It coordinates the Loader, Splitter, and the existing <code>SemanticMemoryService</code>.</li> <li>The service handles the \"Extract -&gt; Transform -&gt; Load\" (ETL) lifecycle of a document.</li> </ul> </li> <li> <p>Shared Configuration for Embeddings:</p> <ul> <li>Instead of creating a separate configuration section for Embeddings, we decided to reuse the existing <code>ai.connections</code> group (used by LLMs).</li> <li>Introduced <code>EmbeddingManager</code> (mirroring <code>LLMManager</code>) to manage the lifecycle of embedding providers using these shared connection settings.</li> </ul> </li> <li> <p>Factory Pattern for Dependency Injection:</p> <ul> <li>Implemented <code>DocumentIngestionFactory</code> to resolve the complex dependency graph (<code>VectorStore</code> + <code>EmbeddingModel</code> -&gt; <code>MemoryService</code> -&gt; <code>IngestionService</code>) based on application settings, simplifying the bootstrap process.</li> </ul> </li> </ol>"},{"location":"adr/ADR-111-modular-document-ingestion-pipeline/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Extensibility: New file formats can be supported simply by implementing a new Loader and registering it, without touching the <code>DocumentIngestionService</code>.</li> <li>Granularity: The pipeline preserves metadata (filename, page numbers) and allows fine-grained control over chunk sizes and overlap, essential for high-quality RAG.</li> <li>Observability: Specific metrics (<code>ai_document_ingestion_duration_seconds</code>, <code>ai_document_pages_loaded_total</code>) were introduced to monitor the throughput and latency of the parsing and splitting phases.</li> <li>Consistency: Reusing <code>ai.connections</code> prevents configuration duplication and simplifies credential management for providers that offer both Chat and Embedding APIs (e.g., OpenAI).</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Compute Overhead: Text splitting and PDF parsing are CPU-intensive operations. In high-load scenarios, this might impact the event loop if not offloaded to a worker thread or separate process (though <code>BaseService</code> structure allows for future offloading).</li> <li>Dependency Growth: Added <code>pypdf</code> as a hard dependency.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>Feature Parity: Currently, the <code>SemanticMemoryService</code> does not support dynamic collection targeting per batch request. The ingestion service currently relies on the default collection configured at startup. This may need refactoring in the future if dynamic multi-tenancy per request is required.</li> </ul> </li> </ul>"},{"location":"adr/ADR-112-refactoring-ai-evaluation-dependencies-and-testing/","title":"ADR-112: Refactoring AI Evaluation Dependencies and Integration Test Strategy","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-12-11</li> </ul>"},{"location":"adr/ADR-112-refactoring-ai-evaluation-dependencies-and-testing/#context","title":"Context","text":"<p>During the development and integration testing of the AI Evaluation module (<code>EvaluationService</code>), several structural and operational issues were identified that prevented a stable and reliable release:</p> <ol> <li>Dependency Injection Failure: The <code>GEvalJudge</code> component required a <code>PromptService</code> to render evaluation templates, but the parent <code>EvaluationService</code> was not designed to inject this dependency. This led to <code>TypeError</code> during service initialization (<code>__init__() missing 1 required positional argument: 'prompt_service'</code>).</li> <li>Configuration Robustness: The <code>GoogleGenAIProvider</code> implementation strictly expected the <code>api_key</code> setting to be a Pydantic <code>SecretStr</code> object. However, depending on the environment loader (Dynaconf vs. raw Environment Variables), this setting often arrived as a plain <code>str</code>, causing <code>AttributeError: 'str' object has no attribute 'get_secret_value'</code> and crashing the connection flow.</li> <li>Integration Test Instability: The initial testing strategy relied on local LLMs (Ollama running Llama 3.2 1B/3B). This proved inviable for CI/CD due to:<ul> <li>High Latency: Single evaluations took over 4 minutes on CPU, leading to timeouts.</li> <li>Resource Contention: Parallel execution of metrics (<code>OLLAMA_NUM_PARALLEL</code>) caused CPU thrashing and failures.</li> <li>Model Hallucinations: Smaller models (1B) failed to strictly adhere to the complex JSON schema required by the <code>GEvalJudge</code> (using <code>instructor</code>), returning invalid tool calls.</li> </ul> </li> </ol>"},{"location":"adr/ADR-112-refactoring-ai-evaluation-dependencies-and-testing/#decision","title":"Decision","text":"<p>To resolve these issues and establish a robust foundation for AI Evaluation, we made the following architectural and implementation decisions:</p> <ol> <li> <p>Explicit Dependency Injection:</p> <ul> <li>Refactored <code>EvaluationService</code> and its factory to explicitly accept <code>PromptService</code> as a constructor argument.</li> <li>This instance is propagated down to the <code>GEvalJudge</code>, ensuring clean separation of concerns and testability.</li> </ul> </li> <li> <p>Polymorphic Secret Resolution in Providers:</p> <ul> <li>Updated the <code>GoogleGenAIProvider</code> (and established a pattern for others) to implement a hybrid check for credentials.</li> <li>The <code>_connect</code> method now detects if <code>api_key</code> is a <code>SecretStr</code> (calling <code>.get_secret_value()</code>) or a raw <code>str</code>, supporting both secure production configurations and simple test environments.</li> </ul> </li> <li> <p>Cloud-Native Integration Testing:</p> <ul> <li>Shifted the integration test suite (<code>test_int_evaluation_workflows.py</code>) to use Google GenAI (Gemini 1.5 Flash) as the Judge backend instead of local models.</li> <li>Configured the test fixture to support <code>GOOGLE_API_KEY</code> injection from <code>.env</code> or environment variables seamlessly via Dynaconf.</li> </ul> </li> </ol>"},{"location":"adr/ADR-112-refactoring-ai-evaluation-dependencies-and-testing/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Stability: Integration tests now pass consistently (<code>PASSED</code>) and are resilient to configuration nuances.</li> <li>Performance: Execution time for evaluation workflows dropped from ~4 minutes to seconds.</li> <li>Maintainability: The dependency graph is explicit; no components are instantiated implicitly within methods, facilitating unit testing with mocks.</li> <li>Correctness: The use of a stronger model (Gemini) for testing ensures that schema validation errors are true positives, not model capability issues.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>External Dependency: Running the full integration suite now requires a valid <code>GOOGLE_API_KEY</code> and internet access, removing the \"air-gapped\" capability of the default test run.</li> <li>Configuration Complexity: Developers must ensure their <code>.env</code> files are correctly set up with API keys for local testing.</li> </ul> </li> <li> <p>Neutral:</p> <ul> <li>Metric thresholds were adjusted (e.g., <code>0.5</code> or <code>0.7</code>) to align with the baseline strictness of the Gemini model.</li> </ul> </li> </ul>"},{"location":"adr/ADR-113-modular-rag-architecture-and-factory-refactoring/","title":"ADR-113: Modular RAG Architecture and Factory Refactoring","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-12-12</li> </ul>"},{"location":"adr/ADR-113-modular-rag-architecture-and-factory-refactoring/#context","title":"Context","text":"<p>The Nala Athomic framework previously provided individual building blocks for AI applications, such as <code>LLMProvider</code>, <code>EmbeddingProvider</code>, and <code>SemanticMemoryService</code>. However, there was no standardized, production-ready orchestration layer for Retrieval-Augmented Generation (RAG).</p> <p>Developers had to manually wire these components together, leading to inconsistent implementations, lack of standardized observability (metrics/tracing), and difficulty in switching between different retrieval or generation strategies (e.g., switching from \"Vector Search\" to \"Hybrid Search\" or from \"Stuff\" to \"Map-Reduce\" generation).</p> <p>Furthermore, the existing Factory implementations (<code>EmbeddingFactory</code>, <code>SemanticMemoryFactory</code>) relied on loose signatures (passing string names or partial configurations), which made dependency injection fragile and harder to validate against the strict Pydantic configuration schemas defined in the project.</p>"},{"location":"adr/ADR-113-modular-rag-architecture-and-factory-refactoring/#decision","title":"Decision","text":"<p>We decided to implement a dedicated RAG Module (<code>nala.athomic.ai.rag</code>) based on the Strategy Pattern and refactor the underlying dependency injection factories to rely strictly on Pydantic Settings objects.</p>"},{"location":"adr/ADR-113-modular-rag-architecture-and-factory-refactoring/#1-rag-service-architecture","title":"1. RAG Service Architecture","text":"<p>We introduced <code>RAGService</code> as the central orchestrator, implementing <code>RAGServiceProtocol</code>. The architecture separates concerns into two distinct strategies:</p> <ul> <li>Retrieval Strategy (<code>RetrievalStrategyProtocol</code>): Responsible solely for fetching <code>SourceDocument</code>s. The default implementation is <code>VectorRetrievalStrategy</code>, which delegates to <code>SemanticMemoryService</code>.</li> <li>Generation Strategy (<code>GenerationStrategyProtocol</code>): Responsible for synthesizing the final answer using an LLM. The default implementation is <code>StuffGenerationStrategy</code>, which concatenates context into a prompt using Jinja2 templates.</li> </ul>"},{"location":"adr/ADR-113-modular-rag-architecture-and-factory-refactoring/#2-factory-refactoring","title":"2. Factory Refactoring","text":"<p>We refactored <code>EmbeddingFactory</code>, <code>SemanticMemoryFactory</code>, and the new <code>RAGFactory</code> to enforce Configuration-Driven Dependency Injection. * Factories now accept full Settings objects (e.g., <code>LLMConnectionSettings</code>, <code>SemanticMemorySettings</code>) instead of individual parameters. * Factories automatically resolve dependencies (like <code>VectorStore</code> or <code>EmbeddingModel</code>) via the global <code>ConnectionManager</code> and <code>EmbeddingManager</code> if they are not explicitly injected.</p>"},{"location":"adr/ADR-113-modular-rag-architecture-and-factory-refactoring/#3-observability-and-streaming","title":"3. Observability and Streaming","text":"<p>The <code>RAGService</code> was designed with \"Day 2\" operations in mind: * Streaming First: Added <code>stream_response</code> yielding typed <code>RAGChunk</code> events (Citations, Content Deltas, Metadata) to support responsive UIs. * Instrumentation: Native integration with OpenTelemetry traces and Prometheus metrics (e.g., <code>rag_operation_duration_seconds</code>, <code>rag_retrieved_chunks_count</code>).</p>"},{"location":"adr/ADR-113-modular-rag-architecture-and-factory-refactoring/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-113-modular-rag-architecture-and-factory-refactoring/#positive","title":"Positive","text":"<ul> <li>Modularity: New retrieval methods (e.g., HyDE, Re-ranking) or generation patterns (e.g., Refine) can be added by implementing new Strategies without modifying the core <code>RAGService</code>.</li> <li>Type Safety: Refactored factories significantly reduce runtime errors by validating configuration through Pydantic before service instantiation.</li> <li>Observability: RAG pipelines are now automatically instrumented, providing visibility into retrieval latency, token usage, and context quality (empty context rates).</li> <li>Standardized API: The <code>RAGRequest</code> and <code>RAGResponse</code> models provide a consistent contract for all RAG operations across the framework.</li> </ul>"},{"location":"adr/ADR-113-modular-rag-architecture-and-factory-refactoring/#negative","title":"Negative","text":"<ul> <li>Breaking Changes: The signature of <code>EmbeddingFactory.create</code> has changed. Code relying on passing a simple string name to this factory must be updated to pass <code>LLMConnectionSettings</code>.</li> <li>Configuration Verbosity: The <code>RAGSettings</code> schema requires explicit definition of connection names (e.g., <code>llm_connection_name</code>), which increases the size of the <code>settings.toml</code> file.</li> </ul>"},{"location":"adr/ADR-113-modular-rag-architecture-and-factory-refactoring/#neutral","title":"Neutral","text":"<ul> <li>Dependency Management: The <code>SemanticMemoryFactory</code> now strictly depends on <code>EmbeddingManager</code> being initialized. This enforces a stricter startup order in the application lifecycle (Database -&gt; AI Connections -&gt; Memory -&gt; RAG).</li> </ul>"},{"location":"adr/ADR-114-centralizing-ai-governance-with-guard-pipeline/","title":"ADR-114: Centralizing AI Governance with Guard Pipeline","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-12-13</li> </ul>"},{"location":"adr/ADR-114-centralizing-ai-governance-with-guard-pipeline/#context","title":"Context","text":"<p>The initial design for AI Governance (Guardrails) placed the execution of checks (<code>AIGuardProtocol.check</code>) directly inside the core <code>BaseLLM</code> provider class (e.g., <code>OpenAIProvider</code>, <code>GoogleGenAIProvider</code>). This tightly coupled the security/governance logic with the underlying LLM communication logic.</p> <p>As the number of guards grew (Rate Limiting, PII Detection, Keyword Blocking) and the need for complex, configurable execution flows emerged (e.g., fail-fast vs. aggregate-violations for input, continue-on-error for output), this tight coupling became a constraint.</p> <p>The existing structure made it difficult to: 1.  Implement Output Guards (Sanitization/Processing) which require modifying the LLM's response after generation. 2.  Manage a sequence of guards with different failure modes (e.g., a critical rate limit vs. a softer PII detection). 3.  Maintain a clean separation of concerns between the LLM Provider (responsible for API communication) and the Governance module (responsible for policy enforcement).</p>"},{"location":"adr/ADR-114-centralizing-ai-governance-with-guard-pipeline/#decision","title":"Decision","text":"<p>The responsibility for AI Governance will be removed from <code>BaseLLM</code> and delegated to a new service layer: the <code>GuardPipeline</code>. This pipeline is instantiated by the <code>LLMManager</code>.</p> <ol> <li>Introduce <code>GuardPipeline</code>: A new service class responsible for orchestrating the execution of both Input Guards (pre-call validation) and Output Guards (post-call processing/sanitization).<ul> <li>It implements a configurable flow for input validation (<code>fail_fast</code> or aggregate and raise <code>MultipleSecurityViolationsError</code> ).</li> <li>It implements a pipeline for output processing, where each guard receives the result of the previous one, with an option to <code>continue_on_output_error</code>.</li> </ul> </li> <li>Refactor <code>BaseLLM</code>: The <code>_run_guards</code> method and the <code>guards</code> attribute are removed from <code>BaseLLM</code>. <code>BaseLLM</code> is now solely responsible for LLM communication, observability, and error wrapping.</li> <li>Centralize Composition in <code>LLMManager</code>: The <code>LLMManager</code> and its factory (<code>LLMManagerFactory</code>) are updated to use the new <code>GuardFactory</code> to compose the <code>GuardPipeline</code> from settings.</li> <li>New Protocols/Guards: Introduce <code>AIGuardInputProtocol</code> and <code>AIGuardOutputProtocol</code>  to formally differentiate between validation (Input) and processing (Output). Implement concrete guards like <code>KeywordBlocklistValidator</code> , <code>RegexPIISanitizer</code> (Input PII logging) , and <code>OutputPIISanitizer</code> (Output PII masking).</li> </ol>"},{"location":"adr/ADR-114-centralizing-ai-governance-with-guard-pipeline/#consequences","title":"Consequences","text":"<ul> <li>Positive:</li> <li>Clean Separation of Concerns: <code>BaseLLM</code> is now cleaner and fully focused on the API/connection layer.</li> <li>Flexible Governance Flow: Enables complex execution logic (e.g., fail-fast on rate limit, aggregate on multiple policy violations) through the <code>GuardPipeline</code>.</li> <li>Support for Output Processing: The new Output Guard protocols enable post-processing and sanitization of LLM responses (e.g., PII Redaction).</li> <li> <p>Simplified LLM Provider Implementation: Concrete LLM providers (subclasses of <code>BaseLLM</code>) no longer need to worry about calling the security/governance chain.</p> </li> <li> <p>Negative:</p> </li> <li>Increased Startup Complexity: The <code>LLMManager</code> now has more dependencies and a more complex startup routine, requiring the <code>GuardFactory</code> to be fully configured before LLM instances are created.</li> <li> <p>Breaking Change (Internal): Requires updating all existing custom <code>BaseLLM</code> implementations to remove the <code>guards</code> dependency and the <code>_run_guards</code> method (already handled in the core codebase).</p> </li> <li> <p>Neutral/Other:</p> </li> <li>Architectural Clarity: The change explicitly defines the LLM Manager as the \"Composition Root\" for the entire AI Service (Connections + Governance).</li> </ul>"},{"location":"adr/ADR-115-standardized-llm-streaming/","title":"ADR-115: Standardized LLM Streaming and Governance","text":"<ul> <li>Status: Proposed</li> <li>Date: 2025-12-14</li> </ul>"},{"location":"adr/ADR-115-standardized-llm-streaming/#context","title":"Context","text":"<p>As the application moves towards interactive user-facing features, the latency of blocking LLM calls (<code>generate</code>) has become a bottleneck for User Experience (UX). Users need to see tokens arriving in real-time.</p> <p>Furthermore, we currently lack a standardized way to enforce AI Governance (Safety Guardrails) across different providers (OpenAI, Google GenAI) consistently. Security checks are often applied ad-hoc or are missing from streaming implementations.</p>"},{"location":"adr/ADR-115-standardized-llm-streaming/#decision","title":"Decision","text":"<p>We will extend the <code>BaseLLM</code> and <code>LLMProviderProtocol</code> to support standardized asynchronous streaming with integrated governance hooks.</p> <ol> <li> <p>Streaming Protocol:</p> <ul> <li>Introduce <code>stream_content(...) -&gt; AsyncIterator[LLMResponseChunk]</code> as a core method in <code>BaseLLM</code>.</li> <li>Implement <code>LLMResponseChunk</code> to normalize disparate provider events (deltas, finish reasons, usage) into a single schema.</li> <li>Implement specific adapters for Google GenAI and OpenAI.</li> </ul> </li> <li> <p>Governance Integration:</p> <ul> <li>Input Guards: Applied synchronously/blocking before the stream starts. If a guard fails, the stream is aborted immediately.</li> <li>Output Guards: For streaming, we accept a trade-off. Since strictly sanitizing a stream (e.g., PII masking) requires buffering that negates latency benefits, output guards will initially be bypassed or used for auditing/flagging only.</li> </ul> </li> </ol>"},{"location":"adr/ADR-115-standardized-llm-streaming/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>UX Improvement: Drastic reduction in Time-to-First-Token (TTFT) for end-users.</li> <li>Provider Agnosticism: The <code>LLMResponseChunk</code> abstracts away the differences between OpenAI's chunks and VertexAI's candidates.</li> <li>Observability: The base implementation automatically handles tracing (<code>llm.stream_content</code>) and metrics (<code>llm_operation_duration_seconds</code>), tracking the full stream lifecycle.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Complexity: Asynchronous iterators introduce complexity in error handling and state management compared to simple awaitable calls.</li> <li>Governance Trade-off: We lose the ability to strictly block harmful output in real-time without introducing significant latency (buffering).</li> </ul> </li> <li> <p>Risk:</p> <ul> <li>Small Model Hallucinations: Smaller models (e.g., Llama 3.2) may leak structural artifacts (like tool schemas) into the content stream. The consumer of the stream needs to be robust against this.</li> </ul> </li> </ul>"},{"location":"adr/ADR-116-rag-pipeline-v2-streaming/","title":"ADR-116: RAG Pipeline V2 (True Streaming and Determinism)","text":"<ul> <li>Status: Proposed</li> <li>Date: 2025-12-14</li> </ul>"},{"location":"adr/ADR-116-rag-pipeline-v2-streaming/#context","title":"Context","text":"<p>The initial RAG (Retrieval-Augmented Generation) implementation utilized a \"simulated\" streaming approach where the entire answer was generated by the LLM before being yielded to the client. This negated the latency benefits of streaming.</p> <p>Additionally, the <code>GenerationStrategy</code> coupled prompt construction with execution, making it difficult to test prompt templates without invoking the LLM. The domain models (<code>SourceDocument</code>) were also tightly coupled to the Memory module.</p>"},{"location":"adr/ADR-116-rag-pipeline-v2-streaming/#decision","title":"Decision","text":"<p>We will refactor the <code>RAGService</code> to support True Streaming and decoupled generation logic.</p> <ol> <li> <p>Stream Orchestration:</p> <ul> <li>The <code>stream_response</code> method will yield a <code>RAGResponseChunk</code> containing sources (citations) immediately after the retrieval step, before the LLM starts generating.</li> <li>The service will then stream LLM content deltas directly to the client as they arrive.</li> <li>A final metadata chunk (latency, tokens) is yielded at the end.</li> </ul> </li> <li> <p>Strategy Pattern Refactoring:</p> <ul> <li>Split <code>GenerationStrategyProtocol</code> into:<ul> <li><code>augment_prompt</code> (Synchronous): Pure logic to build the context-aware prompt.</li> <li><code>generate</code> (Asynchronous): Executes the LLM call.</li> </ul> </li> <li>Enforced Determinism: The default \"Stuff\" strategy will enforce <code>temperature=0.0</code> on the LLM to minimize hallucinations and ensure the model relies strictly on the retrieved context.</li> </ul> </li> <li> <p>Domain Models:</p> <ul> <li>Replace <code>SourceDocument</code> with <code>RAGSource</code> in a dedicated <code>schemas/rag.py</code> to decouple RAG from Memory implementations.</li> </ul> </li> </ol>"},{"location":"adr/ADR-116-rag-pipeline-v2-streaming/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Perceived Performance: Users see citations (\"Checking documents...\") immediately, reducing the perception of waiting.</li> <li>Testability: We can now unit test prompt construction (<code>augment_prompt</code>) deterministically without mocking <code>llm.generate</code>.</li> <li>Consistency: Forcing low temperature reduces variance in RAG answers, crucial for \"Corporate Memory\" use cases.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Breaking Change: The <code>RAGResponse</code> and <code>RAGRequest</code> schemas have changed significantly; existing clients must be updated.</li> <li>Complexity: The client must now handle different types of chunks (<code>sources</code> vs <code>content_delta</code> vs <code>is_final</code>).</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>We are currently manually mapping Memory results to <code>RAGSource</code> inside the strategy. A future mapper layer may be required if data structures diverge.</li> </ul> </li> </ul>"},{"location":"architecture/","title":"Architecture Overview","text":"<p>The <code>athomic-docs</code> follows a clean, layered architecture designed for separation of concerns, testability, and scalability. It is heavily inspired by principles from Domain-Driven Design (DDD) and Clean Architecture.</p> <p>The main layers are:</p> <ul> <li>Domain Layer: Contains the core business logic, entities, and rules. It is the heart of your application and has no dependencies on other layers.</li> <li>Application Layer: Orchestrates the domain logic. It contains use cases and application services that handle requests from the outside world.</li> <li>Infrastructure Layer: Implements external concerns like databases, message brokers, and third-party API clients.</li> <li>Athomic Layer: A cross-cutting \"engine\" that provides foundational capabilities like caching, resilience, and observability to all other layers.</li> </ul>"},{"location":"architecture/dependency-injection/","title":"Dependency Injection &amp; Service Location","text":""},{"location":"architecture/dependency-injection/#overview","title":"Overview","text":"<p>The <code>athomic-docs</code> project manages dependencies and object lifecycles without relying on an external dependency injection framework. Instead, it employs a robust and clear approach using a combination of two classic design patterns:</p> <ol> <li>The Facade Pattern: The <code>Athomic</code> class acts as a central service locator, providing a single, unified entry point to all core framework services.</li> <li>The Factory Pattern: Dedicated <code>Factory</code> classes are responsible for creating and managing singleton instances of specific components, handling their unique dependencies internally.</li> </ol> <p>This approach promotes loose coupling, simplifies testing through easy mocking, and makes the flow of dependency resolution explicit and easy to follow.</p>"},{"location":"architecture/dependency-injection/#the-athomic-facade","title":"The <code>Athomic</code> Fa\u00e7ade","text":"<p>The main entry point to the entire framework is the <code>nala.athomic.facade.Athomic</code> class. During application startup, this class is instantiated once. Its constructor (<code>__init__</code>) is responsible for wiring together the primary, high-level services.</p> <pre><code># From: nala.athomic.facade.py\nclass Athomic:\n    def __init__(self, ...):\n        # ...\n        self.settings = settings or get_settings()\n        self.secrets_manager = SecretsManager(self.settings)\n        self.lifecycle_manager = LifecycleManager(...)\n        self.tracer = get_tracer(__name__)\n        self.plugin_manager: PluginManager = PluginManager()\n        # ...\n</code></pre> <p>The <code>nala.athomic.provider</code> module then makes this single <code>Athomic</code> instance globally accessible via the <code>get_athomic_instance()</code> function, allowing any part of the application to access core services like the <code>LifecycleManager</code> or <code>PluginManager</code> when needed.</p>"},{"location":"architecture/dependency-injection/#the-factory-pattern","title":"The Factory Pattern","text":"<p>For most components within the Athomic Layer, a dedicated factory is responsible for its creation. This pattern provides several key advantages:</p> <ul> <li>Centralized Creation Logic: The logic for creating a complex object (e.g., a <code>ConnectionManager</code> that needs <code>DatabaseSettings</code>) is encapsulated in one place.</li> <li>Singleton Management: Factories typically manage a singleton instance of the object they create, ensuring resources like connection pools are shared efficiently.</li> <li>Simplified Dependency Injection: Components that need a dependency can simply call the factory's <code>create()</code> method without needing to know how to construct it.</li> <li>Testability: Factories usually include a <code>.clear()</code> method, which is crucial for resetting state between tests and ensuring test isolation.</li> </ul>"},{"location":"architecture/dependency-injection/#example-flow","title":"Example Flow","text":"<p>A great example is the <code>MongoOutboxRepository</code>. It needs a database connection to work. Here is how its dependencies are resolved:</p> <ol> <li>The application needs an <code>OutboxStorageProtocol</code> instance.</li> <li>It calls <code>OutboxStorageFactory.create()</code>.</li> <li>The <code>OutboxStorageFactory</code> knows it needs a database connection. It calls <code>connection_manager_factory.create()</code>.</li> <li>The <code>ConnectionManagerFactory</code> creates the singleton <code>ConnectionManager</code>.</li> <li>The <code>OutboxStorageFactory</code> then asks the <code>ConnectionManager</code> for the specific database connection (<code>get_document_db(...)</code>).</li> <li>Finally, it injects the database connection into a new <code>MongoOutboxRepository</code> instance and returns it.</li> </ol> <p>This chain of factory calls ensures that dependencies are resolved correctly and only when needed.</p>"},{"location":"architecture/dependency-injection/#api-reference","title":"API Reference","text":""},{"location":"architecture/dependency-injection/#core-facade-and-provider","title":"Core Fa\u00e7ade and Provider","text":""},{"location":"architecture/dependency-injection/#nala.athomic.facade.Athomic","title":"<code>nala.athomic.facade.Athomic</code>","text":"<p>A Facade that provides a single, simple entry point to all of Athomic's services.</p> <p>This class acts as the main interaction point for an application utilizing the Athomic framework. It manages the initialization, dependency injection, and lifecycle of all infrastructure components, such as secrets management, plugins, and background services.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <code>AppSettings</code> <p>The validated application settings.</p> <code>secrets_manager</code> <code>SecretsManager</code> <p>The manager responsible for resolving secrets.</p> <code>lifecycle_manager</code> <code>LifecycleManager</code> <p>The orchestrator for the service lifecycle.</p> <code>plugin_manager</code> <code>PluginManager</code> <p>The manager for the plugin system.</p> <code>tracer</code> <code>Tracer</code> <p>The OpenTelemetry Tracer instance for this component.</p> <code>observability</code> <p>A namespace for observability components like loggers.</p>"},{"location":"architecture/dependency-injection/#nala.athomic.facade.Athomic.__init__","title":"<code>__init__(domain_initializers_registrar=None, settings=None)</code>","text":"<p>Initializes the Athomic layer.</p> <p>Parameters:</p> Name Type Description Default <code>domain_initializers_registrar</code> <code>Optional[Callable[[], None]]</code> <p>A function from the application layer (e.g., API) that registers all business domain-specific initializers.</p> <code>None</code> <code>settings</code> <code>Optional[AppSettings]</code> <p>An instance of application settings. If not provided, it will be loaded globally. Ideal for dependency injection in tests.</p> <code>None</code>"},{"location":"architecture/dependency-injection/#nala.athomic.facade.Athomic.shutdown","title":"<code>shutdown()</code>  <code>async</code>","text":"<p>Runs the graceful shutdown sequence for all services.</p>"},{"location":"architecture/dependency-injection/#nala.athomic.facade.Athomic.startup","title":"<code>startup()</code>  <code>async</code>","text":"<p>Runs the complete, ordered startup sequence for the application's infrastructure.</p> <p>This method orchestrates the startup of services in the correct dependency order: 1. Resolves all secret references within the configuration. 2. Discovers and loads all available plugins. 3. Calls the 'on_athomic_startup' hook, allowing plugins to initialize. 4. Starts all registered services (e.g., database, messaging).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If any critical step in the startup sequence fails.</p>"},{"location":"architecture/dependency-injection/#nala.athomic.provider","title":"<code>nala.athomic.provider</code>","text":""},{"location":"architecture/dependency-injection/#nala.athomic.provider.clear_athomic_instance","title":"<code>clear_athomic_instance()</code>","text":"<p>For use in tests, to ensure isolation.</p>"},{"location":"architecture/dependency-injection/#nala.athomic.provider.get_athomic_instance","title":"<code>get_athomic_instance()</code>","text":"<p>Returns the singleton instance of the Athomic facade that has already been initialized.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the instance has not yet been created and set.</p>"},{"location":"architecture/dependency-injection/#nala.athomic.provider.set_athomic_instance","title":"<code>set_athomic_instance(instance)</code>","text":"<p>Sets the global singleton instance of the Athomic facade. This function should be called ONLY ONCE during application startup.</p>"},{"location":"architecture/dependency-injection/#example-factories","title":"Example Factories","text":""},{"location":"architecture/dependency-injection/#nala.athomic.database.factory.ConnectionManagerFactory","title":"<code>nala.athomic.database.factory.ConnectionManagerFactory</code>","text":"<p>Manages the singleton instance of the ConnectionManager. Ensures that only one instance of the ConnectionManager exists throughout the application's lifecycle.</p>"},{"location":"architecture/dependency-injection/#nala.athomic.database.factory.ConnectionManagerFactory.clear","title":"<code>clear()</code>  <code>async</code>","text":"<p>Clears the singleton instance of the ConnectionManager. This is primarily used for test isolation.</p>"},{"location":"architecture/dependency-injection/#nala.athomic.database.factory.ConnectionManagerFactory.create","title":"<code>create(settings=None)</code>","text":"<p>Creates and returns the singleton instance of the ConnectionManager.</p> <p>On the first call, it instantiates the ConnectionManager. Subsequent calls return the cached instance.</p>"},{"location":"architecture/dependency-injection/#nala.athomic.serializer.factory.SerializerFactory","title":"<code>nala.athomic.serializer.factory.SerializerFactory</code>","text":"<p>Factory for instantiating message serializers based on messaging backend. Falls back to BaseSerializer if no specific implementation is registered. Caches instances by backend name for singleton-like behavior per backend.</p>"},{"location":"architecture/dependency-injection/#nala.athomic.serializer.factory.SerializerFactory.clear","title":"<code>clear()</code>  <code>classmethod</code>","text":"<p>Clears the singleton cache of serializer instances.</p>"},{"location":"architecture/dependency-injection/#nala.athomic.serializer.factory.SerializerFactory.create","title":"<code>create(settings=None)</code>  <code>classmethod</code>","text":"<p>Creates and returns a singleton instance of the configured SerializerProtocol by delegating to a registered creator.</p>"},{"location":"architecture/domain-layer/","title":"The Domain Layer","text":"<p>The Domain Layer is the core of the application. It contains the business logic, rules, and state that are unique to your specific problem domain. This layer is designed to be completely independent of any infrastructure concerns\u2014it knows nothing about databases, web frameworks, or message brokers.</p>"},{"location":"architecture/domain-layer/#key-concepts","title":"Key Concepts","text":"<ul> <li>Entities &amp; Aggregates: These are the primary objects that model your domain. An Aggregate is a cluster of associated objects that we treat as a single unit for data changes.</li> <li>Value Objects: Objects that represent a descriptive aspect of the domain with no conceptual identity.</li> <li>Domain Events: Objects that represent something that happened in the domain. They are often used to communicate changes between different parts of the application without direct coupling.</li> <li>Repositories: Interfaces defined in the Domain Layer that specify the contract for data persistence (e.g., <code>save(user)</code>, <code>find_by_id(user_id)</code>). The implementation of these interfaces lives in the Infrastructure Layer.</li> </ul> <p>By keeping the Domain Layer pure, you ensure that your most critical business logic is easy to test, maintain, and evolve independently of technology choices.</p>"},{"location":"architecture/athomic-layer/","title":"The Athomic Layer","text":"<p>The Athomic Layer is the heart of the <code>athomic-docs</code> project. It is an IDP that provides a rich set of production-ready, cross-cutting concerns as a service to the rest of the application.</p> <p>Its primary goal is to allow developers to focus on writing business logic (in the Domain and Application layers) without having to repeatedly solve complex infrastructure problems.</p> <p>Athomic provides abstractions and concrete implementations for: -   Data &amp; Persistence (Databases, Caches, Storage) -   Observability (Logging, Tracing, Metrics) -   Security (Secrets, Auth, Crypto) -   Resilience Patterns (Retry, Circuit Breaker, etc.) -   Integrations (Messaging, Tasks, Service Discovery)</p> <p>By using a consistent, protocol-based approach, components within the Athomic Layer are designed to be loosely coupled and easily replaceable or extensible.</p>"},{"location":"backlog/_ice-box/future-backlog/","title":"Nala Athomic Framework \u2014 Master Backlog","text":""},{"location":"backlog/_ice-box/future-backlog/#epic-1-developer-experience-tooling-cli-docs","title":"EPIC 1 \u2014 Developer Experience &amp; Tooling (CLI &amp; Docs)","text":"<p>Goal: Transform Nala from a library into a manageable platform, reducing friction for developers and ops.</p>"},{"location":"backlog/_ice-box/future-backlog/#tasks","title":"Tasks","text":"<ul> <li>[ ] Implement Nala CLI (<code>nala-cli</code>)<ul> <li>Tech: Use <code>Typer</code> or <code>Click</code>.</li> <li>Commands:<ul> <li><code>nala run-api</code>: Starts Uvicorn with structured logging config.</li> <li><code>nala run-worker &lt;name&gt;</code>: Starts specific consumers/background tasks.</li> <li><code>nala db migrate</code>: Wraps Alembic or internal migration tools.</li> <li><code>nala cache clear</code>: Flushes Redis cache/locks.</li> </ul> </li> </ul> </li> <li>[ ] Contributor \"Cookbook\"<ul> <li>Create <code>docs/cookbook/</code>.</li> <li>Guides: \"How to add a Secret Provider\", \"How to add Custom Metrics\", \"How to create a new Event Processor\".</li> </ul> </li> <li>[ ] OpenAPI/Swagger Automation<ul> <li>Add CI/CD step to generate <code>openapi.json</code> and validate against breaking changes (using tools like <code>openapi-diff</code>).</li> </ul> </li> <li>[ ] CredentialProxy Documentation<ul> <li>Add explicit docstrings and <code>typing.Annotated</code> warnings on Pydantic models to clarify that secrets are async proxies.</li> </ul> </li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#epic-2-api-maturity-governance","title":"EPIC 2 \u2014 API Maturity &amp; Governance","text":"<p>Goal: Ensure the API layer is robust, standard-compliant, and resilient to public usage.</p>"},{"location":"backlog/_ice-box/future-backlog/#tasks_1","title":"Tasks","text":"<ul> <li>[ ] API Versioning Strategy<ul> <li>Implement versioning (decide: URI Path <code>/v1/</code> vs Header <code>Accept</code>).</li> <li>Update <code>FastAPI</code> router configuration to support versioned prefixes.</li> </ul> </li> <li>[ ] Idempotency Mechanism<ul> <li>Create Middleware looking for <code>Idempotency-Key</code> header.</li> <li>Store result of successful non-safe operations (POST/PUT) in Redis with TTL.</li> <li>Return cached response if key is seen again.</li> </ul> </li> <li>[ ] Unified Exception Handling<ul> <li>Create a global <code>exception_handler</code> in <code>main.py</code>.</li> <li>Map internal exceptions (<code>StorageOperationError</code>, <code>SecretNotFoundError</code>) to correct HTTP Status Codes (503, 404) automatically.</li> </ul> </li> <li>[ ] Real-time Support (Optional)<ul> <li>POC for WebSockets or SSE (Server-Sent Events) for live updates (e.g., \"Report Generated\").</li> </ul> </li> <li>[ ] GraphQL Support (Optional)<ul> <li>Create a separate module <code>nala.athomic.api.graphql</code> using <code>Strawberry</code>.</li> </ul> </li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#epic-3-security-compliance-athomic-layer","title":"EPIC 3 \u2014 Security &amp; Compliance (Athomic Layer)","text":"<p>Goal: Deepen security controls, auditability, and identity management.</p>"},{"location":"backlog/_ice-box/future-backlog/#tasks_2","title":"Tasks","text":"<ul> <li>[ ] Audit Log System<ul> <li>Create <code>audit_log</code> service.</li> <li>Intercept critical actions (Login, Secret Access, Config Change).</li> <li>Persist with PII Masking/Redaction.</li> </ul> </li> <li>[ ] Extended IAM Providers<ul> <li>Implement generic OAuth2/OIDC provider (Google, Azure AD, Keycloak).</li> <li>Create <code>AccessManager</code> abstraction to centralize RBAC/ABAC logic.</li> </ul> </li> <li>[ ] Secret Usage Debugging<ul> <li>Create a protected endpoint <code>GET /_debug/secrets</code> (SuperAdmin only).</li> <li>Show which secrets are loaded and their provider source (values masked: <code>sk-***123</code>).</li> </ul> </li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#epic-4-quality-assurance-resilience","title":"EPIC 4 \u2014 Quality Assurance &amp; Resilience","text":"<p>Goal: Shift-left testing and ensure the system recovers from chaos.</p>"},{"location":"backlog/_ice-box/future-backlog/#tasks_3","title":"Tasks","text":"<ul> <li>[ ] Strict Static Analysis Pipeline<ul> <li>Configure <code>ruff</code> (linter/formatter) with strict rules.</li> <li>Configure <code>mypy --strict</code> for type safety.</li> <li>Add pre-commit hooks.</li> </ul> </li> <li>[ ] Coverage Enforcement<ul> <li>Integrate <code>pytest-cov</code>.</li> <li>Fail CI if branch coverage &lt; 95%.</li> </ul> </li> <li>[ ] Load &amp; Chaos Testing<ul> <li>Create <code>Locust</code> scenarios simulating realistic user flows.</li> <li>Integrate <code>ChaosToolkit</code> to kill Redis/Kafka pods during load tests to verify recovery.</li> </ul> </li> <li>[ ] Infrastructure Self-Healing<ul> <li>Implement \"Consul Reconnector\": Background task that retries connection if Consul vanishes, without crashing the app.</li> </ul> </li> <li>[ ] DLQ Management API<ul> <li>Endpoints: <code>GET /dlq</code>, <code>POST /dlq/{id}/retry</code>, <code>DELETE /dlq/{id}</code>.</li> <li>Allows Ops to handle failed messages without direct Kafka access.</li> </ul> </li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#epic-5-architecture-internal-refactoring","title":"EPIC 5 \u2014 Architecture &amp; Internal Refactoring","text":"<p>Goal: Clean up technical debt and improve modularity.</p>"},{"location":"backlog/_ice-box/future-backlog/#epic-6-data-event-sourcing-advanced-logic","title":"EPIC 6 \u2014 Data &amp; Event Sourcing Advanced Logic","text":"<p>Goal: Data integrity, lifecycle management, and complex event handling.</p>"},{"location":"backlog/_ice-box/future-backlog/#tasks_4","title":"Tasks","text":"<ul> <li>[ ] Data Validation Pipelines<ul> <li>Implement <code>Validator</code> and <code>Transformer</code> interfaces.</li> <li>Create standard pipelines for PII stripping and format normalization.</li> </ul> </li> <li>[ ] Data Retention (TTL/Purge)<ul> <li>Implement \"Purge Jobs\" for old data (soft-delete physical removal).</li> </ul> </li> <li>[ ] Event Sourcing: \"Waiting Room\" Strategy<ul> <li>Problem: Missing event in sequence (Got 6, missed 5).</li> <li>Solution:<ul> <li>Implement <code>MaxWaitTime</code> (TTL) for orphaned events in buffer (e.g., 1 hour).</li> <li>Implement <code>MaxBufferSize</code> per aggregate.</li> <li>Action: If TTL expires, move orphaned events to a generic <code>CorruptedStreamDLQ</code> and alert, preventing memory leaks.</li> </ul> </li> </ul> </li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#nala-athomic-framework-master-backlog-part-2-advanced-engineering","title":"Nala Athomic Framework \u2014 Master Backlog (Part 2: Advanced Engineering)","text":""},{"location":"backlog/_ice-box/future-backlog/#epic-8-messaging-pipeline-observability-dx","title":"EPIC 8 \u2014 Messaging Pipeline Observability &amp; DX","text":"<p>Focus: Granular visibility into the Payload Processor and DX improvements for KVStore.</p>"},{"location":"backlog/_ice-box/future-backlog/#tasks_5","title":"Tasks","text":"<ul> <li>[ ] Instrument <code>PayloadProcessor</code> Metrics</li> <li>Type: Feature</li> <li>Details: Implement Prometheus metrics.<ul> <li>Histogram: <code>payload_processing_duration_seconds</code> (labels: <code>step</code>, <code>direction</code>).</li> <li>Counter: <code>payload_processing_errors_total</code> (labels: <code>step</code>, <code>direction</code>).</li> </ul> </li> <li>[ ] Deep Tracing for Payload Pipeline</li> <li>Type: Feature</li> <li>Details: Create child spans for <code>serialize</code>, <code>compress</code>, <code>encrypt</code> within the parent publish/consume span. Record exceptions on the specific child span.</li> <li>[ ] Transparent KVStore Serialization</li> <li>Type: Refactor/DX</li> <li>Details: Update <code>BaseKVStoreProvider</code> to use <code>SerializerProtocol</code>.</li> <li>Goal: Allow <code>kv.set(\"key\", pydantic_model)</code> and automatic storage/retrieval without manual serialization in the application layer.</li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#epic-10-architecture-refinement-performance","title":"EPIC 10 \u2014 Architecture Refinement &amp; Performance","text":"<p>Focus: Async correctness, Dependency Injection, and Resource Management.</p>"},{"location":"backlog/_ice-box/future-backlog/#tasks_6","title":"Tasks","text":"<ul> <li>[ ] Centralize Contextual Key Generation</li> <li>Refactor: Create <code>ContextualKeyGenerator</code> class in <code>nala/athomic/context</code>.</li> <li>Action: Unify logic for <code>tenant_id:namespace:key</code> generation.</li> <li>Cleanup: Delete <code>key_resolver.py</code> after migration.</li> <li>[ ] Async Wrapper Optimization (GCS &amp; Fernet)</li> <li>Optimization: Review <code>GcsStorageProvider</code> and <code>FernetCryptoProvider</code>.</li> <li>Action: Ensure CPU-bound crypto ops and Sync I/O are strictly wrapped in <code>asyncio.to_thread</code> or <code>ThreadPoolExecutor</code>.</li> <li>Long-term: Investigate migration to native async libs (<code>gcloud-aio-storage</code>).</li> <li>[ ] MongoDB Atomic Updates (Outbox)</li> <li>Performance: Replace <code>find_one</code> + <code>save</code> pattern in <code>mark_event_attempted</code>.</li> <li>Action: Use <code>find_one_and_update</code> with <code>$inc</code> operator to ensure atomicity and reduce RTT.</li> <li>[ ] Simplify DLQHandlerFactory</li> <li>Refactor: Remove <code>producer</code> argument from <code>create</code>.</li> <li>Action: Factory should internally use <code>ProducerFactory</code> to instantiate the dedicated DLQ producer.</li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#epic-11-cicd-documentation-quality","title":"EPIC 11 \u2014 CI/CD &amp; Documentation Quality","text":"<p>Focus: Automating quality gates and improving contributor experience.</p>"},{"location":"backlog/_ice-box/future-backlog/#tasks_7","title":"Tasks","text":"<ul> <li>[ ] CI Quality Gates</li> <li>Tooling: Implement <code>commitlint</code> (Conventional Commits).</li> <li>Spellcheck: Add <code>codespell</code> to pipeline (ignore <code>*.lock</code>, <code>*.json</code>).</li> <li>Docs: Run <code>mkdocs build --strict</code> in CI.</li> <li>[ ] MkDocs Navigation Fixes</li> <li>Action: Move external MD files (CONTRIBUTING.md) into <code>docs/</code> or fix relative paths to satisfy <code>strict</code> mode.</li> <li>[ ] Migration Scripts</li> <li>Task: Create Beanie migration for <code>api_clients</code> collection and indexes.</li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#epic-12-product-vision-the-control-panel","title":"EPIC 12 \u2014 Product Vision: \"The Control Panel\"","text":"<p>Focus: Admin tools and future differentiation.</p>"},{"location":"backlog/_ice-box/future-backlog/#tasks_8","title":"Tasks","text":"<ul> <li>[ ] DLQ Management API</li> <li><code>GET /admin/dlq/messages</code> (List with pagination)</li> <li><code>POST /admin/dlq/messages/{id}/republish</code> (The retry logic)</li> <li><code>DELETE /admin/dlq/messages/{id}</code></li> <li>[ ] Secret Usage Debugger</li> <li>Endpoint to show which secrets are loaded and their provider source (masked values).</li> <li>[ ] Authorized \"Backoffice\"</li> <li>Build a simple UI (or use existing Admin panel tools) that consumes these APIs.</li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#strategic-notes-architecture-decision-records-candidate","title":"Strategic Notes (Architecture Decision Records candidate)","text":""},{"location":"backlog/_ice-box/future-backlog/#1-the-athomic-usp-positioning","title":"1. The \"Athomic\" USP (Positioning)","text":"<ul> <li>Concept: \"The First Framework with Atomic Probabilistic Dispatch.\"</li> <li>Why: Most frameworks are deterministic. Adding native AI probabilistic routing (LLM-based decision making for function calls) into the kernel is a massive differentiator.</li> <li>Action: Validated. This should be the tagline.</li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#2-event-store-trade-offs","title":"2. Event Store Trade-offs","text":"<ul> <li>Current State: Using Kafka as Event Store with Redis Locks/Versioning.</li> <li>Risk: Write Latency (3x network calls) and Read Performance (Scanning partitions).</li> <li>Decision: Acceptable for now, but roadmap must include Snapshotting strategies or migration to a specialized Event Store (EventStoreDB) if write throughput becomes the bottleneck.</li> </ul>"},{"location":"backlog/todo/TASK-008-outbox-webhook-executor/","title":"\ud83e\udde9 Engineering Task: Implement <code>WebhookExecutor</code> for Agnostic Outbox","text":"<p>Status: To Do Priority: High Parent Task: Generalize Outbox Module to Support Agnostic Executors Linked ADR: ADR - Outbox Module Architecture Definition (Agnostic Outbox Pattern)</p>"},{"location":"backlog/todo/TASK-008-outbox-webhook-executor/#context","title":"\ud83c\udfaf Context","text":"<p>As part of the agnostic outbox pattern, we want to support integration with HTTP endpoints via a new executor: <code>WebhookExecutor</code>.</p> <p>This executor will allow events in the Outbox to be dispatched to configurable webhooks, enabling integrations with external systems (e.g., CRMs, notification services, third-party APIs) through simple HTTP calls with payloads.</p>"},{"location":"backlog/todo/TASK-008-outbox-webhook-executor/#deliverables","title":"\u2705 Deliverables","text":"<ul> <li>[ ] Define and implement <code>WebhookExecutor</code> under <code>nala.athomic.integration.outbox.executors.webhook_executor</code>.</li> <li>[ ] Define configuration contract for the destination URL, headers, and retry behavior (optional).</li> <li>[ ] Support <code>POST</code> method by default with JSON payloads.</li> <li>[ ] Log and trace webhook invocations with request/response status and body (if needed).</li> <li>[ ] Implement retry safety (e.g., avoid retrying on 4xx).</li> <li>[ ] Ensure exceptions are correctly classified as retriable or non-retriable.</li> <li>[ ] Add unit tests with mocked HTTP responses:</li> <li>[ ] Success (200 OK).</li> <li>[ ] Failure (e.g., 500, 404).</li> <li>[ ] Timeout or network error.</li> <li>[ ] Add integration example to <code>/examples</code>.</li> <li>[ ] Document expected <code>payload</code>, <code>destination</code>, and <code>event_type=WEBHOOK</code> usage.</li> </ul>"},{"location":"backlog/todo/TASK-008-outbox-webhook-executor/#technical-notes","title":"\u2699 Technical Notes","text":"<ul> <li>Consider using <code>httpx.AsyncClient()</code> for async support and timeout control.</li> <li>Optionally support dynamic headers through the event payload if explicitly allowed.</li> <li>Ensure the webhook executor can be registered via the router using <code>event_type=WEBHOOK</code>.</li> </ul>"},{"location":"backlog/todo/TASK-008-outbox-webhook-executor/#acceptance-criteria","title":"\ud83d\udee1 Acceptance Criteria","text":"<ul> <li>[ ] Events with <code>event_type=WEBHOOK</code> are routed to <code>WebhookExecutor</code>.</li> <li>[ ] The executor sends HTTP POST requests with the event's payload to the specified <code>destination</code>.</li> <li>[ ] The system logs webhook interactions and handles errors appropriately.</li> <li>[ ] Unit tests and examples validate the executor logic and error handling.</li> </ul>"},{"location":"backlog/todo/TASK-009-outbox-task-executor/","title":"\ud83e\udde9 Engineering Task: Implement <code>TaskExecutor</code> for Internal Async Task Dispatching","text":"<p>Status: To Do Priority: Medium Parent Task: Generalize Outbox Module to Support Agnostic Executors Linked ADR: ADR - Outbox Module Architecture Definition (Agnostic Outbox Pattern)</p>"},{"location":"backlog/todo/TASK-009-outbox-task-executor/#context","title":"\ud83c\udfaf Context","text":"<p>The <code>TaskExecutor</code> will enable the Outbox to dispatch internal Python async functions. This supports use cases where you want the guarantee of execution after persistence without relying on external queues.</p> <p>It is especially useful for async workflows, post-commit hooks, or background processing without adding full-blown messaging infrastructure.</p>"},{"location":"backlog/todo/TASK-009-outbox-task-executor/#deliverables","title":"\u2705 Deliverables","text":"<ul> <li>[ ] Implement <code>TaskExecutor</code> under <code>nala.athomic.integration.outbox.executors.task_executor</code>.</li> <li>[ ] Add support to register and resolve tasks via a <code>TaskRegistry</code>.</li> <li>[ ] The event's <code>destination</code> should reference the task name in the registry.</li> <li>[ ] The payload is passed as a dictionary to the task function.</li> <li>[ ] Tasks must be <code>async def</code> and return <code>None</code> or raise on failure.</li> <li>[ ] Add unit tests:</li> <li>[ ] Successful task execution.</li> <li>[ ] Task not found.</li> <li>[ ] Task raises an exception (retriable vs non-retriable).</li> <li>[ ] Add example usage in <code>/examples</code> directory.</li> <li>[ ] Document usage pattern in README.</li> </ul>"},{"location":"backlog/todo/TASK-009-outbox-task-executor/#technical-notes","title":"\u2699 Technical Notes","text":"<ul> <li>The registry can be a simple dictionary <code>{str: Callable}</code>.</li> <li>Use <code>inspect.iscoroutinefunction()</code> to validate async functions.</li> <li>Avoid importing the registry from business modules; keep the registration local and explicit.</li> <li>Ensure compatibility with the existing <code>OutboxPublisher</code> and router.</li> </ul>"},{"location":"backlog/todo/TASK-009-outbox-task-executor/#acceptance-criteria","title":"\ud83d\udee1 Acceptance Criteria","text":"<ul> <li>[ ] Events with <code>event_type=TASK</code> are routed to <code>TaskExecutor</code>.</li> <li>[ ] The task is resolved and called with the payload.</li> <li>[ ] Exceptions are correctly classified and handled.</li> <li>[ ] Full test coverage is in place.</li> <li>[ ] A working example demonstrates how to register and dispatch a task.</li> </ul>"},{"location":"backlog/todo/TASK-migrations/","title":"Benchmark e Backlog: Sistema de Migra\u00e7\u00f5es Athomic","text":"<p>Data: 12 de Setembro de 2025 Autor: Nala Assistance Status: An\u00e1lise Conclu\u00edda, Backlog Definido</p>"},{"location":"backlog/todo/TASK-migrations/#1-introducao","title":"1. Introdu\u00e7\u00e3o","text":"<p>Este documento apresenta uma an\u00e1lise comparativa (benchmark) entre a biblioteca <code>yoyo-migrations</code> e a solu\u00e7\u00e3o customizada de migra\u00e7\u00f5es que projetamos para o framework Athomic. O objetivo \u00e9 validar nossa abordagem e identificar os pontos-chave a serem implementados para garantir uma ferramenta de alta qualidade, alinhada com os princ\u00edpios do Athomic.</p>"},{"location":"backlog/todo/TASK-migrations/#2-benchmark-de-features-yoyo-migrations-vs-athomic-migration-runner","title":"2. Benchmark de Features: yoyo-migrations vs. Athomic Migration Runner","text":"<p>A biblioteca <code>yoyo-migrations</code> foi usada como refer\u00eancia de mercado para um orquestrador de migra\u00e7\u00f5es agn\u00f3stico e baseado em scripts Python.</p> Feature yoyo-migrations Athomic Migration Runner (Nosso Design) Vantagem Athomic Rastreamento de Migra\u00e7\u00e3o Usa uma tabela dedicada (<code>_yoyo_migrations</code>) para rastrear scripts executados. Usa uma cole\u00e7\u00e3o Beanie (<code>MigrationStatus</code>) com suporte nativo a <code>source</code> para m\u00faltiplos conjuntos de migra\u00e7\u00f5es. \u2705 Superior Suporte a Bancos Agn\u00f3stico. Requer backends espec\u00edficos para cada tipo de banco (SQL, SQLite). N\u00e3o possui backend nativo para NoSQL. Agn\u00f3stico atrav\u00e9s do <code>MigrationBackendProtocol</code>. O <code>MongoBackend</code> \u00e9 a primeira implementa\u00e7\u00e3o. O <code>SqlYoyoBackend</code> (adaptador) permite usar o motor do yoyo para SQL. \u2705 Superior Tipo de Script Suporta arquivos <code>.sql</code> e <code>.py</code> (com fun\u00e7\u00f5es <code>step</code> ou <code>upgrade</code>/<code>downgrade</code>). Foco em <code>.py</code> com <code>upgrade</code>/<code>downgrade</code>. Isso permite que as migra\u00e7\u00f5es usem outros componentes do Athomic (config, logger, etc.) de forma nativa. \u2705 Mais Integrado Suporte <code>asyncio</code> Sim, possui suporte nativo para opera\u00e7\u00f5es ass\u00edncronas. Nativo e Async-first. Toda a arquitetura foi projetada do zero para ser ass\u00edncrona. \u2705 Igual/Superior Interface de Comando (CLI) Fornece uma CLI completa e robusta (<code>apply</code>, <code>rollback</code>, <code>show</code>, etc.). Projetada com <code>Typer</code> para ser uma CLI unificada (<code>athomic db ...</code>), oferecendo uma experi\u00eancia de usu\u00e1rio mais coesa e integrada. \u2705 Superior Gest\u00e3o de Transa\u00e7\u00f5es Excelente para SQL. Executa cada migra\u00e7\u00e3o em uma transa\u00e7\u00e3o, garantindo atomicidade. Ponto de Melhoria para MongoDB. A atomicidade precisa ser gerenciada manualmente nos scripts de migra\u00e7\u00e3o que realizam m\u00faltiplas opera\u00e7\u00f5es. Para SQL, o adaptador para <code>yoyo</code> herda essa funcionalidade. \ud83d\udfe1 Inferior (Mongo) Hooks P\u00f3s-Execu\u00e7\u00e3o Permite configurar \"post-apply hooks\" para executar c\u00f3digo Python ap\u00f3s as migra\u00e7\u00f5es. Solu\u00e7\u00e3o mais robusta via Event Bus. O runner publicar\u00e1 um evento (<code>athomic.migrations.applied</code>), permitindo que qualquer servi\u00e7o reaja de forma desacoplada. \u2705 Superior Manuten\u00e7\u00e3o Gerenciada pela comunidade de c\u00f3digo aberto. Gerenciada pela equipe do Athomic, garantindo total alinhamento com o framework. \u26aa\ufe0f Trade-off"},{"location":"backlog/todo/TASK-migrations/#conclusao-do-benchmark","title":"Conclus\u00e3o do Benchmark","text":"<p>A solu\u00e7\u00e3o customizada para o Athomic, embora demande um esfor\u00e7o inicial de implementa\u00e7\u00e3o, se mostra superior em termos de integra\u00e7\u00e3o, consist\u00eancia e flexibilidade para o ecossistema do framework. Ela adota os melhores conceitos do <code>yoyo-migrations</code> e os eleva ao integr\u00e1-los nativamente com os outros primitivos do Athomic (configura\u00e7\u00e3o, eventos, CLI unificada).</p>"},{"location":"backlog/todo/TASK-migrations/#3-backlog-de-implementacao","title":"3. Backlog de Implementa\u00e7\u00e3o","text":"<p>Com base na an\u00e1lise e no design que finalizamos, aqui est\u00e1 um backlog simples com as tarefas necess\u00e1rias para construir o m\u00f3dulo de migra\u00e7\u00e3o.</p>"},{"location":"backlog/todo/TASK-migrations/#epico-epic-007-implementar-modulo-de-migracao-de-banco-de-dados-agnostico","title":"\u00c9pico: EPIC-007: Implementar M\u00f3dulo de Migra\u00e7\u00e3o de Banco de Dados Agn\u00f3stico","text":""},{"location":"backlog/todo/TASK-migrations/#tarefas-de-implementacao-core","title":"Tarefas de Implementa\u00e7\u00e3o (Core)","text":"<ul> <li>[ ] TASK-MIG-001: Criar o <code>MigrationBackendProtocol</code> para definir a interface dos backends.</li> <li>[ ] TASK-MIG-002: Implementar o schema Beanie <code>MigrationStatus</code> para rastreamento no MongoDB.</li> <li>[ ] TASK-MIG-003: Implementar a classe <code>MongoBackend</code> que satisfaz o protocolo.</li> <li>[ ] TASK-MIG-004: Criar o <code>migration_backend_registry</code> e registrar o <code>MongoBackend</code>.</li> <li>[ ] TASK-MIG-005: Implementar a <code>MigrationBackendFactory</code> com a l\u00f3gica de resolu\u00e7\u00e3o de configura\u00e7\u00e3o (inline vs. <code>config_ref</code>).</li> <li>[ ] TASK-MIG-006: Implementar o <code>MigrationRunner</code> que orquestra o processo lendo as <code>sources</code> e usando a factory.</li> <li>[ ] TASK-MIG-007: Adicionar a publica\u00e7\u00e3o do evento <code>athomic.migrations.applied</code> no Event Bus ao final do processo de <code>upgrade</code>.</li> </ul>"},{"location":"backlog/todo/TASK-migrations/#tarefas-da-cli","title":"Tarefas da CLI","text":"<ul> <li>[ ] TASK-MIG-008: Estruturar a CLI <code>athomic db</code> com <code>Typer</code> no diret\u00f3rio <code>helpers/athomic_cli/commands/</code>.</li> <li>[ ] TASK-MIG-009: Implementar o comando <code>athomic db upgrade</code>.</li> <li>[ ] TASK-MIG-010: Implementar o comando <code>athomic db status</code> com a tabela formatada (<code>rich</code>).</li> <li>[ ] TASK-MIG-011: Implementar o comando <code>athomic db create</code> para gerar templates de migra\u00e7\u00e3o.</li> </ul>"},{"location":"backlog/todo/TASK-migrations/#melhorias-e-proximos-passos-backlog-futuro","title":"Melhorias e Pr\u00f3ximos Passos (Backlog Futuro)","text":"<ul> <li>[ ] TASK-MIG-012: Implementar a l\u00f3gica de <code>downgrade</code> no runner e na CLI.</li> <li>[ ] TASK-MIG-013: Documentar o padr\u00e3o de uso de transa\u00e7\u00f5es do Motor dentro dos scripts de migra\u00e7\u00e3o para MongoDB.</li> <li>[ ] TASK-MIG-014: (Spike) Implementar o <code>SqlYoyoBackend</code> como prova de conceito para o suporte a bancos relacionais.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/","title":"Athomic (Athomic) \u2014 Product Backlog","text":""},{"location":"backlog/todo/ai-ml-procudt-backlog/#phase-0-foundations-and-technical-hygiene-critical-p0","title":"Phase 0 \u2014 Foundations and Technical Hygiene (Critical, P0)","text":""},{"location":"backlog/todo/ai-ml-procudt-backlog/#01-core-technical-stabilization","title":"0.1 Core Technical Stabilization","text":"<ul> <li>Refactor GcsStorageProvider to fully async (evaluate gcloud aio vs threadpool).</li> <li>Decouple DLQHandlerFactory from producer injection.</li> <li>Externalize RedisLeaseProvider Lua scripts using SHA loading + evalsha.</li> <li>Add granular OTel metrics to PayloadProcessor (serialization, compression, encryption).</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#02-observability-baseline","title":"0.2 Observability Baseline","text":"<ul> <li>Global tracing conventions for LLM, RAG, agents, tools, ingestion and storage.</li> <li>Standardized metric names and histogram buckets.</li> <li>Move \\@monitor_llm into observability module.</li> <li>Provider latency, token usage and error metrics.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#03-ai-foundation-nalaathomicai","title":"0.3 AI Foundation (nala.athomic.ai)","text":"<ul> <li>Pydantic settings for Vertex, OpenAI, Azure and Local providers.</li> <li>LLMProviderProtocol (Ensuring Pydantic V2/Instructor compatibility): ```python async def generate_content(...) async def generate_structured(..., response_model: Type[BaseModel]) async def get_embeddings(...) ```</li> <li>LocalLLMProvider (Ollama / vLLM).</li> <li>Async VertexAIProvider with retries + circuit breakers.</li> <li>AIMetrics for token and latency tracking.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#04-minimal-test-coverage","title":"0.4 Minimal Test Coverage","text":"<ul> <li>LLM providers (vertex, openai, azure, local).</li> <li>Decorators: retry, circuit breaker, monitor_llm.</li> <li>Async storage providers.</li> <li>Observability exporters.</li> <li>VCR cassettes for deterministic tests.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#05-event-bus-messaging-infrastructure-new","title":"0.5 Event Bus &amp; Messaging Infrastructure (New)","text":"<ul> <li>Define Broker Protocol (Producer/Consumer).</li> <li>Implementations: Memory (Dev), Redis Pub/Sub (MVP), Google PubSub (Prod).</li> <li>Standardized Event Envelope (id, timestamp, type, payload, trace_context).</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#06-rate-limiting-cost-control-new","title":"0.6 Rate Limiting &amp; Cost Control (New)","text":"<ul> <li>TokenBucket algorithm implementation.</li> <li>Tenant-based quota management (requests/min, tokens/day).</li> <li>Fail-fast mechanism before Provider invocation.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#phase-1-ai-tools-prompt-engine-and-dispatcher-high-p1","title":"Phase 1 \u2014 AI Tools, Prompt Engine and Dispatcher (High, P1)","text":""},{"location":"backlog/todo/ai-ml-procudt-backlog/#11-tools-layer-nalaathomicaitools","title":"1.1 Tools Layer (nala.athomic.ai.tools)","text":"<ul> <li>\\@ai_tool decorator with metadata.</li> <li>Tool registry with auto discovery + JSON Schema generation.</li> <li>Multi provider formatting support.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#12-action-dispatcher-self-healing","title":"1.2 Action Dispatcher (Self Healing)","text":"<ul> <li>Parse function calling responses.</li> <li>Execute tools with structured error handling.</li> <li>Repair prompts (max_repair_attempts).</li> <li>Multi-step tool chaining.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#13-prompt-engine","title":"1.3 Prompt Engine","text":"<ul> <li>External YAML or Jinja templates.</li> <li>Versioning for prompts.</li> <li>Template loader with slot injection.</li> <li>Prompt A/B testing.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#phase-2-safety-guardrails-and-compliance-high-p1","title":"Phase 2 \u2014 Safety, Guardrails and Compliance (High, P1)","text":""},{"location":"backlog/todo/ai-ml-procudt-backlog/#21-guardrails-framework","title":"2.1 Guardrails Framework","text":"<ul> <li>InputSanitizerProtocol and ContentValidatorProtocol.</li> <li>RegexPIISanitizer for email, phone, cpf, jwt and credit card.</li> <li>KeywordBlocklistValidator.</li> <li>\\@guardrail decorator.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#22-secure-logging-and-telemetry","title":"2.2 Secure Logging and Telemetry","text":"<ul> <li>PII scrubbing across logs, traces and prompts.</li> <li>Store pre and post sanitized snapshots.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#23-secrets-and-credential-management","title":"2.3 Secrets and Credential Management","text":"<ul> <li>Zero downtime key rotation.</li> <li>Dynamic settings reload for Vertex provider on 401/403.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#phase-3-semantic-memory-vector-store-and-document-ingestion-high-p1","title":"Phase 3 \u2014 Semantic Memory, Vector Store and Document Ingestion (High, P1)","text":""},{"location":"backlog/todo/ai-ml-procudt-backlog/#31-vector-store-nalaathomicaimemory","title":"3.1 Vector Store (nala.athomic.ai.memory)","text":"<ul> <li>VectorStoreProtocol for add, search and delete.</li> <li>Multi-tenancy support: Namespace/Collection segregation per tenant.</li> <li>QdrantVectorStore implementation.</li> <li>RedisVectorStore fallback.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#32-semantic-memory-service","title":"3.2 Semantic Memory Service","text":"<ul> <li>Embedding orchestration.</li> <li>Vector persistence with tenancy isolation.</li> <li>Emit <code>memory.created</code> events via Event Bus.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#33-document-ingestion-nalaathomicaidocuments","title":"3.3 Document Ingestion (nala.athomic.ai.documents)","text":"<ul> <li>PDFDocumentLoader, TextDocumentLoader.</li> <li>Chunking strategies:</li> <li>RecursiveCharacterSplitter.</li> <li>MarkdownHeaderSplitter.</li> <li>Ingestion pipeline:</li> <li>download \u2192 extract \u2192 chunk \u2192 embed \u2192 persist.</li> <li>Background workers for CPU heavy operations.</li> <li>Emit <code>document.ingestion.completed</code> events.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#phase-4-rag-core-vector-only-high-p1","title":"Phase 4 \u2014 RAG Core (Vector Only) (High, P1)","text":""},{"location":"backlog/todo/ai-ml-procudt-backlog/#40-rag-evaluation-harness-mvp-shifted-left","title":"4.0 RAG Evaluation Harness (MVP) (Shifted Left)","text":"<ul> <li>Do not proceed without measuring.</li> <li>Simple \"Golden Dataset\" runner.</li> <li>Basic Metrics: Retrieval Precision, Answer Relevance (LLM-as-Judge).</li> <li>Integration with CI for regression testing.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#41-rag-core-nalaathomicairag","title":"4.1 RAG Core (nala.athomic.ai.rag)","text":"<ul> <li>Retrieve top K.</li> <li>Context builder.</li> <li>Template-driven RAG structure.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#42-advanced-rag-enhancements","title":"4.2 Advanced RAG Enhancements","text":"<ul> <li>Cross encoder re-ranking.</li> <li>Query expansion (LLM or HyDE).</li> <li>Metadata filtering.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#43-query-understanding-and-intent-engine","title":"4.3 Query Understanding and Intent Engine","text":"<ul> <li>Intent classification: search, ask, summarize, compare, plan.</li> <li>Query rewriting and optimization pipeline.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#44-context-and-window-optimization","title":"4.4 Context and Window Optimization","text":"<ul> <li>Dynamic token budget allocation.</li> <li>Adaptive context window expansion.</li> <li>Compression RAG:</li> <li>summary embeddings</li> <li>low rank chunk compression.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#phase-45-semantic-cache-high-p1","title":"Phase 4.5 \u2014 Semantic Cache (High, P1)","text":""},{"location":"backlog/todo/ai-ml-procudt-backlog/#451-semantic-cache-layer-nalaathomicaisemantic_cache","title":"4.5.1 Semantic Cache Layer (nala.athomic.ai.semantic_cache)","text":"<ul> <li>Vector similarity lookup before LLM call.</li> <li>Threshold configurable.</li> <li>TTL eviction strategy.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#phase-6a-minimal-agent-runtime-foundational-subset-only","title":"Phase 6A \u2014 Minimal Agent Runtime (Foundational Subset Only)","text":"<p>This subset enables basic agent execution without routing, HITL or sandboxing.</p>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#6a1-agent-runtime-protocol","title":"6A.1 Agent Runtime Protocol","text":"<p>```python run(...) invoke(...) stream(...) persist_state(...) ```</p>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#6a2-base-athomiclanggraphruntime","title":"6A.2 Base AthomicLangGraphRuntime","text":"<ul> <li>Basic graph execution.</li> <li>Simple step loop.</li> <li>Error propagation.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#6a3-state-persistence-and-observability","title":"6A.3 State Persistence and Observability","text":"<ul> <li>Redis checkpointer.</li> <li>AthomicAgentTracing integration with OTel.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#phase-5-knowledge-graph-engine-graphrag-p2","title":"Phase 5 \u2014 Knowledge Graph Engine (GraphRAG) (P2)","text":""},{"location":"backlog/todo/ai-ml-procudt-backlog/#51-knowledge-graph-layer-nalaathomicaigraph","title":"5.1 Knowledge Graph Layer (nala.athomic.ai.graph)","text":"<ul> <li>GraphStoreProtocol: ```python add_node(...) add_edge(...) query_subgraph(...) ```</li> <li>Multi-tenancy support: Label strategy or separate DBs per tenant.</li> <li>Adapters for KuzuDB, Neo4j and Memgraph.</li> <li>Node and edge upsert operations.</li> <li>Graph schema definitions.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#52-entity-and-relation-extraction","title":"5.2 Entity and Relation Extraction","text":"<ul> <li>Structured extraction during ingestion.</li> <li>Map entities to nodes and edges.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#53-graph-augmented-retrieval","title":"5.3 Graph-Augmented Retrieval","text":"<ul> <li>Graph-based retrieval.</li> <li>Hybrid vector + graph retrieval.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#phase-6b-full-agent-runtime-and-agent-factory-medium-p2","title":"Phase 6B \u2014 Full Agent Runtime and Agent Factory (Medium, P2)","text":""},{"location":"backlog/todo/ai-ml-procudt-backlog/#64-agent-factory","title":"6.4 Agent Factory","text":"<ul> <li>Agents with tools, RAG, routing, memory and tracing.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#65-prebuilt-agent-profiles","title":"6.5 Prebuilt Agent Profiles","text":"<ul> <li>Research Agent.</li> <li>RAG Agent.</li> <li>Planning Agent.</li> <li>Tool Execution Agent.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#66-reasoning-monitoring-and-anti-loop-controls","title":"6.6 Reasoning Monitoring and Anti Loop Controls","text":"<ul> <li>Loop detection heuristics.</li> <li>Hallucination detection heuristics.</li> <li>LLM reasoning score.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#67-secure-code-execution-sandboxing","title":"6.7 Secure Code Execution (Sandboxing)","text":"<ul> <li>CodeInterpreterProtocol.</li> <li>Integration with ephemeral environments (E2B or Docker).</li> <li>Safe Python/JS execution.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#68-human-in-the-loop-hitl","title":"6.8 Human-in-the-Loop (HITL)","text":"<ul> <li>Interrupt signals (pause/resume).</li> <li>Approval workflow (wait_for_human).</li> <li>State snapshotting.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#phase-7-smart-routing-and-optimization-medium-p2","title":"Phase 7 \u2014 Smart Routing and Optimization (Medium, P2)","text":""},{"location":"backlog/todo/ai-ml-procudt-backlog/#71-smart-router-nalaathomicairouter","title":"7.1 Smart Router (nala.athomic.ai.router)","text":"<ul> <li>Tier definitions: FAST, STRONG, ULTRA.</li> <li>Routing based on cost, latency, model capability and token length.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#72-fallback-strategies","title":"7.2 Fallback Strategies","text":"<ul> <li>Provider fallback chain.</li> <li>Embeddings fallback.</li> <li>Provider health checks.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#73-cost-optimization-ai","title":"7.3 Cost Optimization AI","text":"<ul> <li>Predictive cost and latency models.</li> <li>RL-based provider selection.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#phase-8-mlops-classic-ml-and-drift-strategic-p3","title":"Phase 8 \u2014 MLOps, Classic ML and Drift (Strategic, P3)","text":""},{"location":"backlog/todo/ai-ml-procudt-backlog/#81-model-storage-nalaathomicml","title":"8.1 Model Storage (nala.athomic.ml)","text":"<ul> <li>ModelStorageProtocol.</li> <li>MLflow adapter.</li> <li>GCS or bucket provider.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#82-inference-service","title":"8.2 Inference Service","text":"<ul> <li>Lazy loading.</li> <li>Async warming.</li> <li>Batch prediction.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#83-drift-and-data-monitoring","title":"8.3 Drift and Data Monitoring","text":"<ul> <li>Async prediction logger.</li> <li>Feature snapshotting.</li> <li>Drift detection.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#84-fine-tuning-and-continual-learning","title":"8.4 Fine Tuning and Continual Learning","text":"<ul> <li>Fine tuning for LLMs and embeddings.</li> <li>Dataset management and deduplication.</li> <li>Continual learning pipeline via feedback.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#85-synthetic-data-generator","title":"8.5 Synthetic Data Generator","text":"<ul> <li>Synthetic QA for RAG improvements.</li> <li>Synthetic structured data.</li> <li>Edge case generation.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#phase-9-developer-experience-and-evaluation","title":"Phase 9 \u2014 Developer Experience and Evaluation","text":""},{"location":"backlog/todo/ai-ml-procudt-backlog/#91-advanced-ai-evaluation-nalaathomicaiqa","title":"9.1 Advanced AI Evaluation (nala.athomic.ai.qa)","text":"<ul> <li>Multi Judge Evaluation (Agreement scoring, Bias detection).</li> <li>Automated Prompt Optimization (DSPy-like).</li> <li>Prompt diffing.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#92-feedback-loop","title":"9.2 Feedback Loop","text":"<ul> <li>Feedback API associated to trace IDs.</li> <li>Export pipeline for fine tuning datasets.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#93-developer-experience-nalaathomicaidx","title":"9.3 Developer Experience (nala.athomic.ai.dx)","text":"<ul> <li>Playground UI for prompts, tools, agents and routing.</li> <li>SSE streaming.</li> <li>Token cost explorer.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#phase-10-unified-observability-and-benchmarking","title":"Phase 10 \u2014 Unified Observability and Benchmarking","text":""},{"location":"backlog/todo/ai-ml-procudt-backlog/#101-unified-observability","title":"10.1 Unified Observability","text":"<ul> <li>Span naming across all modules.</li> <li>End-to-end traces across ingestion \u2192 memory \u2192 graph \u2192 rag \u2192 agent.</li> <li>Deep trace linking into vector metadata.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#102-benchmarking-suite","title":"10.2 Benchmarking Suite","text":"<ul> <li>Provider benchmarking (cost, latency).</li> <li>Cold start agent benchmarks.</li> <li>Embedding latency benchmarking.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#103-ci-cd-enhancements","title":"10.3 CI &amp; CD Enhancements","text":"<ul> <li>Pre-merge RAG regression suite.</li> <li>Agent end-to-end harness.</li> <li>Automated semantic versioning for prompts.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#phase-11-multimodal-real-time-expansion-new-p2","title":"Phase 11 \u2014 Multimodal &amp; Real-Time Expansion (New, P2)","text":""},{"location":"backlog/todo/ai-ml-procudt-backlog/#111-multimodal-provider-support","title":"11.1 Multimodal Provider Support","text":"<ul> <li>Unified multimodal protocol: ```python generate_image transcribe_audio describe_image embed_image analyze_video ```</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#112-multimodal-document-ingestion","title":"11.2 Multimodal Document Ingestion","text":"<ul> <li>OCR.</li> <li>PPTX loader.</li> <li>Excel/CSV loader.</li> <li>Table/layout extraction.</li> <li>Video frame extraction.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#113-multimodal-rag","title":"11.3 Multimodal RAG","text":"<ul> <li>Image embeddings retrieval.</li> <li>Mixed text+image context.</li> <li>Video segment retrieval.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#114-real-time-voice-streaming","title":"11.4 Real-Time Voice &amp; Streaming","text":"<ul> <li>WebSocket bi-directional audio.</li> <li>VAD middleware.</li> <li>Low-latency TTS streaming.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#phase-12-memory-extensions-new-p2","title":"Phase 12 \u2014 Memory Extensions (New, P2)","text":""},{"location":"backlog/todo/ai-ml-procudt-backlog/#121-episodic-memory","title":"12.1 Episodic Memory","text":"<ul> <li>Short term session memory.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#122-working-memory","title":"12.2 Working Memory","text":"<ul> <li>Lightweight reasoning state.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#123-personalized-memory","title":"12.3 Personalized Memory","text":"<ul> <li>Per user or per team context memory.</li> </ul>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#epic-7-ai-innovation-future","title":"EPIC 7 \u2014 AI &amp; Innovation (Future)","text":"<p>Goal: Prepare Nala to be an AI-native framework.</p>"},{"location":"backlog/todo/ai-ml-procudt-backlog/#tasks","title":"Tasks","text":"<ul> <li>[ ] Dev Agent (Internal)<ul> <li>Create agent that reads <code>nala-core-agent-instructions.md</code> and can generate boilerplate code based on prompts.</li> </ul> </li> <li>[ ] AIaaS Modules<ul> <li><code>AnomalyDetector</code> (Logs/Metrics analysis).</li> <li><code>SuggestionEngine</code> (Config optimization).</li> </ul> </li> </ul>"},{"location":"backlog/todo/circuit_breaker/","title":"Epic: Circuit Breaker V2 Enhancements","text":"<p>This epic covers a set of features to improve the observability, dynamic configuration, and resilience of the Circuit Breaker module.</p>"},{"location":"backlog/todo/circuit_breaker/#story-1-enhance-circuit-breaker-with-comprehensive-observability","title":"Story 1: Enhance Circuit Breaker with Comprehensive Observability","text":"<p>User Story: As a DevOps/SRE, I want the Circuit Breaker module to emit events and metrics whenever a circuit changes its state, so that I can monitor service health in real-time, create automated alerts, and diagnose failures more effectively.</p> <p>Acceptance Criteria: 1.  A structured log event must be emitted every time a circuit breaker's state changes (e.g., <code>CLOSED -&gt; OPEN</code>). The log should include <code>circuit_name</code>, <code>previous_state</code>, <code>new_state</code>, and <code>trigger</code> (e.g., failure_threshold_exceeded). 2.  A Prometheus gauge metric named <code>athomic_circuit_breaker_state</code> must be exposed. This metric should have labels for <code>circuit_name</code>, <code>tenant_id</code> (if applicable), and reflect the current state (CLOSED=0, OPEN=1, HALF_OPEN=2). 3.  A Prometheus counter metric named <code>athomic_circuit_breaker_state_changes_total</code> must be incremented on every state change. 4.  Edge Case: Ensure metrics accurately reflect the local instance's perception of the distributed state.</p> <p>Technical Notes: -   Integrate with <code>nala.athomic.observability</code>. -   Use the Observer Pattern: Create <code>CircuitBreakerObserver</code> that attaches to the breaker instance. -   Ensure compatibility with the \"Metrics Style Guide\" (Epic 2 of Observability).</p>"},{"location":"backlog/todo/circuit_breaker/#story-2-implement-dynamic-configuration-for-circuit-breakers","title":"Story 2: Implement Dynamic Configuration for Circuit Breakers","text":"<p>User Story: As a Platform Engineer, I want the ability to update Circuit Breaker policies (e.g., <code>fail_max</code>, <code>reset_timeout_sec</code>) at runtime without a service restart.</p> <p>Acceptance Criteria: 1.  The <code>CircuitBreakerService</code> must listen for configuration changes via <code>live_config</code>. 2.  When configuration updates:     -   If the circuit is <code>CLOSED</code>: Update parameters immediately.     -   If the circuit is <code>OPEN</code>: Do not reset to CLOSED. Either maintain <code>OPEN</code> state with the new timeout or transition to <code>HALF-OPEN</code> to safely test downstream health. 3.  The update process must be thread-safe.</p> <p>Technical Notes: -   Invalidate <code>self._breakers</code> cache safely. -   Critical Safety: Avoid \"Hard Resets\" that could accidentally flood a recovering downstream service during a config push.</p>"},{"location":"backlog/todo/circuit_breaker/#story-3-implement-fail-safe-mode-storage-resilience","title":"Story 3: Implement Fail-Safe Mode (Storage Resilience)","text":"<p>User Story: As an Application Developer, I want the Circuit Breaker to gracefully handle storage (Redis) failures, ensuring that the resilience mechanism itself does not become a cause of application downtime.</p> <p>Acceptance Criteria: 1.  Catch <code>ConnectionError</code> or <code>TimeoutError</code> from the <code>CircuitBreakerStorage</code>. 2.  Fallback Strategy:     -   Primary: Fallback to a local In-Memory counter mechanism (degraded mode, non-distributed).     -   Secondary (if memory fails): Default to <code>CLOSED</code> (pass-through) and log a CRITICAL warning. 3.  Log the storage failure with a dedicated error code (e.g., <code>BREAKER_STORAGE_UNAVAILABLE</code>). 4.  Implement a \"Background Reconnector\" or \"Retry-on-Next-Call\" strategy to restore Redis connectivity automatically.</p> <p>Technical Notes: -   Wrap the storage client in a <code>ResilientStorageProxy</code>. -   Use a boolean flag <code>_is_degraded_mode</code> to toggle between Redis and Local Dictionary logic.</p>"},{"location":"backlog/todo/dependency-injection/","title":"Dependency injection","text":""},{"location":"backlog/todo/dependency-injection/#epic-architecture-refactor-migrate-to-explicit-dependency-injection-di-ioc","title":"EPIC: Architecture Refactor: Migrate to Explicit Dependency Injection (DI) / IoC","text":"<p>Goal: Eliminate all static/global service factories (<code>Factory.create()</code>) and singletons from infrastructure components (excluding fundamental registries and utilities). Centralize the creation and lifecycle management of core Athomic services within a dedicated DI Container class orchestrated by the <code>Athomic</code> Facade.</p> <p>Motivation: 1.  Enhance Testability (Isolation): Allow easy mocking and swapping of infrastructure components (e.g., <code>ConnectionManager</code>, <code>ProducerManager</code>) during unit and integration testing without relying on global state or manual cache clearing. 2.  Improve Decoupling (DIP/SRP): Ensure components like <code>EventSourcingFactory</code> receive their dependencies via the constructor (<code>__init__</code>) rather than resolving them internally via factories, separating the responsibility of use from creation. 3.  Future-Proofing: Prepare the framework for future adoption of a full third-party DI library (e.g., <code>dependency_injector</code>) or complex multi-tenancy scenarios.</p> <p>Acceptance Criteria: * The <code>Athomic</code> Facade is responsible for instantiating the <code>AthomicContainer</code>. * The global <code>connection_manager_factory</code> (and its singleton cache) is eliminated. * The global <code>EventSourcingFactory</code> singleton is eliminated. * The <code>EventSourcingFactory</code> and other top-level factories receive all required dependencies (e.g., <code>AppSettings</code>, <code>ConnectionManager</code>) in their constructors. * All tests requiring isolation (e.g., testing the <code>EventSourcingFactory</code>) can inject mocked dependencies without affecting other tests.</p>"},{"location":"backlog/todo/dependency-injection/#user-stories-modules","title":"User Stories (Modules)","text":"Story ID Module Title Priority US-DI-001 Core/Facade Implement <code>AthomicContainer</code> and refactor <code>Athomic.__init__</code> to use DI. HIGH US-DI-002 Database Eliminate <code>connection_manager_factory</code> global singleton. HIGH US-DI-003 Event Sourcing Migrate <code>EventSourcingFactory</code> to fully constructor-injected dependencies. HIGH US-DI-004 Sagas Migrate <code>SagaManagerFactory</code> and supporting repositories to constructor-injected dependencies. MEDIUM US-DI-005 Auth Migrate <code>AuthProviderFactory</code> logic within <code>AuthExecutorRegistry</code> to accept injected <code>ConnectionManager</code>. MEDIUM"},{"location":"backlog/todo/dependency-injection/#tasks","title":"Tasks","text":"<ul> <li>[ ] Dependency Injection Refactor (FastAPI Integration)<ul> <li>Concept: Bridge <code>AthomicContainer</code> to FastAPI.</li> <li>Create <code>nala.athomic.api.deps.py</code>:   <code>python   def get_kv_store(request: Request) -&gt; KVStoreProtocol:       return request.app.state.container.kv_store</code></li> <li>Refactor Routes: <code>async def route(kv: KVStore = Depends(get_kv_store))</code></li> </ul> </li> <li>[ ] Fix SerializerFactory Circular Dependency<ul> <li>Use Constructor Injection for the Registry instead of import inside methods.</li> </ul> </li> <li>[ ] Modular Imports<ul> <li>Organize <code>pyproject.toml</code> extras (e.g., <code>pip install nala[google]</code>, <code>nala[aws]</code>) to avoid installing unused SDKs.</li> </ul> </li> <li>[ ] Consistent Provider Constructors<ul> <li>Refactor <code>NotificationFactory</code> and others to follow the \"Simple Constructor + Resilience Wrapper\" pattern.</li> </ul> </li> </ul>"},{"location":"backlog/todo/dependency-injection/#3-service-locator-vs-di","title":"3. Service Locator vs. DI","text":"<ul> <li>Observation: Current <code>Factory.create()</code> usage is implicit (Service Locator pattern).</li> <li> <p>Decision: Move towards Explicit Constructor Injection (as detailed in previous interaction) where the <code>Athomic</code> facade acts as the Composition Root.</p> </li> <li> <p>Design Patterns e Inje\u00e7\u00e3o de Depend\u00eancia O uso de Factory, Registry e Protocol est\u00e1 excelente e desacopla bem as implementa\u00e7\u00f5es.</p> </li> </ul> <p>F\u00e1bricas e Globais: Em EventSourcingFactory (src/nala/athomic/event_sourcing/factory.py), voc\u00ea instancia depend\u00eancias chamando connection_manager_factory.create() diretamente.</p> <p>Refatora\u00e7\u00e3o: Embora pr\u00e1tico, isso acopla a f\u00e1brica ao estado global/singleton das outras f\u00e1bricas. Considere passar as inst\u00e2ncias de ConnectionManager e outros servi\u00e7os fundamentais para o construtor da EventSourcingFactory (ou via um container de DI principal no startup). Isso facilita testes unit\u00e1rios onde voc\u00ea n\u00e3o quer mockar globais.</p> <ol> <li>Inje\u00e7\u00e3o de Depend\u00eancia Expl\u00edcita vs. Singleton Global O projeto utiliza uma combina\u00e7\u00e3o de inje\u00e7\u00e3o de depend\u00eancia e inst\u00e2ncias globais (singletons), como o </li> </ol> <p>get_athomic_instance. Embora isso funcione, pode tornar os testes um pouco mais complexos, pois o estado global precisa ser gerenciado (</p> <p>clear_athomic_instance).</p> <p>Sugest\u00e3o: Para componentes cr\u00edticos, considere passar inst\u00e2ncias como depend\u00eancias expl\u00edcitas sempre que poss\u00edvel. Por exemplo, em vez de um middleware ou rota chamar </p> <p>get_athomic_instance(), ele poderia receber a inst\u00e2ncia athomic atrav\u00e9s do request.state, onde ela j\u00e1 foi adicionada durante o lifespan da aplica\u00e7\u00e3o. Isso j\u00e1 \u00e9 feito corretamente para as rotas dos plugins, e o padr\u00e3o poderia ser expandido.</p> <ol> <li>Inje\u00e7\u00e3o de Depend\u00eancia vs. Service Locator Seu padr\u00e3o atual, com f\u00e1bricas que gerenciam singletons (_instance), se assemelha muito ao padr\u00e3o Service Locator. Ele funciona muito bem, mas possui alguns trade-offs em compara\u00e7\u00e3o com um container de Inje\u00e7\u00e3o de Depend\u00eancia (DI) mais expl\u00edcito:</li> </ol> <p>Acoplamento Impl\u00edcito: O c\u00f3digo que chama ConnectionManagerFactory.create() est\u00e1 implicitamente acoplado ao singleton global. Isso pode tornar os testes unit\u00e1rios um pouco mais complexos, exigindo o uso dos m\u00e9todos .clear() nos testes para garantir o isolamento.</p> <p>Visibilidade das Depend\u00eancias: As depend\u00eancias de uma classe n\u00e3o ficam expl\u00edcitas em seu construtor, sendo resolvidas internamente.</p> <p>Sugest\u00e3o: Para aplica\u00e7\u00f5es maiores ou para facilitar ainda mais os testes, voc\u00ea poderia explorar um \"Composition Root\" na inicializa\u00e7\u00e3o da aplica\u00e7\u00e3o (provavelmente dentro do Athomic.startup), onde as inst\u00e2ncias dos servi\u00e7os principais (como ConnectionManager, Producer) s\u00e3o criadas e explicitamente injetadas nos servi\u00e7os que dependem delas, em vez de cada servi\u00e7o chamar sua pr\u00f3pria f\u00e1brica. Frameworks como dependency-injector poderiam ajudar, mas a sua abordagem manual j\u00e1 \u00e9 muito organizada.</p>"},{"location":"backlog/todo/futher_improves/","title":"Futher improves","text":""},{"location":"backlog/todo/futher_improves/#title-add-detailed-exception-logging-in-baseserviceclose","title":"Title: Add Detailed Exception Logging in <code>BaseService.close</code>","text":"<p>Description: The <code>close</code> method in <code>BaseService</code>  currently catches generic <code>Exception</code>s during the shutdown of its background task and the execution of the <code>_close</code> hook. This prevents a failing service from halting the entire application shutdown but can hide the root cause of the failure. The <code>except</code> blocks should be updated to use <code>self.logger.exception(...)</code> to log the full stack trace, which will significantly improve debuggability of shutdown-related issues.</p> <p>Affected Files: * <code>nala/athomic/services/base.py</code> </p> <p>Labels: <code>improvement</code>, <code>observability</code>, <code>logging</code></p> <p>Acceptance Criteria: * The <code>try...except</code> block in the <code>close</code> method must log failures using <code>logger.exception</code>. * Tests should confirm that if a service's <code>_close</code> method raises an exception, it is logged with a full stack trace.</p>"},{"location":"backlog/todo/futher_improves/#title-add-documentation-regarding-dynamic-cache-decorator-application-in-httpclientfactory","title":"Title: Add Documentation Regarding Dynamic Cache Decorator Application in HttpClientFactory","text":"<p>Description: The <code>HttpClientFactory</code> dynamically applies the <code>@cache</code> decorator to the <code>get</code> method of a client instance when caching is enabled for that client. This is a powerful and efficient pattern but can be non-obvious to new maintainers. Add a clear and concise code comment in this section of the factory to explain why and how this is done. The comment should clarify that the <code>get</code> method on a cached client instance is a wrapped object, not the original method from the provider class.</p> <p>Affected Files: * <code>nala/athomic/http/factory.py</code> </p> <p>Labels: <code>documentation</code>, <code>maintainability</code>, <code>low-priority</code></p> <p>Acceptance Criteria: * A descriptive comment is added to the <code>if client_settings.cache...</code> block in <code>HttpClientFactory.create</code>.</p> <ul> <li>ID: <code>BK-001</code></li> <li>Title: Refactor Mongo Outbox Repository to Use Atomic Updates</li> <li>Type: <code>Refactor</code></li> <li>Priority: <code>High</code></li> <li>Description: The current implementation in <code>MongoOutboxRepository</code> uses a <code>find_one()</code> followed by a <code>save()</code> for update operations like <code>mark_event_attempted</code>, <code>mark_event_published</code>, and <code>mark_event_failed</code>. This pattern is not atomic and can lead to race conditions under high load. It is also less performant than a single database operation.</li> <li>Acceptance Criteria:<ul> <li>```</li> <li>[ ] The <code>mark_event_attempted</code> method should be refactored to use a single <code>update_one</code> operation with the <code>$inc</code> operator to increment the <code>attempts</code> field and <code>$set</code> to update <code>last_attempt_at</code>.</li> <li>[ ] The <code>mark_event_published</code> method should use a single <code>update_one</code> operation with <code>$set</code> to change the event's <code>status</code> to <code>PUBLISHED</code>.</li> <li>[ ] The <code>mark_event_failed</code> method should use a single <code>update_one</code> operation with <code>$set</code> to update the <code>last_error</code> and potentially the <code>status</code> fields.</li> <li>[ ] The changes must ensure the operations remain atomic and performant.</li> <li>```</li> </ul> </li> <li>Files:<ul> <li><code>nala/athomic/database/outbox/mongo/mongo_outbox_repository.py</code></li> </ul> </li> </ul> <ul> <li>ID: <code>BK-002</code></li> <li>Title: Ensure Consistent Exception Propagation in BaseKVStore</li> <li>Type: <code>Bug</code></li> <li>Priority: <code>Medium</code></li> <li>Description: The <code>delete</code> method in <code>BaseKVStore</code> catches exceptions, records them in the span, but does not re-raise them. This is inconsistent with the <code>get</code> and <code>set</code> methods, which correctly propagate exceptions. This inconsistency can lead to silent failures where callers are unaware that a delete operation failed.</li> <li>Acceptance Criteria:<ul> <li>```</li> <li>[ ] Add a <code>raise</code> statement within the <code>except</code> block of the <code>delete</code> method in <code>BaseKVStore</code>.</li> <li>[ ] Verify that all data operation methods (<code>get</code>, <code>set</code>, <code>delete</code>, <code>exists</code>, <code>clear</code>, etc.) in <code>BaseKVStore</code> have a consistent error handling strategy (i.e., they all propagate exceptions after logging/tracing).</li> <li>```</li> </ul> </li> <li>Files:<ul> <li><code>nala/athomic/database/kvstore/base.py</code></li> </ul> </li> </ul> <ul> <li>ID: <code>BK-003</code></li> <li>Title: Review Saga Compensation Strategy for Critical Failures</li> <li>Type: <code>Chore</code></li> <li>Priority: <code>Medium</code></li> <li>Description: The current <code>OrchestrationSagaExecutor</code> implements a \"fail-fast\" strategy for compensations. If a compensation step fails, the saga is marked as <code>FAILED</code>, and the compensation chain is halted. For critical transactions, this could leave the system in an inconsistent state that requires manual intervention.</li> <li>Acceptance Criteria:<ul> <li>```</li> <li>[ ] The team should discuss the implications of a failing compensation.</li> <li>[ ] Decide if a retry mechanism should be added to compensation steps.</li> <li>[ ] Consider implementing an alerting mechanism (e.g., publishing an event to a \"dead-letter-saga\" topic) when a compensation fails permanently.</li> <li>[ ] Document the chosen strategy and its trade-offs.</li> <li>```</li> </ul> </li> <li>Files:<ul> <li><code>nala/athomic/resilience/sagas/executors/orchestration_executor.py</code></li> </ul> </li> </ul> <ul> <li>ID: <code>BK-004</code></li> <li>Title: Improve Security in RedisKVClient Connection Logging</li> <li>Type: <code>Refactor</code></li> <li>Priority: <code>Low</code></li> <li>Description: The <code>_connect</code> method in <code>RedisKVClient</code> logs the connection URI with a note that credentials have been redacted. While the intention is good, logging the full URI string, even with a note, poses a potential risk of leaking credentials if the redaction process fails or is incomplete. A more secure approach is to never include sensitive parts of the URI in logs in the first place.</li> <li>Acceptance Criteria:<ul> <li>```</li> <li>[ ] The log message in the <code>_connect</code> method should be modified to explicitly exclude the user/password part of the URI.</li> <li>[ ] The log should still clearly indicate the host, port, and database being connected to for debugging purposes.</li> <li>```</li> </ul> </li> <li>Files:<ul> <li><code>nala/athomic/database/kvstore/providers/redis/client.py</code></li> </ul> </li> </ul> <ul> <li>ID: <code>BK-005</code></li> <li>Title: Refactor Athomic Facade <code>__init__</code> for Simpler Default Settings</li> <li>Type: <code>Refactor</code></li> <li>Priority: <code>Low</code></li> <li>Description: In the <code>Athomic</code> class constructor, the <code>settings</code> argument defaults to <code>None</code>, and is then resolved using an <code>or</code> statement. This can be slightly simplified by making <code>get_settings()</code> the default value directly in the method signature.</li> <li>Acceptance Criteria:<ul> <li>```</li> <li>[ ] Change the <code>__init__</code> method signature in <code>nala/athomic/facade.py</code> to <code>def __init__(self, ..., settings: AppSettings = None):</code>.</li> <li>[ ] Update the first line inside the method to <code>self.settings = settings or get_settings()</code>.</li> <li>```</li> </ul> </li> <li>Files:<ul> <li><code>nala/athomic/facade.py</code></li> </ul> </li> </ul>"},{"location":"backlog/todo/observability/","title":"Observability 2.0 Backlog (Athomic Layer)","text":""},{"location":"backlog/todo/observability/#epic-1-observabilitycore-unified-observability-layer","title":"EPIC 1 \u2014 ObservabilityCore (Unified Observability Layer)","text":""},{"location":"backlog/todo/observability/#goal","title":"Goal","text":"<p>Create a single module that centralizes logging, metrics, tracing, instrumentations and execution context.</p>"},{"location":"backlog/todo/observability/#tasks","title":"Tasks","text":"<ul> <li>[ ] Create module <code>nala/athomic/observability/core.py</code></li> <li>[ ] Register ObservabilityCore as a first-class Athomic service</li> <li>[ ] Centralize initialization of:</li> <li>[ ] logging</li> <li>[ ] tracing</li> <li>[ ] metrics</li> <li>[ ] OTel auto/semiauto instrumentation</li> <li>[ ] execution context propagation (tenant_id, trace_id)</li> <li>[ ] Create <code>ObservabilitySettings</code></li> <li>[ ] Ensure all services enrich logs + metrics with contextual fields</li> <li>[ ] Provide unified API:</li> <li>```python     observability_core.logger     observability_core.metrics     observability_core.tracer     ```</li> </ul>"},{"location":"backlog/todo/observability/#epic-2-metrics-style-guide-standardization","title":"EPIC 2 \u2014 Metrics Style Guide (Standardization)","text":""},{"location":"backlog/todo/observability/#goal_1","title":"Goal","text":"<p>Define strict conventions for naming, labeling and structuring all metrics.</p>"},{"location":"backlog/todo/observability/#tasks_1","title":"Tasks","text":"<ul> <li>[ ] Create file: <code>docs/observability/metrics-style-guide.md</code></li> <li>[ ] Define global naming convention:</li> <li><code>domain_action_metricname</code></li> <li>[ ] Define mandatory labels:</li> <li><code>tenant_id</code>, <code>service</code>, <code>provider</code>, <code>status</code></li> <li>[ ] Define standard histogram buckets</li> <li>[ ] Create templates for:</li> <li>counters  </li> <li>histograms  </li> <li>gauges</li> <li>[ ] Implement <code>@instrumented</code> decorator</li> <li>[ ] Write metric compliance tests</li> <li>[ ] Apply standard metrics to all Athomic modules</li> </ul>"},{"location":"backlog/todo/observability/#epic-3-distributed-tracing-deep-integration","title":"EPIC 3 \u2014 Distributed Tracing Deep Integration","text":""},{"location":"backlog/todo/observability/#goal_2","title":"Goal","text":"<p>Enable full end-to-end tracing coverage across messaging, payload pipeline, KVStore, HTTP clients and core services.</p>"},{"location":"backlog/todo/observability/#tasks_2","title":"Tasks","text":""},{"location":"backlog/todo/observability/#messaging-layer","title":"Messaging Layer","text":"<ul> <li>[ ] Add spans for:</li> <li>message processing</li> <li>batch processing</li> <li>orchestration strategy</li> <li>execution strategy</li> <li>retry / delay logic</li> <li>DLQ forwarding</li> <li>[ ] Include attributes:</li> <li><code>message_id</code>, <code>partition</code>, <code>offset</code>, <code>batch_size</code></li> </ul>"},{"location":"backlog/todo/observability/#payload-pipeline","title":"Payload Pipeline","text":"<ul> <li>[ ] Add spans for each pipeline step:</li> <li>serialize</li> <li>compress</li> <li>encrypt</li> <li>[ ] Register exceptions as span events</li> <li>[ ] Add attributes:</li> <li>original_size</li> <li>final_size</li> <li>algorithm</li> <li>step_latency</li> </ul>"},{"location":"backlog/todo/observability/#kvstore","title":"KVStore","text":"<ul> <li>[ ] Add spans for:</li> <li><code>get</code>, <code>set</code>, <code>delete</code>, <code>exists</code>, <code>expire</code></li> <li>[ ] Add attributes:</li> <li><code>key_type</code>, <code>size</code>, <code>ttl</code>, <code>provider_name</code></li> </ul>"},{"location":"backlog/todo/observability/#http-client","title":"HTTP Client","text":"<ul> <li>[ ] Wrap httpx requests with spans</li> <li>[ ] Add retry information into spans</li> <li>[ ] Add attributes:</li> <li><code>status_code</code>, <code>url_base</code>, <code>latency_ms</code></li> </ul>"},{"location":"backlog/todo/observability/#epic-4-official-grafana-dashboards","title":"EPIC 4 \u2014 Official Grafana Dashboards","text":""},{"location":"backlog/todo/observability/#goal_3","title":"Goal","text":"<p>Provide prebuilt, production-ready dashboards to reduce operational effort.</p>"},{"location":"backlog/todo/observability/#tasks_3","title":"Tasks","text":"<ul> <li>[ ] Create folder <code>observability/dashboards/</code></li> <li>[ ] Dashboard: API</li> <li>[ ] Latency per endpoint</li> <li>[ ] Throughput</li> <li>[ ] Error rates</li> <li>[ ] Latency heatmap</li> <li>[ ] Dashboard: Messaging</li> <li>[ ] Consumer lag</li> <li>[ ] Messages per second</li> <li>[ ] DLQ rate</li> <li>[ ] Retry/Delay stats</li> <li>[ ] Dashboard: Payload Pipeline</li> <li>[ ] Encode/Decode latency</li> <li>[ ] Compression ratio</li> <li>[ ] Crypto overhead</li> <li>[ ] Dashboard: Redis / KVStore</li> <li>[ ] Cache hit/miss</li> <li>[ ] Key size distribution</li> <li>[ ] TTL distribution</li> <li>[ ] Dashboard: Lifecycle</li> <li>[ ] Service uptime</li> <li>[ ] Service failure counters</li> <li>[ ] Provide CLI command:</li> <li>```bash     nala dashboards export     ```</li> </ul>"},{"location":"backlog/todo/observability/#epic-5-health-readiness-system","title":"EPIC 5 \u2014 Health &amp; Readiness System","text":""},{"location":"backlog/todo/observability/#goal_4","title":"Goal","text":"<p>Implement enterprise-grade health and readiness checks with deep validation.</p>"},{"location":"backlog/todo/observability/#tasks_4","title":"Tasks","text":""},{"location":"backlog/todo/observability/#health-checks","title":"Health Checks","text":"<ul> <li>[ ] Redis health check</li> <li>[ ] MongoDB health check</li> <li>[ ] Kafka health check</li> <li>[ ] Vault health check</li> <li>[ ] Multi-cloud secrets providers health</li> <li>[ ] File/Blob storage provider health</li> </ul>"},{"location":"backlog/todo/observability/#readiness-gates","title":"Readiness Gates","text":"<ul> <li>[ ] Integrate readiness with Athomic lifecycle manager</li> <li>[ ] Block:</li> <li>consumer startup</li> <li>producer startup</li> <li>background tasks</li> <li>outbox publisher</li> <li>[ ] Export readiness via:</li> <li>HTTP  </li> <li>metrics (<code>readiness_status</code>)</li> </ul>"},{"location":"backlog/todo/observability/#deep-checks","title":"Deep Checks","text":"<ul> <li>[ ] Connectivity  </li> <li>[ ] Permissions / IAM roles  </li> <li>[ ] Latency tests  </li> <li>[ ] Secret TTL verification  </li> <li>[ ] Queue/buffer overload checks  </li> </ul>"},{"location":"backlog/todo/observability/#epic-6-auto-instrumentation-layer","title":"EPIC 6 \u2014 Auto-Instrumentation Layer","text":""},{"location":"backlog/todo/observability/#goal_5","title":"Goal","text":"<p>Eliminate the need for manual observability hooks across Athomic modules.</p>"},{"location":"backlog/todo/observability/#tasks_5","title":"Tasks","text":"<ul> <li>[ ] Create decorators:</li> <li>```python     @observed_async     @observed_sync     ```</li> <li>[ ] Auto-inject:</li> <li>trace_id  </li> <li>tenant_id  </li> <li>correlation_id  </li> <li>[ ] Auto-instrument:</li> <li>KVStore</li> <li>Secrets Providers</li> <li>Producers</li> <li>Consumers</li> <li>Outbox</li> <li>HTTP Client</li> <li>Storage providers</li> </ul>"},{"location":"backlog/todo/observability/#epic-7-advanced-structured-logging","title":"EPIC 7 \u2014 Advanced Structured Logging","text":""},{"location":"backlog/todo/observability/#goal_6","title":"Goal","text":"<p>Provide rich, contextual, fully structured logging.</p>"},{"location":"backlog/todo/observability/#tasks_6","title":"Tasks","text":"<ul> <li>[ ] Create <code>StructuredLogger</code> abstraction</li> <li>[ ] Enrich logs with:</li> <li><code>trace_id</code></li> <li><code>tenant_id</code></li> <li><code>service</code></li> <li><code>environment</code></li> <li>[ ] Support JSON + human-readable formats</li> <li>[ ] Implement global maskers</li> <li>[ ] Add log rotation via settings</li> <li>[ ] Remove legacy prints and non-structured logs</li> </ul>"},{"location":"backlog/todo/observability/#epic-8-observability-cli-tooling","title":"EPIC 8 \u2014 Observability CLI Tooling","text":""},{"location":"backlog/todo/observability/#goal_7","title":"Goal","text":"<p>Provide CLI tooling for observability debugging, validation and configuration.</p>"},{"location":"backlog/todo/observability/#tasks_7","title":"Tasks","text":"<ul> <li>[ ] Command: <code>nala observability init</code></li> <li>[ ] Command: <code>nala tracing test</code></li> <li>[ ] Command: <code>nala metrics list</code></li> <li>[ ] Command: <code>nala health run</code></li> <li>[ ] Validate metric naming conventions</li> <li>[ ] Validate dashboards</li> <li>[ ] Generate observability boilerplate</li> </ul>"},{"location":"backlog/todo/observability/#epic-9-full-observability-documentation","title":"EPIC 9 \u2014 Full Observability Documentation","text":""},{"location":"backlog/todo/observability/#goal_8","title":"Goal","text":"<p>Create complete and clear documentation for all observability components.</p>"},{"location":"backlog/todo/observability/#tasks_8","title":"Tasks","text":"<ul> <li>[ ] Add \u201cObservability\u201d section in docs</li> <li>[ ] Document:</li> <li>logs  </li> <li>metrics  </li> <li>tracing  </li> <li>dashboards  </li> <li>health &amp; readiness  </li> <li>style guides  </li> <li>[ ] Include:</li> <li>code examples  </li> <li>best practices  </li> <li>architecture diagrams  </li> </ul>"},{"location":"backlog/todo/observability/#consolidated-roadmap-sequence","title":"Consolidated Roadmap Sequence","text":"<ol> <li>ObservabilityCore  </li> <li>Metrics Style Guide  </li> <li>Full tracing instrumentation  </li> <li>Grafana dashboards  </li> <li>Health &amp; readiness system  </li> <li>Structured logging  </li> <li>Auto-instrumentation layer  </li> <li>CLI tooling  </li> <li>Complete documentation</li> </ol>"},{"location":"backlog/todo/resilience/","title":"\ud83d\udccb Engineering Backlog: Athomic Resilience Layer","text":""},{"location":"backlog/todo/resilience/#13-develop-athomictoolguard-class","title":"1.3 Develop <code>AthomicToolGuard</code> Class","text":"<ul> <li>Description: Create the wrapper class that dynamically applies decorators to tool functions.</li> <li>Acceptance Criteria:<ul> <li>[ ] Implement method <code>execute_safely(func, **kwargs)</code>.</li> <li>[ ] Catch infrastructure exceptions and raise custom <code>AthomicInfrastructureError</code>.</li> <li>[ ] Implement structured logging (Info on call, Error on catch).</li> </ul> </li> </ul>"},{"location":"backlog/todo/resilience/#epic-3-vertex-ai-integration-agentic-ux","title":"\ud83e\udd16 Epic 3: Vertex AI Integration (Agentic UX)","text":"<p>Focus: How the LLM interacts with the resilience layer.</p>"},{"location":"backlog/todo/resilience/#31-implement-agentresponseformatter","title":"3.1 Implement <code>AgentResponseFormatter</code>","text":"<ul> <li>Description: The function that translates technical exceptions into System Instructions for the LLM.</li> <li>Acceptance Criteria:<ul> <li>[ ] Intercept <code>CircuitOpenError</code>.</li> <li>[ ] Return clear string: <code>\"SYSTEM_ALERT: [Reason]. STOP RETRYING.\"</code></li> <li>[ ] Security: Ensure raw Python stack traces are NOT leaked in the returned string.</li> </ul> </li> </ul>"},{"location":"backlog/todo/resilience/#32-observability-tracing","title":"3.2 Observability &amp; Tracing","text":"<ul> <li>Description: Instrument execution for monitoring.</li> <li>Acceptance Criteria:<ul> <li>[ ] Metrics: <code>athomic_tool_execution_count</code>, <code>athomic_circuit_breaker_state</code> (Gauge), <code>athomic_retry_count</code>.</li> <li>[ ] Logs compatible with Cloud Logging (Structured JSON with <code>trace_id</code>).</li> </ul> </li> </ul>"},{"location":"backlog/todo/resilience/#epic-4-quality-assurance-qa","title":"\ud83e\uddea Epic 4: Quality Assurance (QA)","text":""},{"location":"backlog/todo/resilience/#41-unit-testing-suite","title":"4.1 Unit Testing Suite","text":"<ul> <li>Description: Isolated tests for backoff algorithms and state transitions.</li> <li>Acceptance Criteria:<ul> <li>[ ] 90%+ Code Coverage on decorators.</li> <li>[ ] Mock time to test <code>recovery_timeout</code> transitions without waiting.</li> </ul> </li> </ul>"},{"location":"backlog/todo/resilience/#42-integration-testing-simulation","title":"4.2 Integration Testing (Simulation)","text":"<ul> <li>Description: Simulate a full flow where a \"Mock API\" fails intermittently.</li> <li>Acceptance Criteria:<ul> <li>[ ] Verify <code>Guard</code> stops calling the tool once Circuit is OPEN.</li> <li>[ ] Verify the LLM receives the correct <code>SYSTEM_ALERT</code>.</li> </ul> </li> </ul>"},{"location":"backlog/todo/resilience/#epic-9-resilience-hardening-specific-user-stories","title":"EPIC 9 \u2014 Resilience Hardening (Specific User Stories)","text":"<p>Focus: Handling edge cases, race conditions, and fail-safes.</p>"},{"location":"backlog/todo/resilience/#story-refactor-cache-invalidation-logic-race-condition-prevention","title":"Story: Refactor Cache Invalidation Logic (Race Condition Prevention)","text":"<ul> <li>[ ] Logic Change: Move <code>@invalidate_cache</code> execution to before the decorated function call.</li> <li>[ ] Validation: Update <code>InvalidationHandler</code> and tests to ensure cache is cleared before DB write to prevent stale data usage during the operation.</li> </ul>"},{"location":"backlog/todo/resilience/#story-improve-fallback-strategy-error-handling","title":"Story: Improve Fallback Strategy Error Handling","text":"<ul> <li>[ ] Refine Strategy: <code>FallbackOnMissOrErrorStrategy</code> must differentiate between <code>None</code> (Cache Miss) and <code>KVStoreError</code> (Failure).</li> <li>[ ] Behavior: Only trigger fallback chain on Exceptions, not on cache misses.</li> </ul>"},{"location":"backlog/todo/resilience/#story-circuit-breaker-storage-safety","title":"Story: Circuit Breaker Storage Safety","text":"<ul> <li>[ ] Strict Startup: <code>CircuitBreakerStorageFactory</code> must FAIL (raise <code>ServiceConnectionError</code>) if Redis is configured but unavailable.</li> <li>[ ] Removal: Remove the automatic silent fallback to <code>CircuitMemoryStorage</code> (it creates split-brain states in distributed systems).</li> </ul>"},{"location":"backlog/todo/resilience/#story-self-healing-deadlock-prevention","title":"Story: Self-Healing Deadlock Prevention","text":"<ul> <li>[ ] Async Fix: In <code>FallbackKVProvider.heal()</code>, use <code>asyncio.gather(..., return_exceptions=True)</code>.</li> <li>[ ] Goal: Ensure one failing provider doesn't halt the healing process of others.</li> </ul>"},{"location":"athomic/cloud_native_mesh_mode/","title":"Nala Athomic: Cloud-Native &amp; Service Mesh Mode","text":""},{"location":"athomic/cloud_native_mesh_mode/#executive-summary","title":"Executive Summary","text":"<p>Starting from the current version, the Nala framework officially supports Mesh Mode (Cloud-Native). This feature transforms the application from a \"Smart Client\" (which manages its own resilience, discovery, and networking logic) into a \"Dumb Client\" (which delegates these responsibilities to the infrastructure layer, such as Istio, Linkerd, or AWS App Mesh).</p>"},{"location":"athomic/cloud_native_mesh_mode/#how-to-enable","title":"How to Enable","text":"<p>Activation is controlled via a single environment variable or configuration setting in <code>settings.toml</code>.</p> <p>Environment Variable: ```bash NALA_DEPLOYMENT_MODE=\"mesh\" ```</p> <p>settings.toml: ```toml [default] deployment_mode = \"mesh\" ```</p>"},{"location":"athomic/cloud_native_mesh_mode/#architectural-changes-bimodal-operation","title":"Architectural Changes (Bimodal Operation)","text":"<p>When <code>DEPLOYMENT_MODE=\"mesh\"</code>, the framework automatically adjusts the behavior of the following components:</p> Component Standalone Mode (Default) Mesh Mode (New) Rationale Service Discovery Actively queries Consul to resolve IP/Port. Uses Native K8s DNS (<code>svc.cluster.local</code>). The Envoy Proxy intercepts DNS and handles load balancing. Avoids unnecessary dependency on Consul. Resilience (Retry) Applies Retry policies (Backoff, Jitter) within the app. Disabled (No-Op). Executes only once. Prevents \"Retry Storms\" (App tries 3x * Mesh tries 3x = 9 requests). The Mesh handles retries. Circuit Breaker Manages circuit state (Open/Closed) in Memory/Redis. Disabled (Pass-through). The Mesh handles upstream failure detection (Outlier Detection) and ejects unhealthy pods. Security (mTLS) Can manage client certificates and strict SSL validation. Offload. Ignores <code>verify=True</code> and client certs. The Sidecar (Envoy) manages the encrypted mTLS tunnel between services transparently. Observability Propagates proprietary headers (<code>x-request-id</code>). Injects W3C headers (<code>traceparent</code>) via OpenTelemetry. Ensures Distributed Tracing (Istio/Jaeger) can stitch application spans with network spans."},{"location":"athomic/cloud_native_mesh_mode/#kubernetes-migration-guide-istio","title":"Kubernetes Migration Guide (Istio)","text":"<p>When deploying Nala services into a cluster with Sidecar Injection enabled, follow these steps:</p>"},{"location":"athomic/cloud_native_mesh_mode/#1-clean-application-configuration","title":"1. Clean Application Configuration","text":"<p>Remove manual networking configurations from your <code>settings.toml</code>. Do not define static IPs or complex failover logic in the application code.</p>"},{"location":"athomic/cloud_native_mesh_mode/#2-update-deploymentyaml","title":"2. Update Deployment.yaml","text":"<p>Inject the mode switch into your Pod definition:</p> <p>```yaml env:   - name: NALA_DEPLOYMENT_MODE     value: \"mesh\"   # Optional: Pass the namespace for the K8s DNS Provider   - name: KUBERNETES_NAMESPACE     valueFrom:       fieldRef:         fieldPath: metadata.namespace ```</p>"},{"location":"athomic/cloud_native_mesh_mode/#3-configure-virtualservice-istio","title":"3. Configure VirtualService (Istio)","text":"<p>Move the retry logic that previously resided in the Python code to the Istio manifest:</p> <p>```yaml apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata:   name: payment-service-route spec:   hosts:   - payment-service   http:   - route:     - destination:         host: payment-service     retries:       attempts: 3       perTryTimeout: 2s       retryOn: gateway-error,connect-failure,refused-stream ```</p>"},{"location":"athomic/cloud_native_mesh_mode/#troubleshooting","title":"Troubleshooting","text":""},{"location":"athomic/cloud_native_mesh_mode/#symptom-broken-tracing-graphs","title":"Symptom: Broken Tracing Graphs","text":"<p>Observation: Application spans appear disconnected from the Envoy/Mesh spans in Jaeger/Zipkin. Cause: Failure to propagate W3C headers. Fix: In Mesh Mode, Nala automatically calls <code>propagate.inject</code> to add <code>traceparent</code>. Ensure you are not manually clearing headers in custom middleware.</p>"},{"location":"athomic/cloud_native_mesh_mode/#symptom-sslcertificate-errors","title":"Symptom: SSL/Certificate Errors","text":"<p>Observation: Logs show errors like <code>SSLCertVerificationError</code> when calling upstream services. Cause: The application is trying to validate the Sidecar's self-signed certificate or the upstream certificate. Fix: Ensure <code>DEPLOYMENT_MODE=\"mesh\"</code> is set. This forces the <code>HttpClientFactory</code> to set <code>verify=False</code> and trust the local Proxy connection.</p>"},{"location":"athomic/compression/","title":"Payload Compression","text":""},{"location":"athomic/compression/#overview","title":"Overview","text":"<p>The Compression module provides an abstraction for compressing and decompressing data. Its primary use case is to act as a step in the Payload Processing Pipeline, where it can transparently compress message payloads before they are sent to a broker and decompress them on the consumer side. This reduces network bandwidth and storage costs.</p>"},{"location":"athomic/compression/#how-it-works","title":"How It Works","text":"<p>The module follows a provider pattern, with the <code>GzipProvider</code> being the default implementation. It is integrated into the <code>PayloadProcessor</code> via the <code>CompressionStepAdapter</code>. When <code>\"compression\"</code> is added as a step to the payload pipeline in your configuration, this adapter is automatically included in the processing chain.</p>"},{"location":"athomic/compression/#usage","title":"Usage","text":"<p>Usage is typically declarative via configuration. See the Payload Processing Pipeline documentation for an example of how to add the <code>compression</code> step to your pipeline.</p>"},{"location":"athomic/compression/#api-reference","title":"API Reference","text":""},{"location":"athomic/compression/#nala.athomic.compression.protocol.CompressionProtocol","title":"<code>nala.athomic.compression.protocol.CompressionProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the contract for a data compression provider.</p> <p>Any class that implements this protocol can be used by Athomic's components, such as the payload processing pipeline, to compress and decompress data. This allows for different compression algorithms (e.g., gzip, brotli) to be used interchangeably.</p>"},{"location":"athomic/compression/#nala.athomic.compression.protocol.CompressionProtocol.compress","title":"<code>compress(data)</code>  <code>async</code>","text":"<p>Compresses the given byte string.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>The raw byte string to be compressed.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The compressed byte string.</p>"},{"location":"athomic/compression/#nala.athomic.compression.protocol.CompressionProtocol.decompress","title":"<code>decompress(data)</code>  <code>async</code>","text":"<p>Decompresses the given byte string.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>The compressed byte string to be decompressed.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The original, decompressed byte string.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the data is corrupt or in an invalid format.</p>"},{"location":"athomic/compression/#nala.athomic.compression.providers.gzip_provider.GzipProvider","title":"<code>nala.athomic.compression.providers.gzip_provider.GzipProvider</code>","text":"<p>               Bases: <code>CompressionProtocol</code></p> <p>A concrete compression provider that uses the gzip algorithm.</p> <p>This class implements the <code>CompressionProtocol</code> by delegating the core compression and decompression logic to a specific algorithm instance obtained from the <code>CompressionAlgorithmFactory</code>. This design decouples the provider from the algorithm's implementation details.</p> <p>Attributes:</p> Name Type Description <code>algorithm</code> <code>CompressionAlgorithmProtocol</code> <p>The algorithm instance that performs the actual compression and decompression.</p>"},{"location":"athomic/compression/#nala.athomic.compression.providers.gzip_provider.GzipProvider.__init__","title":"<code>__init__(algorithm_name='gzip')</code>","text":"<p>Initializes the GzipProvider.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm_name</code> <code>str</code> <p>The name of the compression algorithm to create via the factory. Defaults to \"gzip\".</p> <code>'gzip'</code>"},{"location":"athomic/compression/#nala.athomic.compression.providers.gzip_provider.GzipProvider.compress","title":"<code>compress(data)</code>  <code>async</code>","text":"<p>Compresses data by delegating to the configured algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>The raw byte string to be compressed.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The gzip-compressed byte string.</p>"},{"location":"athomic/compression/#nala.athomic.compression.providers.gzip_provider.GzipProvider.decompress","title":"<code>decompress(data)</code>  <code>async</code>","text":"<p>Decompresses data by delegating to the configured algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>The gzip-compressed byte string to be decompressed.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The original, decompressed byte string.</p>"},{"location":"athomic/config/","title":"Configuration Management","text":""},{"location":"athomic/config/#overview","title":"Overview","text":"<p>The configuration module is a robust, type-safe system that provides the foundation for all configurable components within the Athomic Layer. It is built upon two key libraries:</p> <ul> <li>Dynaconf: For layered configuration loading from multiple sources (files, environment variables, Vault, etc.).</li> <li>Pydantic: For strict, type-safe validation of the configuration structure at startup.</li> </ul> <p>This combination ensures that the application starts with a valid and predictable configuration, failing fast if any required settings are missing or malformed.</p>"},{"location":"athomic/config/#key-features","title":"Key Features","text":"<ul> <li>Layered Loading: Settings are loaded from <code>.toml</code> files and can be individually overridden by environment variables.</li> <li>Environment Switching: Easily switch between environments (e.g., <code>development</code>, <code>production</code>) by setting a single environment variable.</li> <li>Type Safety: The entire configuration is parsed into a strongly-typed Pydantic model (<code>AppSettings</code>), providing auto-completion and static analysis benefits.</li> <li>Integrated Secret Management: Handles sensitive values securely, either by reading them directly or by resolving references to external secret managers like Vault at runtime.</li> </ul>"},{"location":"athomic/config/#how-it-works","title":"How It Works","text":"<p>The configuration is loaded and validated in a two-step process during application startup:</p> <ol> <li>Loading with Dynaconf: The <code>DynaconfLoader</code> reads all specified <code>.toml</code> files, merges them based on the current environment, and then applies any overrides from environment variables.</li> <li>Validation with Pydantic: The raw dictionary loaded by Dynaconf is passed to the root <code>AppSettings</code> Pydantic model. Pydantic recursively validates the entire structure, coercing types and raising a <code>ValidationError</code> if any part of the configuration is invalid.</li> </ol>"},{"location":"athomic/config/#accessing-settings","title":"Accessing Settings","text":"<p>The primary way to access configuration throughout the application is via the <code>get_settings()</code> function. This function is cached, ensuring that the loading and validation process happens only once.</p> <pre><code>from nala.athomic.config import get_settings\n\nsettings = get_settings()\nprint(settings.app_name)\nprint(settings.database.default_document_connection)\n</code></pre>"},{"location":"athomic/config/#configuration-files-environments","title":"Configuration Files &amp; Environments","text":"<p>By default, the application loads configuration from <code>settings.toml</code>. You can create additional files for different environments, like <code>development.toml</code> or <code>production.toml</code>.</p> <p>To activate a specific environment, set the <code>NALA_ENV_FOR_DYNACONF</code> environment variable. Dynaconf will then automatically merge the base <code>settings.toml</code> with the environment-specific file.</p> <pre><code>export NALA_ENV_FOR_DYNACONF=production\n# The application will now load settings from settings.toml and merge them with production.toml\n</code></pre>"},{"location":"athomic/config/#environment-variables-override","title":"Environment Variables Override","text":"<p>Any setting can be overridden using environment variables. This is particularly useful for containerized deployments (e.g., Docker, Kubernetes).</p> <ul> <li>Prefix: All variables must start with <code>NALA_</code>.</li> <li>Separator: Nested keys are separated by a double underscore <code>__</code>.</li> </ul> <p>For example, to override the database name for the default MongoDB connection, you would set the following environment variable:</p> <pre><code>export NALA_DATABASE__DOCUMENTS__DEFAULT_MONGO__PROVIDER__DATABASE_NAME=\"my_production_db\"\n</code></pre>"},{"location":"athomic/config/#secret-management","title":"Secret Management","text":"<p>The configuration system has first-class support for handling secrets securely.</p>"},{"location":"athomic/config/#1-direct-value-from-environment","title":"1. Direct Value (from Environment)","text":"<p>For secrets provided directly as environment variables, you can use Pydantic's <code>SecretStr</code> type in the configuration model. This ensures the value is automatically masked when logged or printed.</p>"},{"location":"athomic/config/#2-secret-reference-from-vault-etc","title":"2. Secret Reference (from Vault, etc.)","text":"<p>For a more secure approach, you can configure a reference to a secret stored in an external manager. This is done using the <code>SecretValue</code> model, which requires a <code>path</code> and a <code>key</code>.</p> <pre><code># In your settings.toml\n[default.database.documents.default_mongo.provider]\n# This does not contain the actual password.\n# It's a reference to a secret stored in Vault at path 'database/mongo' with key 'password'.\npassword = { path = \"database/mongo\", key = \"password\" } # pragma: allowlist secret\n</code></pre> <p>At startup, the <code>SecretsManager</code> traverses the entire configuration and replaces these <code>SecretValue</code> objects with a lazy-loading proxy. The actual secret is only fetched from the provider (e.g., Vault) at the moment it is first needed.</p>"},{"location":"athomic/config/#api-reference","title":"API Reference","text":""},{"location":"athomic/config/#nala.athomic.config.get_settings","title":"<code>nala.athomic.config.get_settings()</code>  <code>cached</code>","text":"<p>Loads, validates, and returns the application settings as a singleton.</p> <p>This is the primary entry point for accessing application configuration. It uses <code>@lru_cache</code> to ensure that the potentially expensive process of loading and validating settings from files and environment variables happens only once.</p> <p>Returns:</p> Name Type Description <code>AppSettings</code> <code>AppSettings</code> <p>A validated Pydantic model containing the entire</p> <code>AppSettings</code> <p>application configuration.</p>"},{"location":"athomic/config/#nala.athomic.config.schemas.app_settings.AppSettings","title":"<code>nala.athomic.config.schemas.app_settings.AppSettings</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The root Pydantic model for the entire application's configuration.</p>"},{"location":"athomic/config/#nala.athomic.config.schemas.models.SecretValue","title":"<code>nala.athomic.config.schemas.models.SecretValue</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a structured reference to a secret in an external provider.</p> <p>Instead of hardcoding secrets in configuration, this model is used to point to a secret's location (e.g., in HashiCorp Vault). The <code>SecretsManager</code> resolves these references at runtime to fetch the actual secret value.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>str</code> <p>The path or mount point in the secrets backend where the group of secrets is stored (e.g., 'database/production').</p> <code>key</code> <code>str</code> <p>The specific key within the given path that holds the secret value (e.g., 'password').</p>"},{"location":"athomic/context/","title":"Context Management","text":""},{"location":"athomic/context/#overview","title":"Overview","text":"<p>The Context Management module is the backbone for multi-tenancy, distributed tracing, and enriched logging throughout the Athomic Layer. It provides a safe and reliable way to carry request-scoped state through your application without passing arguments down the call stack.</p> <p>It is built on top of Python's native <code>contextvars</code>, making it fully compatible with <code>asyncio</code> and ensuring that context is never accidentally shared between concurrent requests or tasks.</p>"},{"location":"athomic/context/#core-use-cases","title":"Core Use Cases","text":"<ul> <li>Multi-Tenancy: Automatically isolates data by carrying the <code>tenant_id</code> from the initial request to the database layer.</li> <li>Distributed Tracing: Propagates <code>trace_id</code> and <code>span_id</code> across function calls and even to background tasks.</li> <li>Enriched Logging: Allows log records to be automatically enriched with contextual data like <code>request_id</code> and <code>user_id</code>.</li> <li>Consistent Keying: Provides a <code>ContextKeyGenerator</code> for creating standardized keys for caches, locks, and rate limiters.</li> </ul>"},{"location":"athomic/context/#core-components","title":"Core Components","text":""},{"location":"athomic/context/#contextvarmanager","title":"<code>ContextVarManager</code>","text":"<p>This is the central registry that manages the lifecycle of all context variables. It handles their creation, default values, and tracks which variables should be propagated to background tasks.</p>"},{"location":"athomic/context/#context_vars-module","title":"<code>context_vars</code> Module","text":"<p>This module defines all the context variables available in the application (e.g., <code>tenant_id</code>, <code>request_id</code>, <code>user_id</code>, <code>locale</code>) and provides simple getter and setter functions for each one. This is the primary API you will interact with.</p> <pre><code># Example of using the getters and setters\nfrom nala.athomic.context import context_vars\n\n# Set a value (typically in a middleware)\ntoken = context_vars.set_tenant_id(\"tenant-123\")\n\n# Get a value anywhere in the call stack\ncurrent_tenant = context_vars.get_tenant_id() # Returns \"tenant-123\"\n\n# Reset the value (restores the previous state)\ncontext_vars.get_tenant_id().reset(token)\n</code></pre>"},{"location":"athomic/context/#executioncontext","title":"<code>ExecutionContext</code>","text":"<p>This is a simple data class that captures a snapshot of all current context variables at a specific moment in time. Its main purpose is to facilitate context propagation.</p>"},{"location":"athomic/context/#context-propagation","title":"Context Propagation","text":"<p>One of the biggest challenges in a microservices architecture is maintaining context across different processes, especially when dispatching background tasks. This module provides a simple and effective solution.</p> <ol> <li><code>capture_context()</code>: Before a background task is sent to a broker, this function is called. It reads all context variables marked for propagation and returns them in a serializable dictionary.</li> <li><code>restore_context()</code>: On the worker side, this function is used as a context manager (<code>with restore_context(...)</code>). It takes the dictionary captured in step 1 and temporarily applies those values to the <code>contextvars</code> for the duration of the task execution.</li> </ol> <p>This ensures that a background task for sending an email, for example, will have the same <code>trace_id</code> and <code>tenant_id</code> as the API request that triggered it.</p>"},{"location":"athomic/context/#example","title":"Example","text":"<pre><code># In your API endpoint\nfrom nala.athomic.context import capture_context\nfrom my_app.tasks import send_welcome_email_task\n\nasync def create_user(user_data: dict):\n    # ... create user ...\n\n    # Capture the current context before enqueuing the task\n    context_to_propagate = capture_context()\n\n    # Pass the context in the task's keyword arguments\n    await send_welcome_email_task.delay(\n        user_id=user.id,\n        _nala_context=context_to_propagate\n    )\n\n# In your worker/task definition\nfrom nala.athomic.context import restore_context\nfrom nala.athomic.integration.tasks import task\n\n@task()\nasync def send_welcome_email_task(user_id: int, _nala_context: dict):\n    # The `run_task_with_context` decorator usually handles this automatically,\n    # but this is how it works internally.\n    with restore_context(_nala_context):\n        # Now, inside this block, get_tenant_id() and get_trace_id()\n        # will return the values from the original API request.\n        await send_email_to(user_id)\n</code></pre> <p>Note: The <code>@run_task_with_context</code> decorator automates this process for you.</p>"},{"location":"athomic/context/#contextual-key-generation","title":"Contextual Key Generation","text":"<p>To ensure consistency in caching, locking, and rate limiting, the <code>ContextKeyGenerator</code> automatically creates keys that are namespaced and aware of the current context.</p> <p>A key is typically generated with the following structure: <code>static_prefix:context_prefix:namespace:logical_key</code></p> <ul> <li><code>static_prefix</code>: A global prefix (e.g., \"nala\").</li> <li><code>context_prefix</code>: Automatically includes the <code>tenant_id</code> and/or <code>user_id</code> if enabled in the configuration.</li> <li><code>namespace</code>: A logical grouping for the key (e.g., \"cache\", \"ratelimit\").</li> <li><code>logical_key</code>: The specific identifier for the resource.</li> </ul> <pre><code>from nala.athomic.context import ContextKeyGenerator\n\n# Assume tenant_id='tenant-123' is set in the context\nkey_gen = ContextKeyGenerator(namespace=\"cache\")\ncache_key = key_gen.generate(\"user-profile\", \"user-456\")\n\n# Result: \"nala:tenant-123:cache:user-profile:user-456\"\n</code></pre>"},{"location":"athomic/context/#api-reference","title":"API Reference","text":""},{"location":"athomic/context/#nala.athomic.context.context_vars","title":"<code>nala.athomic.context.context_vars</code>","text":""},{"location":"athomic/context/#nala.athomic.context.context_vars.clear_all_context","title":"<code>clear_all_context()</code>","text":"<p>Clears all context variables to their default values.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.clear_correlation_id","title":"<code>clear_correlation_id()</code>","text":"<p>Clears the correlation_id context variable.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.clear_feature_flags","title":"<code>clear_feature_flags()</code>","text":"<p>Resets the feature_flags context variable to an empty dict.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.clear_locale","title":"<code>clear_locale()</code>","text":"<p>Resets the locale context variable to its default ('pt-BR').</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.clear_request_id","title":"<code>clear_request_id()</code>","text":"<p>Clears the request_id context variable.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.clear_role","title":"<code>clear_role()</code>","text":"<p>Clears the role context variable.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.clear_session_id","title":"<code>clear_session_id()</code>","text":"<p>Clears the session_id context variable.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.clear_source_ip","title":"<code>clear_source_ip()</code>","text":"<p>Clears the source_ip context variable.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.clear_span_id","title":"<code>clear_span_id()</code>","text":"<p>Clears the span_id context variable.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.clear_tenant_id","title":"<code>clear_tenant_id()</code>","text":"<p>Clears the tenant_id context variable.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.clear_timeout_cancelled","title":"<code>clear_timeout_cancelled()</code>","text":"<p>Resets the timeout_cancelled flag to False.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.clear_timeout_deadline","title":"<code>clear_timeout_deadline()</code>","text":"<p>Clears the timeout_deadline context variable.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.clear_trace_id","title":"<code>clear_trace_id()</code>","text":"<p>Clears the trace_id context variable.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.clear_user_id","title":"<code>clear_user_id()</code>","text":"<p>Clears the user_id context variable.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.get_correlation_id","title":"<code>get_correlation_id()</code>","text":"<p>Retrieves the correlation ID for linking related events across services.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.get_current_context_dic","title":"<code>get_current_context_dic()</code>","text":"<p>Returns a dictionary with the state of all context variables.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.get_feature_flags","title":"<code>get_feature_flags()</code>","text":"<p>Retrieves the dictionary of active feature flags.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.get_locale","title":"<code>get_locale()</code>","text":"<p>Retrieves the locale/language code for the current request.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.get_request_id","title":"<code>get_request_id()</code>","text":"<p>Retrieves the unique identifier for the current request context.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.get_role","title":"<code>get_role()</code>","text":"<p>Retrieves the role of the authenticated user for the current context.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.get_session_id","title":"<code>get_session_id()</code>","text":"<p>Retrieves the session identifier for the current context.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.get_source_ip","title":"<code>get_source_ip()</code>","text":"<p>Retrieves the source IP address of the client making the request.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.get_span_id","title":"<code>get_span_id()</code>","text":"<p>Retrieves the ID for the current span within the distributed trace.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.get_tenant_id","title":"<code>get_tenant_id()</code>","text":"<p>Retrieves the tenant ID for the current multi-tenancy context.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.get_timeout_deadline","title":"<code>get_timeout_deadline()</code>","text":"<p>Retrieves the absolute Unix timestamp of when the current operation must complete.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.get_trace_id","title":"<code>get_trace_id()</code>","text":"<p>Retrieves the distributed trace ID for the current context.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.get_user_id","title":"<code>get_user_id()</code>","text":"<p>Retrieves the authenticated user ID for the current context.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.get_vector_clock","title":"<code>get_vector_clock()</code>","text":"<p>Retrieves the VectorClock for the current causality context.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.is_timeout_cancelled","title":"<code>is_timeout_cancelled()</code>","text":"<p>Checks if the current operation has been marked as cancelled due to timeout.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.set_correlation_id","title":"<code>set_correlation_id(value)</code>","text":"<p>Sets the correlation ID for the current context, returning a token to reset the change.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.set_feature_flags","title":"<code>set_feature_flags(value)</code>","text":"<p>Sets the feature flags dictionary for the current context, returning a token to reset the change.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.set_locale","title":"<code>set_locale(value)</code>","text":"<p>Sets the locale for the current context, returning a token to reset the change.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.set_request_id","title":"<code>set_request_id(value)</code>","text":"<p>Sets the request ID for the current context, returning a token to reset the change.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.set_role","title":"<code>set_role(value)</code>","text":"<p>Sets the user role for the current context, returning a token to reset the change.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.set_session_id","title":"<code>set_session_id(value)</code>","text":"<p>Sets the session ID for the current context, returning a token to reset the change.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.set_source_ip","title":"<code>set_source_ip(value)</code>","text":"<p>Sets the source IP for the current context, returning a token to reset the change.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.set_span_id","title":"<code>set_span_id(value)</code>","text":"<p>Sets the span ID for the current context, returning a token to reset the change.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.set_tenant_id","title":"<code>set_tenant_id(value)</code>","text":"<p>Sets the tenant ID for the current context, returning a token to reset the change.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.set_timeout_cancelled","title":"<code>set_timeout_cancelled(value)</code>","text":"<p>Sets the timeout cancelled flag for the current context, returning a token to reset the change.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.set_timeout_deadline","title":"<code>set_timeout_deadline(value)</code>","text":"<p>Sets the timeout deadline for the current context, returning a token to reset the change.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.set_trace_id","title":"<code>set_trace_id(value)</code>","text":"<p>Sets the trace ID for the current context, returning a token to reset the change.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.set_user_id","title":"<code>set_user_id(value)</code>","text":"<p>Sets the user ID for the current context, returning a token to reset the change.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.set_vector_clock","title":"<code>set_vector_clock(value)</code>","text":"<p>Sets the VectorClock for the current context, returning a token to reset the change.</p>"},{"location":"athomic/context/#nala.athomic.context.propagation","title":"<code>nala.athomic.context.propagation</code>","text":""},{"location":"athomic/context/#nala.athomic.context.propagation.capture_context","title":"<code>capture_context()</code>","text":"<p>Captures the current values of all context variables marked for propagation.</p> <p>This function iterates through the centrally registered context variables in the <code>context_var_manager</code>, collects the values of those flagged with <code>propagate=True</code>, and returns them as a dictionary. This dictionary is suitable for serialization and passing to background tasks or events.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary containing the current context values to be propagated.</p>"},{"location":"athomic/context/#nala.athomic.context.propagation.restore_context","title":"<code>restore_context(context_dict)</code>","text":"<p>A context manager to temporarily set context variables from a dictionary.</p> <p>This is used on the worker side (e.g., in a background task or event handler) to re-establish the context that existed when the job was enqueued. It safely resets all variables to their previous state upon exiting the <code>with</code> block.</p> <p>Parameters:</p> Name Type Description Default <code>context_dict</code> <code>Optional[Dict[str, Any]]</code> <p>The dictionary of context values captured before the           task was enqueued.</p> required"},{"location":"athomic/context/#nala.athomic.context.generator.ContextKeyGenerator","title":"<code>nala.athomic.context.generator.ContextKeyGenerator</code>","text":"<p>A centralized service for creating consistent and context-sensitive keys.</p> <p>This class standardizes key generation for features like caching, locking, and rate limiting. It automatically prepends keys with a static prefix and contextual information (e.g., tenant ID) based on the application's configuration, ensuring key consistency and multi-tenancy support.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <code>ContextSettings</code> <p>The context configuration settings.</p> <code>static_prefix</code> <code>str</code> <p>A global prefix for all generated keys (e.g., 'nala').</p> <code>use_tenant</code> <code>bool</code> <p>If True, the tenant ID is included in the key.</p> <code>use_user</code> <code>bool</code> <p>If True, the user ID is included in the key.</p> <code>separator</code> <code>str</code> <p>The character used to separate key parts.</p> <code>namespace</code> <code>str</code> <p>The specific namespace for this key generator instance.</p>"},{"location":"athomic/context/#nala.athomic.context.generator.ContextKeyGenerator.__init__","title":"<code>__init__(*, namespace=None, settings=None)</code>","text":"<p>Initializes the ContextKeyGenerator.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>Optional[str]</code> <p>A specific namespace for the keys generated by this instance. Overrides the default namespace from settings if provided.</p> <code>None</code> <code>settings</code> <code>Optional[ContextSettings]</code> <p>The context configuration settings. If not provided, global settings are used.</p> <code>None</code>"},{"location":"athomic/context/#nala.athomic.context.generator.ContextKeyGenerator.generate","title":"<code>generate(*key_parts)</code>","text":"<p>Generates a final key from multiple parts with context and namespace.</p> <p>The final key structure is: <code>static_prefix:context_prefix:namespace:part1:part2:...</code></p> <p>Parameters:</p> Name Type Description Default <code>*key_parts</code> <code>Any</code> <p>A variable number of logical parts to be appended to the key.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The fully constructed, context-aware key.</p>"},{"location":"athomic/context/#nala.athomic.context.generator.ContextKeyGenerator.generate_for_function","title":"<code>generate_for_function(func, args, kwargs)</code>","text":"<p>Generates a unique and deterministic key for a function call.</p> <p>This method is ideal for function caching decorators. It creates a stable key by combining the function's qualified name with a SHA256 hash of its serialized arguments, ensuring that the same call always produces the same key.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>The function being called.</p> required <code>args</code> <code>Tuple</code> <p>The positional arguments passed to the function.</p> required <code>kwargs</code> <code>Dict</code> <p>The keyword arguments passed to the function.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A unique key representing the specific function call.</p>"},{"location":"athomic/context/#nala.athomic.context.execution.ExecutionContext","title":"<code>nala.athomic.context.execution.ExecutionContext</code>","text":"<p>A data class that provides a snapshot of the current execution context.</p> <p>This class consolidates all context variables (e.g., tenant_id, request_id, trace_id) from the <code>context_vars</code> module into a single, cohesive object. Its primary purpose is to offer a clean way to access context information and to capture the context at a specific point in time for propagation to background tasks or asynchronous events.</p> <p>Attributes:</p> Name Type Description <code>tenant_id</code> <code>Optional[str]</code> <p>The identifier for the current tenant.</p> <code>request_id</code> <code>Optional[str]</code> <p>The unique ID for the current request.</p> <code>trace_id</code> <code>Optional[str]</code> <p>The ID for the distributed trace.</p> <code>span_id</code> <code>Optional[str]</code> <p>The ID for the current span within a trace.</p> <code>user_id</code> <code>Optional[str]</code> <p>The identifier for the authenticated user.</p> <code>role</code> <code>Optional[str]</code> <p>The role of the authenticated user.</p> <code>locale</code> <code>Optional[str]</code> <p>The locale/language for the current request.</p> <code>source_ip</code> <code>Optional[str]</code> <p>The IP address of the original client.</p> <code>session_id</code> <code>Optional[str]</code> <p>The session identifier.</p> <code>correlation_id</code> <code>Optional[str]</code> <p>An ID to correlate logs and events across services.</p> <code>feature_flags</code> <code>Optional[Dict[str, bool]]</code> <p>A dictionary of active feature flags.</p>"},{"location":"athomic/context/#nala.athomic.context.execution.ExecutionContext.__init__","title":"<code>__init__(tenant_id=None, request_id=None, trace_id=None, span_id=None, user_id=None, role=None, locale=None, source_ip=None, session_id=None, correlation_id=None, feature_flags=None)</code>","text":"<p>Initializes the ExecutionContext.</p> <p>If a value for a parameter is not explicitly provided, the constructor will fetch the current value from the global <code>context_vars</code>. This allows for both creating a snapshot of the current context (<code>ExecutionContext()</code>) and creating a custom context for specific purposes (like testing or background jobs).</p> <p>Parameters:</p> Name Type Description Default <code>tenant_id</code> <code>Optional[str]</code> <p>The unique identifier for the tenant.</p> <code>None</code> <code>request_id</code> <code>Optional[str]</code> <p>The unique ID for the request.</p> <code>None</code> <code>trace_id</code> <code>Optional[str]</code> <p>The distributed trace ID.</p> <code>None</code> <code>span_id</code> <code>Optional[str]</code> <p>The current span ID.</p> <code>None</code> <code>user_id</code> <code>Optional[str]</code> <p>The identifier for the user.</p> <code>None</code> <code>role</code> <code>Optional[str]</code> <p>The user's role.</p> <code>None</code> <code>locale</code> <code>Optional[str]</code> <p>The request's locale.</p> <code>None</code> <code>source_ip</code> <code>Optional[str]</code> <p>The client's IP address.</p> <code>None</code> <code>session_id</code> <code>Optional[str]</code> <p>The session ID.</p> <code>None</code> <code>correlation_id</code> <code>Optional[str]</code> <p>The correlation ID.</p> <code>None</code> <code>feature_flags</code> <code>Optional[Dict[str, bool]]</code> <p>A dictionary of feature flags.</p> <code>None</code>"},{"location":"athomic/context/#nala.athomic.context.execution.ExecutionContext.__repr__","title":"<code>__repr__()</code>","text":"<p>Returns a string representation of the object.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A developer-friendly string representation of the context.</p>"},{"location":"athomic/context/#nala.athomic.context.execution.ExecutionContext.to_dict","title":"<code>to_dict()</code>","text":"<p>Serializes the context object to a dictionary.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary representation of the execution context,</p> <code>Dict[str, Any]</code> <p>suitable for logging or propagating to other services.</p>"},{"location":"athomic/credentials/","title":"Credential Resolution","text":""},{"location":"athomic/credentials/#overview","title":"Overview","text":"<p>The Credentials module provides the core components for the \"just-in-time\" secret resolution system used throughout the Athomic Layer. Its purpose is to securely handle sensitive values that might be raw strings, protected Pydantic <code>SecretStr</code> types, or lazy-loading <code>CredentialProxy</code> objects.</p>"},{"location":"athomic/credentials/#key-components","title":"Key Components","text":"<ul> <li><code>CredentialProxy</code>: A lightweight object that replaces a secret reference in the configuration at startup. It holds a function that knows how to fetch the real secret from a provider but doesn't execute it until the secret is actually needed.</li> <li><code>CredentialResolve</code>: A mixin class that provides the <code>_resolve_credential</code> and <code>_decode_credential_to_str</code> helper methods. Any service that needs to handle a potentially unresolved secret (like a database provider needing a password) inherits from this mixin to safely get the final secret value.</li> </ul> <p>For a more detailed explanation, see the Secrets Management documentation.</p>"},{"location":"athomic/credentials/#api-reference","title":"API Reference","text":""},{"location":"athomic/credentials/#nala.athomic.credentials.proxy.CredentialProxy","title":"<code>nala.athomic.credentials.proxy.CredentialProxy</code>","text":"<p>A lazy-loading proxy for a secret value.</p> <p>It holds a reference to the secrets provider and the secret's location (path/key). It fetches the secret's actual value \"just-in-time\" only when its <code>get()</code> method is explicitly awaited. This ensures that the application always uses the most up-to-date credential, supporting secret rotation.</p>"},{"location":"athomic/credentials/#nala.athomic.credentials.proxy.CredentialProxy.get","title":"<code>get()</code>  <code>async</code>","text":"<p>Fetches the secret's value just-in-time from the secrets provider.</p> <p>Returns:</p> Type Description <code>Optional[Any]</code> <p>A Resolved secret value, or None if not found.</p>"},{"location":"athomic/credentials/#nala.athomic.credentials.base.CredentialResolve","title":"<code>nala.athomic.credentials.base.CredentialResolve</code>","text":"<p>A mixin class providing standardized methods for resolving credentials.</p> <p>This class is intended to be inherited by services or providers that need to handle sensitive data. It centralizes the logic for unwrapping various credential types\u2014such as raw strings, Pydantic's <code>SecretStr</code>, or a lazy-loading <code>CredentialProxy</code>\u2014into their final, usable form.</p>"},{"location":"athomic/database/","title":"Database Connection Management","text":""},{"location":"athomic/database/#overview","title":"Overview","text":"<p>The <code>athomic.database</code> module provides a centralized, lifecycle-managed system for all data store connections within the application. Its primary component, the <code>ConnectionManager</code>, acts as a registry and orchestrator for all configured data stores, including Document Databases (like MongoDB) and Key-Value Stores (like Redis).</p> <p>This system is crucial for building robust applications as it ensures that:</p> <ul> <li>All database connections are established and checked for readiness during application startup.</li> <li>Dependencies are respected, so services that require a database connection only start after the connection is ready.</li> <li>All connections are gracefully closed during application shutdown.</li> </ul>"},{"location":"athomic/database/#core-components","title":"Core Components","text":""},{"location":"athomic/database/#connectionmanager","title":"<code>ConnectionManager</code>","text":"<p>The <code>ConnectionManager</code> is the heart of the database module. It is a <code>BaseService</code> whose lifecycle is managed by the main <code>LifecycleManager</code>. Its responsibilities are:</p> <ul> <li>Reading Configuration: It inspects the <code>[database]</code> section of your settings file to discover all configured connections.</li> <li>Creating Providers: For each configured connection, it uses a dedicated factory (e.g., <code>DocumentsDatabaseFactory</code>, <code>KVStoreFactory</code>) to instantiate the correct provider client.</li> <li>Managing Lifecycle: It concurrently starts (<code>connect</code>) and checks the readiness (<code>wait_ready</code>) of all provider instances during startup, and gracefully closes them during shutdown.</li> <li>Acting as a Registry: It provides getter methods (<code>get_document_db</code>, <code>get_kv_store</code>) for other services to retrieve active and ready-to-use database clients.</li> </ul>"},{"location":"athomic/database/#connectionmanagerfactory","title":"<code>ConnectionManagerFactory</code>","text":"<p>This is a singleton factory responsible for creating the single instance of the <code>ConnectionManager</code>. Any component in the application that needs access to a database connection should use this factory to get the <code>ConnectionManager</code>.</p>"},{"location":"athomic/database/#how-it-works","title":"How It Works","text":""},{"location":"athomic/database/#startup-sequence","title":"Startup Sequence","text":"<ol> <li>During application startup, the main <code>LifecycleManager</code> starts the <code>ConnectionManager</code> service.</li> <li>The <code>ConnectionManager</code> reads the settings and identifies all connections defined under <code>[database.documents]</code> and <code>[database.kvstore]</code>.</li> <li>It then concurrently calls the <code>connect()</code> and <code>wait_ready()</code> methods on each of these connection providers.</li> <li>The <code>ConnectionManager</code> itself becomes \"ready\" only after all its managed connections are successfully established.</li> </ol>"},{"location":"athomic/database/#retrieving-a-connection","title":"Retrieving a Connection","text":"<p>A service that needs to interact with a database follows these steps:</p> <ol> <li>Get the singleton <code>ConnectionManager</code> instance from the <code>ConnectionManagerFactory</code>.</li> <li>Call the appropriate getter method with the name of the desired connection, for example: <code>connection_manager.get_document_db(\"default_mongo\")</code>.</li> </ol> <p>This call will return a client instance that is guaranteed to be connected and ready to use.</p>"},{"location":"athomic/database/#configuration","title":"Configuration","text":"<p>You can define multiple connections for both document databases and key-value stores in your <code>settings.toml</code> file. This is useful for connecting to different databases for different purposes (e.g., one for application data, another for caching).</p> <pre><code>[default.database]\n# Define the names of the default connections to be used by other modules.\ndefault_document_connection = \"default_mongo\"\ndefault_kvstore_connection = \"default_redis\"\n\n# A dictionary of all available document database connections.\n[default.database.documents]\n  [default.database.documents.default_mongo]\n  enabled = true\n  backend = \"mongo\"\n    [default.database.documents.default_mongo.provider]\n    url = \"mongodb://user:pass@localhost:27017\" # pragma: allowlist secret\n    database_name = \"main_db\"\n\n# A dictionary of all available key-value store connections.\n[default.database.kvstore]\n  [default.database.kvstore.default_redis]\n  enabled = true\n  namespace = \"cache\"\n    [default.database.kvstore.default_redis.provider]\n    backend = \"redis\"\n    uri = \"redis://localhost:6379/0\"\n\n  [default.database.kvstore.another_redis]\n  enabled = true\n  namespace = \"sessions\"\n    [default.database.kvstore.another_redis.provider]\n    backend = \"redis\"\n    uri = \"redis://localhost:6379/1\"\n</code></pre>"},{"location":"athomic/database/#api-reference","title":"API Reference","text":""},{"location":"athomic/database/#nala.athomic.database.manager.ConnectionManager","title":"<code>nala.athomic.database.manager.ConnectionManager</code>","text":"<p>               Bases: <code>BaseService</code></p> <p>Orchestrates the lifecycle of all database connection managers. Acts as a Facade to access different types of database connections, delegating the responsibility to specialized managers.</p>"},{"location":"athomic/database/#nala.athomic.database.manager.ConnectionManager.get_document_db","title":"<code>get_document_db(name=None)</code>","text":"<p>Delegates the call to the document database manager.</p>"},{"location":"athomic/database/#nala.athomic.database.manager.ConnectionManager.get_graph_db","title":"<code>get_graph_db(name=None)</code>","text":"<p>Delegates the call to the graph manager.</p>"},{"location":"athomic/database/#nala.athomic.database.manager.ConnectionManager.get_kv_store","title":"<code>get_kv_store(name=None)</code>","text":"<p>Delegates the call to the K/V store manager.</p>"},{"location":"athomic/database/#nala.athomic.database.manager.ConnectionManager.get_vector_store","title":"<code>get_vector_store(name=None)</code>","text":"<p>Delegates the call to the vector manager to retrieve a vector store client.</p>"},{"location":"athomic/database/#nala.athomic.database.factory.ConnectionManagerFactory","title":"<code>nala.athomic.database.factory.ConnectionManagerFactory</code>","text":"<p>Manages the singleton instance of the ConnectionManager. Ensures that only one instance of the ConnectionManager exists throughout the application's lifecycle.</p>"},{"location":"athomic/database/#nala.athomic.database.factory.ConnectionManagerFactory.clear","title":"<code>clear()</code>  <code>async</code>","text":"<p>Clears the singleton instance of the ConnectionManager. This is primarily used for test isolation.</p>"},{"location":"athomic/database/#nala.athomic.database.factory.ConnectionManagerFactory.create","title":"<code>create(settings=None)</code>","text":"<p>Creates and returns the singleton instance of the ConnectionManager.</p> <p>On the first call, it instantiates the ConnectionManager. Subsequent calls return the cached instance.</p>"},{"location":"athomic/database/#nala.athomic.database.protocol.DatabaseClientProtocol","title":"<code>nala.athomic.database.protocol.DatabaseClientProtocol</code>","text":"<p>               Bases: <code>BaseServiceProtocol</code>, <code>Protocol</code></p> <p>Protocol for a document database provider.</p> <p>Defines the contract for managing a database connection and providing access to the native database instance (e.g., a Motor/Beanie database object). It inherits from ConnectionServiceProtocol to be managed by the application's lifecycle.</p>"},{"location":"athomic/database/#nala.athomic.database.protocol.DatabaseClientProtocol.get_database","title":"<code>get_database()</code>  <code>async</code>","text":"<p>Returns the native database instance required by repositories to perform operations.</p>"},{"location":"athomic/events/","title":"Internal Event Bus","text":""},{"location":"athomic/events/#overview","title":"Overview","text":"<p>The Internal Event Bus provides a lightweight, in-process publish/subscribe mechanism that allows different components within your application to communicate without being directly coupled to one another. It is a powerful tool for building modular and maintainable systems.</p> <p>Important Distinction: The Internal Event Bus is designed for intra-service communication (communication within a single service instance or across instances of the same service if using Redis). For durable, guaranteed, inter-service communication (between different microservices), you should use the more robust Messaging Module.</p>"},{"location":"athomic/events/#key-features","title":"Key Features","text":"<ul> <li>Decoupled Communication: Allows components to react to events happening elsewhere in the application without needing a direct reference.</li> <li>Automatic Context Propagation: This is a critical feature. The event bus automatically captures the <code>ExecutionContext</code> (<code>trace_id</code>, <code>tenant_id</code>, etc.) when an event is published and restores it when the subscriber callback is executed. This ensures perfect continuity for tracing and multi-tenancy, even in an event-driven flow.</li> <li>Multiple Backends: Supports both an in-memory provider for single-instance applications and a Redis Pub/Sub provider for multi-instance scenarios.</li> </ul>"},{"location":"athomic/events/#how-it-works","title":"How It Works","text":"<p>The module implements the classic Publish/Subscribe pattern:</p> <ol> <li>Subscribe: A component (the \"subscriber\") registers an asynchronous callback function for a specific event name (e.g., <code>\"user.created\"</code>).</li> <li>Publish: Another component (the \"publisher\") publishes an event with that name and a data payload.</li> <li>Dispatch: The event bus finds all registered callbacks for that event name and executes them concurrently, passing the payload to each one.</li> </ol>"},{"location":"athomic/events/#available-providers","title":"Available Providers","text":"<ul> <li><code>LocalEventBus</code>: The default, in-memory provider. It is extremely fast and requires no external dependencies. It is the perfect choice for decoupling components within a single service instance.</li> <li><code>RedisEventBus</code>: This provider uses Redis Pub/Sub as its backend. This allows multiple instances of the same service to share the same event bus, enabling simple real-time communication across processes without the overhead of a full message broker like Kafka.</li> </ul>"},{"location":"athomic/events/#usage-example","title":"Usage Example","text":"<p>Imagine a <code>UserService</code> that needs to notify an <code>AuditService</code> whenever a new user is created, without the two services knowing about each other.</p>"},{"location":"athomic/events/#subscriber-auditservice","title":"Subscriber (<code>AuditService</code>)","text":"<pre><code># In your_app/services/audit_service.py\nfrom nala.athomic.events import get_event_bus\nfrom nala.athomic.observability import get_logger\n\nlogger = get_logger(__name__)\n\nclass AuditService:\n    def __init__(self):\n        # Get the singleton event bus instance\n        event_bus = get_event_bus()\n        # Subscribe the `on_user_created` method to the \"user.created\" event\n        event_bus.subscribe(\"user.created\", self.on_user_created)\n\n    async def on_user_created(self, payload: dict):\n        # This method is executed when the event is published.\n        # It runs with the same context (trace_id, etc.) as the publisher.\n        user_id = payload.get(\"id\")\n        logger.info(f\"Auditing creation of user {user_id}\")\n</code></pre>"},{"location":"athomic/events/#publisher-userservice","title":"Publisher (<code>UserService</code>)","text":"<pre><code># In your_app/services/user_service.py\nfrom nala.athomic.events import get_event_bus\n\nclass UserService:\n    def __init__(self):\n        self.event_bus = get_event_bus()\n\n    async def create_user(self, name: str) -&gt; dict:\n        user = {\"id\": \"user-123\", \"name\": name}\n\n        # ... logic to save the user to the database ...\n\n        # Publish an event with the user data as the payload\n        await self.event_bus.publish(\"user.created\", payload=user)\n\n        return user\n</code></pre>"},{"location":"athomic/events/#configuration","title":"Configuration","text":"<p>The event bus is configured under the <code>[events]</code> section in your <code>settings.toml</code>.</p>"},{"location":"athomic/events/#local-provider-default","title":"Local Provider (Default)","text":"<pre><code>[default.events]\nenabled = true\n\n  [default.events.provider]\n  backend = \"local\"\n</code></pre>"},{"location":"athomic/events/#redis-provider","title":"Redis Provider","text":"<pre><code>[default.events]\nenabled = true\n\n  [default.events.provider]\n  backend = \"redis\"\n\n    # The redis provider reuses a KVStore connection configuration\n    [default.events.provider.provider]\n    # The key_prefix is used to namespace the Redis channels\n    key_prefix = \"app_events\"\n\n      [default.events.provider.provider.provider]\n      backend = \"redis\"\n      uri = \"redis://localhost:6379/2\"\n</code></pre>"},{"location":"athomic/events/#api-reference","title":"API Reference","text":""},{"location":"athomic/events/#nala.athomic.events.protocol.EventBusProtocol","title":"<code>nala.athomic.events.protocol.EventBusProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the standard interface for an internal event bus.</p> <p>Implementations of this protocol handle publishing events and managing subscriptions within the application, enabling decoupled, in-process (or cross-process with backends like Redis) communication.</p>"},{"location":"athomic/events/#nala.athomic.events.protocol.EventBusProtocol.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Closes any open connections or releases resources.</p>"},{"location":"athomic/events/#nala.athomic.events.protocol.EventBusProtocol.publish","title":"<code>publish(event_name, payload)</code>  <code>async</code>","text":"<p>Publishes an event to all registered subscribers for that event name.</p> <p>Implementations should automatically capture and include the current ExecutionContext within the payload (e.g., under a '_nala_context' key) before dispatching to subscribers.</p> <p>Parameters:</p> Name Type Description Default <code>event_name</code> <code>str</code> <p>The string identifier of the event being published.</p> required <code>payload</code> <code>Dict[str, Any]</code> <p>A dictionary containing the event data.</p> required"},{"location":"athomic/events/#nala.athomic.events.protocol.EventBusProtocol.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Initializes the event bus backend if necessary (e.g., connections).</p>"},{"location":"athomic/events/#nala.athomic.events.protocol.EventBusProtocol.subscribe","title":"<code>subscribe(event_name, callback)</code>","text":"<p>Registers an async callback to be executed when an event is published.</p> <p>Parameters:</p> Name Type Description Default <code>event_name</code> <code>str</code> <p>The name of the event to subscribe to.</p> required <code>callback</code> <code>Callable[[Dict[str, Any]], Awaitable[None]]</code> <p>An async function that will receive the event payload (including context). Callbacks should handle their own exceptions to prevent disrupting the event bus.</p> required"},{"location":"athomic/events/#nala.athomic.events.protocol.EventBusProtocol.unsubscribe","title":"<code>unsubscribe(event_name, callback)</code>","text":"<p>Unregisters a previously subscribed callback for a specific event.</p> <p>Parameters:</p> Name Type Description Default <code>event_name</code> <code>str</code> <p>The name of the event from which to unsubscribe.</p> required <code>callback</code> <code>Callable</code> <p>The specific callback function instance to remove.</p> required"},{"location":"athomic/events/#nala.athomic.events.factory.get_event_bus","title":"<code>nala.athomic.events.factory.get_event_bus()</code>","text":"<p>Convenience function that delegates to the factory.</p>"},{"location":"athomic/events/#nala.athomic.events.providers.local_events_provider.LocalEventBus","title":"<code>nala.athomic.events.providers.local_events_provider.LocalEventBus</code>","text":"<p>               Bases: <code>EventBusBase</code></p> <p>An in-memory event bus implementation for in-process communication.</p> <p>This provider executes subscribed callbacks immediately and concurrently upon publication, avoiding the need for a separate continuous polling loop (<code>_run_loop</code>). It is designed for high-speed, decoupled communication within a single application instance, making it ideal for testing and development environments.</p>"},{"location":"athomic/events/#nala.athomic.events.providers.local_events_provider.LocalEventBus.__init__","title":"<code>__init__(settings)</code>","text":"<p>Initializes the LocalEventBus.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>EventsSettings</code> <p>The event bus configuration settings.</p> required"},{"location":"athomic/events/#nala.athomic.events.providers.redis_events_provider.RedisEventBus","title":"<code>nala.athomic.events.providers.redis_events_provider.RedisEventBus</code>","text":"<p>               Bases: <code>EventBusBase</code></p> <p>An event bus provider implementation using Redis Pub/Sub for cross-process communication.</p> <p>This provider enables multiple application instances to share the same event bus. It manages the connection to Redis, dynamically subscribes to channels based on active subscribers, and processes incoming events in a continuous background loop.</p> <p>Attributes:</p> Name Type Description <code>redis_events_settings</code> <code>RedisEventsSettings</code> <p>The Redis-specific configuration model.</p> <code>kvstore_settings</code> <code>KVStoreSettings</code> <p>The underlying KVStore configuration used to connect to Redis.</p> <code>kvstore</code> <code>KVStoreProtocol</code> <p>The instance of the KVStore client.</p> <code>_redis_client</code> <code>Optional[Redis]</code> <p>The raw asynchronous Redis client instance.</p> <code>_pubsub_client</code> <code>Optional[PubSub]</code> <p>The Redis PubSub client used for listening to channels.</p>"},{"location":"athomic/events/#nala.athomic.events.providers.redis_events_provider.RedisEventBus.__init__","title":"<code>__init__(settings, kv_store=None)</code>","text":"<p>Initializes the RedisEventBus.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>EventsSettings</code> <p>The event bus configuration settings.</p> required <code>kv_store</code> <code>Optional[KVStoreProtocol]</code> <p>Optional pre-configured KV store instance for dependency injection.</p> <code>None</code>"},{"location":"athomic/events/#nala.athomic.events.providers.redis_events_provider.RedisEventBus.before_start","title":"<code>before_start()</code>  <code>async</code>","text":"<p>Establishes the connection to Redis and initializes the PubSub client.</p> <p>This hook is called by the <code>BaseService</code> lifecycle before the main processing loop starts.</p> <p>Raises:</p> Type Description <code>EventBusError</code> <p>If the connection to Redis fails.</p>"},{"location":"athomic/events/#nala.athomic.events.providers.redis_events_provider.RedisEventBus.before_stop","title":"<code>before_stop()</code>  <code>async</code>","text":"<p>Performs cleanup before the service stops. Currently a no-op as connection closure is handled by the KVStore client if it's managed globally.</p>"},{"location":"athomic/http/","title":"Resilient HTTP Client","text":""},{"location":"athomic/http/#overview","title":"Overview","text":"<p>The HTTP module provides a factory for creating pre-configured, resilient, and observable HTTP clients for communicating with external services. Instead of creating <code>httpx</code> or <code>requests</code> clients directly in your code, you define named clients in your configuration, and Athomic assembles them with a rich set of features.</p> <p>This approach centralizes the configuration of external services and ensures that all outgoing HTTP traffic from your application is handled in a consistent, resilient, and observable way.</p>"},{"location":"athomic/http/#key-features","title":"Key Features","text":"<ul> <li>Centralized Configuration: Define all your external service clients (base URL, authentication, timeouts) in one place.</li> <li>Built-in Resilience: Automatically apply Retry and Circuit Breaker policies to your HTTP calls simply by referencing them in the configuration.</li> <li>Automatic Observability: Every request made through a Athomic client is automatically instrumented with Prometheus metrics (latency, request count, error rate) and OpenTelemetry traces.</li> <li>Automatic Context Propagation: Critical context headers like <code>x-request-id</code>, <code>x-trace-id</code>, and <code>x-tenant-id</code> are automatically injected into every outgoing request.</li> <li>Declarative Caching: Enable and configure caching for <code>GET</code> requests directly in your settings.</li> <li>Extensible Authentication: Built-in support for Bearer Token and API Key authentication.</li> </ul>"},{"location":"athomic/http/#how-it-works","title":"How It Works","text":"<p>You don't instantiate clients directly. Instead, you use the singleton <code>HttpClientFactory</code> to get a pre-configured client by name.</p> <p>When you request a client (e.g., <code>HttpClientFactory.create(\"payment_api\")</code>), the factory performs the following steps:</p> <ol> <li>Reads the configuration for the <code>\"payment_api\"</code> client from your <code>settings.toml</code>.</li> <li>Instantiates the base provider (e.g., <code>HttpxProvider</code>).</li> <li>Creates and attaches the configured authentication strategy (e.g., <code>BearerTokenAuth</code>).</li> <li>Builds a <code>ResilienceOrchestrator</code> and injects the configured <code>Retry</code> and <code>Circuit Breaker</code> handlers.</li> <li>If caching is enabled for the client, it wraps the <code>get</code> method with the <code>@cache</code> decorator.</li> <li>Returns a fully assembled, ready-to-use client instance (and caches it for future calls).</li> </ol>"},{"location":"athomic/http/#usage-example","title":"Usage Example","text":"<p>First, define your client in <code>settings.toml</code> (see Configuration section below). Then, get and use the client in your code:</p> <pre><code>from nala.athomic.http import HttpClientFactory\n\n# Get the pre-configured client for the \"payment_api\"\npayment_api_client = HttpClientFactory.create(\"payment_api\")\n\nasync def process_payment(payment_data: dict):\n    try:\n        # This call is automatically resilient (retries, circuit breaker)\n        # and observable (traces, metrics).\n        response = await payment_api_client.post(\"/v1/payments\", json=payment_data)\n        return response\n    except Exception as e:\n        # Handle specific HTTP errors if needed\n        print(f\"Failed to process payment: {e}\")\n        raise\n</code></pre>"},{"location":"athomic/http/#configuration","title":"Configuration","text":"<p>This module's power comes from its declarative configuration. You define all clients under the <code>[http.clients]</code> section in your <code>settings.toml</code>.</p> <pre><code>[default.http]\nenabled = true\n\n  # Define a dictionary of named clients\n  [default.http.clients]\n\n    # --- Example 1: A client for an external payment API ---\n    [default.http.clients.payment_api]\n    base_url = \"[https://api.paymentservice.com](https://api.paymentservice.com)\"\n    timeout = 15.0 # seconds\n\n      # Use Bearer Token authentication\n      [default.http.clients.payment_api.auth]\n      method = \"bearer\"\n      # Token is a secret reference\n      token = { path = \"services/payment\", key = \"api_token\" }\n\n      # Apply resilience policies defined in the [resilience] section\n      retry_policy_name = \"default\"\n      circuit_breaker_policy_name = \"external_services\"\n\n    # --- Example 2: A client for an internal service with caching ---\n    [default.http.clients.user_service]\n    base_url = \"[http://user-service.internal:8000](http://user-service.internal:8000)\"\n    timeout = 5.0\n\n      # Use API Key authentication\n      [default.http.clients.user_service.auth]\n      method = \"api_key\"\n      header_name = \"X-Internal-API-Key\"\n      key = { path = \"services/user\", key = \"api_key\" }\n\n      # Enable and configure caching for GET requests\n      [default.http.clients.user_service.cache]\n      enabled = true\n      key_prefix = \"user_service_cache\"\n\n        # Uses a KVStore connection named \"cache_redis\"\n        [default.http.clients.user_service.cache.kv_store_connection_name]\n        name = \"cache_redis\"\n\n        # Apply a default TTL wrapper to the cache\n        [[default.http.clients.user_service.cache.kv_store_connection_name.wrappers]]\n        name = \"default_ttl\"\n          [default.http.clients.user_service.cache.kv_store_connection_name.wrappers.config]\n          default_ttl_seconds = 300 # 5 minutes\n</code></pre>"},{"location":"athomic/http/#api-reference","title":"API Reference","text":""},{"location":"athomic/http/#nala.athomic.http.protocol.HttpClientProtocol","title":"<code>nala.athomic.http.protocol.HttpClientProtocol</code>","text":"<p>               Bases: <code>BaseServiceProtocol</code>, <code>Protocol</code></p> <p>Defines the contract for a resilient, lifecycle-managed HTTP client.</p> <p>Any class that implements this protocol can be used by the framework to make external HTTP requests. Inheriting from <code>BaseServiceProtocol</code> ensures that the client's lifecycle (e.g., connection pooling) is managed by Athomic's central <code>LifecycleManager</code>.</p>"},{"location":"athomic/http/#nala.athomic.http.protocol.HttpClientProtocol.delete","title":"<code>delete(url, **kwargs)</code>  <code>async</code>","text":"<p>Sends an HTTP DELETE request.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL path for the request, relative to the client's base URL.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to be passed to the underlying HTTP client library (e.g., <code>params</code>, <code>headers</code>).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The parsed response body.</p> <p>Raises:</p> Type Description <code>HTTPRequestError</code> <p>If the server responds with a 4xx or 5xx status.</p> <code>HTTPTimeoutError</code> <p>If the request times out.</p> <code>ServiceNotReadyError</code> <p>If the client has not been started.</p>"},{"location":"athomic/http/#nala.athomic.http.protocol.HttpClientProtocol.get","title":"<code>get(url, **kwargs)</code>  <code>async</code>","text":"<p>Sends an HTTP GET request.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL path for the request, relative to the client's base URL.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to be passed to the underlying HTTP client library (e.g., <code>params</code>, <code>headers</code>).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The parsed response body.</p> <p>Raises:</p> Type Description <code>HTTPRequestError</code> <p>If the server responds with a 4xx or 5xx status.</p> <code>HTTPTimeoutError</code> <p>If the request times out.</p> <code>ServiceNotReadyError</code> <p>If the client has not been started.</p>"},{"location":"athomic/http/#nala.athomic.http.protocol.HttpClientProtocol.patch","title":"<code>patch(url, **kwargs)</code>  <code>async</code>","text":"<p>Sends an HTTP PATCH request.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL path for the request, relative to the client's base URL.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to be passed to the underlying HTTP client library (e.g., <code>json</code>, <code>data</code>, <code>headers</code>).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The parsed response body.</p> <p>Raises:</p> Type Description <code>HTTPRequestError</code> <p>If the server responds with a 4xx or 5xx status.</p> <code>HTTPTimeoutError</code> <p>If the request times out.</p> <code>ServiceNotReadyError</code> <p>If the client has not been started.</p>"},{"location":"athomic/http/#nala.athomic.http.protocol.HttpClientProtocol.post","title":"<code>post(url, **kwargs)</code>  <code>async</code>","text":"<p>Sends an HTTP POST request.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL path for the request, relative to the client's base URL.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to be passed to the underlying HTTP client library (e.g., <code>json</code>, <code>data</code>, <code>headers</code>).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The parsed response body.</p> <p>Raises:</p> Type Description <code>HTTPRequestError</code> <p>If the server responds with a 4xx or 5xx status.</p> <code>HTTPTimeoutError</code> <p>If the request times out.</p> <code>ServiceNotReadyError</code> <p>If the client has not been started.</p>"},{"location":"athomic/http/#nala.athomic.http.protocol.HttpClientProtocol.put","title":"<code>put(url, **kwargs)</code>  <code>async</code>","text":"<p>Sends an HTTP PUT request.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL path for the request, relative to the client's base URL.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to be passed to the underlying HTTP client library (e.g., <code>json</code>, <code>data</code>, <code>headers</code>).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The parsed response body.</p> <p>Raises:</p> Type Description <code>HTTPRequestError</code> <p>If the server responds with a 4xx or 5xx status.</p> <code>HTTPTimeoutError</code> <p>If the request times out.</p> <code>ServiceNotReadyError</code> <p>If the client has not been started.</p>"},{"location":"athomic/http/#nala.athomic.http.factory.HttpClientFactory","title":"<code>nala.athomic.http.factory.HttpClientFactory</code>","text":"<p>Factory to create configured and resilient HTTP client instances.</p> <p>This factory orchestrates the creation of <code>HttpClientProtocol</code> instances. It assembles a complete client by: 1. Loading the client's specific configuration by name. 2. Enforcing Mesh-mode overrides (e.g., disabling mTLS) if applicable. 3. Instantiating the correct backend provider (e.g., Httpx). 4. Creating and attaching the configured authentication strategy. 5. Building a resilience pipeline with Retry and Circuit Breaker handlers. 6. Applying a cache decorator to the <code>get</code> method if configured. Created instances are cached by name to ensure a single client instance per configuration is used throughout the application.</p>"},{"location":"athomic/http/#nala.athomic.http.factory.HttpClientFactory.create","title":"<code>create(name)</code>  <code>classmethod</code>","text":"<p>Creates or retrieves a named, fully-configured HTTP client instance.</p> <p>This is the main factory method. It builds a complete client, including its provider, authentication, resilience (retry, circuit breaker), and caching layers, based on the application's configuration.</p> <p>If the application is running in MESH mode, this method automatically adjusts client settings to offload security concerns (mTLS/SSL) to the infrastructure sidecar.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The unique name of the client configuration to create, as defined in the settings file.</p> required <p>Returns:</p> Type Description <code>HttpClientProtocol</code> <p>A fully initialized and resilient HTTP client instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the client configuration for the given <code>name</code> is not found.</p> <code>RuntimeError</code> <p>If the configured provider class is not found in the registry.</p>"},{"location":"athomic/http/#nala.athomic.http.base.HTTPClientBase","title":"<code>nala.athomic.http.base.HTTPClientBase</code>","text":"<p>               Bases: <code>BaseService</code>, <code>HttpClientProtocol</code></p> <p>Abstract base class for resilient HTTP Client providers.</p> <p>This class implements the Template Method pattern, providing a shared, robust framework for making HTTP requests. It handles common concerns like service lifecycle, resilience orchestration (via <code>ResilienceOrchestrator</code>), automatic context propagation in headers, and comprehensive observability (metrics and tracing).</p> <p>Concrete subclasses must only implement the <code>_request</code> method to handle the specifics of the underlying HTTP client library.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <code>HttpClientSettings</code> <p>The configuration for this specific client instance.</p> <code>resilience_orchestrator</code> <p>The orchestrator that applies resilience patterns like retry and circuit breaker to the request.</p>"},{"location":"athomic/http/#nala.athomic.http.base.HTTPClientBase.__init__","title":"<code>__init__(settings, service_name, resilience_orchestrator)</code>","text":"<p>Initializes the HTTPClientBase.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>HttpClientSettings</code> <p>The configuration for this specific HTTP client.</p> required <code>service_name</code> <code>str</code> <p>A unique name for the service, used for observability.</p> required <code>resilience_orchestrator</code> <code>ResilienceOrchestrator</code> <p>An orchestrator instance to apply resilience patterns to the outgoing requests.</p> required"},{"location":"athomic/http/#nala.athomic.http.base.HTTPClientBase.delete","title":"<code>delete(url, **kwargs)</code>  <code>async</code>","text":"<p>Sends an HTTP DELETE request.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL path for the request, relative to the client's base URL.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the HTTP request.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The parsed response body.</p>"},{"location":"athomic/http/#nala.athomic.http.base.HTTPClientBase.get","title":"<code>get(url, **kwargs)</code>  <code>async</code>","text":"<p>Sends an HTTP GET request.</p> <p>This is a convenience method that delegates to the main <code>_execute_request</code> orchestrator.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL path for the request, relative to the client's base URL.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the HTTP request.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The parsed response body.</p>"},{"location":"athomic/http/#nala.athomic.http.base.HTTPClientBase.patch","title":"<code>patch(url, **kwargs)</code>  <code>async</code>","text":"<p>Sends an HTTP PATCH request.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL path for the request, relative to the client's base URL.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the HTTP request.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The parsed response body.</p>"},{"location":"athomic/http/#nala.athomic.http.base.HTTPClientBase.post","title":"<code>post(url, **kwargs)</code>  <code>async</code>","text":"<p>Sends an HTTP POST request.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL path for the request, relative to the client's base URL.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the HTTP request.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The parsed response body.</p>"},{"location":"athomic/http/#nala.athomic.http.base.HTTPClientBase.put","title":"<code>put(url, **kwargs)</code>  <code>async</code>","text":"<p>Sends an HTTP PUT request.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL path for the request, relative to the client's base URL.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the HTTP request.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The parsed response body.</p>"},{"location":"athomic/lifecycle/","title":"Lifecycle Management","text":""},{"location":"athomic/lifecycle/#overview","title":"Overview","text":"<p>The <code>LifecycleManager</code> is the central orchestrator responsible for managing the startup and shutdown sequences of all registered <code>BaseService</code> instances in the application.</p>"},{"location":"athomic/lifecycle/#how-it-works","title":"How It Works","text":"<ol> <li>Registration: Services are registered with the <code>LifecycleManager</code>, optionally declaring dependencies on other services.</li> <li>Startup: When <code>manager.start_all()</code> is called, it builds a dependency graph and starts the services in the correct topological order. It uses <code>asyncio.gather</code> to start independent services concurrently, optimizing startup time.</li> <li>Readiness: The manager waits for all services to become \"ready\" by calling their <code>wait_ready()</code> method. The application is only considered fully started after all essential services are ready.</li> <li>Shutdown: When <code>manager.stop_all()</code> is called, it shuts down all services in the reverse order of their startup, ensuring a graceful teardown.</li> </ol> <p>This system guarantees that a service that depends on a database connection will only start after the database connection manager is ready, preventing race conditions and startup errors.</p>"},{"location":"athomic/lifecycle/#api-reference","title":"API Reference","text":""},{"location":"athomic/lifecycle/#nala.athomic.lifecycle.manager.LifecycleManager","title":"<code>nala.athomic.lifecycle.manager.LifecycleManager</code>","text":"<p>Orchestrates the application lifecycle for all Athomic and Domain services.</p> <p>This class is the central coordinator for service startup and shutdown. It ensures that dependencies are respected by starting and stopping services based on a priority system. It first registers all framework-level infrastructure services and then allows the application to register its own domain-specific services before managing their execution order.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <code>AppSettings</code> <p>The application's configuration settings.</p>"},{"location":"athomic/lifecycle/#nala.athomic.lifecycle.manager.LifecycleManager.__init__","title":"<code>__init__(domain_initializers_register=None, settings=None)</code>","text":"<p>Initializes the LifecycleManager.</p> <p>This constructor registers all of Athomic's internal infrastructure services and then executes the provided domain initializer callback to allow the application to register its own services into the lifecycle registry.</p> <p>Parameters:</p> Name Type Description Default <code>domain_initializers_register</code> <code>Optional[Callable[[], None]]</code> <p>An optional function that, when called, registers all domain-specific initializers and services.</p> <code>None</code> <code>settings</code> <code>Optional[AppSettings]</code> <p>The application settings instance. If not provided, global settings will be used.</p> <code>None</code>"},{"location":"athomic/lifecycle/#nala.athomic.lifecycle.manager.LifecycleManager.shutdown","title":"<code>shutdown()</code>  <code>async</code>","text":"<p>Executes the graceful shutdown sequence for all registered services.</p> <p>It retrieves services and stops them in reverse priority order to correctly handle dependencies during shutdown. A timeout is applied to each <code>stop</code> operation, and any errors are logged without halting the shutdown of other services.</p>"},{"location":"athomic/lifecycle/#nala.athomic.lifecycle.manager.LifecycleManager.startup","title":"<code>startup()</code>  <code>async</code>","text":"<p>Executes the startup sequence for all registered services.</p> <p>It retrieves all services from the registry, sorted by their priority, and starts each one sequentially. A timeout is applied to each service's startup to prevent the application from hanging.</p>"},{"location":"athomic/lifecycle/#nala.athomic.lifecycle.registry.LifecycleRegistry","title":"<code>nala.athomic.lifecycle.registry.LifecycleRegistry</code>","text":"<p>               Bases: <code>BaseInstanceRegistry[BaseServiceProtocol]</code></p> <p>A specialized registry for services with a manageable lifecycle.</p> <p>This class extends BaseInstanceRegistry by adding a priority system to control the startup and shutdown order of services. Services with a lower priority number are started first and stopped last, which is essential for managing dependencies (e.g., ensuring a database service is available before a repository service that uses it).</p> <p>Attributes:</p> Name Type Description <code>_registry_with_priority</code> <code>List[Tuple[int, str, BaseServiceProtocol]]</code> <p>An internal list that stores services along with their priority, sorted on insertion.</p>"},{"location":"athomic/lifecycle/#nala.athomic.lifecycle.registry.LifecycleRegistry.__init__","title":"<code>__init__(protocol)</code>","text":"<p>Initializes the LifecycleRegistry.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>BaseServiceProtocol</code> <p>The protocol that all registered items must conform to.</p> required"},{"location":"athomic/lifecycle/#nala.athomic.lifecycle.registry.LifecycleRegistry.clear","title":"<code>clear()</code>  <code>async</code>","text":"<p>Stops all services and completely clears the registry.</p> <p>This method overrides the parent <code>clear</code> to also purge the internal priority list, ensuring a complete reset for testing or re-initialization.</p>"},{"location":"athomic/lifecycle/#nala.athomic.lifecycle.registry.LifecycleRegistry.get_services_by_priority","title":"<code>get_services_by_priority()</code>","text":"<p>Returns all registered services, sorted by startup priority.</p> <p>Returns:</p> Type Description <code>List[BaseServiceProtocol]</code> <p>List[BaseServiceProtocol]: A list of service instances ordered from</p> <code>List[BaseServiceProtocol]</code> <p>lowest to highest priority, ready for startup.</p>"},{"location":"athomic/lifecycle/#nala.athomic.lifecycle.registry.LifecycleRegistry.register","title":"<code>register(name, item_instance, priority=100)</code>","text":"<p>Registers a service instance with a specific priority.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The unique string name for the service instance.</p> required <code>item_instance</code> <code>BaseServiceProtocol</code> <p>The service object to register.</p> required <code>priority</code> <code>Optional[int]</code> <p>The startup priority. Lower numbers are started earlier and stopped later. Defaults to 100.</p> <code>100</code>"},{"location":"athomic/lineage/","title":"Data Lineage","text":""},{"location":"athomic/lineage/#overview","title":"Overview","text":"<p>The Data Lineage module provides a robust, asynchronous system for generating, transporting, and persisting lineage events that comply with the OpenLineage standard.</p> <p>Data lineage tracks the origin, movement, and transformation of data. In the Athomic Layer, this is achieved through a Provider-Collector Architecture, which decouples the application (Producer) from the storage backends (Collector).</p>"},{"location":"athomic/lineage/#architecture-components","title":"Architecture &amp; Components","text":"<p>The system is split into two main phases to ensure high performance and scalability:</p>"},{"location":"athomic/lineage/#1-the-producer-application-side","title":"1. The Producer (Application Side)","text":"<p>The <code>LineageProducer</code> is a lightweight service embedded in the application.</p> <ul> <li>Responsibility: Constructs <code>LineageEvent</code> models and publishes them to a messaging topic (<code>platform.lineage.events</code>).</li> <li>Performance: It does not connect to databases or external APIs directly, ensuring zero impact on the application's request latency.</li> <li>Identity: Configured with the \"Producer URI\" and \"Namespace\" to identify the source of events.</li> </ul>"},{"location":"athomic/lineage/#2-the-collector-worker-side","title":"2. The Collector (Worker Side)","text":"<p>The <code>LineageCollectorService</code> is a dedicated background service (or separate microservice).</p> <ul> <li>Responsibility: Consumes events from the lineage topic, hydrates them, and delegates persistence to the <code>LineageManager</code>.</li> <li>Composite Storage: Supports writing to multiple backends simultaneously (e.g., Document DB for audit + Graph DB for analysis).</li> <li>Fail-Fast: If writing to any configured store fails, the collector triggers a retry (DLQ flow), ensuring data consistency.</li> </ul>"},{"location":"athomic/lineage/#how-it-works","title":"How It Works","text":"<ol> <li>Instrumentation: When a function decorated with <code>@track_lineage</code> executes, or a message is consumed, the <code>LineageProducer</code> is invoked.</li> <li>Emission: The producer creates a <code>START</code>, <code>COMPLETE</code>, or <code>FAIL</code> event and publishes it to the message broker.</li> <li>Ingestion: The <code>LineageCollectorService</code> picks up the message asynchronously.</li> <li>Persistence: The <code>LineageManager</code> routes the event to the configured Composite Store, which broadcasts it to all active backends (e.g., MongoDB, Neo4j, Logging).</li> </ol>"},{"location":"athomic/lineage/#storage-backends","title":"Storage Backends","text":"<p>The system supports multiple storage backends via the <code>LineageStoreProtocol</code>.</p> <ul> <li><code>DocumentLineageStore</code> (MongoDB): Stores the raw OpenLineage event payload. Ideal for audit trails and historical debugging.</li> <li><code>Neo4jLineageStore</code> (Graph): Projects the event into a property graph (Nodes: <code>Job</code>, <code>Run</code>, <code>Dataset</code>). Ideal for impact analysis and visualization.</li> <li><code>LoggingLineageStore</code>: Writes the lineage event as a structured JSON log entry.</li> <li><code>OpenLineageStore</code>: Sends the event directly to an external HTTP API (like Marquez).</li> </ul>"},{"location":"athomic/lineage/#usage","title":"Usage","text":""},{"location":"athomic/lineage/#the-track_lineage-decorator","title":"The <code>@track_lineage</code> Decorator","text":"<p>To track a business operation, apply the decorator to your function.</p> <pre><code>from nala.athomic.integration.messaging.lineage import track_lineage\nfrom nala.athomic.lineage.models import LineageDataset\n\ninput_data = LineageDataset(namespace=\"mongo\", name=\"users_collection\")\n\n@track_lineage(\n    job_name=\"process_user_report\",\n    inputs=[input_data]\n)\nasync def generate_report(user_id: str):\n    # ... business logic ...\n    # Automatic START event emitted before execution.\n    # Automatic COMPLETE/FAIL event emitted after execution.\n    return \"report_generated\"\n</code></pre>"},{"location":"athomic/lineage/#configuration","title":"Configuration","text":"<p>Configuration is split between the Producer (Identity) and the Store (Persistence).</p> <pre><code># 1. Enable Lineage Globally\n[lineage]\nenabled = true\ncollector_topic = \"platform.lineage.events\"\n\n# 2. Configure Identity (Who am I?)\n[lineage.producer]\njob_namespace = \"my-service-ns\"\nproducer_uri = \"[https://github.com/my-org/my-service](https://github.com/my-org/my-service)\"\n\n# 3. Configure Storage Backends (Where does data go?)\n[lineage.store]\n# Use 'composite' to enable multiple backends\nbackend = \"composite\"\nbackends = [\"document\", \"graph\"]\n\n    # --- MongoDB Store Settings ---\n    [lineage.store.document]\n    collection_name = \"audit_lineage_events\"\n    document_connection_name = \"default_mongo\"\n\n    # --- Neo4j Store Settings ---\n    [lineage.store.graph]\n    graph_connection_name = \"analytics_graph\"\n</code></pre>"},{"location":"athomic/lineage/#api-reference","title":"API Reference","text":""},{"location":"athomic/lineage/#nala.athomic.integration.messaging.lineage.producer.LineageProducer","title":"<code>nala.athomic.integration.messaging.lineage.producer.LineageProducer</code>","text":"<p>               Bases: <code>BaseService</code></p> <p>Service responsible for constructing and dispatching Data Lineage events.</p>"},{"location":"athomic/lineage/#nala.athomic.integration.messaging.lineage.producer.LineageProducer.record_consumption","title":"<code>record_consumption(event_type, job_name, job_namespace=None, run_id=None, inputs=None, outputs=None, producer_uri=None)</code>  <code>async</code>","text":"<p>Records a lineage event by constructing the model and publishing it.</p>"},{"location":"athomic/lineage/#nala.athomic.integration.messaging.lineage.producer.LineageProducer.record_message_consumption","title":"<code>record_message_consumption(topic, group_id, source_namespace)</code>  <code>async</code>","text":"<p>Helper method specifically for Consumer Processors. Maps messaging concepts (Topic, Group) to Lineage concepts (Dataset, Job).</p>"},{"location":"athomic/lineage/#nala.athomic.lineage.collector.service.LineageCollectorService","title":"<code>nala.athomic.lineage.collector.service.LineageCollectorService</code>","text":"<p>               Bases: <code>BaseService</code></p> <p>Service responsible for consuming LineageEvents from the messaging system and delegating their persistence to the underlying lineage store(s) managed by the LineageManager.</p> <p>This service uses the standard BaseService lifecycle hooks (_connect/_close) to manage the Orchestrator dependency safely.</p>"},{"location":"athomic/lineage/#nala.athomic.lineage.collector.service.LineageCollectorService.__init__","title":"<code>__init__(lineage_manager)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>lineage_manager</code> <code>LineageManager</code> <p>The orchestrator responsible for initializing and providing access to the configured lineage stores.</p> required"},{"location":"athomic/lineage/#nala.athomic.lineage.collector.service.LineageCollectorService.handle_lineage_event","title":"<code>handle_lineage_event(event, raw_message=None, **kwargs)</code>  <code>async</code>","text":"<p>Handles an incoming LineageEvent and persists it using the active lineage store provided by the LineageManager.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Union[LineageEvent, Dict[str, Any]]</code> <p>The deserialized payload (LineageEvent model or raw dict).</p> required <code>raw_message</code> <code>Optional[Any]</code> <p>The raw message object (optional).</p> <code>None</code> <code>**kwargs</code> <p>Captures additional metadata injected by the messaging framework       (e.g., message_key, raw_headers).</p> <code>{}</code>"},{"location":"athomic/lineage/#nala.athomic.lineage.manager.LineageManager","title":"<code>nala.athomic.lineage.manager.LineageManager</code>","text":"<p>               Bases: <code>BaseService</code></p> <p>Orchestrates the lineage storage subsystem.</p> <p>This manager follows the Provider pattern similar to the Database module. It dynamically discovers enabled storage backends based on the provided configuration and the registered store creators.</p> <p>It is responsible for: 1. initializing active stores based on configuration. 2. Managing the lifecycle (startup/shutdown) of these stores. 3. Providing a unified interface (Composite or Single) to the collector.</p>"},{"location":"athomic/lineage/#nala.athomic.lineage.manager.LineageManager.get_store","title":"<code>get_store()</code>","text":"<p>Returns the active lineage store instance.</p>"},{"location":"athomic/lineage/#nala.athomic.lineage.models.LineageEvent","title":"<code>nala.athomic.lineage.models.LineageEvent</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Standard OpenLineage-compatible event structure.</p> <p>This model acts as the Data Transfer Object (DTO) for the lineage collector and storage layers. It captures the 'who', 'what', 'when', and 'where' of data processing.</p>"},{"location":"athomic/notification/","title":"Notifications","text":""},{"location":"athomic/notification/#overview","title":"Overview","text":"<p>The Notification module provides a reliable, extensible, and provider-agnostic system for sending notifications, with an initial focus on email. It is designed to decouple your application's business logic from the specific details of how notifications are delivered.</p>"},{"location":"athomic/notification/#key-features","title":"Key Features","text":"<ul> <li>Multi-Provider Support: Comes with built-in providers for SMTP and a console logger, and can be easily extended to support others like SendGrid or Twilio.</li> <li>Resilient by Default: Network-based providers like the <code>SMTPProvider</code> are automatically configured with a retry policy, making them resilient to transient failures.</li> <li>Structured Payloads: Uses Pydantic models like <code>EmailPayload</code> to ensure that all necessary data for a notification is provided in a structured and validated way.</li> <li>Centralized Configuration: All provider settings, including credentials and retry policies, are managed in a single configuration section.</li> </ul>"},{"location":"athomic/notification/#how-it-works","title":"How It Works","text":"<p>The module is built around a few key abstractions:</p> <ol> <li><code>NotificationProtocol</code>: A simple contract that all providers must implement. It defines a primary <code>send(payload)</code> method.</li> <li><code>NotificationPayload</code>: A set of Pydantic models that define the structure for different types of notifications (e.g., <code>EmailPayload</code>).</li> <li><code>NotificationFactory</code>: A singleton factory that is the main entry point for the application. It reads the configuration and instantiates the correct provider (e.g., <code>SMTPProvider</code>). If notifications are disabled in the settings, it safely returns a <code>NullNotificationProvider</code> that performs no action, preventing application errors.</li> <li>Provider Creators: For providers with complex dependencies, like the <code>SMTPProvider</code> which requires a <code>ResilienceOrchestrator</code>, a dedicated creator class handles the dependency injection, ensuring the provider is properly assembled.</li> </ol>"},{"location":"athomic/notification/#available-providers","title":"Available Providers","text":"<ul> <li><code>SMTPProvider</code>: Sends emails using a standard SMTP server. This provider is automatically wrapped with a retry mechanism.</li> <li><code>ConsoleNotificationProvider</code>: Does not send any real notifications. Instead, it prints the content of the notification payload to the console, making it perfect for local development and testing.</li> <li><code>NullNotificationProvider</code>: A \"no-op\" provider that does absolutely nothing. It is used as a safe fallback when the notification module is disabled in the configuration.</li> </ul>"},{"location":"athomic/notification/#usage-example","title":"Usage Example","text":"<p>Sending an email is a simple, two-step process: build the payload and send it using the factory.</p> <pre><code>from nala.athomic.notification import NotificationFactory\nfrom nala.athomic.notification.payloads import EmailPayload\n\nasync def send_welcome_email(user_email: str, user_name: str):\n    # 1. Create the structured payload\n    email = EmailPayload(\n        to=[user_email],\n        subject=f\"Welcome, {user_name}!\",\n        body=\"Thank you for joining our platform.\",\n        html_body=\"&lt;p&gt;Thank you for joining our platform.&lt;/p&gt;\"\n    )\n\n    try:\n        # 2. Get the configured provider and send the payload\n        notifier = NotificationFactory.create()\n        success = await notifier.send(email)\n\n        if success:\n            print(\"Welcome email sent successfully!\")\n        else:\n            print(\"Failed to send welcome email.\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n</code></pre>"},{"location":"athomic/notification/#configuration","title":"Configuration","text":"<p>Notifications are configured under the <code>[notifications]</code> section in your <code>settings.toml</code>.</p>"},{"location":"athomic/notification/#console-provider-example-for-development","title":"Console Provider Example (for Development)","text":"<pre><code>[default.notifications]\nenabled = true\ndefault_from_email = \"dev@nala.com\"\n\n  [default.notifications.provider]\n  backend = \"console\"\n</code></pre>"},{"location":"athomic/notification/#smtp-provider-example-for-production","title":"SMTP Provider Example (for Production)","text":"<pre><code>[default.notifications]\nenabled = true\ndefault_from_email = \"noreply@nala.com\"\n\n  # Configure a retry policy for sending emails\n  [default.notifications.retry]\n  attempts = 3\n  wait_min_seconds = 1.0\n  backoff = 2.0\n\n  # Configure the SMTP provider\n  [default.notifications.provider]\n  backend = \"smtp\"\n  host = \"smtp.mailprovider.com\"\n  port = 587\n  username = \"my-user\"\n  # The password should always be a secret reference\n  password = { path = \"smtp/prod\", key = \"password\" } # pragma: allowlist secret\n  use_tls = true\n</code></pre>"},{"location":"athomic/notification/#api-reference","title":"API Reference","text":""},{"location":"athomic/notification/#nala.athomic.notification.protocol.NotificationProtocol","title":"<code>nala.athomic.notification.protocol.NotificationProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol defining the standard interface for notification providers.</p>"},{"location":"athomic/notification/#nala.athomic.notification.protocol.NotificationProtocol.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Closes connections or releases resources, if applicable.</p>"},{"location":"athomic/notification/#nala.athomic.notification.protocol.NotificationProtocol.is_available","title":"<code>is_available()</code>  <code>async</code>","text":"<p>Checks if the provider is configured and ready.</p>"},{"location":"athomic/notification/#nala.athomic.notification.protocol.NotificationProtocol.send","title":"<code>send(payload)</code>  <code>async</code>","text":"<p>Sends a notification based on the provided payload.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>NotificationPayload</code> <p>A structured data object (e.g., EmailPayload, SMSPayload)      containing all necessary information for the notification.</p> required <p>Returns:     bool: True if sending was successful, False otherwise.</p>"},{"location":"athomic/notification/#nala.athomic.notification.factory.NotificationFactory","title":"<code>nala.athomic.notification.factory.NotificationFactory</code>","text":"<p>Factory responsible for creating and managing the singleton instance of the configured NotificationProtocol provider.</p> <p>It uses a Registry for provider discovery and implements a fallback to NullProvider if the notifications module is disabled in settings.</p>"},{"location":"athomic/notification/#nala.athomic.notification.factory.NotificationFactory.clear","title":"<code>clear()</code>  <code>classmethod</code>","text":"<p>Clears the singleton instance.</p> <p>Primarily used for test isolation to ensure fresh setup between tests.</p>"},{"location":"athomic/notification/#nala.athomic.notification.factory.NotificationFactory.create","title":"<code>create(settings=None)</code>  <code>classmethod</code>","text":"<p>Retrieves or creates the singleton instance of the NotificationProtocol.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[NotificationSettings]</code> <p>Optional explicit settings to override application configuration.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no provider creator is registered for the configured backend.</p> <code>RuntimeError</code> <p>If initialization of the concrete provider fails.</p> <p>Returns:</p> Name Type Description <code>NotificationProtocol</code> <code>NotificationProtocol</code> <p>The shared singleton instance.</p>"},{"location":"athomic/notification/#nala.athomic.notification.payloads.EmailPayload","title":"<code>nala.athomic.notification.payloads.EmailPayload</code>","text":"<p>               Bases: <code>BasePayload</code></p> <p>Specific payload structure required for sending email notifications.</p>"},{"location":"athomic/notification/#nala.athomic.notification.providers.smtp_provider.SMTPProvider","title":"<code>nala.athomic.notification.providers.smtp_provider.SMTPProvider</code>","text":"<p>               Bases: <code>BaseNotification</code>, <code>CredentialResolve</code></p> <p>SMTPProvider implementation specialized for EmailPayload, using a resilience orchestrator.</p>"},{"location":"athomic/notification/#nala.athomic.notification.providers.smtp_provider.SMTPProvider.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Closes any open resources. (No-op for smtplib as connection is per-send).</p>"},{"location":"athomic/notification/#nala.athomic.notification.providers.smtp_provider.SMTPProvider.is_available","title":"<code>is_available()</code>  <code>async</code>","text":"<p>Verifies SMTP availability by sending a NOOP command.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the SMTP connection was successful, False otherwise.</p>"},{"location":"athomic/payload/","title":"Payload Processing Pipeline","text":""},{"location":"athomic/payload/#overview","title":"Overview","text":"<p>The Payload Processing module provides a flexible and powerful implementation of the Pipes and Filters architectural pattern. Its core component, the <code>PayloadProcessor</code>, orchestrates a configurable pipeline of transformation steps that are applied to data payloads, primarily within the messaging system.</p> <p>This architecture allows complex processing sequences\u2014such as serialization, encryption, compression, and large message handling (Claim Check)\u2014to be composed declaratively in your configuration file. It decouples the messaging producers and consumers from the specifics of the payload format, making the system highly extensible.</p>"},{"location":"athomic/payload/#how-the-pipeline-works","title":"How The Pipeline Works","text":"<p>The <code>PayloadProcessor</code> manages an ordered list of \"steps,\" where each step is a component that implements the <code>ProcessingStepProtocol</code> (defining <code>encode</code> and <code>decode</code> methods).</p>"},{"location":"athomic/payload/#encoding-flow-on-publish","title":"Encoding Flow (On Publish)","text":"<p>When a message is published, the pipeline is executed in the forward order as defined in your configuration. The output of one step becomes the input for the next.</p> <ul> <li><code>Domain Object</code> -&gt; [Serializer] -&gt; <code>Bytes</code> -&gt; [Crypto] -&gt; <code>Encrypted Bytes</code> -&gt; [Compression] -&gt; <code>Compressed Bytes</code> -&gt; <code>Message Broker</code></li> </ul>"},{"location":"athomic/payload/#decoding-flow-on-consume","title":"Decoding Flow (On Consume)","text":"<p>When a message is consumed, the pipeline is executed in reverse order.</p> <ul> <li><code>Message Broker</code> -&gt; <code>Compressed Bytes</code> -&gt; [Decompression] -&gt; <code>Encrypted Bytes</code> -&gt; [Decryption] -&gt; <code>Bytes</code> -&gt; [Deserializer] -&gt; <code>Domain Object</code></li> </ul> <p>This symmetric design ensures that transformations are applied and reverted correctly and transparently.</p>"},{"location":"athomic/payload/#available-pipeline-steps","title":"Available Pipeline Steps","text":"<p>Athomic provides several built-in steps that you can include in your payload pipeline:</p> <ul> <li><code>serializer</code>: (Required first step for encoding). Converts Python objects (like Pydantic models) into a byte representation (e.g., JSON, Protobuf) and vice-versa.</li> <li><code>crypto</code>: Encrypts the byte payload using the configured cryptographic provider (e.g., Fernet) to ensure end-to-end security.</li> <li><code>compression</code>: Compresses the byte payload to reduce its size, saving network bandwidth and storage costs.</li> <li><code>claim_check</code>: Implements the Claim Check pattern. If a payload exceeds a size threshold, it's uploaded to a file storage provider (like GCS), and a small reference is sent through the broker instead.</li> </ul>"},{"location":"athomic/payload/#configuration","title":"Configuration","text":"<p>You configure the pipeline in your <code>settings.toml</code> file under the <code>[integration.messaging.payload]</code> section. The <code>pipeline</code> is a list of tables, and their order matters.</p>"},{"location":"athomic/payload/#example-full-pipeline","title":"Example: Full Pipeline","text":"<p>This example configures a pipeline that first serializes the object to JSON, then encrypts it, and finally compresses it.</p> <pre><code>[default.integration.messaging.payload]\n\n# The pipeline is a list of steps. Order is crucial.\n# Encoding runs top-to-bottom. Decoding runs bottom-to-top.\n[[default.integration.messaging.payload.pipeline]]\nstep = \"serializer\"\n  # Optional settings for this specific step can be provided.\n  [default.integration.messaging.payload.pipeline.settings]\n  backend = \"orjson\"\n\n[[default.integration.messaging.payload.pipeline]]\nstep = \"crypto\"\n  # No settings needed here; it will use the global [security.crypto] config.\n\n[[default.integration.messaging.payload.pipeline]]\nstep = \"compression\"\n</code></pre>"},{"location":"athomic/payload/#extending-the-pipeline-custom-steps","title":"Extending the Pipeline (Custom Steps)","text":"<p>The pipeline is fully extensible. To add your own custom processing step (e.g., for data validation or transformation):</p> <ol> <li>Create a class that implements the <code>ProcessingStepProtocol</code>.</li> <li>Create a factory class that implements the <code>StepCreatorProtocol</code>.</li> <li>Register an instance of your creator in the <code>payload_step_registry</code> with a unique name.</li> </ol> <p>Once registered, your custom step can be added to the <code>pipeline</code> in your configuration just like the built-in ones.</p>"},{"location":"athomic/payload/#api-reference","title":"API Reference","text":""},{"location":"athomic/payload/#nala.athomic.payload.processor.PayloadProcessor","title":"<code>nala.athomic.payload.processor.PayloadProcessor</code>","text":"<p>An orchestrator that executes a dynamic pipeline of processing steps (Pipes and Filters pattern).</p> <p>This class is agnostic to the nature of the steps (serialization, encryption, compression), focusing only on the correct order of execution for both outbound (encode) and inbound (decode) flows.</p>"},{"location":"athomic/payload/#nala.athomic.payload.processor.PayloadProcessor.__init__","title":"<code>__init__(pipeline_steps)</code>","text":"<p>Initializes the processor with an ordered list of pipeline steps.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_steps</code> <code>List[ProcessingStepProtocol]</code> <p>An ordered list of steps to be executed during encoding.</p> required"},{"location":"athomic/payload/#nala.athomic.payload.processor.PayloadProcessor.decode","title":"<code>decode(raw_data, **kwargs)</code>  <code>async</code>","text":"<p>Executes the decoding pipeline in reverse order (backward).</p> <p>The flow is typically: Raw Bytes -&gt; [Decompress -&gt; Decrypt -&gt; Deserialize] -&gt; Domain Object.</p> <p>Parameters:</p> Name Type Description Default <code>raw_data</code> <code>bytes</code> <p>The initial payload (transport bytes).</p> required <code>**kwargs</code> <code>Any</code> <p>Contextual arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The final domain object or data structure.</p>"},{"location":"athomic/payload/#nala.athomic.payload.processor.PayloadProcessor.encode","title":"<code>encode(data_object, **kwargs)</code>  <code>async</code>","text":"<p>Executes the encoding pipeline in the defined order (forward).</p> <p>The flow is typically: Domain Object -&gt; [Serialize -&gt; Encrypt -&gt; Compress] -&gt; Raw Bytes.</p> <p>Parameters:</p> Name Type Description Default <code>data_object</code> <code>Any</code> <p>The initial payload (usually a domain object or model).</p> required <code>**kwargs</code> <code>Any</code> <p>Contextual arguments to be passed to each step.</p> <code>{}</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the final output of the pipeline is not bytes.</p> <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The final, transport-ready payload.</p>"},{"location":"athomic/payload/#nala.athomic.payload.protocol.ProcessingStepProtocol","title":"<code>nala.athomic.payload.protocol.ProcessingStepProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the contract for a single, bidirectional (encode/decode) step in a payload processing pipeline.</p> <p>Examples of steps include serialization (JSON/Protobuf), compression (Gzip), or encryption (AES).</p>"},{"location":"athomic/payload/#nala.athomic.payload.protocol.ProcessingStepProtocol.decode","title":"<code>decode(data, **kwargs)</code>  <code>async</code>","text":"<p>Processes data in the inbound flow (e.g., preparing data for consumption).</p> <p>The flow direction is typically from raw bytes/transport format to the final domain object expected by the consumer.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The received payload (can be bytes or a transport format).</p> required <code>**kwargs</code> <code>Any</code> <p>Contextual arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The decoded payload (usually the final domain object).</p>"},{"location":"athomic/payload/#nala.athomic.payload.protocol.ProcessingStepProtocol.encode","title":"<code>encode(data, **kwargs)</code>  <code>async</code>","text":"<p>Processes data in the outbound flow (e.g., preparing data for transmission).</p> <p>The flow direction is typically from domain object to raw bytes/transport format.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The payload to be processed (can be an object or bytes).</p> required <code>**kwargs</code> <code>Any</code> <p>Contextual arguments (e.g., schema version, compression level).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The processed payload (usually bytes for intermediate steps).</p>"},{"location":"athomic/payload/#nala.athomic.payload.factory.PayloadProcessorFactory","title":"<code>nala.athomic.payload.factory.PayloadProcessorFactory</code>","text":"<p>A Singleton Factory responsible for assembling the final PayloadProcessor.</p> <p>It dynamically retrieves step creators from a central registry based on a structured pipeline configuration (Pipes and Filters pattern), ensuring the pipeline assembly logic runs only once.</p>"},{"location":"athomic/payload/#nala.athomic.payload.factory.PayloadProcessorFactory.clear","title":"<code>clear()</code>  <code>classmethod</code>","text":"<p>Clears the singleton instance. Used primarily for test isolation.</p>"},{"location":"athomic/payload/#nala.athomic.payload.factory.PayloadProcessorFactory.create","title":"<code>create(settings)</code>  <code>classmethod</code>","text":"<p>Retrieves or creates the singleton instance of the fully assembled PayloadProcessor pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>PayloadSettings</code> <p>The configuration specifying the order and type of steps       in the processing pipeline.</p> required <p>Returns:</p> Name Type Description <code>PayloadProcessor</code> <code>PayloadProcessor</code> <p>The shared, single instance of the processor.</p>"},{"location":"athomic/payload/#nala.athomic.payload.creators_protocol.StepCreatorProtocol","title":"<code>nala.athomic.payload.creators_protocol.StepCreatorProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the contract for an object (a specialized Factory) that knows how to create a specific ProcessingStepProtocol implementation.</p> <p>This protocol is used by the PayloadProcessorFactory to dynamically construct the processing pipeline based on configuration, enabling Dependency Injection for complex steps.</p>"},{"location":"athomic/payload/#nala.athomic.payload.creators_protocol.StepCreatorProtocol.create","title":"<code>create(settings)</code>","text":"<p>Creates and returns a concrete instance of a ProcessingStepProtocol.</p> <p>The creation logic within the implementer should handle the necessary setup and dependency injection (e.g., retrieving a key for an EncryptionStep).</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[Dict[str, Any]]</code> <p>The specific configuration dictionary for this step       from the main PayloadSettings.</p> required <p>Returns:</p> Name Type Description <code>ProcessingStepProtocol</code> <code>ProcessingStepProtocol</code> <p>The fully initialized processing step.</p>"},{"location":"athomic/plugins/","title":"Plugin System","text":""},{"location":"athomic/plugins/#overview","title":"Overview","text":"<p>The Athomic Plugin System provides a powerful and clean way to extend the core functionality of the application without modifying its source code. Plugins can introduce new API routes, register custom services into the lifecycle, or execute logic at specific points in the application's startup sequence.</p> <p>The system is built on Python's native <code>importlib.metadata</code> and entry points, making it a standard and robust way to create a pluggable architecture.</p>"},{"location":"athomic/plugins/#how-it-works","title":"How It Works","text":"<ol> <li>Entry Point: A plugin is a separate, installable Python package that defines an entry point in its <code>pyproject.toml</code> under the <code>[project.entry-points.\"nala.plugins\"]</code> group.</li> <li>Discovery: At startup, the <code>PluginManager</code> discovers all installed packages that declare this entry point.</li> <li>Hooks: Plugins define functions decorated with <code>@hookimpl</code>. The <code>PluginManager</code> registers these functions as implementations for specific, predefined hooks.</li> <li>Execution: When the core application reaches a certain point, it calls <code>plugin_manager.call_hook(\"hook_name\")</code>, which executes all registered implementations for that hook.</li> </ol>"},{"location":"athomic/plugins/#available-hooks","title":"Available Hooks","text":"<ul> <li><code>on_athomic_startup(athomic: Athomic)</code>: Called early in the startup sequence. Ideal for registering custom services with the <code>LifecycleRegistry</code>.</li> <li><code>get_api_routers() -&gt; List[APIRouter]</code>: Called by the web server layer. Each plugin can return a list of FastAPI <code>APIRouter</code> objects, which will be automatically mounted into the main application.</li> </ul>"},{"location":"athomic/plugins/#creating-a-plugin","title":"Creating a Plugin","text":"<p>See the <code>plugins/nala-plugin-greetings</code> directory for a complete example of a simple plugin.</p>"},{"location":"athomic/plugins/#api-reference","title":"API Reference","text":""},{"location":"athomic/plugins/#nala.athomic.plugins.hooks.hookimpl","title":"<code>nala.athomic.plugins.hooks.hookimpl(func)</code>","text":"<p>Decorator to explicitly mark a function as a hook implementation.</p> <p>This metadata allows the PluginManager to discover and register the implementation with the corresponding hook specification.</p>"},{"location":"athomic/plugins/#nala.athomic.plugins.manager.PluginManager","title":"<code>nala.athomic.plugins.manager.PluginManager</code>","text":"<p>               Bases: <code>PluginManagerProtocol</code></p> <p>Mediates the interaction between the Athomic core and external plugins.</p> <p>It implements the Mediator, Observer, and Registry patterns to handle plugin discovery, registration, and resilient hook execution.</p>"},{"location":"athomic/plugins/#nala.athomic.plugins.manager.PluginManager.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the PluginManager and its internal hook registry.</p>"},{"location":"athomic/plugins/#nala.athomic.plugins.manager.PluginManager.call_hook","title":"<code>call_hook(hook_name, **kwargs)</code>  <code>async</code>","text":"<p>Executes all registered implementations for a given hook with resilience, observability (tracing/metrics), and failure notification.</p>"},{"location":"athomic/plugins/#nala.athomic.plugins.manager.PluginManager.discover_and_load","title":"<code>discover_and_load()</code>  <code>async</code>","text":"<p>Discovers plugins using Python's entry points (group=\"nala.plugins\"), loads their modules, and registers all functions marked with @hookimpl.</p>"},{"location":"athomic/plugins/#nala.athomic.plugins.manager.PluginManager.get_api_routers","title":"<code>get_api_routers()</code>  <code>async</code>","text":"<p>Public contract method for the API layer. It calls the 'get_api_routers' hook, aggregates the results (a list of lists), and flattens them into a single list of router objects.</p>"},{"location":"athomic/serializer/","title":"Serializer","text":""},{"location":"athomic/serializer/#overview","title":"Overview","text":"<p>The Serializer module is a core component of the Athomic Layer responsible for the crucial task of converting Python objects into a byte representation (serialization) and back (deserialization). This functionality is essential for any operation that involves data persistence or network transmission, such as:</p> <ul> <li>Storing data in a Key-Value store (e.g., Redis).</li> <li>Publishing messages to a message broker (e.g., Kafka).</li> <li>Caching function results.</li> </ul> <p>The module is designed with extensibility in mind, built upon a protocol-centric architecture. This allows developers to easily add new serialization formats without modifying the core components that rely on it.</p>"},{"location":"athomic/serializer/#core-concepts","title":"Core Concepts","text":"<p>The Serializer module is built on three key components that work together:</p>"},{"location":"athomic/serializer/#1-serializerprotocol","title":"1. <code>SerializerProtocol</code>","text":"<p>This is the abstract contract that every serializer provider must implement. It defines the standard interface for all serialization and deserialization operations, ensuring that any component can use any serializer interchangeably. The key methods include:</p> <ul> <li><code>serialize_value(value)</code>: Converts a Python object into bytes.</li> <li><code>deserialize_value(value)</code>: Converts bytes back into a Python object.</li> <li><code>serialize_key(key)</code>: A dedicated method for serializing message keys.</li> <li><code>serialize_headers(headers)</code>: A dedicated method for serializing message headers.</li> </ul>"},{"location":"athomic/serializer/#2-serializerregistry","title":"2. <code>SerializerRegistry</code>","text":"<p>This is a central registry that maps string identifiers (backend names) to the concrete serializer classes that implement the <code>SerializerProtocol</code>. For example, it maps the name <code>\"orjson\"</code> to the <code>OrjsonSerializer</code> class. This allows the factory to be decoupled from the implementations.</p>"},{"location":"athomic/serializer/#3-serializerfactory","title":"3. <code>SerializerFactory</code>","text":"<p>This is the primary entry point for the rest of the application to obtain a serializer instance. It reads the application's configuration, finds the configured backend name in the <code>SerializerRegistry</code>, and instantiates the correct class. It manages a singleton instance to ensure the same serializer is reused throughout the application.</p>"},{"location":"athomic/serializer/#available-providers","title":"Available Providers","text":"<p>The Athomic Layer comes with several built-in serializer providers:</p> <ul> <li><code>JsonPydanticSerializer</code>: A robust JSON serializer that enhances deserialization by validating the incoming data against a target Pydantic model. This is great for ensuring data integrity at the boundaries of your application.</li> <li><code>OrjsonSerializer</code>: A high-performance JSON serializer that uses the <code>orjson</code> library for significant speed improvements over the standard <code>json</code> library.</li> <li><code>ProtobufSerializer</code>: A highly efficient, schema-based serializer for Google Protocol Buffers. It can optionally integrate with a Schema Registry to handle schema validation and wire format encapsulation, making it ideal for high-throughput messaging systems.</li> </ul>"},{"location":"athomic/serializer/#configuration","title":"Configuration","text":"<p>The serializer is configured under the <code>serializer</code> section in your settings file (e.g., <code>settings.toml</code>). You can select the backend and provide specific configurations for features like Schema Registry.</p> <pre><code>[default.serializer]\n# The name of the specific serializer implementation to use (e.g., 'json_pydantic', 'protobuf', 'orjson').\nbackend = \"orjson\"\n\n# The fundamental data format that this serializer handles.\nformat = \"json\"\n\n# --- Schema Registry (for Protobuf/Avro) ---\nschema_validation_enabled = false\nschema_registry_url = \"http://localhost:8081\"\nhandler = \"confluent_protobuf\"\n</code></pre>"},{"location":"athomic/serializer/#api-reference","title":"API Reference","text":"<p>The following sections are auto-generated from the source code docstrings.</p>"},{"location":"athomic/serializer/#serializerprotocol","title":"<code>SerializerProtocol</code>","text":""},{"location":"athomic/serializer/#nala.athomic.serializer.protocol.SerializerProtocol","title":"<code>nala.athomic.serializer.protocol.SerializerProtocol</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Defines the bidirectional contract for message serializers.</p> <p>This protocol handles the conversion of payloads, keys, and headers between Python objects/dictionaries and the raw byte format suitable for transport-specific backends (e.g., Kafka, Pub/Sub) or storage layers (e.g., Cache, Outbox). It is a critical component of the message processing pipeline.</p>"},{"location":"athomic/serializer/#nala.athomic.serializer.protocol.SerializerProtocol.__init__","title":"<code>__init__(settings)</code>","text":"<p>Initializes the serializer with its required configuration settings.</p>"},{"location":"athomic/serializer/#nala.athomic.serializer.protocol.SerializerProtocol.deserialize_headers","title":"<code>deserialize_headers(headers, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Deserializes the message headers from the backend's raw format into a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>headers</code> <code>Optional[Any]</code> <p>The serialized headers from the broker,                      which may be a list of tuples or a raw object.</p> required <code>**kwargs</code> <code>Any</code> <p>Contextual arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[Dict[str, str]]</code> <p>Optional[Dict[str, str]]: The deserialized headers as a dictionary of key-value strings, or None.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If deserialization fails.</p>"},{"location":"athomic/serializer/#nala.athomic.serializer.protocol.SerializerProtocol.deserialize_key","title":"<code>deserialize_key(key, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Deserializes the raw byte message key back into a string or object.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>bytes</code> <p>The serialized message key as bytes.</p> required <code>**kwargs</code> <code>Any</code> <p>Contextual arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: The deserialized key (typically a string), or None.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If deserialization fails.</p>"},{"location":"athomic/serializer/#nala.athomic.serializer.protocol.SerializerProtocol.deserialize_value","title":"<code>deserialize_value(value, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Deserializes the raw byte message payload back into a Python object.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>bytes</code> <p>The serialized message payload as bytes.</p> required <code>**kwargs</code> <code>Any</code> <p>Contextual arguments, potentially including a <code>target_type</code>             (e.g., a Pydantic model) for validation.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The deserialized message value (e.g., dictionary, Pydantic model instance).</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If deserialization fails (e.g., invalid JSON/Protobuf format, schema mismatch).</p>"},{"location":"athomic/serializer/#nala.athomic.serializer.protocol.SerializerProtocol.serialize_headers","title":"<code>serialize_headers(headers, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Serializes the message headers into a list of backend-compatible byte tuples.</p> <p>Parameters:</p> Name Type Description Default <code>headers</code> <code>Optional[Dict[str, str]]</code> <p>Optional dictionary of string-based headers.</p> required <code>**kwargs</code> <code>Any</code> <p>Contextual arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[list[tuple[str, bytes]]]</code> <p>Optional[list[tuple[str, bytes]]]: A list of (key, value_bytes) tuples, or None.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If serialization fails (e.g., non-string key or value).</p>"},{"location":"athomic/serializer/#nala.athomic.serializer.protocol.SerializerProtocol.serialize_key","title":"<code>serialize_key(key, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Serializes the message key for partitioning or routing.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Optional[Any]</code> <p>The optional message key.</p> required <code>**kwargs</code> <code>Any</code> <p>Contextual arguments (e.g., topic name).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[bytes]</code> <p>Optional[bytes]: The serialized key as raw bytes, or None.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If serialization fails.</p>"},{"location":"athomic/serializer/#nala.athomic.serializer.protocol.SerializerProtocol.serialize_value","title":"<code>serialize_value(value, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Serializes the message payload for outbound transmission or storage.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The original message payload (e.g., Pydantic model, dictionary).</p> required <code>**kwargs</code> <code>Any</code> <p>Contextual arguments (e.g., topic name, schema version).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[bytes]</code> <p>Optional[bytes]: The serialized payload as raw bytes, or None if the input was None.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If serialization fails (e.g., unhandled data type, schema incompatibility).</p>"},{"location":"athomic/serializer/#serializerfactory","title":"<code>SerializerFactory</code>","text":""},{"location":"athomic/serializer/#nala.athomic.serializer.factory.SerializerFactory","title":"<code>nala.athomic.serializer.factory.SerializerFactory</code>","text":"<p>Factory for instantiating message serializers based on messaging backend. Falls back to BaseSerializer if no specific implementation is registered. Caches instances by backend name for singleton-like behavior per backend.</p>"},{"location":"athomic/serializer/#nala.athomic.serializer.factory.SerializerFactory.clear","title":"<code>clear()</code>  <code>classmethod</code>","text":"<p>Clears the singleton cache of serializer instances.</p>"},{"location":"athomic/serializer/#nala.athomic.serializer.factory.SerializerFactory.create","title":"<code>create(settings=None)</code>  <code>classmethod</code>","text":"<p>Creates and returns a singleton instance of the configured SerializerProtocol by delegating to a registered creator.</p>"},{"location":"athomic/serializer/#jsonpydanticserializer","title":"<code>JsonPydanticSerializer</code>","text":""},{"location":"athomic/serializer/#nala.athomic.serializer.providers.json_pydantic_serializer.JsonPydanticSerializer","title":"<code>nala.athomic.serializer.providers.json_pydantic_serializer.JsonPydanticSerializer</code>","text":"<p>               Bases: <code>BaseSerializer</code></p> <p>A JSON serializer implementation optimized for data validation using Pydantic models.</p> <p>This serializer inherits the core functionality for header and key handling, as well as default serialization (<code>serialize_value</code>), from <code>BaseSerializer</code>. Its specialized role is to enhance the deserialization process by enforcing the schema defined by a target Pydantic model.</p>"},{"location":"athomic/serializer/#nala.athomic.serializer.providers.json_pydantic_serializer.JsonPydanticSerializer.deserialize_value","title":"<code>deserialize_value(value, target_type=dict, **kwargs)</code>  <code>async</code>","text":"<p>Deserializes a JSON payload from bytes and validates it against a target Pydantic model.</p> <p>If a <code>target_type</code> (that is a subclass of <code>BaseModel</code>) is provided, the data is validated against that schema before being returned.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>bytes | None</code> <p>The raw JSON payload as bytes.</p> required <code>target_type</code> <code>Any</code> <p>The expected class for the deserialized result, typically                a <code>pydantic.BaseModel</code> subclass. Defaults to <code>dict</code>.</p> <code>dict</code> <p>Returns:</p> Type Description <code>Any | None</code> <p>Any | None: The validated <code>target_type</code> instance or the raw data (<code>dict</code>/<code>list</code>).</p> <p>Raises:</p> Type Description <code>DeserializationError</code> <p>If JSON decoding fails or if Pydantic validation fails.</p>"},{"location":"athomic/serializer/#orjsonserializer","title":"<code>OrjsonSerializer</code>","text":""},{"location":"athomic/serializer/#nala.athomic.serializer.providers.orjson_serializer.OrjsonSerializer","title":"<code>nala.athomic.serializer.providers.orjson_serializer.OrjsonSerializer</code>","text":"<p>               Bases: <code>BaseSerializer</code></p> <p>A high-performance serializer that uses the <code>orjson</code> library to encode and decode JSON.</p> <p>This class provides a fast, dedicated JSON implementation for message payloads, inheriting all common features (logging, tracing, header/key handling) from <code>BaseSerializer</code>.</p>"},{"location":"athomic/serializer/#nala.athomic.serializer.providers.orjson_serializer.OrjsonSerializer.__init__","title":"<code>__init__(settings)</code>","text":"<p>Initializes the OrjsonSerializer.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>SerializerSettings</code> <p>The configuration for this serializer instance.</p> required"},{"location":"athomic/serializer/#nala.athomic.serializer.providers.orjson_serializer.OrjsonSerializer.deserialize_value","title":"<code>deserialize_value(data, **kwargs)</code>  <code>async</code>","text":"<p>Deserializes a JSON payload from bytes back into a Python object (dict/list).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The serialized message payload, expected as bytes.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The deserialized Python object.</p> <p>Raises:</p> Type Description <code>DeserializationError</code> <p>If orjson fails to decode the payload (e.g., invalid JSON structure).</p>"},{"location":"athomic/serializer/#nala.athomic.serializer.providers.orjson_serializer.OrjsonSerializer.serialize_value","title":"<code>serialize_value(value, **kwargs)</code>  <code>async</code>","text":"<p>Serializes a value into a JSON byte string using orjson.</p> <p>It uses the custom default encoder to handle complex types like Pydantic models.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The original message payload.</p> required <p>Returns:</p> Type Description <code>Optional[bytes]</code> <p>Optional[bytes]: The serialized payload as raw bytes.</p> <p>Raises:</p> Type Description <code>SerializationError</code> <p>If orjson fails to serialize the object (e.g., non-serializable type).</p>"},{"location":"athomic/serializer/#protobufserializer","title":"<code>ProtobufSerializer</code>","text":""},{"location":"athomic/serializer/#nala.athomic.serializer.providers.protobuf_serializer.ProtobufSerializer","title":"<code>nala.athomic.serializer.providers.protobuf_serializer.ProtobufSerializer</code>","text":"<p>               Bases: <code>BaseSerializer</code></p> <p>Serializes and deserializes messages using Google Protocol Buffers.</p> <p>This serializer is optimized for performance and type safety. It conditionally delegates schema validation and wire format encapsulation (Confluent Wire Format) to a specialized Schema Handler (e.g., Avro, Protobuf, etc.) when configured. This design keeps the class decoupled (SRP) and focused on Protobuf operations.</p> <p>Attributes:</p> Name Type Description <code>handler</code> <code>Optional[SchemaHandlerProtocol]</code> <p>The component responsible for                                        Schema Registry interaction and wire format handling.</p>"},{"location":"athomic/serializer/#nala.athomic.serializer.providers.protobuf_serializer.ProtobufSerializer.__init__","title":"<code>__init__(settings)</code>","text":"<p>Initializes the serializer.</p> <p>It attempts to inject a schema handler if schema validation is enabled in the configuration.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>SerializerSettings</code> <p>The specific configuration for this serializer instance.</p> required"},{"location":"athomic/serializer/#nala.athomic.serializer.providers.protobuf_serializer.ProtobufSerializer.deserialize_value","title":"<code>deserialize_value(data, **kwargs)</code>  <code>async</code>","text":"<p>Deserializes raw bytes into a Protobuf message instance.</p> <p>If the payload is in the Confluent Wire Format (starts with MAGIC_BYTE), it delegates the process to the schema handler for schema ID lookup. Otherwise, it attempts standard Protobuf deserialization.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The raw message data, expected as bytes.</p> required <code>**kwargs</code> <code>Any</code> <p>Contextual arguments (must include 'target_type' and 'topic' if schema handler is active).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The deserialized Protobuf message instance.</p> <p>Raises:</p> Type Description <code>DeserializationError</code> <p>If 'target_type' is missing, data is malformed,                   or standard deserialization fails.</p>"},{"location":"athomic/serializer/#nala.athomic.serializer.providers.protobuf_serializer.ProtobufSerializer.serialize_value","title":"<code>serialize_value(value, **kwargs)</code>  <code>async</code>","text":"<p>Serializes a Protobuf message into raw bytes.</p> <p>Delegates to the schema handler if configured to include the wire format (magic byte + schema ID). Otherwise, performs standard Protobuf serialization.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The Protobuf message instance to serialize.</p> required <code>**kwargs</code> <code>Any</code> <p>Contextual arguments (must include 'topic' if schema handler is active).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[bytes]</code> <p>Optional[bytes]: The serialized payload.</p> <p>Raises:</p> Type Description <code>SerializationError</code> <p>If the value is not a Protobuf message or if the                 required 'topic' is missing when a schema handler is used.</p>"},{"location":"athomic/services/","title":"Base Service &amp; Lifecycle","text":""},{"location":"athomic/services/#overview","title":"Overview","text":"<p>The <code>BaseService</code> is a fundamental building block within the Athomic Layer. It provides a standardized protocol (<code>BaseServiceProtocol</code>) for all long-running or stateful components that have a defined lifecycle (e.g., a database connection manager, a message consumer, a background poller).</p>"},{"location":"athomic/services/#key-lifecycle-methods","title":"Key Lifecycle Methods","text":"<ul> <li><code>start()</code>: Initializes the service and its dependencies.</li> <li><code>stop()</code>: Gracefully shuts down the service and releases resources.</li> <li><code>wait_ready()</code>: An awaitable method that resolves only when the service is fully initialized and ready to be used.</li> <li><code>is_ready()</code>: A boolean property to check the current readiness state.</li> </ul> <p>This consistent interface allows the Lifecycle Management system to orchestrate the application's startup and shutdown sequences in a reliable and predictable order.</p>"},{"location":"athomic/storage/","title":"File Storage Abstraction","text":""},{"location":"athomic/storage/#overview","title":"Overview","text":"<p>The Storage module provides a unified, protocol-based interface for interacting with various file and object storage backends, such as the local filesystem or cloud providers like Google Cloud Storage (GCS).</p> <p>This abstraction is essential for decoupling application logic from the underlying storage technology. Its primary use cases include: -   Handling large message payloads via the Claim Check pattern. -   General application file storage (e.g., user uploads, generated reports).</p>"},{"location":"athomic/storage/#core-concepts","title":"Core Concepts","text":""},{"location":"athomic/storage/#storageprotocol","title":"<code>StorageProtocol</code>","text":"<p>This is the abstract contract that all storage providers must implement. It defines a standard set of asynchronous operations for object storage: -   <code>upload(source_path, destination_path)</code> -   <code>download(source_path, destination_path)</code> -   <code>delete(path)</code> -   <code>get_url(path)</code> -   <code>exists(path)</code></p>"},{"location":"athomic/storage/#storagefactory","title":"<code>StorageFactory</code>","text":"<p>A singleton factory that creates the configured storage provider instance. It ensures that a single, shared client is used throughout the application, managing resources efficiently.</p>"},{"location":"athomic/storage/#available-providers","title":"Available Providers","text":""},{"location":"athomic/storage/#localstorageprovider","title":"<code>LocalStorageProvider</code>","text":"<p>This provider stores files on the local filesystem. It is ideal for local development and testing. A key feature is its built-in path traversal protection, which ensures that all file operations are securely contained within the configured <code>base_path</code>, preventing security vulnerabilities.</p>"},{"location":"athomic/storage/#gcsstorageprovider","title":"<code>GcsStorageProvider</code>","text":"<p>This is a production-ready provider for Google Cloud Storage (GCS). It handles authentication using either a service account JSON file or Application Default Credentials (ADC). It also supports generating temporary, secure signed URLs for accessing private objects.</p>"},{"location":"athomic/storage/#integration-with-messaging-the-claim-check-pattern","title":"Integration with Messaging: The Claim Check Pattern","text":"<p>A powerful feature built on the Storage module is the implementation of the Claim Check pattern for the messaging system. This pattern solves the problem of sending large messages through brokers that have size limits.</p> <p>Here\u2019s how it works: 1.  You add the <code>\"claim_check\"</code> step to your messaging payload processing pipeline in your configuration. 2.  When a producer sends a message, the <code>ClaimCheckStepAdapter</code> intercepts it. 3.  If the message's size exceeds a configured threshold (e.g., 250KB), the adapter automatically uploads the large payload to the configured storage provider (e.g., GCS). 4.  The original message payload is replaced with a small JSON object\u2014the \"claim check\"\u2014that contains a reference to the storage path. 5.  On the consumer side, the <code>ClaimCheckStepAdapter</code> detects the claim check, downloads the original large payload from storage, and transparently passes it to the next step in the processing pipeline.</p> <p>This entire process is seamless to the developer and provides a robust solution for handling large payloads.</p>"},{"location":"athomic/storage/#configuration","title":"Configuration","text":"<p>The storage provider is configured under the <code>[storage]</code> section in your <code>settings.toml</code>.</p>"},{"location":"athomic/storage/#local-storage-example","title":"Local Storage Example","text":"<pre><code>[default.storage]\nenabled = true\n\n  [default.storage.provider]\n  backend = \"local\"\n  # The base directory where files will be stored.\n  base_path = \"./.storage\"\n</code></pre>"},{"location":"athomic/storage/#google-cloud-storage-gcs-example","title":"Google Cloud Storage (GCS) Example","text":"<pre><code>[default.storage]\nenabled = true\n\n  [default.storage.provider]\n  backend = \"gcs\"\n  bucket_name = \"my-gcs-bucket-name\"\n\n  # Optional: Path to your service account JSON key file.\n  # If omitted, Application Default Credentials (ADC) are used.\n  credentials_path = \"path/to/your/gcs-credentials.json\"\n</code></pre>"},{"location":"athomic/storage/#api-reference","title":"API Reference","text":""},{"location":"athomic/storage/#nala.athomic.storage.protocol.StorageProtocol","title":"<code>nala.athomic.storage.protocol.StorageProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol defining the standard interface for storage operations. Implementations will handle interaction with specific backends like local filesystem, S3, GCS, Azure Blob, etc.</p>"},{"location":"athomic/storage/#nala.athomic.storage.protocol.StorageProtocol.delete","title":"<code>delete(path)</code>  <code>async</code>","text":"<p>Deletes an object from the storage backend.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path (key or blob name) of the object to delete.</p> required <p>Raises:</p> Type Description <code>StorageOperationError</code> <p>If the deletion fails. (Often idempotent if object doesn't exist)</p>"},{"location":"athomic/storage/#nala.athomic.storage.protocol.StorageProtocol.download","title":"<code>download(source_path, destination_path)</code>  <code>async</code>","text":"<p>Downloads an object from the storage backend to a local destination path.</p> <p>Parameters:</p> Name Type Description Default <code>source_path</code> <code>str</code> <p>The path (key or blob name) of the object in the storage backend.</p> required <code>destination_path</code> <code>str</code> <p>The local path where the file should be saved.</p> required <p>Raises:</p> Type Description <code>ObjectNotFoundError</code> <p>If the source_path does not exist in the storage.</p> <code>StorageOperationError</code> <p>If the download fails for other reasons.</p>"},{"location":"athomic/storage/#nala.athomic.storage.protocol.StorageProtocol.exists","title":"<code>exists(path)</code>  <code>async</code>","text":"<p>Checks if an object exists at the specified path in the storage backend.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path (key or blob name) to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the object exists, False otherwise.</p> <p>Raises:</p> Type Description <code>StorageOperationError</code> <p>If the check fails due to connection or permission issues.</p>"},{"location":"athomic/storage/#nala.athomic.storage.protocol.StorageProtocol.get_url","title":"<code>get_url(path, expires_in=3600)</code>  <code>async</code>","text":"<p>Generates a public or pre-signed URL to access an object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path (key or blob name) of the object.</p> required <code>expires_in</code> <code>Optional[int]</code> <p>Optional duration (in seconds) for which the URL should be valid         (relevant for pre-signed URLs). Defaults to 1 hour.</p> <code>3600</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The accessible URL as a string, or None if the provider cannot generate URLs</p> <code>Optional[str]</code> <p>or the object doesn't exist.</p> <p>Raises:</p> Type Description <code>StorageOperationError</code> <p>If URL generation fails.</p>"},{"location":"athomic/storage/#nala.athomic.storage.protocol.StorageProtocol.upload","title":"<code>upload(source_path, destination_path, metadata=None)</code>  <code>async</code>","text":"<p>Uploads a file from a local source path to a destination path in the storage backend.</p> <p>Parameters:</p> Name Type Description Default <code>source_path</code> <code>str</code> <p>The local path of the file to upload.</p> required <code>destination_path</code> <code>str</code> <p>The target path (key or blob name) in the storage backend.</p> required <code>metadata</code> <code>Optional[Dict[str, str]]</code> <p>Optional dictionary of metadata to associate with the object.</p> <code>None</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the source_path does not exist.</p> <code>StorageOperationError</code> <p>If the upload fails for other reasons (permissions, connection, etc.).</p>"},{"location":"athomic/storage/#nala.athomic.storage.factory.StorageFactory","title":"<code>nala.athomic.storage.factory.StorageFactory</code>","text":"<p>A factory responsible for creating and managing the singleton instance of the configured <code>StorageProvider</code>.</p> <p>This factory ensures that only one instance of the storage provider (e.g., <code>LocalStorageProvider</code>, <code>GcsStorageProvider</code>) is created during the application's lifecycle, managing resource efficiency and centralized configuration access.</p>"},{"location":"athomic/storage/#nala.athomic.storage.factory.StorageFactory.clear","title":"<code>clear()</code>  <code>classmethod</code>","text":"<p>Clears the cached singleton instance. Intended for use in test cleanup.</p>"},{"location":"athomic/storage/#nala.athomic.storage.factory.StorageFactory.create","title":"<code>create(settings=None)</code>  <code>classmethod</code>","text":"<p>Creates or retrieves the singleton instance of the configured storage provider.</p> <p>The creation process validates settings, retrieves the correct provider class from the <code>storage_registry</code>, and instantiates it with the corresponding configuration.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[StorageSettings]</code> <p>Optional explicit settings to override                                   global configuration.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>StorageProtocol</code> <code>StorageProtocol</code> <p>The fully initialized singleton instance of the storage provider.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the storage module is disabled or configuration is missing/invalid.</p> <code>ValueError</code> <p>If the configured storage backend is not registered.</p>"},{"location":"athomic/storage/#nala.athomic.storage.providers.local_storage.LocalStorageProvider","title":"<code>nala.athomic.storage.providers.local_storage.LocalStorageProvider</code>","text":"<p>               Bases: <code>StorageBase</code></p> <p>A storage provider implementation that saves and retrieves files from the local filesystem.</p> <p>This provider is primarily intended for development, testing, and single-instance deployments. It inherits base lifecycle management and observability from <code>StorageBase</code>. All synchronous file I/O operations are wrapped using <code>asyncio.to_thread</code> to prevent blocking the asynchronous event loop.</p> <p>It includes critical path traversal defense to ensure all operations are contained within the configured <code>base_path</code>.</p> <p>Attributes:</p> Name Type Description <code>base_path</code> <code>Path</code> <p>The resolved absolute path to the root storage directory.</p>"},{"location":"athomic/storage/#nala.athomic.storage.providers.local_storage.LocalStorageProvider.__init__","title":"<code>__init__(config)</code>","text":"<p>Initializes the provider and ensures the base storage directory exists.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>LocalStorageProviderSettings</code> <p>The configuration settings for the local storage.</p> required <p>Raises:</p> Type Description <code>StorageOperationError</code> <p>If the base directory cannot be created.</p>"},{"location":"athomic/storage/#nala.athomic.storage.providers.gcs_storage.GcsStorageProvider","title":"<code>nala.athomic.storage.providers.gcs_storage.GcsStorageProvider</code>","text":"<p>               Bases: <code>StorageBase</code></p> <p>A storage provider implementation for Google Cloud Storage (GCS).</p> <p>This provider uses the synchronous GCS client (<code>google.cloud.storage</code>) and wraps all blocking I/O operations with <code>asyncio.to_thread</code> to maintain compatibility with the asynchronous event loop. It handles initialization with service account credentials or Application Default Credentials (ADC).</p> <p>Attributes:</p> Name Type Description <code>bucket_name</code> <code>str</code> <p>The name of the target GCS bucket.</p> <code>client</code> <code>Client</code> <p>The underlying synchronous GCS client instance.</p> <code>bucket</code> <code>Bucket</code> <p>The reference to the configured GCS bucket.</p>"},{"location":"athomic/storage/#nala.athomic.storage.providers.gcs_storage.GcsStorageProvider.__init__","title":"<code>__init__(config)</code>","text":"<p>Initializes the GcsStorageProvider.</p> <p>It attempts to create the synchronous GCS client and resolve the target bucket.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>GcsStorageProviderSettings</code> <p>The configuration settings for GCS.</p> required <p>Raises:</p> Type Description <code>StorageOperationError</code> <p>If the GCS client fails to initialize (e.g., due to invalid credentials).</p>"},{"location":"athomic/storage/#nala.athomic.storage.payload.adapter.ClaimCheckStepAdapter","title":"<code>nala.athomic.storage.payload.adapter.ClaimCheckStepAdapter</code>","text":"<p>               Bases: <code>ProcessingStepProtocol</code></p> <p>An adapter that implements the Claim Check pattern as a payload processing step.</p> <p>This component is integrated into the messaging payload pipeline to handle messages whose size exceeds a configurable threshold. It stores the large payload in external blob storage (<code>StorageProtocol</code>) and replaces the original message body with a small \"claim check\" (a reference to the storage location).</p> <p>Attributes:</p> Name Type Description <code>settings</code> <p>Configuration for the Claim Check pattern, including the size threshold.</p> <code>storage</code> <code>Optional[StorageProtocol]</code> <p>The configured instance of the <code>StorageProtocol</code> (e.g., GCS, S3).</p>"},{"location":"athomic/storage/#nala.athomic.storage.payload.adapter.ClaimCheckStepAdapter.__init__","title":"<code>__init__(settings=None)</code>","text":"<p>Initializes the adapter and resolves the storage dependency and configuration settings.</p>"},{"location":"athomic/storage/#nala.athomic.storage.payload.adapter.ClaimCheckStepAdapter.decode","title":"<code>decode(data, **kwargs)</code>  <code>async</code>","text":"<p>Executes the Claim Check logic for inbound messages.</p> <p>If the message contains the claim check header and payload structure, it downloads the original, large payload from storage.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>The raw payload bytes received from the message broker.</p> required <code>**kwargs</code> <code>Any</code> <p>Contextual arguments (must include <code>headers</code>).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The original, large payload bytes.</p> <p>Raises:</p> Type Description <code>StorageOperationError</code> <p>If the download fails or the claim check path is missing.</p>"},{"location":"athomic/storage/#nala.athomic.storage.payload.adapter.ClaimCheckStepAdapter.encode","title":"<code>encode(data, **kwargs)</code>  <code>async</code>","text":"<p>Executes the Claim Check logic for outbound messages.</p> <p>If the payload size exceeds the threshold, the data is uploaded to storage, and the original payload is replaced by a JSON envelope containing the path.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>The raw payload bytes from the previous pipeline step.</p> required <code>**kwargs</code> <code>Any</code> <p>Contextual arguments (must include <code>headers</code>).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The original payload or the small claim check JSON envelope.</p> <p>Raises:</p> Type Description <code>StorageOperationError</code> <p>If the upload operation fails.</p>"},{"location":"athomic/ai/agents/","title":"AI Agents","text":""},{"location":"athomic/ai/agents/#overview","title":"Overview","text":"<p>The AI Agents module provides a robust, configuration-driven framework for building autonomous agents capable of reasoning and tool execution. It implements the ReAct (Reasoning + Acting) pattern, allowing Large Language Models to interact with external environments, execute tools, and observe results to solve complex, multi-step problems.</p>"},{"location":"athomic/ai/agents/#key-features","title":"Key Features","text":"<ul> <li>ReAct Loop: Orchestrates the \"Think -&gt; Act -&gt; Observe\" cycle automatically.</li> <li>Profile-Based Configuration: Define agent behaviors, system prompts, and toolsets entirely in <code>settings.toml</code> without changing code.</li> <li>Tool Abstraction: Decouples the agent from specific tool implementations using a robust <code>Executor</code> strategy.</li> <li>Resilience: Handles tool execution errors gracefully, feeding errors back to the LLM for self-correction.</li> <li>Observability: Full tracing of the reasoning chain (thoughts, actions, observations).</li> </ul>"},{"location":"athomic/ai/agents/#how-it-works","title":"How It Works","text":"<ol> <li>Initialization: The <code>AgentFactory</code> reads a specific \"Agent Profile\" from configuration. It resolves the required LLM connection via the <code>LLMManager</code> and prepares the <code>ToolExecutor</code>.</li> <li>Execution Loop (<code>AgentService.run</code>):<ul> <li>Think: The agent sends the conversation history (plus system prompt) to the LLM.</li> <li>Decide: The LLM returns either a final answer or a <code>ToolCall</code>.</li> <li>Act: If a tool is called, the <code>SyncToolExecutor</code> executes it safely.</li> <li>Observe: The tool's output (result or error) is added to the history as a <code>TOOL</code> role message.</li> <li>Repeat: The loop continues until the agent provides a final answer or the <code>max_iterations</code> limit is reached.</li> </ul> </li> </ol>"},{"location":"athomic/ai/agents/#usage-example","title":"Usage Example","text":"<pre><code>from nala.athomic.ai.agents.factory import AgentFactory\n\nasync def run_math_task():\n    # 1. Create the agent defined in 'math_agent' profile\n    agent = AgentFactory.create(profile_name=\"math_agent\")\n\n    # 2. Run the task\n    # The agent will automatically call the 'calculator' tool if needed\n    response = await agent.run(\"What is the square root of 144 multiplied by 5?\")\n\n    print(response) \n    # Output: \"The result is 60.\"\n</code></pre>"},{"location":"athomic/ai/agents/#configuration","title":"Configuration","text":"<p>Agents are configured under <code>[ai.agents]</code> in <code>settings.toml</code>. You define reusable Profiles.</p> <pre><code>[default.ai.agents]\nenabled = true\ndefault_profile = \"default_assistant\"\n\n  # --- Profile: Math Agent ---\n  [default.ai.agents.profiles.connections.math_agent]\n  name = \"math_agent\"\n  description = \"An agent capable of performing calculations.\"\n  system_prompt = \"You are a helpful assistant. Use tools for math.\"\n\n  # References a connection defined in [ai.connections]\n  connection_name = \"openai_gpt4\"\n\n  # Constraints\n  max_iterations = 10\n  max_execution_time_seconds = 60\n\n  # Allowed Tools (must be registered in ToolRegistry)\n  tools = [\"calculator_tool\", \"unit_converter\"]\n</code></pre>"},{"location":"athomic/ai/agents/#api-reference","title":"API Reference","text":""},{"location":"athomic/ai/agents/#nala.athomic.ai.agents.service.AgentService","title":"<code>nala.athomic.ai.agents.service.AgentService</code>","text":"<p>               Bases: <code>BaseService</code></p> <p>The core runtime service for an AI Agent.</p> <p>This service implements the ReAct (Reasoning + Acting) loop, orchestrating interactions between the LLM (Reasoning Engine) and the Tool Executor (Acting Engine).</p>"},{"location":"athomic/ai/agents/#nala.athomic.ai.agents.service.AgentService.__init__","title":"<code>__init__(settings, llm, executor)</code>","text":"<p>Initializes the AgentService.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>AgentProfileSettings</code> <p>Configuration profile defining agent behavior and constraints.</p> required <code>llm</code> <code>BaseLLM</code> <p>The Language Model provider instance.</p> required <code>executor</code> <code>BaseToolExecutor</code> <p>The strategy for executing tool calls. Must be a BaseToolExecutor       to support lifecycle and tool resolution management.</p> required"},{"location":"athomic/ai/agents/#nala.athomic.ai.agents.service.AgentService.run","title":"<code>run(input_message, history=None, **kwargs)</code>  <code>async</code>","text":"<p>Executes the agent loop for a given user input.</p> <p>Orchestrates the Think-Act-Observe cycle until the agent produces a final response or reaches the iteration limit.</p> <p>Parameters:</p> Name Type Description Default <code>input_message</code> <code>str</code> <p>The user's query or instruction.</p> required <code>history</code> <code>Optional[List[ChatMessage]]</code> <p>Optional existing conversation history.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional context or overrides.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The final text response from the agent.</p> <p>Raises:</p> Type Description <code>ServiceNotReadyError</code> <p>If the service is not started.</p> <code>TimeoutError</code> <p>If execution exceeds max_execution_time_seconds.</p> <code>AgentMaxIterationsError</code> <p>If the loop limit is reached without conclusion.</p>"},{"location":"athomic/ai/agents/#nala.athomic.ai.agents.factory.AgentFactory","title":"<code>nala.athomic.ai.agents.factory.AgentFactory</code>","text":"<p>Factory responsible for creating fully configured AgentService instances. Delegates LLM resolution to the central LLMManager using the profile's configuration.</p>"},{"location":"athomic/ai/agents/#nala.athomic.ai.agents.factory.AgentFactory.create","title":"<code>create(profile_name=None)</code>  <code>classmethod</code>","text":"<p>Creates a new AgentService instance based on the specified profile.</p>"},{"location":"athomic/ai/agents/#nala.athomic.ai.agents.executors.base.BaseToolExecutor","title":"<code>nala.athomic.ai.agents.executors.base.BaseToolExecutor</code>","text":"<p>               Bases: <code>BaseService</code>, <code>ToolExecutorProtocol</code>, <code>ABC</code></p> <p>Abstract base class for tool execution strategies.</p> <p>Manages the lifecycle of tools and standardizes execution error handling. Acts as a Service itself, ensuring all dependent tools are ready when connected.</p>"},{"location":"athomic/ai/agents/#nala.athomic.ai.agents.executors.base.BaseToolExecutor.__init__","title":"<code>__init__(tool_registry=None)</code>","text":"<p>Initializes the executor as a service.</p> <p>Parameters:</p> Name Type Description Default <code>tool_registry</code> <code>Optional[ToolRegistry]</code> <p>Registry containing available tools. If None,            a default registry instance is created.</p> <code>None</code>"},{"location":"athomic/ai/agents/#nala.athomic.ai.agents.executors.base.BaseToolExecutor.execute_batch","title":"<code>execute_batch(tool_calls)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Executes a batch of tool calls.</p> <p>Must be implemented by concrete strategies (e.g., Sync, Async, Parallel).</p>"},{"location":"athomic/ai/agents/#nala.athomic.ai.agents.executors.base.BaseToolExecutor.get_tools_for_agent","title":"<code>get_tools_for_agent(tool_names)</code>","text":"<p>Retrieves and validates a list of tools required by an Agent.</p> <p>Parameters:</p> Name Type Description Default <code>tool_names</code> <code>List[str]</code> <p>List of tool identifiers requested by the agent profile.</p> required <p>Returns:</p> Type Description <code>List[AIToolProtocol]</code> <p>List of ready-to-use tool instances.</p>"},{"location":"athomic/ai/agents/#nala.athomic.config.schemas.ai.agents.AgentProfileSettings","title":"<code>nala.athomic.config.schemas.ai.agents.AgentProfileSettings</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for a specific Agent Profile. Defines the behavior, capabilities, and constraints of a single agent type.</p>"},{"location":"athomic/ai/documents/","title":"Document Ingestion (RAG ETL)","text":""},{"location":"athomic/ai/documents/#overview","title":"Overview","text":"<p>The Document Ingestion module provides a high-level orchestration service for transforming unstructured data (PDFs, Text files) into semantic vectors persisted in a Vector Store. It implements a complete Extract-Transform-Load (ETL) pipeline designed for Retrieval-Augmented Generation (RAG).</p>"},{"location":"athomic/ai/documents/#key-features","title":"Key Features","text":"<ul> <li>Modular Loaders: Support for multiple file formats (<code>.txt</code>, <code>.pdf</code>, <code>.md</code>) via a plugin-like registry.</li> <li>Semantic Splitting: Uses <code>RecursiveCharacterTextSplitter</code> to break documents into chunks while preserving semantic context and metadata.</li> <li>Managed Pipeline: Coordinates loading, splitting, embedding, and upserting in a single method call.</li> <li>Observability: Metrics tracks pages loaded, chunks created, and processing latency.</li> </ul>"},{"location":"athomic/ai/documents/#how-it-works","title":"How It Works","text":"<ol> <li>Extract (Load): The service detects the file extension and selects the appropriate <code>DocumentLoader</code> (e.g., <code>PDFLoader</code> using <code>pypdf</code>) to extract raw text and metadata (page numbers, filenames).</li> <li>Transform (Split): The raw text is passed to a <code>TextSplitter</code> which divides it into chunks based on configurable <code>chunk_size</code> and <code>chunk_overlap</code>.</li> <li>Load (Persist): The chunks are sent to the <code>SemanticMemoryService</code>, which generates embeddings and upserts them into the configured <code>VectorStore</code>.</li> </ol>"},{"location":"athomic/ai/documents/#usage-example","title":"Usage Example","text":"<pre><code>from nala.athomic.ai.documents import DocumentIngestionFactory\n\nasync def ingest_knowledge_base(file_path: str):\n    # 1. Get the pre-configured service\n    ingestion_service = DocumentIngestionFactory.create_default()\n    await ingestion_service.connect()\n\n    # 2. Ingest the file\n    # Handles loading, splitting, embedding, and storage automatically.\n    memory_ids = await ingestion_service.ingest(\n        source=file_path,\n        filename=\"company_policy.pdf\",\n        metadata={\"category\": \"hr_policy\", \"version\": \"1.0\"}\n    )\n\n    print(f\"Successfully created {len(memory_ids)} vector records.\")\n</code></pre>"},{"location":"athomic/ai/documents/#configuration","title":"Configuration","text":"<p>Configure ingestion parameters under <code>[ai.documents]</code> in <code>settings.toml</code>.</p> <pre><code>[default.ai.documents]\nenabled = true\n\n# Chunking strategy parameters\ndefault_chunk_size = 1000\ndefault_chunk_overlap = 200\n</code></pre>"},{"location":"athomic/ai/documents/#api-reference","title":"API Reference","text":""},{"location":"athomic/ai/documents/#nala.athomic.ai.documents.service.DocumentIngestionService","title":"<code>nala.athomic.ai.documents.service.DocumentIngestionService</code>","text":"<p>               Bases: <code>BaseService</code></p> <p>Orchestrates the document ingestion pipeline.</p> <p>This service coordinates the loading, splitting, and persisting of documents into the semantic memory system. It acts as a Facade over the specialized components (Loaders, Splitters, Memory).</p> <p>Flow: 1. Resolve Loader based on file extension. 2. Load raw Documents. 3. Split Documents into Chunks. 4. Persist Chunks to Semantic Memory (Embed + Store).</p>"},{"location":"athomic/ai/documents/#nala.athomic.ai.documents.service.DocumentIngestionService.__init__","title":"<code>__init__(memory_service)</code>","text":"<p>Initialize the ingestion service.</p> <p>Parameters:</p> Name Type Description Default <code>memory_service</code> <code>SemanticMemoryService</code> <p>The initialized SemanticMemoryService instance used             for embedding and storing the document chunks.</p> required"},{"location":"athomic/ai/documents/#nala.athomic.ai.documents.service.DocumentIngestionService.ingest","title":"<code>ingest(source, filename, metadata=None, collection_name=None)</code>  <code>async</code>","text":"<p>Ingests a document into the semantic memory.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, Path, bytes]</code> <p>The file path or raw bytes.</p> required <code>filename</code> <code>str</code> <p>Name of the file (critical for extension detection).</p> required <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Additional context tags (e.g., user_id, category).</p> <code>None</code> <code>collection_name</code> <code>Optional[str]</code> <p>Target vector store collection (currently unused due to MemoryService limitations).</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of generated Memory IDs.</p> <p>Raises:</p> Type Description <code>UnsupportedFileFormatError</code> <p>If no loader exists for the file extension.</p> <code>DocumentIngestionError</code> <p>If any step of the pipeline fails.</p>"},{"location":"athomic/ai/documents/#nala.athomic.ai.documents.factory.DocumentIngestionFactory","title":"<code>nala.athomic.ai.documents.factory.DocumentIngestionFactory</code>","text":"<p>Factory responsible for creating fully configured DocumentIngestionService instances.</p> <p>It acts as a composition root, wiring together the Vector Store, Embedding Model (via Manager), and Memory Service.</p>"},{"location":"athomic/ai/documents/#nala.athomic.ai.documents.factory.DocumentIngestionFactory.create_default","title":"<code>create_default()</code>  <code>classmethod</code>","text":"<p>Creates a DocumentIngestionService using the default application settings.</p> <p>This method leverages the global Managers (EmbeddingManager) to resolve shared resources, ensuring consistent configuration across the app.</p> <p>Returns:</p> Type Description <code>DocumentIngestionService</code> <p>A configured and ready-to-connect DocumentIngestionService.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required configurations or providers are missing.</p> <code>RuntimeError</code> <p>If modules are disabled.</p>"},{"location":"athomic/ai/documents/#nala.athomic.ai.documents.registry.DocumentLoaderRegistry","title":"<code>nala.athomic.ai.documents.registry.DocumentLoaderRegistry</code>","text":"<p>               Bases: <code>BaseRegistry[Type[DocumentLoaderProtocol]]</code></p> <p>Registry for Document Loaders mapped by file extension. Allows runtime resolution of the correct strategy to read a file.</p>"},{"location":"athomic/ai/documents/#nala.athomic.ai.documents.registry.DocumentLoaderRegistry.get_loader_for_extension","title":"<code>get_loader_for_extension(extension)</code>","text":"<p>Instantiates a loader for the given extension.</p> <p>Parameters:</p> Name Type Description Default <code>extension</code> <code>str</code> <p>File extension (e.g., '.pdf', '.txt').</p> required <p>Returns:</p> Type Description <code>Optional[DocumentLoaderProtocol]</code> <p>An instance of the specific loader, or None if not supported.</p>"},{"location":"athomic/ai/embeddings/","title":"Embeddings","text":""},{"location":"athomic/ai/embeddings/#overview","title":"Overview","text":"<p>The Embeddings module provides a unified interface for text-to-vector transformation services. It is the engine powering the Vector Store and Document Ingestion modules.</p> <p>With the introduction of the <code>EmbeddingManager</code>, this module now supports robust lifecycle management and connection pooling, mirroring the architecture of the LLM module.</p>"},{"location":"athomic/ai/embeddings/#key-features","title":"Key Features","text":"<ul> <li>Unified Management: The <code>EmbeddingManager</code> handles the initialization and reuse of embedding providers.</li> <li>Shared Configuration: Reuses the <code>[ai.connections]</code> configuration group, allowing you to use the same provider settings (e.g., OpenAI API Key) for both Chat and Embeddings.</li> <li>Asymmetric Support: Supports models that differentiate between query and document embeddings (e.g., Vertex AI Gecko).</li> </ul>"},{"location":"athomic/ai/embeddings/#usage-example","title":"Usage Example","text":"<pre><code>from nala.athomic.ai.embeddings import embedding_manager\n\nasync def get_vector(text: str):\n    # 1. Get a client by name (defined in settings)\n    # The manager handles initialization/caching.\n    model = embedding_manager.get_client(\"openai_default\")\n\n    # 2. Generate vector\n    vector = await model.embed_query(text)\n    return vector\n</code></pre>"},{"location":"athomic/ai/embeddings/#configuration","title":"Configuration","text":"<p>Embeddings use the shared <code>[ai.connections]</code> pool. You specify which connection to use as default for embeddings in the root <code>[ai]</code> section.</p> <pre><code>[default.ai]\nenabled = true\n# Default connection for Chat/Generation\ndefault_connection_name = \"gpt4\"\n# Default connection specifically for Embeddings\ndefault_embeddings_connection_name = \"text_embedding_3\"\n\n  # Define the connection once\n  [default.ai.connections.connections.text_embedding_3]\n  backend = \"openai\"\n  default_model = \"text-embedding-3-small\"\n\n    [default.ai.connections.connections.text_embedding_3.provider]\n    api_key = \"...\"\n</code></pre>"},{"location":"athomic/ai/embeddings/#api-reference","title":"API Reference","text":""},{"location":"athomic/ai/embeddings/#nala.athomic.ai.embeddings.manager.EmbeddingManager","title":"<code>nala.athomic.ai.embeddings.manager.EmbeddingManager</code>","text":"<p>               Bases: <code>BaseManager[BaseEmbedding, LLMConnectionSettings]</code></p> <p>A specialized lifecycle manager for Embedding Model connections.</p> <p>This class orchestrates the initialization, connection, and shutdown of multiple Embedding providers. It shares the 'connections' configuration pool with LLMs, allowing efficient reuse of provider settings.</p> <p>It inherits robust lifecycle management from <code>BaseManager</code>, ensuring that requested Embedding connections are established during startup and gracefully closed during shutdown.</p>"},{"location":"athomic/ai/embeddings/#nala.athomic.ai.embeddings.manager.EmbeddingManager.__init__","title":"<code>__init__(settings=None)</code>","text":"<p>Initializes the EmbeddingManager.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[AISettings]</code> <p>The root AI settings object. If None, loads from global settings.</p> <code>None</code>"},{"location":"athomic/ai/embeddings/#nala.athomic.ai.embeddings.factory.EmbeddingFactory","title":"<code>nala.athomic.ai.embeddings.factory.EmbeddingFactory</code>","text":"<p>Factory for creating Embedding Model instances.</p>"},{"location":"athomic/ai/embeddings/#nala.athomic.ai.embeddings.factory.EmbeddingFactory.create","title":"<code>create(settings)</code>  <code>staticmethod</code>","text":"<p>Creates a configured instance of a BaseEmbedding provider.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>LLMConnectionSettings</code> <p>The configuration settings for this connection.</p> required <p>Returns:</p> Type Description <code>BaseEmbedding</code> <p>An initialized instance of the embedding provider.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the backend is not registered.</p>"},{"location":"athomic/ai/embeddings/#nala.athomic.ai.embeddings.protocol.EmbeddingModelProtocol","title":"<code>nala.athomic.ai.embeddings.protocol.EmbeddingModelProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol defining the contract for text-to-vector transformation services.</p>"},{"location":"athomic/ai/embeddings/#nala.athomic.ai.embeddings.protocol.EmbeddingModelProtocol.embed_documents","title":"<code>embed_documents(texts, **kwargs)</code>  <code>async</code>","text":"<p>Generates embeddings for a list of documents to be stored.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[str]</code> <p>List of strings to vectorize.</p> required <code>**kwargs</code> <code>Any</code> <p>Provider specific options (e.g. dimensionality).</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[List[float]]</code> <p>A list of vectors (one per input text).</p>"},{"location":"athomic/ai/embeddings/#nala.athomic.ai.embeddings.protocol.EmbeddingModelProtocol.embed_query","title":"<code>embed_query(text, **kwargs)</code>  <code>async</code>","text":"<p>Generates an embedding for a single user query. Some providers (like Vertex) optimize this differently from documents.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The query string.</p> required <code>**kwargs</code> <code>Any</code> <p>Provider specific options.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[float]</code> <p>A single vector list of floats.</p>"},{"location":"athomic/ai/governance/","title":"AI Governance (Guardrails)","text":""},{"location":"athomic/ai/governance/#overview","title":"Overview","text":"<p>The AI Governance module, often referred to as \"Guardrails,\" is responsible for enforcing security, compliance, and operational policies when interacting with Large Language Models (LLMs). Its main purpose is to decouple policy enforcement from specific LLM providers (e.g., OpenAI, Gemini), centralizing the logic in a robust and extensible pipeline.</p>"},{"location":"athomic/ai/governance/#integration","title":"Integration","text":"<p>The <code>GuardPipeline</code> is now integrated directly into the <code>BaseLLM</code>. This ensures that governance is applied consistently regardless of whether the LLM is called via <code>LLMManager</code>, <code>AgentService</code>, or <code>RAGService</code>.</p>"},{"location":"athomic/ai/governance/#execution-flow","title":"Execution Flow","text":"<ol> <li> <p>Input Guards (Blocking):</p> <ul> <li>Run synchronously before <code>generate</code> or <code>stream_content</code>.</li> <li>If a guard fails (e.g., Blocklist, Rate Limit), the operation is aborted immediately, and a <code>GovernanceViolationError</code> is raised.</li> </ul> </li> <li> <p>Output Guards (Context Dependent):</p> <ul> <li>Unary Calls (<code>generate</code>): Run after the full response is received. Can modify/redact the response content (e.g., PII Sanitization) before returning it to the caller.</li> <li>Streaming Calls (<code>stream_content</code>): Due to the complexity of sanitizing partial token streams without adding significant latency (buffering), output guards in streaming mode currently operate in Audit/Pass-through mode. They log violations but do not block the stream in real-time.</li> </ul> </li> </ol>"},{"location":"athomic/ai/governance/#available-guards","title":"Available Guards","text":"<p>The following governance components are currently implemented and ready for configuration:</p> Guard Class Type Description <code>RateLimitValidator</code> Input Prevents abuse by limiting the number of calls per client or token. <code>KeywordBlocklistValidator</code> Input Checks prompts against a configured list of banned terms, raising a violation if found. <code>RegexPIISanitizer</code> Input Identifies and logs PII data in the prompt for compliance purposes. <code>OutputPIISanitizer</code> Output Masks or redacts identified PII data in the LLM's final response (Non-streaming only)."},{"location":"athomic/ai/governance/#configuration-example","title":"Configuration Example","text":"<pre><code># Settings example for an active configuration\nai_governance:\n  enabled: true\n  input_policy: \"fail_fast\" # or \"aggregate\"\n  guards:\n    rate_limit:\n      enabled: true\n      max_calls: 100\n    keyword_blocking:\n      enabled: true\n      keywords: [\"banned_term\", \"policy_violation\"]\n    pii_sanitization:\n      enabled: true\n      redact_output: true\n</code></pre>"},{"location":"athomic/ai/llm/","title":"Large Language Models (LLM)","text":""},{"location":"athomic/ai/llm/#overview","title":"Overview","text":"<p>The <code>athomic.ai.llm</code> module provides a unified, provider-agnostic interface for interacting with Large Language Models (LLMs) such as OpenAI (GPT-4) and Google Vertex AI (Gemini). It is designed to solve common challenges in production AI applications:</p> <ul> <li>Vendor Lock-in: Switch providers via configuration without changing code.</li> <li>Real-Time Streaming: Native support for asynchronous token streaming with standardized chunks.</li> <li>Structured Data: Reliably extract Pydantic models from LLM outputs using a standardized API (<code>generate_structured</code>).</li> <li>Observability: Automatic tracing (OpenTelemetry) and metrics (Prometheus) for every call (unary or streaming).</li> <li>Embedded Governance: Enforce rate limits and safety checks directly within the provider pipeline.</li> </ul>"},{"location":"athomic/ai/llm/#core-concepts","title":"Core Concepts","text":""},{"location":"athomic/ai/llm/#llmproviderprotocol","title":"<code>LLMProviderProtocol</code>","text":"<p>The contract that all LLM providers must implement. It defines three primary operations:</p> <ul> <li><code>generate_content(prompt, ...)</code>: Generates unstructured text (blocking).</li> <li><code>stream_content(prompt, ...)</code>: Generates unstructured text incrementally (async iterator).</li> <li><code>generate_structured(prompt, response_model, ...)</code>: Generates a structured object adhering to a specific Pydantic schema.</li> </ul>"},{"location":"athomic/ai/llm/#basellm-governance","title":"<code>BaseLLM</code> &amp; Governance","text":"<p>The abstract base class now handles AI Governance automatically: 1.  Input Guards: Executed before the request is sent to the provider. These are blocking (e.g., Rate Limiting, Prompt Injection check). 2.  Output Guards: Executed after the response is received. For streaming, these currently act in audit mode.</p>"},{"location":"athomic/ai/llm/#usage-example","title":"Usage Example","text":""},{"location":"athomic/ai/llm/#basic-text-generation-blocking","title":"Basic Text Generation (Blocking)","text":"<pre><code>from nala.athomic.ai.llm.factory import LLMFactory\nfrom nala.athomic.config import get_settings\n\nasync def chat_with_ai(user_input: str):\n    # 1. Create the provider from global settings\n    settings = get_settings().ai.llm.connections[\"default\"]\n    llm = LLMFactory.create(settings)\n\n    # 2. Generate content (waits for full completion)\n    response = await llm.generate_content(\n        prompt=user_input,\n        system_message=\"You are a helpful assistant.\"\n    )\n    return response.content\n</code></pre>"},{"location":"athomic/ai/llm/#real-time-streaming","title":"Real-Time Streaming","text":"<pre><code>import sys\nfrom nala.athomic.ai.llm.factory import LLMFactory\n\nasync def stream_chat(user_input: str):\n    llm = LLMFactory.create_default()\n\n    # stream_content yields LLMResponseChunk objects\n    async for chunk in llm.stream_content(prompt=user_input):\n\n        # 1. Process incremental text (Token Delta)\n        if chunk.content_delta:\n            sys.stdout.write(chunk.content_delta)\n            sys.stdout.flush()\n\n        # 2. Handle metadata on finish (Usage, Stop Reason)\n        if chunk.is_final:\n            print(f\"n[Meta] Finished. Tokens: {chunk.usage.total_tokens}\")\n</code></pre>"},{"location":"athomic/ai/llm/#structured-output-json-extraction","title":"Structured Output (JSON Extraction)","text":"<pre><code>from pydantic import BaseModel, Field\nfrom nala.athomic.ai.llm.factory import LLMFactory\n\nclass UserIntent(BaseModel):\n    intent: str = Field(..., description=\"The user's intention (buy, sell, support)\")\n    confidence: float\n\nasync def analyze_intent(text: str):\n    llm = LLMFactory.create_default()\n\n    # Guaranteed to return a UserIntent instance or raise StructuredOutputError\n    intent: UserIntent = await llm.generate_structured(\n        prompt=f\"Analyze this text: {text}\",\n        response_model=UserIntent\n    )\n\n    return intent\n</code></pre>"},{"location":"athomic/ai/llm/#configuration","title":"Configuration","text":"<p>LLM connections are configured in <code>settings.toml</code>.</p> <pre><code>[default.ai.llm]\n# The default connection to use\ndefault_connection_name = \"gpt4_main\"\n\n  [default.ai.llm.connections.gpt4_main]\n  backend = \"openai\"\n  default_model = \"gpt-4-turbo\"\n  timeout = 30.0\n\n    [default.ai.llm.connections.gpt4_main.provider]\n    # OpenAI specific settings\n    api_key = { path = \"ai/openai\", key = \"api_key\" }\n    organization_id = \"org-123\"\n</code></pre>"},{"location":"athomic/ai/llm/#api-reference","title":"API Reference","text":""},{"location":"athomic/ai/llm/#nala.athomic.ai.llm.protocol.LLMProviderProtocol","title":"<code>nala.athomic.ai.llm.protocol.LLMProviderProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol defining the contract for Large Language Model (LLM) interactions. Focused on text generation and structured data extraction.</p>"},{"location":"athomic/ai/llm/#nala.athomic.ai.llm.protocol.LLMProviderProtocol.generate_content","title":"<code>generate_content(prompt=None, system_message=None, **kwargs)</code>  <code>async</code>","text":"<p>Generates unstructured text content based on a given prompt.</p>"},{"location":"athomic/ai/llm/#nala.athomic.ai.llm.protocol.LLMProviderProtocol.generate_structured","title":"<code>generate_structured(prompt, response_model, system_message=None, max_retries=3, **kwargs)</code>  <code>async</code>","text":"<p>Generates structured data conforming to a specific Pydantic schema.</p>"},{"location":"athomic/ai/llm/#nala.athomic.ai.llm.protocol.LLMProviderProtocol.stream_content","title":"<code>stream_content(prompt=None, system_message=None, **kwargs)</code>  <code>async</code>","text":"<p>Generates unstructured text content incrementally as an asynchronous stream.</p> <p>This method is critical for low-latency user experiences (UX).</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Optional[str]</code> <p>The user input text.</p> <code>None</code> <code>system_message</code> <code>Optional[str]</code> <p>Optional system instruction.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional generation parameters.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncIterator[LLMResponseChunk]</code> <p>An asynchronous iterator yielding LLMResponseChunk objects.</p>"},{"location":"athomic/ai/llm/#nala.athomic.ai.llm.base.BaseLLM","title":"<code>nala.athomic.ai.llm.base.BaseLLM</code>","text":"<p>               Bases: <code>BaseService</code>, <code>LLMProviderProtocol</code>, <code>Generic[S]</code>, <code>ABC</code></p> <p>Abstract base class for all LLM providers (Vertex, OpenAI, etc.).</p> <p>This class implements the Template Method pattern to enforce: 1. Lifecycle Management (Startup/Shutdown via BaseService). 2. Unified Observability (Tracing, Metrics, Logging). 3. Error Handling boundaries.</p> <p>Note: Governance (Guards) is handled by the LLMManager/GuardPipeline before calling this provider.</p>"},{"location":"athomic/ai/llm/#nala.athomic.ai.llm.base.BaseLLM.generate","title":"<code>generate(prompt=None, system_message=None, tools=None, **kwargs)</code>  <code>async</code>","text":"<p>Public method for generation (Text or Tool Calls) with observability.</p>"},{"location":"athomic/ai/llm/#nala.athomic.ai.llm.base.BaseLLM.generate_structured","title":"<code>generate_structured(prompt, response_model, system_message=None, max_retries=3, **kwargs)</code>  <code>async</code>","text":"<p>Public method for structured generation with observability.</p>"},{"location":"athomic/ai/llm/#nala.athomic.ai.llm.base.BaseLLM.record_token_usage","title":"<code>record_token_usage(prompt_tokens, completion_tokens)</code>","text":"<p>Helper method to record token usage metrics to Prometheus.</p>"},{"location":"athomic/ai/llm/#nala.athomic.ai.llm.base.BaseLLM.stream_content","title":"<code>stream_content(prompt=None, system_message=None, tools=None, **kwargs)</code>  <code>async</code>","text":"<p>Public method for streamed generation (Text or Tool Calls) with observability. The LLM's response is yielded incrementally.</p>"},{"location":"athomic/ai/llm/#nala.athomic.ai.schemas.llms.LLMResponseChunk","title":"<code>nala.athomic.ai.schemas.llms.LLMResponseChunk</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a single, incremental chunk of the response during streaming.</p>"},{"location":"athomic/ai/llm/#nala.athomic.ai.llm.factory.LLMFactory","title":"<code>nala.athomic.ai.llm.factory.LLMFactory</code>","text":"<p>Factory responsible for creating instances of the configured LLM provider.</p>"},{"location":"athomic/ai/llm/#nala.athomic.ai.llm.factory.LLMFactory.create","title":"<code>create(settings)</code>  <code>classmethod</code>","text":"<p>Creates and returns an instance of the LLM provider based on the settings.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>LLMConnectionSettings</code> <p>The specific connection settings containing the backend type       and provider-specific configurations.</p> required <p>Returns:</p> Type Description <code>BaseLLM</code> <p>An initialized instance adhering to BaseLLM.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the backend is not specified or not registered.</p>"},{"location":"athomic/ai/memory/","title":"Semantic Memory Service","text":""},{"location":"athomic/ai/memory/#overview","title":"Overview","text":"<p>The <code>SemanticMemoryService</code> acts as an orchestration facade for Retrieval-Augmented Generation (RAG). It combines the capabilities of Embeddings and Vector Stores to provide a high-level API for storing and recalling semantic information.</p> <p>Instead of manually wiring an embedding model to a vector database in every application service, developers use this unified service to \"add\" memories (text) and \"recall\" them (search).</p>"},{"location":"athomic/ai/memory/#features","title":"Features","text":"<ul> <li>Orchestration: Automatically handles the flow: <code>Text -&gt; Embedding -&gt; Vector Record -&gt; DB Upsert</code>.</li> <li>Asymmetric Support: Correctly uses <code>embed_documents</code> for storage and <code>embed_query</code> for retrieval.</li> <li>Standardized Metadata: Automatically manages creation timestamps and metadata persistence.</li> <li>Tracing: Provides a holistic trace span (<code>memory.add</code>, <code>memory.recall</code>) encompassing the entire RAG operation.</li> </ul>"},{"location":"athomic/ai/memory/#usage","title":"Usage","text":"<pre><code>from nala.athomic.ai.memory.factory import SemanticMemoryFactory\n\nasync def remember_user_preference(user_id: str, preference: str):\n    # 1. Initialize the service (automatically wires DB and AI)\n    memory_service = SemanticMemoryFactory.create(collection_name=\"user_context\")\n\n    # 2. Add a memory with metadata\n    await memory_service.add(\n        content=preference,\n        metadata={\"user_id\": user_id, \"type\": \"preference\"}\n    )\n\nasync def get_relevant_context(user_id: str, question: str):\n    memory_service = SemanticMemoryFactory.create(collection_name=\"user_context\")\n\n    # 3. Recall relevant memories\n    # The 'filters' argument is passed down to the Vector Store\n    results = await memory_service.recall(\n        query=question,\n        limit=3,\n        filters={\"user_id\": user_id}\n    )\n\n    return [res.memory.content for res in results]\n</code></pre>"},{"location":"athomic/ai/memory/#api-reference","title":"API Reference","text":""},{"location":"athomic/ai/memory/#nala.athomic.ai.memory.service.SemanticMemoryService","title":"<code>nala.athomic.ai.memory.service.SemanticMemoryService</code>","text":"<p>               Bases: <code>SemanticMemoryServiceProtocol</code></p> <p>Concrete implementation of the Semantic Memory Service.</p> <p>Orchestrates the flow: Content -&gt; Embedding (via EmbeddingModelProtocol) -&gt; Vector Store. Respects the distinction between embedding documents for storage and queries for retrieval.</p>"},{"location":"athomic/ai/memory/#nala.athomic.ai.memory.service.SemanticMemoryService.__init__","title":"<code>__init__(vector_store, embedding_model, collection_name='semantic_memories')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>vector_store</code> <code>VectorStoreProtocol</code> <p>The infrastructure client for vector persistence.</p> required <code>embedding_model</code> <code>EmbeddingModelProtocol</code> <p>The AI provider conforming to existing protocol.</p> required <code>collection_name</code> <code>str</code> <p>The logical collection name to store memories.</p> <code>'semantic_memories'</code>"},{"location":"athomic/ai/memory/#nala.athomic.ai.memory.types.Memory","title":"<code>nala.athomic.ai.memory.types.Memory</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a unit of semantic memory.</p>"},{"location":"athomic/ai/memory/#nala.athomic.ai.memory.types.MemorySearchResult","title":"<code>nala.athomic.ai.memory.types.MemorySearchResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a recalled memory with its relevance score.</p>"},{"location":"athomic/ai/rag/","title":"Retrieval-Augmented Generation (RAG)","text":""},{"location":"athomic/ai/rag/#overview","title":"Overview","text":"<p>The RAG module provides an orchestration service for combining semantic search (Memory) with generative capabilities (LLM). It implements a Stream-First architecture designed for high-performance user experiences, where users need to verify sources (\"citations\") immediately while the answer is being generated.</p>"},{"location":"athomic/ai/rag/#key-features","title":"Key Features","text":"<ul> <li>True Streaming: Yields retrieval sources immediately, then streams the LLM answer token-by-token.</li> <li>Deterministic Generation: Enforces strict parameters (<code>temperature=0.0</code>) on the LLM to minimize hallucinations based on retrieved context.</li> <li>Strategy Pattern: Decouples Retrieval (getting data) from Generation (synthesizing answer), allowing easy swapping of algorithms (e.g., \"Stuff\", \"Map-Reduce\").</li> <li>Observability: Full pipeline tracing (<code>rag.retrieve</code> -&gt; <code>rag.generate</code>) with granular metrics for retrieval quality and generation latency.</li> </ul>"},{"location":"athomic/ai/rag/#architecture","title":"Architecture","text":""},{"location":"athomic/ai/rag/#the-rag-pipeline","title":"The RAG Pipeline","text":"<ol> <li>Retrieval Phase:<ul> <li>The <code>RetrievalStrategy</code> executes (e.g., Vector Search).</li> <li>Converts raw memory results into standardized <code>RAGSource</code> objects.</li> </ul> </li> <li>Augmentation Phase:<ul> <li>The <code>GenerationStrategy.augment_prompt()</code> method constructs the final prompt, injecting the sources into the template (e.g., XML formatting <code>&lt;doc id='1'&gt;...&lt;/doc&gt;</code>).</li> </ul> </li> <li>Generation Phase:<ul> <li>The service invokes the LLM using the augmented prompt.</li> <li>Response chunks are yielded to the client.</li> </ul> </li> </ol>"},{"location":"athomic/ai/rag/#usage-example","title":"Usage Example","text":""},{"location":"athomic/ai/rag/#streaming-response-recommended","title":"Streaming Response (Recommended)","text":"<pre><code>from nala.athomic.ai.rag.factory import RAGFactory\nfrom nala.athomic.ai.schemas.rag import RAGRequest\n\nasync def ask_knowledge_base(question: str):\n    # 1. Initialize Service\n    rag_service = RAGFactory.create()\n    await rag_service.connect()\n\n    request = RAGRequest(query=question)\n    print(f\"Question: {question}\")\n\n    # 2. Iterate over RAGResponseChunk objects\n    async for chunk in rag_service.stream_response(request):\n\n        # A. Sources arrive first (Immediate Feedback)\n        if chunk.sources:\n            doc_ids = [s.document_id for s in chunk.sources]\n            print(f\"n[Sources Found]: {doc_ids}\")\n\n        # B. Content streams in real-time\n        if chunk.content_delta:\n            print(chunk.content_delta, end=\"\", flush=True)\n\n        # C. Final metadata (Latency, Tokens, etc.)\n        if chunk.is_final:\n            print(f\"\\n\\n[Done] Latency: {chunk.metadata['latency_ms']}ms\")\n</code></pre>"},{"location":"athomic/ai/rag/#configuration","title":"Configuration","text":"<p>RAG is configured under <code>[ai.rag]</code> in <code>settings.toml</code>.</p> <pre><code>[default.ai.rag]\nenabled = true\n\n# Strategies\nretrieval_strategy = \"vector_similarity\"\ngeneration_strategy = \"stuff\" # Concatenates context into prompt\n\n# Prompting\ndefault_prompt_template = \"rag/default_qa\"\n\n# Constraints\nmax_context_length = 4000\n</code></pre>"},{"location":"athomic/ai/rag/#api-reference","title":"API Reference","text":""},{"location":"athomic/ai/rag/#nala.athomic.ai.rag.service.RAGService","title":"<code>nala.athomic.ai.rag.service.RAGService</code>","text":"<p>               Bases: <code>BaseService</code></p> <p>Concrete implementation of the RAG pipeline with full observability.</p> <p>Integrates: - Semantic Memory (Retrieval) via Strategies. - Prompt Engineering (Augmentation) via PromptService. - LLM (Generation) via Strategies. - Prometheus Metrics &amp; OpenTelemetry Tracing.</p>"},{"location":"athomic/ai/rag/#nala.athomic.ai.rag.service.RAGService.generate_response","title":"<code>generate_response(request)</code>  <code>async</code>","text":"<p>Executes the full RAG pipeline: Retrieve -&gt; Augment -&gt; Generate (Non-streaming).</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>RAGRequest</code> <p>The RAG request containing the query and optional model override.</p> required"},{"location":"athomic/ai/rag/#nala.athomic.ai.rag.service.RAGService.retrieve_context","title":"<code>retrieve_context(request)</code>  <code>async</code>","text":"<p>Executes ONLY the retrieval step (Public API). Delegates to the internal helper which handles metrics/errors.</p> <p>Returns:</p> Type Description <code>List[RAGSource]</code> <p>List[RAGSource]: The retrieved document snippets.</p>"},{"location":"athomic/ai/rag/#nala.athomic.ai.rag.service.RAGService.stream_response","title":"<code>stream_response(request)</code>  <code>async</code>","text":"<p>Executes the pipeline and yields RAGResponseChunks for real-time streaming.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>RAGRequest</code> <p>The RAG request containing the query and optional model override.</p> required <p>Yields:     AsyncIterator[RAGResponseChunk]: Stream of chunks including sources, content delta, and final metadata.</p>"},{"location":"athomic/ai/rag/#nala.athomic.ai.schemas.rag.RAGRequest","title":"<code>nala.athomic.ai.schemas.rag.RAGRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Standard input payload for RAG operations. This is the complete replacement for the legacy 'RAGRequest'.</p>"},{"location":"athomic/ai/rag/#nala.athomic.ai.schemas.rag.RAGResponseChunk","title":"<code>nala.athomic.ai.schemas.rag.RAGResponseChunk</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents an incremental chunk of the final RAG response during streaming.</p>"},{"location":"athomic/ai/rag/#nala.athomic.ai.rag.generation.protocol.GenerationStrategyProtocol","title":"<code>nala.athomic.ai.rag.generation.protocol.GenerationStrategyProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol defining the contract for RAG generation strategies (e.g., Stuff, Map-Reduce).</p>"},{"location":"athomic/ai/rag/#nala.athomic.ai.rag.generation.protocol.GenerationStrategyProtocol.augment_prompt","title":"<code>augment_prompt(query, sources, prompt_service, template_name, **kwargs)</code>","text":"<p>Augments the raw user query with the retrieved sources to create the final, context-aware prompt string. This is a synchronous operation.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The user's original query.</p> required <code>sources</code> <code>List[RAGSource]</code> <p>The retrieved context documents (RAGSource objects).</p> required <code>prompt_service</code> <code>PromptService</code> <p>Service to render the final prompt template.</p> required <code>template_name</code> <code>str</code> <p>The template to use (e.g., 'rag/default_qa').</p> required <code>**kwargs</code> <code>Any</code> <p>Additional parameters.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The final prompt string ready to be sent to the LLM.</p>"},{"location":"athomic/ai/rag/#nala.athomic.ai.rag.generation.protocol.GenerationStrategyProtocol.generate","title":"<code>generate(query, sources, llm, prompt_service, template_name, **kwargs)</code>  <code>async</code>","text":"<p>Generates the final answer based on the query and retrieved sources.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The user's original query.</p> required <code>sources</code> <code>List[RAGSource]</code> <p>The retrieved context documents (RAGSource objects).</p> required <code>llm</code> <code>BaseLLM</code> <p>The LLM provider instance to use for completion.</p> required <code>prompt_service</code> <code>PromptService</code> <p>Service to render the final prompt template.</p> required <code>template_name</code> <code>str</code> <p>The template to use (e.g., 'rag/default_qa').</p> required <code>**kwargs</code> <code>Any</code> <p>Additional generation parameters.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The final generated answer string.</p>"},{"location":"athomic/config/schemas/","title":"Configuration Schemas","text":""},{"location":"athomic/config/schemas/#overview","title":"Overview","text":"<p>The <code>config/schemas</code> directory is the heart of the Athomic Layer's type-safe configuration system. It contains a collection of Pydantic models that precisely define the structure, data types, default values, and validation rules for every setting in the application.</p> <p>This approach provides several key benefits: -   Type Safety: Eliminates configuration errors at startup by validating the entire settings structure. -   Auto-Completion: Developers get full auto-completion and static analysis for configuration objects in their IDE. -   Clear Structure: The directory structure of the schemas directly mirrors the structure of the <code>settings.toml</code> file, making the configuration intuitive to navigate.</p> <p>The root of this system is the <code>AppSettings</code> model, which composes all other schema models into a single, unified configuration object.</p>"},{"location":"athomic/config/schemas/#schema-module-structure","title":"Schema Module Structure","text":"<p>Below is a list of the primary modules within the <code>schemas</code> directory. Each module corresponds to a major feature or component of the Athomic Layer.</p> <ul> <li> <p><code>app_settings.py</code>: The main Pydantic model that aggregates all other configuration models.</p> </li> <li> <p><code>context/</code>:</p> <ul> <li><code>context_config.py</code>: Defines settings for the Context Management module, including which variables to propagate.</li> </ul> </li> <li> <p><code>control/</code>:</p> <ul> <li><code>control_config.py</code>: Root model for runtime control features.</li> <li><code>feature_flags_config.py</code>: Schemas for the Feature Flags system.</li> <li><code>live_config.py</code>: Schemas for the Live Configuration watcher and state services.</li> <li><code>scheduler_config.py</code>: Schemas for the Task Scheduler.</li> </ul> </li> <li> <p><code>database/</code>:</p> <ul> <li><code>database_config.py</code>: Main model for all Data &amp; Persistence connections.</li> <li><code>document/</code>: Schemas for Document Store providers (e.g., MongoDB).</li> <li><code>kvstore/</code>: Schemas for Key-Value Store providers (e.g., Redis) and their wrappers.</li> <li><code>migrations_config.py</code>: Schemas for the Database Migrations engine.</li> </ul> </li> <li> <p><code>events/</code>:</p> <ul> <li><code>events_config.py</code>: Schemas for the Internal Event Bus.</li> </ul> </li> <li> <p><code>http/</code>:</p> <ul> <li><code>http_config.py</code>: Schemas for the Resilient HTTP Client.</li> </ul> </li> <li> <p><code>integration/</code>:</p> <ul> <li><code>integration_config.py</code>: Root model for all external integrations.</li> <li><code>consul_config.py</code>: Schemas for the Consul Client.</li> <li><code>discovery_config.py</code>: Schemas for the Service Discovery system.</li> <li><code>messaging/</code>: A rich set of schemas for the Messaging module, including producer, consumer, DLQ, and republisher settings.</li> <li><code>tasks/</code>: Schemas for Background Task brokers.</li> </ul> </li> <li> <p><code>lineage/</code>:</p> <ul> <li><code>lineage_config.py</code>: Defines settings for the Data Lineage module. Now includes:<ul> <li><code>LineageProducerSettings</code>: Identity configuration.</li> <li><code>LineageStoreSettings</code>: Composite storage configuration.</li> <li>Specific backend settings (<code>DocumentLineageStoreSettings</code>, <code>GraphLineageStoreSettings</code>, etc.).</li> </ul> </li> </ul> </li> <li> <p><code>observability/</code>:</p> <ul> <li><code>observability_config.py</code>: Main model for the observability stack.</li> <li><code>log/logging_config.py</code>: Schemas for Structured Logging.</li> <li><code>metrics/metrics_config.py</code>: Schemas for Prometheus Metrics.</li> </ul> </li> <li> <p><code>performance/</code>:</p> <ul> <li><code>performance_config.py</code>: Root model for performance features.</li> <li><code>cache_config.py</code>: Schemas for the Caching system.</li> <li><code>compression_config.py</code>: Schemas for HTTP Compression.</li> </ul> </li> <li> <p><code>resilience/</code>:</p> <ul> <li><code>resilience_config.py</code>: The main model that aggregates all resilience patterns.</li> <li>Contains individual schema files for each pattern (e.g., <code>retry_config.py</code>, <code>circuit_breaker_config.py</code>).</li> </ul> </li> <li> <p><code>security/</code>:</p> <ul> <li><code>security_config.py</code>: The main model for all security features.</li> <li><code>auth_config.py</code>: Schemas for Authentication (JWT, API Key).</li> <li><code>crypto_config.py</code>: Schemas for Cryptography.</li> <li><code>secrets/</code>: Schemas for Secrets Management.</li> </ul> </li> <li> <p><code>serializer/</code>:</p> <ul> <li><code>serializer_config.py</code>: Schemas for the Serializer module.</li> </ul> </li> <li> <p><code>storage/</code>:</p> <ul> <li><code>storage_config.py</code>: Schemas for the File Storage abstraction.</li> </ul> </li> </ul>"},{"location":"athomic/control/feature_flags/","title":"Feature Flags","text":"<p>feature_flags# Feature Flags</p>"},{"location":"athomic/control/feature_flags/#overview","title":"Overview","text":"<p>The Feature Flags module provides a powerful system for dynamically enabling or disabling features in your application at runtime, without requiring a code deployment. This is an essential tool for continuous delivery, A/B testing, and safely rolling out new functionality.</p>"},{"location":"athomic/control/feature_flags/#key-features","title":"Key Features","text":"<ul> <li>Multiple Backends: Supports fetching flags from a local configuration dictionary, Redis, or Consul KV.</li> <li>Declarative Usage: Protect an entire function or code path with the simple <code>@feature_enabled</code> decorator.</li> <li>Contextual Evaluation: Flags can be evaluated based on the current <code>ExecutionContext</code> (e.g., enable a feature only for a specific <code>tenant_id</code>).</li> <li>Multivariate Flags: Supports variants, which are flags that can return a string, number, or JSON object instead of just true/false.</li> </ul>"},{"location":"athomic/control/feature_flags/#how-it-works","title":"How It Works","text":"<p>The system is orchestrated by the <code>FeatureFlagFactory</code>, which creates a provider instance based on your configuration. The primary way to interact with the system is through two utility functions: -   <code>is_feature_enabled(flag_key)</code>: Checks if a boolean flag is active. -   <code>get_feature_variant(flag_key)</code>: Retrieves the value of a multivariate flag.</p> <p>For convenience, the <code>@feature_enabled</code> decorator wraps this logic, protecting an entire function.</p>"},{"location":"athomic/control/feature_flags/#usage-example","title":"Usage Example","text":"<pre><code>from nala.athomic.control import feature_enabled, is_feature_enabled, get_feature_variant\nfrom nala.athomic.context import context_vars\n\n# --- Using the decorator ---\n@feature_enabled(flag_key=\"new-dashboard-feature\", context_kwargs=[\"tenant_id\"])\nasync def show_new_dashboard(tenant_id: str):\n    # This code will only run if the \"new-dashboard-feature\" is enabled.\n    # The check can be contextual based on the tenant_id.\n    return {\"dashboard\": \"v2\"}\n\n# --- Using the utility functions ---\nasync def get_user_limit() -&gt; int:\n    # Get a variant value, with a default of 100\n    limit = await get_feature_variant(\"user-concurrent-limit\", default=100)\n    return limit\n</code></pre>"},{"location":"athomic/control/feature_flags/#configuration","title":"Configuration","text":"<pre><code>[default.control.feature_flags]\nenabled = true\n# The primary backend to fetch flags from.\nbackend = \"consul\"\n# TTL in seconds for caching flags from external providers (Redis, Consul).\ncache_ttl_seconds = 60\n\n  [default.control.feature_flags.provider]\n  backend = \"consul\"\n  # The base path in Consul KV where flags are stored.\n  kv_prefix = \"config/flags\"\n</code></pre>"},{"location":"athomic/control/live_config/","title":"Live Configuration (Hot-Reloading)","text":""},{"location":"athomic/control/live_config/#overview","title":"Overview","text":"<p>The Live Configuration system provides a powerful mechanism to change application settings at runtime without requiring a restart or deployment. This is a critical feature for operational agility, allowing you to tune parameters, toggle features, or respond to incidents in real-time.</p>"},{"location":"athomic/control/live_config/#key-features","title":"Key Features","text":"<ul> <li>Dynamic Updates: Settings can be changed in a central source (like Consul KV) and are automatically reflected in the application.</li> <li>Seamless Integration: Works through a special Pydantic model, <code>LiveConfigModel</code>, which makes accessing dynamic values transparent to the developer.</li> <li>Event-Driven: Uses the internal Event Bus for efficient, decoupled communication between the watcher and the state service.</li> </ul>"},{"location":"athomic/control/live_config/#how-it-works-a-two-part-system","title":"How It Works: A Two-Part System","text":"<ol> <li> <p><code>ConfigWatcherService</code>: This is a background service that continuously long-polls an external configuration provider (e.g., Consul) for changes under a specific key prefix.</p> <ul> <li>When a change is detected, it compares the new values with its last known state.</li> <li>For each key that has changed, it publishes a <code>config.updated</code> event on the internal event bus with the key and the new value.</li> </ul> </li> <li> <p><code>LiveConfigService</code>: This service subscribes to the <code>config.updated</code> event. It maintains an in-memory, flattened dictionary of the application's most current configuration state. When it receives an update event, it updates its internal state.</p> </li> <li> <p><code>LiveConfigModel</code>: Your Pydantic settings models can inherit from <code>LiveConfigModel</code>. When you access an attribute on such a model, it automatically intercepts the call and queries the <code>LiveConfigService</code> for the latest value, falling back to the statically configured value if none is found.</p> </li> </ol>"},{"location":"athomic/control/live_config/#api-reference","title":"API Reference","text":""},{"location":"athomic/control/live_config/#nala.athomic.control.live_config.service.LiveConfigService","title":"<code>nala.athomic.control.live_config.service.LiveConfigService</code>","text":"<p>               Bases: <code>BaseService</code></p> <p>Maintains the current state of the application's live configuration.</p> <p>This service acts as an in-memory cache for configuration values that can be updated at runtime. It subscribes to internal <code>config.updated</code> events, which are published by the <code>ConfigWatcherService</code>, and updates its internal state accordingly. Other components, like <code>LiveConfigModel</code>, query this service to get the most up-to-date configuration values.</p> <p>Attributes:</p> Name Type Description <code>event_bus</code> <code>EventBusProtocol</code> <p>The internal event bus used to receive update notifications.</p> <code>_config_state</code> <code>Dict[str, Any]</code> <p>The in-memory dictionary holding the flattened, most current configuration state.</p>"},{"location":"athomic/control/live_config/#nala.athomic.control.live_config.service.LiveConfigService.__init__","title":"<code>__init__(settings=None, event_bus=None)</code>","text":"<p>Initializes the LiveConfigService.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[LiveConfigSettings]</code> <p>The configuration for the live config system. If not provided, global settings are used.</p> <code>None</code> <code>event_bus</code> <code>Optional[EventBusProtocol]</code> <p>An instance of the event bus. If not provided, the global instance is retrieved.</p> <code>None</code>"},{"location":"athomic/control/live_config/#nala.athomic.control.live_config.service.LiveConfigService.get","title":"<code>get(key, default=None)</code>","text":"<p>Gets the latest value of a configuration key from the in-memory state.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The configuration key in flattened format (e.g., 'resilience.retry.attempts').</p> required <code>default</code> <code>Optional[Any]</code> <p>The value to return if the key is not found.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The current configuration value or the default.</p>"},{"location":"athomic/control/live_config/#nala.athomic.control.live_config.watcher_service.ConfigWatcherService","title":"<code>nala.athomic.control.live_config.watcher_service.ConfigWatcherService</code>","text":"<p>               Bases: <code>BaseService</code></p> <p>Watches a configuration prefix in an external provider for changes.</p> <p>This background service actively monitors a specified key prefix in a dynamic configuration backend (e.g., Consul KV). When it detects a change, it compares the new values with its last known state and publishes <code>config.updated</code> events on the internal event bus for each changed key.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <code>LiveConfigSettings</code> <p>The settings for the live config system.</p> <code>config_provider</code> <code>ConfigProvidersProtocol</code> <p>The client for the external configuration source.</p> <code>event_bus</code> <code>EventBusProtocol</code> <p>The internal event bus for publishing updates.</p>"},{"location":"athomic/control/live_config/#nala.athomic.control.live_config.watcher_service.ConfigWatcherService.__init__","title":"<code>__init__(settings=None, config_provider=None, event_bus=None)</code>","text":"<p>Initializes the ConfigWatcherService.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[LiveConfigSettings]</code> <p>The configuration for the live config system.</p> <code>None</code> <code>config_provider</code> <code>Optional[ConfigProvidersProtocol]</code> <p>The provider for the external configuration source.</p> <code>None</code> <code>event_bus</code> <code>Optional[EventBusProtocol]</code> <p>The internal event bus for publishing updates.</p> <code>None</code>"},{"location":"athomic/control/live_config/#nala.athomic.config.schemas.live.LiveConfigModel","title":"<code>nala.athomic.config.schemas.live.LiveConfigModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A Pydantic BaseModel that provides transparent dynamic configuration.</p> <p>Subclasses of <code>LiveConfigModel</code> can have their attributes updated at runtime. When an attribute is accessed, this class intercepts the call and attempts to fetch the latest value from the central <code>LiveConfigService</code>. This enables hot-reloading of configuration without an application restart.</p> <p>The mapping of dynamic fields is controlled in two ways, with configuration from files taking precedence:</p> <ol> <li> <p>Via Configuration File (<code>settings.toml</code>):     In the TOML section for your schema, add a <code>_live_keys_config</code> map:     <code>toml     [my_module]     _live_keys_config = { pydantic_attr_name = \"live.config.key\" }     PYDANTIC_ATTR_NAME = \"static_default\"</code></p> </li> <li> <p>Via Code (<code>model_config</code>):     In the Pydantic schema class, define a <code>live_keys</code> dictionary in <code>model_config</code>.</p> </li> </ol> <p>Attributes:</p> Name Type Description <code>_instance_live_keys</code> <code>Dict[str, str]</code> <p>A dictionary mapping attribute names in this model to their corresponding keys in the live configuration backend. This map is populated from the <code>_live_keys_config</code> section in the TOML configuration file.</p>"},{"location":"athomic/control/live_config/#nala.athomic.config.schemas.live.LiveConfigModel.__getattribute__","title":"<code>__getattribute__(name)</code>","text":"<p>Overrides attribute access to fetch dynamic values from the live config service.</p> <p>This method implements the core logic for live configuration: 1. Retrieves the static value (loaded at application startup). 2. Checks if the requested attribute is mapped as a \"live key\". 3. If it is, queries the <code>LiveConfigService</code> for the most up-to-date value. 4. Returns the live value if found; otherwise, the static value is used    as a fallback. 5. If the attribute is not a live key, the static value is returned directly.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the attribute being accessed.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The latest value of the attribute from the live config service,</p> <code>Any</code> <p>or the statically configured value as a fallback.</p>"},{"location":"athomic/control/live_config/#nala.athomic.config.schemas.live.LiveConfigModel.__init__","title":"<code>__init__(**data)</code>","text":"<p>Initializes the model and extracts the live key mapping.</p> <p>This constructor intercepts the <code>_live_keys_config</code> dictionary from the incoming data before passing the rest to the standard Pydantic validator. This allows the model to store the live configuration mapping without treating it as a standard model field.</p> <p>Parameters:</p> Name Type Description Default <code>**data</code> <code>Any</code> <p>The raw data dictionary used to populate the model, which may include the <code>_live_keys_config</code> metadata key.</p> <code>{}</code>"},{"location":"athomic/control/scheduler/","title":"Task Scheduler","text":""},{"location":"athomic/control/scheduler/#overview","title":"Overview","text":"<p>The Scheduler module provides a persistent, distributed mechanism for scheduling tasks to be executed at a specific time in the future or after a certain delay. It is distinct from the main Background Tasks module, which is for immediate asynchronous execution.</p> <p>The scheduler is essential for features like: -   Implementing the Delayed Messages strategy. -   Scheduling recurring batch jobs. -   Triggering future actions (e.g., ending a trial period).</p>"},{"location":"athomic/control/scheduler/#how-it-works","title":"How It Works","text":"<p>The system is composed of a persistent storage backend and a background worker. 1.  Scheduling: When you call <code>scheduler.schedule()</code>, a <code>GenericTask</code> object (containing the handler path and payload) is created. This object is stored in a KV store (like Redis), and its execution time is added as a score to a sorted set. 2.  Polling (Worker): The <code>SchedulerWorkerService</code> is a background service that periodically polls the sorted set for tasks whose execution time has passed. 3.  Dispatching: The worker atomically fetches and removes a due task from the schedule and then enqueues it into the main application Task Broker for actual execution by a standard worker.</p> <p>This design decouples scheduling from execution, making the system highly scalable and resilient.</p>"},{"location":"athomic/control/scheduler/#api-reference","title":"API Reference","text":""},{"location":"athomic/control/scheduler/#nala.athomic.control.scheduler.protocol.SchedulerProtocol","title":"<code>nala.athomic.control.scheduler.protocol.SchedulerProtocol</code>","text":"<p>               Bases: <code>BaseServiceProtocol</code>, <code>Protocol</code></p> <p>Defines the contract for a task scheduling system. Abstracts the logic of delaying or scheduling the execution of a task, whether in memory, via broker, or in persistent storage.</p>"},{"location":"athomic/control/scheduler/#nala.athomic.control.scheduler.protocol.SchedulerProtocol.cancel","title":"<code>cancel(task_id)</code>  <code>async</code>","text":"<p>Attempts to cancel a scheduled task.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>The ID of the task to be canceled.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the cancellation was successful, False otherwise.</p>"},{"location":"athomic/control/scheduler/#nala.athomic.control.scheduler.protocol.SchedulerProtocol.schedule","title":"<code>schedule(task, *, delay, task_id=None)</code>  <code>async</code>","text":"<p>Schedules a task to be executed after a certain delay.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Any</code> <p>The task object to be scheduled (can be a Pydantic model, dict, etc.).</p> required <code>delay</code> <code>timedelta</code> <p>The waiting time from now.</p> required <code>task_id</code> <code>str | None</code> <p>An optional ID for the task. If not provided, one will be generated.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The ID of the scheduled task.</p>"},{"location":"athomic/control/scheduler/#nala.athomic.control.scheduler.protocol.SchedulerProtocol.schedule_at","title":"<code>schedule_at(task, *, execution_time, task_id=None)</code>  <code>async</code>","text":"<p>Schedules a task to be executed at an exact moment in the future.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Any</code> <p>The task object to be scheduled.</p> required <code>execution_time</code> <code>datetime</code> <p>The exact date and time (UTC) for execution.</p> required <code>task_id</code> <code>str | None</code> <p>An optional ID for the task.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The ID of the scheduled task.</p>"},{"location":"athomic/control/scheduler/#nala.athomic.control.scheduler.schemas.GenericTask","title":"<code>nala.athomic.control.scheduler.schemas.GenericTask</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a generic, serializable task for the scheduler.</p> <p>This model acts as a standard envelope for any task that needs to be scheduled for delayed or future execution. It decouples the scheduler from the task's implementation by referencing the handler as a string and encapsulating all necessary data within the <code>payload</code>.</p> <p>Attributes:</p> Name Type Description <code>task_id</code> <code>UUID</code> <p>A unique identifier for this specific task instance.</p> <code>handler</code> <code>str</code> <p>The string identifier of the callable handler that will execute the task's logic.</p> <code>payload</code> <code>Dict[str, Any]</code> <p>A dictionary containing the data required by the handler to perform the task.</p>"},{"location":"athomic/control/scheduler/#nala.athomic.control.scheduler.worker.SchedulerWorkerService","title":"<code>nala.athomic.control.scheduler.worker.SchedulerWorkerService</code>","text":"<p>               Bases: <code>BaseService</code></p> <p>A background service that polls a persistent schedule and dispatches due tasks.</p> <p>This worker is the execution engine for the custom KV-based scheduler. It periodically polls a sorted set in a KV store (e.g., Redis) to find tasks whose scheduled execution time has passed. Once a task is claimed, it is dispatched to the primary application task broker (e.g., Taskiq) for actual execution. It uses an exponential backoff strategy to reduce polling frequency during idle periods.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <p>The worker-specific configuration settings.</p> <code>kv_store</code> <code>Optional[KVStoreProtocol]</code> <p>The client for the KV store that holds the schedule.</p> <code>task_broker</code> <code>Optional[TaskBrokerProtocol]</code> <p>The client for the main task broker where tasks are dispatched.</p> <code>tasks_key</code> <code>str</code> <p>The key for the sorted set of scheduled tasks.</p> <code>details_prefix</code> <code>str</code> <p>The key prefix for storing task details.</p>"},{"location":"athomic/control/scheduler/#nala.athomic.control.scheduler.worker.SchedulerWorkerService.__init__","title":"<code>__init__(settings)</code>","text":"<p>Initializes the SchedulerWorkerService.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>SchedulerSettings</code> <p>The main scheduler configuration, from which worker-specific settings are extracted.</p> required"},{"location":"athomic/database/documents/","title":"Document Stores","text":""},{"location":"athomic/database/documents/#overview","title":"Overview","text":"<p>The <code>athomic.database.documents</code> module provides the abstraction layer for connecting to and managing document-oriented databases within the Athomic Layer. It follows a provider-based pattern, allowing different backends like MongoDB to be used interchangeably without affecting the services that depend on them.</p> <p>The primary responsibility of this module is to provide a lifecycle-managed, ready-to-use database client instance that can be passed to data repositories or Object-Document Mappers (ODMs) like Beanie.</p>"},{"location":"athomic/database/documents/#how-it-works","title":"How It Works","text":"<p>The components of this module are designed to work together seamlessly with the central <code>ConnectionManager</code>:</p> <ol> <li>Configuration: In your <code>settings.toml</code>, you define one or more connections under <code>[database.documents]</code>. Each connection specifies a <code>backend</code> (e.g., \"mongo\").</li> <li>Factory Creation: The <code>ConnectionManager</code> calls the <code>DocumentsDatabaseFactory</code> for each configured connection.</li> <li>Registry Lookup: The factory uses the <code>backend</code> name to look up the corresponding provider class (e.g., <code>MongoProvider</code>) in the <code>DocumentDatabaseRegistry</code>.</li> <li>Instantiation: The factory instantiates the provider class with its specific configuration.</li> <li>Lifecycle Management: The <code>ConnectionManager</code> then manages the <code>start</code> and <code>stop</code> lifecycle of the created provider instance.</li> </ol> <p>All providers must adhere to the <code>DocumentsDatabaseProtocol</code>, which guarantees a consistent interface for obtaining the native database client.</p>"},{"location":"athomic/database/documents/#available-providers","title":"Available Providers","text":""},{"location":"athomic/database/documents/#mongoprovider","title":"<code>MongoProvider</code>","text":"<p>This is the primary, production-ready provider for connecting to a MongoDB server or replica set. It is built on top of the official asynchronous driver <code>motor</code>. The provider handles: -   Securely resolving the connection URI. -   Establishing the connection and verifying it with a <code>ping</code> command. -   Providing the native <code>AsyncIOMotorDatabase</code> object that can be used directly by libraries like Beanie.</p>"},{"location":"athomic/database/documents/#localdbprovider","title":"<code>LocalDBProvider</code>","text":"<p>A simple, in-memory provider designed for testing and development. It simulates the connection lifecycle (<code>start</code>, <code>stop</code>, <code>wait_ready</code>) without requiring any external database dependencies. This is extremely useful for fast and isolated unit tests.</p>"},{"location":"athomic/database/documents/#configuration","title":"Configuration","text":"<p>Below is an example of how to configure a MongoDB connection in your <code>settings.toml</code>. You can define multiple connections with different names (e.g., <code>default_mongo</code>, <code>logs_mongo</code>).</p> <pre><code>[default.database.documents.default_mongo]\n# Enables or disables this specific document database connection.\nenabled = true\n\n# The name of the backend. This is the key used in the registry.\nbackend = \"mongo\"\n\n  # Provider-specific settings, validated by a Pydantic model.\n  [default.database.documents.default_mongo.provider]\n  # The connection URI for the MongoDB instance.\n  # This can be a direct value or a secret reference.\n  url = \"mongodb://user:pass@localhost:27017\" # pragma: allowlist secret\n\n  # The name of the database to use.\n  database_name = \"main_app_db\"\n</code></pre>"},{"location":"athomic/database/documents/#api-reference","title":"API Reference","text":""},{"location":"athomic/database/documents/#nala.athomic.database.documents.protocol.DocumentsDatabaseProtocol","title":"<code>nala.athomic.database.documents.protocol.DocumentsDatabaseProtocol</code>","text":"<p>               Bases: <code>BaseServiceProtocol</code>, <code>Protocol</code></p> <p>Protocol for a document database provider.</p> <p>Defines the contract for managing a database connection and providing access to the native database instance (e.g., a Motor/Beanie database object). It inherits from ConnectionServiceProtocol to be managed by the application's lifecycle.</p>"},{"location":"athomic/database/documents/#nala.athomic.database.documents.protocol.DocumentsDatabaseProtocol.get_database","title":"<code>get_database()</code>  <code>async</code>","text":"<p>Returns the native database instance required by repositories to perform operations.</p>"},{"location":"athomic/database/documents/#nala.athomic.database.documents.factory.DocumentsDatabaseFactory","title":"<code>nala.athomic.database.documents.factory.DocumentsDatabaseFactory</code>","text":"<p>Factory responsible for creating a singleton instance of the configured document database provider.</p>"},{"location":"athomic/database/documents/#nala.athomic.database.documents.factory.DocumentsDatabaseFactory.create","title":"<code>create(settings)</code>  <code>classmethod</code>","text":"<p>Creates and returns a singleton instance of the document database provider.</p>"},{"location":"athomic/database/documents/#nala.athomic.database.documents.providers.mongo_provider.MongoProvider","title":"<code>nala.athomic.database.documents.providers.mongo_provider.MongoProvider</code>","text":"<p>               Bases: <code>DocumentsDatabaseBase</code>, <code>CredentialResolve</code></p> <p>A Document Database Provider implementation for MongoDB.</p> <p>This class manages the connection lifecycle, state, and client access for a MongoDB instance using the asynchronous <code>motor.motor_asyncio</code> library. It integrates credential resolution to securely handle the connection URI. It inherits all core lifecycle and state management logic from <code>DocumentsDatabaseBase</code>.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <code>MongoSettings</code> <p>The MongoDB-specific configuration settings.</p> <code>client</code> <code>AsyncIOMotorClient | None</code> <p>The underlying Motor client instance.</p> <code>_db</code> <code>AsyncIOMotorDatabase | None</code> <p>The active database instance.</p>"},{"location":"athomic/database/documents/#nala.athomic.database.documents.providers.mongo_provider.MongoProvider.__init__","title":"<code>__init__(settings=None)</code>","text":"<p>Initializes the MongoProvider.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[DocumentsSettings]</code> <p>The document database configuration settings.</p> <code>None</code>"},{"location":"athomic/database/graph/","title":"Graph Databases","text":""},{"location":"athomic/database/graph/#overview","title":"Overview","text":"<p>The <code>athomic.database.graph</code> module provides the abstraction layer for connecting to and managing graph-oriented databases within the Athomic Layer. It allows the application to model complex relationships, such as data lineage, social networks, or impact analysis graphs, using a high-performance, asynchronous interface.</p> <p>Similar to the Document Stores and Key-Value Stores modules, it follows a provider-based pattern, decoupling the application logic from the underlying graph technology.</p>"},{"location":"athomic/database/graph/#core-concepts","title":"Core Concepts","text":""},{"location":"athomic/database/graph/#graphdatabaseprotocol","title":"<code>GraphDatabaseProtocol</code>","text":"<p>This is the abstract contract that all graph database providers must implement. It defines the standard interface for executing queries (e.g., Cypher) and managing the connection lifecycle.</p>"},{"location":"athomic/database/graph/#graphmanager","title":"<code>GraphManager</code>","text":"<p>The <code>GraphManager</code> is the orchestrator for graph connections. It is responsible for: -   Multi-Tenancy: Managing multiple named connections (e.g., <code>default</code>, <code>analytics</code>, <code>fraud_detection</code>) defined in the configuration. -   Lifecycle: Ensuring that all configured graph drivers are connected at startup and gracefully closed at shutdown. -   Registry: Providing access to specific client instances via <code>get_client(\"connection_name\")</code>.</p>"},{"location":"athomic/database/graph/#neo4jprovider","title":"<code>Neo4jProvider</code>","text":"<p>This is the primary, production-ready provider implementation. It wraps the official Neo4j Python Driver (Async), adding critical enterprise features: -   Observability: Automatically wraps queries with OpenTelemetry tracing spans and Prometheus metrics. -   Resilience: Manages connection pooling and retry logic for transient network errors. -   Security: Integrates with the Credentials module to resolve database passwords securely at runtime.</p>"},{"location":"athomic/database/graph/#how-it-works","title":"How It Works","text":"<ol> <li>Configuration: You define your graph connections in <code>settings.toml</code> under the <code>[database.graph]</code> section.</li> <li>Initialization: At application startup, the central ConnectionManager initializes the <code>GraphManager</code>.</li> <li>Factory Creation: The <code>GraphManager</code> uses the <code>GraphDatabaseFactory</code> to instantiate the configured backend (e.g., <code>Neo4jProvider</code>) for each defined connection.</li> <li>Usage: Services inject the <code>GraphDatabaseProtocol</code> via the connection manager to execute queries without knowing the implementation details.</li> </ol>"},{"location":"athomic/database/graph/#usage-example","title":"Usage Example","text":""},{"location":"athomic/database/graph/#basic-query-execution","title":"Basic Query Execution","text":"<p>To interact with the graph database, retrieve the client from the connection manager and use the <code>execute_query</code> method.</p> <pre><code>from nala.athomic.database.factory import connection_manager_factory\n\nasync def get_service_dependencies(service_name: str):\n    # 1. Get the connection manager (Singleton)\n    manager = connection_manager_factory.create()\n\n    # 2. Get the default graph client\n    # You can also pass a specific name: manager.get_graph_db(\"analytics\")\n    graph_db = manager.get_graph_db() \n\n    # 3. Define a Cypher query\n    query = \"\"\"\n    MATCH (s:Service {name: $name})-[:CALLS]-&gt;(target:Service)\n    RETURN target.name as dependency, target.version as version\n    \"\"\"\n\n    # 4. Execute asynchronously\n    # The provider handles the session management automatically\n    results = await graph_db.execute_query(\n        query, \n        parameters={\"name\": service_name}\n    )\n\n    return [record[\"dependency\"] for record in results]\n</code></pre>"},{"location":"athomic/database/graph/#configuration","title":"Configuration","text":"<p>Graph connections are configured using the standard Connection Group pattern found in <code>settings.toml</code>.</p> <pre><code>[default.database.graph]\n# The name of the default connection to return when none is specified.\ndefault_connection_name = \"default_graph\"\n\n  # --- Connection 1: The main application graph ---\n  [default.database.graph.connections.default_graph]\n  enabled = true\n  backend = \"neo4j\"\n\n    [default.database.graph.connections.default_graph.provider]\n    backend = \"neo4j\"\n    # Connection URI (supports bolt://, neo4j://, neo4j+s://)\n    uri = \"bolt://localhost:7687\"\n    user = \"neo4j\"\n    # Secure password reference via the Secrets module\n    password = { path = \"database/neo4j\", key = \"password\" } # pragma: allowlist secret\n\n    # Connection pool tuning\n    max_connection_pool_size = 50\n    max_connection_lifetime = 3600\n\n  # --- Connection 2: A separate analytics graph ---\n  [default.database.graph.connections.analytics_graph]\n  enabled = true\n  backend = \"neo4j\"\n    [default.database.graph.connections.analytics_graph.provider]\n    backend = \"neo4j\"\n    uri = \"neo4j+s://analytics-cluster.graph.io:7687\"\n    user = \"analytics_svc\"\n    password = { path = \"database/neo4j-analytics\", key = \"password\" } # pragma: allowlist secret\n</code></pre>"},{"location":"athomic/database/graph/#api-reference","title":"API Reference","text":""},{"location":"athomic/database/graph/#nala.athomic.database.graph.protocol.GraphDatabaseProtocol","title":"<code>nala.athomic.database.graph.protocol.GraphDatabaseProtocol</code>","text":"<p>               Bases: <code>BaseServiceProtocol</code>, <code>Protocol</code></p> <p>Defines the contract for a Graph Database provider. This ensures the application is decoupled from specific graph technology drivers.</p>"},{"location":"athomic/database/graph/#nala.athomic.database.graph.protocol.GraphDatabaseProtocol.get_driver","title":"<code>get_driver()</code>  <code>async</code>","text":"<p>Returns the underlying native driver instance.</p>"},{"location":"athomic/database/graph/#nala.athomic.database.graph.protocol.GraphDatabaseProtocol.run_query","title":"<code>run_query(query, parameters=None)</code>  <code>async</code>","text":"<p>Executes a Cypher (or equivalent) query and returns a list of records.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query string.</p> required <code>parameters</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of parameters to bind to the query.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of dictionaries representing the result records.</p>"},{"location":"athomic/database/graph/#nala.athomic.database.graph.manager.GraphManager","title":"<code>nala.athomic.database.graph.manager.GraphManager</code>","text":"<p>               Bases: <code>BaseManager[GraphDatabaseProtocol, GraphSettings]</code></p> <p>A specialized lifecycle manager for Graph Database connections.</p> <p>This class orchestrates the initialization, connection, and shutdown of multiple graph database providers (e.g., 'default', 'analytics'). It acts as the single point of access for retrieving specific graph clients within the application.</p> <p>It inherits robust lifecycle management from <code>BaseManager</code>, ensuring that all configured graph connections are established during startup and gracefully closed during shutdown.</p>"},{"location":"athomic/database/graph/#nala.athomic.database.graph.manager.GraphManager.__init__","title":"<code>__init__(settings)</code>","text":"<p>Initializes the GraphManager.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>DatabaseSettings</code> <p>The root database settings object containing the 'graph'       connection group configuration.</p> required"},{"location":"athomic/database/graph/#nala.athomic.database.graph.providers.neo4j.Neo4jProvider","title":"<code>nala.athomic.database.graph.providers.neo4j.Neo4jProvider</code>","text":"<p>               Bases: <code>GraphDatabaseBase</code>, <code>CredentialResolve</code></p> <p>Concrete implementation of the GraphDatabaseProtocol for Neo4j.</p>"},{"location":"athomic/database/graph/#nala.athomic.database.graph.providers.neo4j.Neo4jProvider.__init__","title":"<code>__init__(settings)</code>","text":"<p>Initializes the Neo4jProvider instance.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>GraphSettings</code> <p>The root database settings object containing the 'graph'                      connection group configuration.</p> required"},{"location":"athomic/database/graph/#nala.athomic.database.graph.factory.GraphDatabaseFactory","title":"<code>nala.athomic.database.graph.factory.GraphDatabaseFactory</code>","text":"<p>Factory for creating GraphDatabaseProtocol instances.</p>"},{"location":"athomic/database/graph/#nala.athomic.database.graph.factory.GraphDatabaseFactory.create","title":"<code>create(settings=None)</code>  <code>classmethod</code>","text":"<p>Creates a graph provider instance.</p> <p>If 'settings' is NOT provided, it resolves the DEFAULT connection from the global AppSettings (ConnectionGroupSettings).</p>"},{"location":"athomic/database/kvstore/","title":"Key-Value Stores","text":""},{"location":"athomic/database/kvstore/#overview","title":"Overview","text":"<p>The Key-Value (KV) Store module provides a powerful and extensible abstraction layer for interacting with key-value data stores like Redis. It is a fundamental building block used by many other Athomic modules, including:</p> <ul> <li>Caching: For storing the results of expensive operations.</li> <li>Rate Limiting: For tracking request counts.</li> <li>Feature Flags: For fetching flag configurations.</li> <li>Distributed Locking: For managing lock states.</li> <li>Sagas &amp; Schedulers: For persisting state and scheduled tasks.</li> </ul> <p>The architecture is designed to be highly flexible, featuring a protocol-based design, multiple backend providers, and a powerful wrapper system that allows for adding cross-cutting functionality declaratively.</p>"},{"location":"athomic/database/kvstore/#core-concepts","title":"Core Concepts","text":""},{"location":"athomic/database/kvstore/#kvstoreprotocol","title":"<code>KVStoreProtocol</code>","text":"<p>This is the contract that all KV store providers must implement. It defines a rich set of asynchronous operations, including not only basic CRUD (<code>get</code>, <code>set</code>, <code>delete</code>, <code>exists</code>) but also advanced commands for sorted sets (<code>zadd</code>, <code>zrem</code>, <code>zpopbyscore</code>), which are critical for implementing components like the task scheduler.</p>"},{"location":"athomic/database/kvstore/#kvstorefactory","title":"<code>KVStoreFactory</code>","text":"<p>This is the main entry point for creating a KV store client. The factory is responsible for: 1.  Reading the configuration for a named connection. 2.  Instantiating the correct base provider (e.g., <code>RedisKVClient</code>) using a registry. 3.  Applying a chain of wrappers to the base provider, adding functionality like multi-tenancy and default TTLs.</p>"},{"location":"athomic/database/kvstore/#wrapper-decorator-pattern","title":"Wrapper (Decorator) Pattern","text":"<p>A key architectural feature of the KV Store module is its use of the Decorator pattern. You can \"wrap\" a base client with additional functionality defined in your configuration. This makes the system extremely flexible. For example, you can add multi-tenant key resolution to any KV store (Redis, in-memory, or a future DynamoDB provider) just by adding a wrapper to the configuration.</p>"},{"location":"athomic/database/kvstore/#available-providers","title":"Available Providers","text":"<ul> <li><code>RedisKVClient</code>: The primary, production-ready provider for Redis. It uses the high-performance <code>redis-py</code> async client and supports the full <code>KVStoreProtocol</code>, including atomic sorted set operations implemented with Lua scripting for maximum safety.</li> <li><code>LocalKVClient</code>: A simple, in-memory provider that simulates a KV store using Python dictionaries. It is perfect for fast, dependency-free unit and integration testing.</li> </ul>"},{"location":"athomic/database/kvstore/#available-wrappers","title":"Available Wrappers","text":"<ul> <li><code>KeyResolvingKVClient</code>: This wrapper automatically prepends contextual prefixes to all keys before they hit the database. It uses the <code>ContextKeyGenerator</code> to include the <code>namespace</code>, <code>tenant_id</code>, and <code>user_id</code> (if configured), making multi-tenancy transparent.</li> <li><code>DefaultTTLKvClient</code>: This wrapper automatically applies a default Time-To-Live (TTL) to all <code>set</code> operations if one is not explicitly provided in the method call. This is useful for ensuring that cache keys, for example, always expire.</li> </ul>"},{"location":"athomic/database/kvstore/#configuration","title":"Configuration","text":"<p>The true power of this module is visible in its configuration. You define a base provider and then apply a list of wrappers to it.</p> <pre><code># In settings.toml, under [default.database.kvstore]\n\n[default.database.kvstore.default_redis]\nenabled = true\n# The namespace is used by the KeyResolvingKVClient wrapper\nnamespace = \"cache\"\n\n  # 1. Define the base provider (e.g., Redis)\n  [default.database.kvstore.default_redis.provider]\n  backend = \"redis\"\n  uri = \"redis://localhost:6379/0\"\n\n  # 2. Define an ordered list of wrappers to apply\n  [[default.database.kvstore.default_redis.wrappers]]\n  name = \"default_ttl\" # This must match a name in the KVStoreWrapperRegistry\n  enabled = true\n    # Custom settings for this wrapper instance\n    [default.database.kvstore.default_redis.wrappers.config]\n    default_ttl_seconds = 3600 # 1 hour\n\n  [[default.database.kvstore.default_redis.wrappers]]\n  name = \"key_resolver\"\n  enabled = true\n    # The 'namespace' from the top level will be used by this wrapper\n    [default.database.kvstore.default_redis.wrappers.config]\n    # No extra config needed here, it uses the parent's namespace\n</code></pre> <p>In this example, when a <code>set(\"my-key\", \"value\")</code> call is made, the final key in Redis will look something like <code>nala:tenant-123:cache:my-key</code>, and it will have a default TTL of 1 hour.</p>"},{"location":"athomic/database/kvstore/#api-reference","title":"API Reference","text":""},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol","title":"<code>nala.athomic.database.kvstore.protocol.KVStoreProtocol</code>","text":"<p>               Bases: <code>BaseServiceProtocol</code>, <code>Protocol</code></p> <p>Defines the contract for an asynchronous Key-Value store provider.</p> <p>This protocol specifies the standard interface for all key-value store implementations within the framework. It includes basic CRUD operations, sorted set commands, and lifecycle management. Any class that conforms to this protocol can be used as a KV store backend for features like caching, batch operate, rate limiting, and feature flags.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.clear","title":"<code>clear()</code>  <code>async</code>","text":"<p>Removes all keys from the current database.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Closes the connection to the backend service.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.connect","title":"<code>connect()</code>  <code>async</code>","text":"<p>Establishes the connection to the backend service.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.delete","title":"<code>delete(key)</code>  <code>async</code>","text":"<p>Deletes a key from the store.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to delete.</p> required"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.delete_many","title":"<code>delete_many(keys)</code>  <code>async</code>","text":"<p>Deletes multiple keys in a single operation.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>List[str]</code> <p>A list of keys to delete.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of keys actually deleted.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.exists","title":"<code>exists(key)</code>  <code>async</code>","text":"<p>Checks if a key exists in the store.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the key exists, False otherwise.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.get","title":"<code>get(key)</code>  <code>async</code>","text":"<p>Retrieves a value by its key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the item to retrieve.</p> required <p>Returns:</p> Type Description <code>Optional[Any]</code> <p>The deserialized value, or None if the key is not found.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.get_final_client","title":"<code>get_final_client()</code>  <code>async</code>","text":"<p>Returns the underlying raw asynchronous client instance.</p> <p>This provides an escape hatch to access provider-specific features not covered by the protocol (e.g., calling a unique Redis command).</p> <p>Returns:</p> Type Description <code>Any</code> <p>The raw provider-specific async client instance (e.g., <code>redis.asyncio.Redis</code>).</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.get_sync_client","title":"<code>get_sync_client()</code>","text":"<p>Returns an underlying raw synchronous client instance, if applicable.</p> <p>Returns:</p> Type Description <code>Any</code> <p>The raw provider-specific sync client instance (e.g., <code>redis.Redis</code>).</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If a synchronous client is not available or supported by the provider.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.hdel","title":"<code>hdel(key, fields)</code>  <code>async</code>","text":"<p>Removes the specified fields from the hash stored at key.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.hgetall","title":"<code>hgetall(key)</code>  <code>async</code>","text":"<p>Returns all fields and values of the hash stored at key.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.hset","title":"<code>hset(key, field, value)</code>  <code>async</code>","text":"<p>Sets field in the hash stored at key to value.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.increment","title":"<code>increment(key, amount=1)</code>  <code>async</code>","text":"<p>Atomically increments the integer value of a key by a given amount.</p> <p>If the key does not exist, it is set to 0 before performing the operation.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the item to increment.</p> required <code>amount</code> <code>int</code> <p>The amount to increment by. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>int</code> <p>The value of the key after the increment.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.is_available","title":"<code>is_available()</code>  <code>async</code>","text":"<p>Checks if the provider is connected and ready to accept operations.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the service is ready, False otherwise.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.set","title":"<code>set(key, value, ttl=None, nx=False)</code>  <code>async</code>","text":"<p>Sets a key-value pair, with an optional TTL.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the item to set.</p> required <code>value</code> <code>Any</code> <p>The value to store. It will be serialized by the provider.</p> required <code>ttl</code> <code>Optional[int]</code> <p>Optional time-to-live for the key in seconds.</p> <code>None</code> <code>nx</code> <code>bool</code> <p>If True, set the key only if it does not already exist.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the operation was successful.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.set_many","title":"<code>set_many(mapping, ttl=None, nx=False)</code>  <code>async</code>","text":"<p>Sets multiple key-value pairs in a single operation.</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>Dict[str, Any]</code> <p>A dictionary of key-value pairs to store.</p> required <code>ttl</code> <code>Optional[int]</code> <p>Optional TTL in seconds applied to all keys in this batch.</p> <code>None</code> <code>nx</code> <code>bool</code> <p>If True, only sets keys that do not exist (atomic per key).</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, bool]</code> <p>A dictionary mapping each key to a boolean indicating success.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.zadd","title":"<code>zadd(key, mapping)</code>  <code>async</code>","text":"<p>Adds one or more members with scores to a sorted set.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>mapping</code> <code>Mapping[str, float]</code> <p>A dictionary of member-score pairs to add.</p> required"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.zpopbyscore","title":"<code>zpopbyscore(key, max_score)</code>  <code>async</code>","text":"<p>Atomically removes and returns the member with the lowest score.</p> <p>The member returned is the one with the lowest score that is less than or equal to the <code>max_score</code>.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>max_score</code> <code>float</code> <p>The maximum score to consider for popping a member.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The member name as a string, or None if no qualifying members exist.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.zrangebyscore","title":"<code>zrangebyscore(key, min_score, max_score)</code>  <code>async</code>","text":"<p>Returns members of a sorted set within a score range.</p> <p>Retrieves all members in a sorted set with scores between <code>min_score</code> and <code>max_score</code> (inclusive).</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>min_score</code> <code>float</code> <p>The minimum score of the range.</p> required <code>max_score</code> <code>float</code> <p>The maximum score of the range.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of member names.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.zrem","title":"<code>zrem(key, members)</code>  <code>async</code>","text":"<p>Removes one or more members from a sorted set.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>members</code> <code>list[str]</code> <p>A list of members to remove.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of members that were removed from the set.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.factory.KVStoreFactory","title":"<code>nala.athomic.database.kvstore.factory.KVStoreFactory</code>","text":"<p>               Bases: <code>FactoryProtocol</code></p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.factory.KVStoreFactory.create","title":"<code>create(settings)</code>  <code>classmethod</code>","text":"<p>Factory that creates a KVStore client based on the provided KVStoreSettings. Applies wrappers as defined INSIDE the kv_config.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.providers.redis.client.RedisKVClient","title":"<code>nala.athomic.database.kvstore.providers.redis.client.RedisKVClient</code>","text":"<p>               Bases: <code>BaseKVStore</code>, <code>CredentialResolve</code></p> <p>A concrete KVStoreProvider implementation for Redis using the aioredis library.</p> <p>This class manages the asynchronous connection lifecycle, handles low-level Redis commands (including atomic operations via Lua scripts), and securely resolves connection credentials.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <code>RedisSettings</code> <p>The Redis-specific configuration settings.</p> <code>_client</code> <code>Redis | None</code> <p>The underlying asynchronous Redis client instance.</p> <code>_claim_script_sha</code> <code>Optional[str]</code> <p>The SHA hash of the Lua script used for ZPOPBYSCORE.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.providers.redis.client.RedisKVClient.__init__","title":"<code>__init__(settings=None, serializer=None)</code>","text":"<p>Initializes the RedisKVClient.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[KVStoreSettings]</code> <p>The configuration for this KV store instance.</p> <code>None</code> <code>serializer</code> <code>Optional[SerializerProtocol]</code> <p>The serializer used for values.</p> <code>None</code>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.providers.redis.client.RedisKVClient.get_final_client","title":"<code>get_final_client()</code>  <code>async</code>","text":"<p>Returns the raw asynchronous Redis client instance (Public Contract).</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.providers.redis.client.RedisKVClient.get_sync_client","title":"<code>get_sync_client()</code>","text":"<p>Creates and returns a NEW instance of the synchronous Redis client (Public Contract).</p> <p>This instance is not managed by the provider's lifecycle.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the Redis URI is not available for connection string construction.</p> <code>RuntimeError</code> <p>If the synchronous client creation fails.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.key_resolver_kv_client.KeyResolvingKVClient","title":"<code>nala.athomic.database.kvstore.wrappers.key_resolver_kv_client.KeyResolvingKVClient</code>","text":"<p>               Bases: <code>WrapperBase</code></p> <p>A decorator (Wrapper) for the KVStore client that dynamically resolves keys using a ContextKeyGenerator.</p> <p>This wrapper applies a contextual prefix (e.g., namespace, tenant ID, user ID) to the logical key provided by the caller before forwarding the operation to the underlying KV store. This is essential for supporting multi-tenancy and avoiding key collisions in shared storage systems.</p> <p>Attributes:</p> Name Type Description <code>resolver</code> <code>ContextKeyGenerator</code> <p>The utility used to generate context-aware keys.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.key_resolver_kv_client.KeyResolvingKVClient.__init__","title":"<code>__init__(client, settings=None, wrapper_settings=None, context_settings=None)</code>","text":"<p>Initializes the KeyResolvingKVClient and sets up the ContextKeyGenerator.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>KVStoreProtocol</code> <p>The underlying KVStore client being wrapped.</p> required <code>settings</code> <code>Optional[KVStoreSettings]</code> <p>The global KVStore settings.</p> <code>None</code> <code>wrapper_settings</code> <code>Optional[KeyResolvingWrapperSettings]</code> <p>Specific settings for this wrapper.</p> <code>None</code> <code>context_settings</code> <code>Optional[ContextSettings]</code> <p>Optional settings for the key generator context.</p> <code>None</code>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.key_resolver_kv_client.KeyResolvingKVClient.delete","title":"<code>delete(key)</code>  <code>async</code>","text":"<p>Deletes a key after resolving the contextual key.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.key_resolver_kv_client.KeyResolvingKVClient.delete_many","title":"<code>delete_many(keys)</code>  <code>async</code>","text":"<p>Deletes multiple keys after resolving their contextual keys.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>List[str]</code> <p>The list of logical keys to delete.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of keys successfully deleted.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.key_resolver_kv_client.KeyResolvingKVClient.exists","title":"<code>exists(key)</code>  <code>async</code>","text":"<p>Checks for key existence after resolving the contextual key.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.key_resolver_kv_client.KeyResolvingKVClient.get","title":"<code>get(key)</code>  <code>async</code>","text":"<p>Retrieves a value after resolving the contextual key.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.key_resolver_kv_client.KeyResolvingKVClient.hdel","title":"<code>hdel(key, fields)</code>  <code>async</code>","text":"<p>Delegates the hdel operation after resolving the contextual key.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.key_resolver_kv_client.KeyResolvingKVClient.hgetall","title":"<code>hgetall(key)</code>  <code>async</code>","text":"<p>Delegates the hgetall operation after resolving the contextual key.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.key_resolver_kv_client.KeyResolvingKVClient.hset","title":"<code>hset(key, field, value)</code>  <code>async</code>","text":"<p>Delegates the hset operation after resolving the contextual key.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.key_resolver_kv_client.KeyResolvingKVClient.set","title":"<code>set(key, value, ttl=None, nx=False)</code>  <code>async</code>","text":"<p>Sets a value after resolving the contextual key.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.key_resolver_kv_client.KeyResolvingKVClient.set_many","title":"<code>set_many(mapping, ttl=None, nx=False)</code>  <code>async</code>","text":"<p>Sets multiple values after resolving their contextual keys.</p> <p>It resolves all keys in the batch, forwards the operation to the client, and then maps the results back to the original keys so the caller remains agnostic of the prefixes.</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>Dict[str, Any]</code> <p>A dictionary of logical key-value pairs.</p> required <code>ttl</code> <code>Optional[int]</code> <p>Time-To-Live in seconds.</p> <code>None</code> <code>nx</code> <code>bool</code> <p>If True, set only if the key does not exist.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, bool]</code> <p>Dict[str, bool]: A map of logical keys to success status.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.key_resolver_kv_client.KeyResolvingKVClient.zadd","title":"<code>zadd(key, mapping)</code>  <code>async</code>","text":"<p>Delegates the zadd operation after resolving the contextual key.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.key_resolver_kv_client.KeyResolvingKVClient.zpopbyscore","title":"<code>zpopbyscore(key, max_score)</code>  <code>async</code>","text":"<p>Delegates the zpopbyscore operation after resolving the contextual key.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.key_resolver_kv_client.KeyResolvingKVClient.zrangebyscore","title":"<code>zrangebyscore(key, min_score, max_score)</code>  <code>async</code>","text":"<p>Delegates the zrangebyscore operation after resolving the contextual key.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.key_resolver_kv_client.KeyResolvingKVClient.zrem","title":"<code>zrem(key, members)</code>  <code>async</code>","text":"<p>Delegates the zrem operation after resolving the contextual key.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.default_ttl_kv_client.DefaultTTLKvClient","title":"<code>nala.athomic.database.kvstore.wrappers.default_ttl_kv_client.DefaultTTLKvClient</code>","text":"<p>               Bases: <code>WrapperBase</code></p> <p>A decorator (Wrapper) for the KVStore client that automatically applies a default Time-To-Live (TTL).</p> <p>This class intercepts the <code>set</code> operation and injects a configured default TTL if no explicit TTL is provided by the caller. It ensures that keys are automatically expired, preventing cache overflow and maintaining consistency.</p> <p>Attributes:</p> Name Type Description <code>default_ttl</code> <code>Optional[int]</code> <p>The default TTL in seconds extracted from configuration, or None.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.default_ttl_kv_client.DefaultTTLKvClient.__init__","title":"<code>__init__(client, settings=None, wrapper_settings=None)</code>","text":"<p>Initializes the wrapper, resolving the configured default TTL value.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>KVStoreProtocol</code> <p>The underlying KVStore client being wrapped.</p> required <code>settings</code> <code>Optional[KVStoreSettings]</code> <p>Optional global KVStore settings.</p> <code>None</code> <code>wrapper_settings</code> <code>Optional[DefaultTTLWrapperSettings]</code> <p>Specific settings for this wrapper instance.</p> <code>None</code>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.default_ttl_kv_client.DefaultTTLKvClient.set","title":"<code>set(key, value, ttl=None, nx=False)</code>  <code>async</code>","text":"<p>Sets a key-value pair, applying the default TTL if no explicit TTL is given.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the item to set.</p> required <code>value</code> <code>Any</code> <p>The value to store.</p> required <code>ttl</code> <code>Optional[int]</code> <p>Explicit time-to-live in seconds provided by the caller.</p> <code>None</code> <code>nx</code> <code>bool</code> <p>If True, set the key only if it does not already exist.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the operation was successful.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.default_ttl_kv_client.DefaultTTLKvClient.set_many","title":"<code>set_many(mapping, ttl=None, nx=False)</code>  <code>async</code>","text":"<p>Sets multiple key-value pairs, applying the default TTL if no explicit TTL is given.</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>Dict[str, Any]</code> <p>The dictionary of keys and values.</p> required <code>ttl</code> <code>Optional[int]</code> <p>Explicit time-to-live in seconds for the batch.</p> <code>None</code> <code>nx</code> <code>bool</code> <p>If True, set keys only if they do not exist.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, bool]</code> <p>Dict[str, bool]: The result map from the underlying client.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.default_ttl_kv_client.DefaultTTLKvClient.zadd","title":"<code>zadd(key, mapping)</code>  <code>async</code>","text":"<p>Delegates the zadd operation to the wrapped client.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.default_ttl_kv_client.DefaultTTLKvClient.zpopbyscore","title":"<code>zpopbyscore(key, max_score)</code>  <code>async</code>","text":"<p>Delegates the zpopbyscore operation to the wrapped client.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.default_ttl_kv_client.DefaultTTLKvClient.zrangebyscore","title":"<code>zrangebyscore(key, min_score, max_score)</code>  <code>async</code>","text":"<p>Delegates the zrangebyscore operation to the wrapped client.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.default_ttl_kv_client.DefaultTTLKvClient.zrem","title":"<code>zrem(key, members)</code>  <code>async</code>","text":"<p>Delegates the zrem operation to the wrapped client.</p>"},{"location":"athomic/database/migrations/","title":"Database Migrations","text":""},{"location":"athomic/database/migrations/#overview","title":"Overview","text":"<p>The Database Migrations module provides a programmatic, version-controlled way to manage your database schema and data transformations over time. It is a lightweight but powerful engine designed to be database-agnostic, with a built-in implementation for MongoDB.</p> <p>When the application starts, the <code>MigrationRunner</code> automatically detects and applies any pending migrations, ensuring that your database schema is always in the state required by the current version of the code.</p>"},{"location":"athomic/database/migrations/#how-it-works","title":"How It Works","text":"<p>The migration process is orchestrated by the <code>MigrationRunner</code> service:</p> <ol> <li>Discovery: At startup, the runner inspects the module <code>paths</code> defined in your configuration to find all migration script files (e.g., <code>001_initial.py</code>).</li> <li>State Check: It connects to the database and queries a special history collection (e.g., <code>athomic_migration_status</code>) to determine the revision number of the last migration that was successfully applied.</li> <li>Execution: The runner compares the list of discovered scripts with the database's history. For every script with a <code>revision</code> number greater than the last applied one, it executes the script's <code>upgrade</code> function in sequential order.</li> <li>Recording: After each script is executed successfully, a new record is added to the history collection, marking the new revision as applied.</li> </ol> <p>This entire process is wrapped with observability, providing detailed logs, metrics, and tracing for each migration operation.</p>"},{"location":"athomic/database/migrations/#how-to-create-a-migration-script","title":"How to Create a Migration Script","text":"<p>Creating a new migration is straightforward.</p>"},{"location":"athomic/database/migrations/#1-create-the-file","title":"1. Create the File","text":"<p>In your application's migrations directory (e.g., <code>src/your_app/migrations/versions/</code>), create a new Python file. The filename is important and must follow this format:</p> <p><code>[NNN]_[description].py</code></p> <ul> <li><code>[NNN]</code>: A three-digit, zero-padded revision number (e.g., <code>001</code>, <code>002</code>).</li> <li><code>[description]</code>: A short, descriptive name for the migration (e.g., <code>create_user_indexes</code>).</li> </ul> <p>Example: <code>001_create_user_indexes.py</code></p>"},{"location":"athomic/database/migrations/#2-define-the-script-content","title":"2. Define the Script Content","text":"<p>Each migration script must contain two key elements:</p> <ul> <li>A <code>revision</code> variable holding the integer revision number.</li> <li>An <code>async def upgrade(db)</code> function that receives the native database client instance (for MongoDB, this is a <code>motor.core.AgnosticDatabase</code>).</li> </ul>"},{"location":"athomic/database/migrations/#example-migration-script","title":"Example Migration Script","text":"<p>This example creates a unique index on the <code>users</code> collection.</p> <pre><code># src/your_app/migrations/versions/001_create_user_indexes.py\nfrom motor.core import AgnosticDatabase\nfrom pymongo import ASCENDING, IndexModel\n\n# The sequential revision number for this migration.\nrevision = 1\n\nasync def upgrade(db: AgnosticDatabase) -&gt; None:\n    \"\"\"\n    Applies the migration to create a unique index on the 'email' field\n    of the 'users' collection.\n    \"\"\"\n    users_collection = db.get_collection(\"users\")\n\n    email_index = IndexModel(\n        [(\"email\", ASCENDING)],\n        name=\"user_email_unique_idx\",\n        unique=True\n    )\n\n    await users_collection.create_indexes([email_index])\n\n# (Optional) You can also define an `async def downgrade(db)` function\n# to support reverting the migration, although this is not currently used by the runner.\n</code></pre>"},{"location":"athomic/database/migrations/#configuration","title":"Configuration","text":"<p>To enable and configure the migration runner, you need to add a <code>[database.migrations]</code> section to your <code>settings.toml</code>.</p> <pre><code>[default.database.migrations]\n# A master switch to enable or disable the migration engine.\nenabled = true\n\n  # A dictionary of named database connections to run migrations against.\n  [default.database.migrations.connections.my_app_db]\n  # The migration engine to use.\n  engine = \"mongodb\"\n\n  # The name of the connection defined under '[database.documents]' to use.\n  db_connection_name = \"default_mongo\"\n\n  # The collection/table used to store migration history.\n  version_collection = \"my_app_migrations\"\n\n  # A list of Python module paths where your migration scripts are located.\n  # The runner will search these directories for migration files.\n  paths = [\"my_app.migrations.versions\"]\n</code></pre>"},{"location":"athomic/database/migrations/#api-reference","title":"API Reference","text":""},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.runner.MigrationRunner","title":"<code>nala.athomic.database.migrations.runner.MigrationRunner</code>","text":"<p>Orchestrates the database migration process with distributed locking support.</p> <p>This class is the core engine for applying database migrations. It is database-agnostic and delegates database-specific operations to a configured backend provider.</p> <p>To prevent race conditions in clustered environments (e.g., Kubernetes), this runner utilizes the <code>LockingProtocol</code> to acquire distributed locks for each database connection before attempting to migrate.</p> <p>Attributes:</p> Name Type Description <code>app_settings</code> <p>The root application settings object.</p> <code>migrations_settings</code> <p>The specific configuration for the migration engine.</p> <code>connection_manager</code> <p>The manager for obtaining database providers.</p> <code>locking_provider</code> <p>The provider used for distributed locking (e.g., RedisLock).</p> <code>logger</code> <p>A pre-configured logger instance for the runner.</p> <code>tracer</code> <p>An OpenTelemetry tracer instance for the runner.</p> <code>hostname</code> <p>The hostname of the current pod/machine.</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.runner.MigrationRunner.__init__","title":"<code>__init__(settings, connection_manager, locking_provider=None)</code>","text":"<p>Initializes the MigrationRunner.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>AppSettings</code> <p>The application's root configuration object.</p> required <code>connection_manager</code> <code>ConnectionManager</code> <p>The manager for obtaining database providers.</p> required <code>locking_provider</code> <code>Optional[LockingProtocol]</code> <p>Optional locking provider to enable distributed locking.             If None, migrations run without locking protection.</p> <code>None</code>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.runner.MigrationRunner.upgrade","title":"<code>upgrade()</code>  <code>async</code>","text":"<p>Runs the 'upgrade' process for all configured database connections.</p> <p>Iterates through configured connections and triggers the upgrade process. Uses distributed locking if a provider is available.</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.protocol.MigrationBackendProtocol","title":"<code>nala.athomic.database.migrations.protocol.MigrationBackendProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the contract for a database-specific migration backend.</p> <p>This allows the MigrationRunner to be agnostic of the underlying database. All I/O operations are designed to be asynchronous.</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.protocol.MigrationBackendProtocol.apply_migration","title":"<code>apply_migration(source_name, revision, path)</code>  <code>async</code>","text":"<p>Loads and executes the 'upgrade' function from a migration script.</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.protocol.MigrationBackendProtocol.connect","title":"<code>connect()</code>  <code>async</code>","text":"<p>Initializes the connection to the database.</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.protocol.MigrationBackendProtocol.disconnect","title":"<code>disconnect()</code>  <code>async</code>","text":"<p>Closes the connection to the database.</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.protocol.MigrationBackendProtocol.get_applied_migrations","title":"<code>get_applied_migrations()</code>  <code>async</code>","text":"<p>Gets the latest applied revision number for each migration source.</p> <p>Returns:</p> Name Type Description <code>Dict[str, int]</code> <p>A dictionary mapping a source_name to its latest revision_number.</p> <code>Example</code> <code>Dict[str, int]</code> <p>{\"athomic\": 2, \"my_app_billing\": 1}</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.protocol.MigrationBackendProtocol.record_migration","title":"<code>record_migration(source_name, revision, description)</code>  <code>async</code>","text":"<p>Records that a migration has been successfully applied.</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.protocol.MigrationBackendProtocol.remove_migration_record","title":"<code>remove_migration_record(source_name, revision)</code>  <code>async</code>","text":"<p>Removes the record of a migration after it has been reverted.</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.protocol.MigrationBackendProtocol.revert_migration","title":"<code>revert_migration(source_name, revision, path)</code>  <code>async</code>","text":"<p>Loads and executes the 'downgrade' function from a migration script.</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.providers.mongodb.MongoDbBackend","title":"<code>nala.athomic.database.migrations.providers.mongodb.MongoDbBackend</code>","text":"<p>               Bases: <code>BaseMigrationBackend</code></p> <p>A migration backend for managing schema changes in a MongoDB database.</p> <p>This class implements the <code>MigrationBackendProtocol</code> for MongoDB, handling database connection retrieval, migration script execution, and version history persistence using MongoDB's aggregation and document operations. It supports dependency injection for testing.</p> <p>Attributes:</p> Name Type Description <code>db_connection_name</code> <code>str</code> <p>The logical name of the database connection to use.</p> <code>connection_manager</code> <code>ConnectionManager</code> <p>The manager for obtaining database providers.</p> <code>db_provider</code> <code>DocumentsDatabaseProtocol</code> <p>The active MongoDB client provider.</p> <code>migration_status_model</code> <code>Type[Document]</code> <p>The Beanie Document model used for storing migration history.</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.providers.mongodb.MongoDbBackend.__init__","title":"<code>__init__(settings, version_target, connection_name, connection_manager=None, db_provider=None, migration_status_model=MigrationStatus)</code>","text":"<p>Initializes the MongoDB Migration Backend, resolving dependencies via factories or injection.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>MigrationConnectionSettings</code> <p>Specific configuration for this migration connection.</p> required <code>version_target</code> <code>str</code> <p>The name of the collection to store version history (deprecated in favor of <code>migration_status_model</code>).</p> required <code>connection_name</code> <code>str</code> <p>The logical name of this migration connection.</p> required <code>connection_manager</code> <code>Optional[ConnectionManager]</code> <p>Optional injected connection manager.</p> <code>None</code> <code>db_provider</code> <code>Optional[DocumentsDatabaseProtocol]</code> <p>Optional injected database provider.</p> <code>None</code> <code>migration_status_model</code> <code>Type[Document]</code> <p>The model used for migration history persistence.</p> <code>MigrationStatus</code>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.providers.mongodb.MongoDbBackend.connect","title":"<code>connect()</code>  <code>async</code>","text":"<p>Ensures the MongoDB provider is connected and ready for migration operations.</p> <p>Raises:</p> Type Description <code>MigrationDatabaseError</code> <p>If the configured database connection cannot be retrieved or fails to connect.</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.providers.mongodb.MongoDbBackend.disconnect","title":"<code>disconnect()</code>  <code>async</code>","text":"<p>Performs a non-operation (no-op) as the MongoDB connection lifecycle is managed by the central ConnectionManager.</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.providers.mongodb.MongoDbBackend.get_applied_migrations","title":"<code>get_applied_migrations()</code>  <code>async</code>","text":"<p>Fetches the latest applied revision number for each migration source from the database.</p> <p>Uses a MongoDB aggregation pipeline to find the maximum <code>revision</code> grouped by <code>source_name</code>.</p> <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>Dict[str, int]: A dictionary mapping each <code>source_name</code> to its latest applied <code>revision</code> number.</p> <p>Raises:</p> Type Description <code>MigrationDatabaseError</code> <p>If the database query fails.</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.providers.mongodb.MongoDbBackend.record_migration","title":"<code>record_migration(source_name, revision, description)</code>  <code>async</code>","text":"<p>Records a successfully applied migration in the version tracking collection.</p> <p>Parameters:</p> Name Type Description Default <code>source_name</code> <code>str</code> <p>The source of the migration (e.g., 'athomic', 'app_billing').</p> required <code>revision</code> <code>int</code> <p>The revision number of the applied migration.</p> required <code>description</code> <code>str</code> <p>A brief description of the migration.</p> required <p>Raises:</p> Type Description <code>MigrationDatabaseError</code> <p>If the record insertion fails.</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.providers.mongodb.MongoDbBackend.remove_migration_record","title":"<code>remove_migration_record(source_name, revision)</code>  <code>async</code>","text":"<p>Removes the record of a migration after it has been reverted (downgraded).</p> <p>Parameters:</p> Name Type Description Default <code>source_name</code> <code>str</code> <p>The source of the migration.</p> required <code>revision</code> <code>int</code> <p>The revision number to remove.</p> required <p>Raises:</p> Type Description <code>MigrationDatabaseError</code> <p>If the deletion operation fails.</p>"},{"location":"athomic/database/outbox/","title":"Transactional Outbox Pattern","text":""},{"location":"athomic/database/outbox/#overview","title":"Overview","text":"<p>The Transactional Outbox pattern is a critical resilience mechanism that ensures reliable, at-least-once delivery of events in a distributed system. It solves the classic \"dual-write\" problem, where an application needs to both commit a change to its database and publish a corresponding event to a message broker.</p> <p>Without the outbox pattern, if the database commit succeeds but the message broker publish fails, the system becomes inconsistent. The Athomic outbox implementation guarantees that an event is published if and only if the business transaction that created it is successfully committed.</p>"},{"location":"athomic/database/outbox/#how-it-works","title":"How It Works","text":"<p>The pattern is implemented in two distinct parts: saving the event atomically and publishing it reliably.</p>"},{"location":"athomic/database/outbox/#1-saving-the-event-the-write","title":"1. Saving the Event (The \"Write\")","text":"<p>Instead of publishing an event directly to a message broker, the application saves the event record to a dedicated <code>outbox_events</code> collection within the same database and transaction as the business data.</p> <p>The primary way to achieve this is by using the <code>@publish_on_success</code> decorator on your service methods. This decorator wraps your business logic and ensures that the outbox event is only saved to the database if your function completes without raising an exception.</p>"},{"location":"athomic/database/outbox/#example-usage","title":"Example Usage","text":"<pre><code># In your application's service layer\nfrom nala.athomic.database.outbox import OutboxEventType, publish_on_success\n\nclass DocumentService:\n    @publish_on_success(\n        event_name=\"DocumentCreated\",\n        event_type=OutboxEventType.MESSAGING,\n        destination_topic=\"document.events.v1\",\n        # The payload_mapper receives the result of the decorated function\n        payload_mapper=lambda result, **kwargs: result.model_dump()\n    )\n    async def create_document(self, content: str, user_id: str, db_session) -&gt; Document:\n        \"\"\"\n        Creates a document and ensures a 'DocumentCreated' event is reliably published.\n\n        The `db_session` is passed by the repository and used by the decorator\n        to ensure the document and the outbox event are saved in the same transaction.\n        \"\"\"\n        new_doc = Document(content=content, owner_id=user_id)\n        await self.repository.save(new_doc, session=db_session)\n        return new_doc\n</code></pre>"},{"location":"athomic/database/outbox/#2-publishing-the-event-the-poll","title":"2. Publishing the Event (The \"Poll\")","text":"<p>A separate, continuously running background service called the <code>OutboxPublisher</code> polls the <code>outbox_events</code> collection for new, <code>PENDING</code> events.</p> <p>When the publisher finds pending events, it: 1.  Publishes them to the actual message broker (e.g., Kafka). 2.  Upon successful publication, it updates the event's status in the database to <code>PUBLISHED</code>.</p> <p>This polling mechanism ensures that even if the application crashes immediately after the database transaction commits, the event will eventually be found and published when the service restarts.</p>"},{"location":"athomic/database/outbox/#guaranteed-event-ordering-fifo","title":"Guaranteed Event Ordering (FIFO)","text":"<p>The outbox pattern in Athomic also supports guaranteed First-In, First-Out (FIFO) processing for events that belong to the same logical entity (or \"aggregate\"). This is essential for use cases where the order of events matters (e.g., <code>OrderCreated</code>, <code>OrderUpdated</code>, <code>OrderShipped</code>).</p> <p>To enable ordering, you simply need to provide an <code>aggregate_key</code> when creating the event. The <code>MongoOutboxRepository</code> will then atomically generate a <code>sequence_id</code> for each event within that aggregate. The <code>OutboxPublisher</code> is designed to process events for the same aggregate key in strict sequential order.</p>"},{"location":"athomic/database/outbox/#configuration","title":"Configuration","text":"<p>The <code>OutboxPublisher</code> is configured under the <code>[integration.outbox_publisher]</code> section in your <code>settings.toml</code>. Here you can enable the service, define the polling interval, and configure its storage backend.</p> <pre><code>[default.integration.outbox_publisher]\nenabled = true\npoll_interval_seconds = 5.0\nfetch_limit = 100 # Max number of unordered events to fetch per cycle\n\n  [default.integration.outbox_publisher.storage]\n  backend = \"mongo\"\n  max_attempts = 3\n    [default.integration.outbox_publisher.storage.mongo]\n    collection_name = \"outbox_events\"\n    db_connection_name = \"default_mongo\"\n</code></pre>"},{"location":"athomic/database/outbox/#api-reference","title":"API Reference","text":""},{"location":"athomic/database/outbox/#nala.athomic.integration.outbox.decorators.publish_on_success","title":"<code>nala.athomic.integration.outbox.decorators.publish_on_success(event_name, event_type, destination_topic, payload_mapper, key_mapper=None, include_context=True, storage=None)</code>","text":"<p>Decorator implementing the Transactional Outbox pattern. It ensures that an event is saved to the persistent Outbox storage (and thus will be published later) only if the decorated function completes without raising an exception.</p> <p>The storage operation is intended to be executed within the same database transaction as the main function's work.</p> <p>Parameters:</p> Name Type Description Default <code>event_name</code> <code>str</code> <p>A descriptive name for the domain event (e.g., 'DocumentCreated').</p> required <code>event_type</code> <code>OutboxEventType</code> <p>The type of the event (e.g., Command, Event, Query).</p> required <code>destination_topic</code> <code>Optional[str]</code> <p>The messaging topic where the event will eventually be published.</p> required <code>payload_mapper</code> <code>Callable[..., Dict[str, Any]]</code> <p>A callable that accepts the decorated function's result,             args, and kwargs, and returns the event payload (Dict[str, Any]).</p> required <code>key_mapper</code> <code>Optional[Callable[..., Optional[str]]]</code> <p>An optional callable to generate a message key (str) for         partitioning purposes.</p> <code>None</code> <code>include_context</code> <code>Optional[bool]</code> <p>If True, automatically embeds the current ExecutionContext              into the event payload under the '_nala_context' key. Defaults to True.</p> <code>True</code> <code>storage</code> <code>Optional[OutboxStorageProtocol]</code> <p>An optional pre-configured OutboxStorageProtocol instance. If None,      the OutboxStorageFactory is used to create one.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable[P, Coroutine[Any, R, Any]]</code> <p>The decorated asynchronous wrapper function.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.protocol.OutboxStorageProtocol","title":"<code>nala.athomic.database.outbox.protocol.OutboxStorageProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol defining the storage operations for the Outbox pattern.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.protocol.OutboxStorageProtocol.delete_processed_event","title":"<code>delete_processed_event(event_id)</code>  <code>async</code>","text":"<p>(Optional but recommended for cleanup) Deletes an event that has been successfully published or marked as permanently failed (e.g., after reaching max attempts).</p> <p>Parameters:</p> Name Type Description Default <code>event_id</code> <code>UUID</code> <p>The unique ID (UUID) of the event to delete.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the event was found and deleted, False otherwise.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.protocol.OutboxStorageProtocol.get_distinct_pending_aggregate_keys","title":"<code>get_distinct_pending_aggregate_keys()</code>  <code>async</code>","text":"<p>Retrieves a list of unique aggregate_keys that have events in the PENDING state. This is used by the publisher to shard and process aggregates independently.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of unique aggregate key strings.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.protocol.OutboxStorageProtocol.get_hot_aggregates","title":"<code>get_hot_aggregates(top_n=10)</code>  <code>async</code>","text":"<p>Retrieves the top N aggregate keys with the highest number of pending events.</p> <p>Parameters:</p> Name Type Description Default <code>top_n</code> <code>int</code> <p>The number of hot aggregates to retrieve.</p> <code>10</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list of dictionaries, each containing 'aggregate_key' and 'count'.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.protocol.OutboxStorageProtocol.get_pending_events","title":"<code>get_pending_events(limit=100)</code>  <code>async</code>","text":"<p>Retrieves a batch of events currently in the PENDING state, ordered by creation time (oldest first).</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>The maximum number of events to retrieve.</p> <code>100</code> <p>Returns:</p> Type Description <code>List[OutboxEventData]</code> <p>A list of pending OutboxEventData objects.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.protocol.OutboxStorageProtocol.get_pending_events_for_aggregate","title":"<code>get_pending_events_for_aggregate(aggregate_key)</code>  <code>async</code>","text":"<p>Retrieves all PENDING events for a specific aggregate_key, ordered by sequence_id.</p> <p>Parameters:</p> Name Type Description Default <code>aggregate_key</code> <code>str</code> <p>The aggregate key to fetch events for.</p> required <p>Returns:</p> Type Description <code>List[OutboxEventData]</code> <p>A list of pending OutboxEventData objects for the given aggregate, correctly ordered.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.protocol.OutboxStorageProtocol.mark_event_attempted","title":"<code>mark_event_attempted(event_id)</code>  <code>async</code>","text":"<p>Marks an event as attempted by incrementing its attempt counter and updating the last attempt timestamp.</p> <p>Parameters:</p> Name Type Description Default <code>event_id</code> <code>UUID</code> <p>The unique ID (UUID) of the event to mark.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the event was found and marked, False otherwise.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.protocol.OutboxStorageProtocol.mark_event_failed","title":"<code>mark_event_failed(event_id, error_message)</code>  <code>async</code>","text":"<p>Marks an event as failed, increments the attempt counter, and stores the error message.</p> <p>Parameters:</p> Name Type Description Default <code>event_id</code> <code>UUID</code> <p>The unique ID (UUID) of the event.</p> required <code>error_message</code> <code>str</code> <p>A description of the error encountered during publishing.</p> required <p>Returns:</p> Type Description <code>Optional[int]</code> <p>The new attempt count if the event was found and marked, None otherwise.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.protocol.OutboxStorageProtocol.mark_event_published","title":"<code>mark_event_published(event_id)</code>  <code>async</code>","text":"<p>Marks a specific event as successfully published by updating its status.</p> <p>Parameters:</p> Name Type Description Default <code>event_id</code> <code>UUID</code> <p>The unique ID (UUID) of the event to mark.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the event was found and marked, False otherwise.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.protocol.OutboxStorageProtocol.save_event","title":"<code>save_event(event, db_session=None)</code>  <code>async</code>","text":"<p>Saves an event to the outbox storage within the provided database session/transaction. Supports agnostic usage (messaging, webhooks, tasks...).</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>OutboxEventData</code> <p>An OutboxEventData object containing the event details.</p> required <p>Returns:</p> Type Description <code>OutboxEventData</code> <p>The saved OutboxEventData object.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.domain_models.OutboxEventData","title":"<code>nala.athomic.database.outbox.domain_models.OutboxEventData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A pure domain model for an outbox event.</p> <p>This Pydantic model represents a single event to be processed reliably. It is designed to be independent of any specific database or ORM, containing all necessary data for the Outbox Publisher to process it. It supports both unordered events and ordered (FIFO) events via the <code>aggregate_key</code> and <code>sequence_id</code> fields.</p> <p>Attributes:</p> Name Type Description <code>event_id</code> <code>UUID</code> <p>The unique identifier for this specific event instance.</p> <code>event_name</code> <code>str</code> <p>A human-readable name identifying the event (e.g., \"user.created\").</p> <code>event_type</code> <code>Optional[OutboxEventType]</code> <p>The category of the event, determining how it's handled.</p> <code>payload</code> <code>Dict[str, Any]</code> <p>The actual data or body of the event.</p> <code>status</code> <code>OutboxEventStatus</code> <p>The current processing status of the event.</p> <code>destination_topic</code> <code>Optional[str]</code> <p>The target topic or queue for 'messaging' type events.</p> <code>message_key</code> <code>Optional[str]</code> <p>The partitioning key for 'messaging' type events.</p> <code>aggregate_key</code> <code>Optional[str]</code> <p>The identifier used to group events to ensure ordered processing. All events with the same <code>aggregate_key</code> are processed in strict FIFO order.</p> <code>sequence_id</code> <code>Optional[int]</code> <p>A monotonically increasing number for events within the same <code>aggregate_key</code> to enforce ordering.</p> <code>attempts</code> <code>int</code> <p>The number of times processing has been attempted for this event.</p> <code>last_attempt_at</code> <code>Optional[datetime]</code> <p>The timestamp of the last processing attempt.</p> <code>last_error</code> <code>Optional[str]</code> <p>The error message from the last failed attempt.</p> <code>created_at</code> <code>datetime</code> <p>The UTC timestamp of when the event was created.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.mongo.mongo_outbox_repository.MongoOutboxRepository","title":"<code>nala.athomic.database.outbox.mongo.mongo_outbox_repository.MongoOutboxRepository</code>","text":"<p>               Bases: <code>OutboxStorageProtocol</code></p> <p>A MongoDB-backed repository for storing and managing outbox events.</p> <p>This class provides a concrete implementation of the <code>OutboxStorageProtocol</code> using MongoDB as the persistence layer. It leverages <code>beanie</code> for object-document mapping and uses atomic operations and aggregation pipelines for efficient and reliable event handling.</p> <p>Attributes:</p> Name Type Description <code>storage_settings</code> <p>Configuration specific to the outbox storage.</p> <code>db_provider</code> <p>The database client provider for MongoDB connections.</p> <code>logger</code> <p>A pre-configured logger instance.</p> <code>service_name</code> <p>The identifier for this service in observability systems.</p> <code>tracer</code> <p>An OpenTelemetry tracer instance.</p> <code>sequence_generator</code> <p>A component for generating sequential IDs for ordered events.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.mongo.mongo_outbox_repository.MongoOutboxRepository.__init__","title":"<code>__init__(storage_settings=None, db_provider=None, sequence_generator=None)</code>","text":"<p>Initializes the MongoOutboxRepository.</p> <p>Parameters:</p> Name Type Description Default <code>storage_settings</code> <code>Optional[OutboxStorageSettings]</code> <p>Configuration for the outbox storage layer.</p> <code>None</code> <code>db_provider</code> <code>Optional[DatabaseClientProtocol]</code> <p>A required database client provider instance.</p> <code>None</code> <code>sequence_generator</code> <code>Optional[SequenceGeneratorProtocol]</code> <p>An optional custom sequence generator.</p> <code>None</code>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.mongo.mongo_outbox_repository.MongoOutboxRepository.delete_processed_event","title":"<code>delete_processed_event(event_id)</code>  <code>async</code>","text":"<p>Deletes a processed event from the collection for cleanup purposes.</p> <p>Parameters:</p> Name Type Description Default <code>event_id</code> <code>UUID</code> <p>The unique ID of the event to delete.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the event was found and deleted, False otherwise.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.mongo.mongo_outbox_repository.MongoOutboxRepository.get_distinct_pending_aggregate_keys","title":"<code>get_distinct_pending_aggregate_keys()</code>  <code>async</code>","text":"<p>Finds all unique aggregate keys that have pending events.</p> <p>Uses MongoDB's <code>distinct</code> command to efficiently get a list of all <code>aggregate_key</code> values that have events in the 'PENDING' state. This is used by the publisher to shard the processing workload.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of unique aggregate key strings.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.mongo.mongo_outbox_repository.MongoOutboxRepository.get_hot_aggregates","title":"<code>get_hot_aggregates(top_n=10)</code>  <code>async</code>","text":"<p>Finds the top N aggregate keys with the most pending events.</p> <p>This method executes a MongoDB aggregation pipeline to group events by <code>aggregate_key</code>, count them, sort by the count, and return the top results. This is primarily used for monitoring to identify potential bottlenecks.</p> <p>Parameters:</p> Name Type Description Default <code>top_n</code> <code>int</code> <p>The number of hot aggregates to retrieve.</p> <code>10</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list of dictionaries, each containing 'aggregate_key' and 'count'.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.mongo.mongo_outbox_repository.MongoOutboxRepository.get_pending_events","title":"<code>get_pending_events(limit=100)</code>  <code>async</code>","text":"<p>Retrieves a batch of pending, unordered events.</p> <p>This method queries MongoDB for events that are in the 'PENDING' state and do not have an <code>aggregate_key</code>, sorted by their creation time to ensure older events are processed first.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>The maximum number of events to retrieve.</p> <code>100</code> <p>Returns:</p> Type Description <code>List[OutboxEventData]</code> <p>A list of pending <code>OutboxEventData</code> objects.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.mongo.mongo_outbox_repository.MongoOutboxRepository.get_pending_events_for_aggregate","title":"<code>get_pending_events_for_aggregate(aggregate_key)</code>  <code>async</code>","text":"<p>Retrieves all pending events for a specific aggregate, sorted by sequence.</p> <p>Parameters:</p> Name Type Description Default <code>aggregate_key</code> <code>str</code> <p>The aggregate key to fetch events for.</p> required <p>Returns:</p> Type Description <code>List[OutboxEventData]</code> <p>A list of pending <code>OutboxEventData</code> objects for the given aggregate.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.mongo.mongo_outbox_repository.MongoOutboxRepository.mark_event_attempted","title":"<code>mark_event_attempted(event_id)</code>  <code>async</code>","text":"<p>Increments an event's attempt counter and updates its last attempt timestamp.</p> <p>Parameters:</p> Name Type Description Default <code>event_id</code> <code>UUID</code> <p>The unique ID of the event to update.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the event was found and updated, False otherwise.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.mongo.mongo_outbox_repository.MongoOutboxRepository.mark_event_failed","title":"<code>mark_event_failed(event_id, exception)</code>  <code>async</code>","text":"<p>Marks an event as failed, storing the error and updating its status.</p> <p>This method updates the event with the error message. It checks if the exception is non-retriable or if the maximum attempt count has been reached to decide whether to move the event to the final 'FAILED' state or keep it as 'PENDING' for a future retry.</p> <p>Parameters:</p> Name Type Description Default <code>event_id</code> <code>UUID</code> <p>The unique ID of the event.</p> required <code>exception</code> <code>Exception</code> <p>The exception that caused the failure.</p> required <p>Returns:</p> Type Description <code>Optional[int]</code> <p>The new attempt count if the event was found and marked, None otherwise.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.mongo.mongo_outbox_repository.MongoOutboxRepository.mark_event_published","title":"<code>mark_event_published(event_id)</code>  <code>async</code>","text":"<p>Marks a specific event's status as PUBLISHED.</p> <p>Parameters:</p> Name Type Description Default <code>event_id</code> <code>UUID</code> <p>The unique ID of the event to mark as published.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the event was found and updated, False otherwise.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.mongo.mongo_outbox_repository.MongoOutboxRepository.save_event","title":"<code>save_event(event, db_session=None)</code>  <code>async</code>","text":"<p>Saves an event to the outbox collection within a transaction.</p> <p>If the event includes an <code>aggregate_key</code>, this method first atomically generates a new <code>sequence_id</code> before inserting the event document into the database.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>OutboxEventData</code> <p>The domain model of the event to save.</p> required <code>db_session</code> <code>Optional[ClientSession]</code> <p>An optional MongoDB client session for transactional operations.</p> <code>None</code> <p>Returns:</p> Type Description <code>OutboxEventData</code> <p>The saved event, now potentially populated with a <code>sequence_id</code>.</p>"},{"location":"athomic/database/vector/","title":"Vector Stores","text":""},{"location":"athomic/database/vector/#overview","title":"Overview","text":"<p>The <code>athomic.database.vector</code> module provides a high-level, provider-agnostic abstraction for interacting with Vector Databases. It is the foundation for implementing Semantic Search, Similarity Matching, and Retrieval-Augmented Generation (RAG) features within the Nala framework.</p> <p>By decoupling the application logic from specific vendors (like Qdrant or Weaviate), this module ensures that your AI and search capabilities remain portable, testable, and consistent with the rest of the application ecosystem.</p>"},{"location":"athomic/database/vector/#key-features","title":"Key Features","text":"<ul> <li>Vendor Agnostic: Switch between backends via configuration without changing a single line of business logic.</li> <li>Unified Data Model: Uses standardized DTOs (<code>VectorRecord</code>, <code>SearchResult</code>) to normalize data exchange across different providers.</li> <li>Context-Aware: Automatically handles multi-tenancy by injecting the current <code>tenant_id</code> into storage operations via the <code>ContextAwareVectorStore</code> wrapper.</li> <li>Lifecycle Management: Fully integrated with the application's lifecycle for connection pooling and graceful shutdown.</li> <li>Observability: Automatic instrumentation for tracing and metrics on all vector operations.</li> </ul>"},{"location":"athomic/database/vector/#core-concepts","title":"Core Concepts","text":""},{"location":"athomic/database/vector/#vectorstoreprotocol","title":"<code>VectorStoreProtocol</code>","text":"<p>This is the abstract contract that all vector store providers must implement. It defines the standard asynchronous operations required for vector manipulation:</p> <ul> <li><code>create_collection(name, dimension, ...)</code>: Manages schema/collection creation.</li> <li><code>upsert(collection, records)</code>: Inserts or updates vector records.</li> <li><code>search(collection, vector, ...)</code>: Performs approximate nearest neighbor (ANN) search.</li> <li><code>delete(collection, ids, ...)</code>: Removes records.</li> </ul>"},{"location":"athomic/database/vector/#vectormanager","title":"<code>VectorManager</code>","text":"<p>Similar to the ConnectionManager, the <code>VectorManager</code> orchestrates the lifecycle of vector database connections. It reads the configuration, initializes the appropriate factory, and holds the references to the active clients.</p>"},{"location":"athomic/database/vector/#vectorstorefactory","title":"<code>VectorStoreFactory</code>","text":"<p>The factory responsible for instantiating the concrete provider (e.g., <code>QdrantVectorStore</code>) and dynamically applying any configured wrappers (middleware) to it.</p>"},{"location":"athomic/database/vector/#data-models","title":"Data Models","text":"<p>To ensure consistency, the module does not expose vendor-specific objects (like Qdrant's <code>PointStruct</code>). Instead, it uses internal Pydantic models:</p> <ul> <li><code>VectorRecord</code>: Represents a data point to be stored.<ul> <li><code>id</code>: Unique identifier (UUID or string).</li> <li><code>vector</code>: The dense vector embedding (list of floats).</li> <li><code>payload</code>: A dictionary of metadata associated with the vector.</li> </ul> </li> <li><code>SearchResult</code>: Represents a generic search hit.<ul> <li><code>id</code>: The record ID.</li> <li><code>score</code>: The similarity score.</li> <li><code>payload</code>: The retrieved metadata.</li> </ul> </li> </ul>"},{"location":"athomic/database/vector/#available-providers","title":"Available Providers","text":""},{"location":"athomic/database/vector/#qdrantvectorstore","title":"<code>QdrantVectorStore</code>","text":"<p>The primary provider implementation for Qdrant. It supports: -   High-performance gRPC or HTTP connections. -   Automatic payload indexing for tenant isolation. -   Efficient batching for upserts.</p>"},{"location":"athomic/database/vector/#weaviatevectorstore","title":"<code>WeaviateVectorStore</code>","text":"<p>Implementation for Weaviate, utilizing its schema-based approach and GraphQL-like interface for retrieval.</p>"},{"location":"athomic/database/vector/#usage-example","title":"Usage Example","text":"<p>The following example demonstrates how to store and retrieve embeddings using the <code>memory_service</code> pattern, which leverages the vector store internally.</p> <pre><code>from nala.athomic.database.factory import connection_manager_factory\nfrom nala.athomic.database.vector.types import VectorRecord\n\nasync def index_document_embeddings(doc_id: str, vector: list[float], content: str):\n    # 1. Get the connection manager\n    manager = connection_manager_factory.create()\n\n    # 2. Get the default vector store client\n    # This client is already connected and wrapped (e.g., with context awareness)\n    store = manager.get_vector_store()\n\n    # 3. Create a standardized record\n    record = VectorRecord(\n        id=doc_id,\n        vector=vector,\n        payload={\n            \"content\": content,\n            \"type\": \"knowledge_base\",\n            # Note: tenant_id is NOT manually added here; \n            # the ContextAware wrapper handles it automatically.\n        }\n    )\n\n    # 4. Upsert\n    await store.upsert(collection_name=\"documents\", records=[record])\n\nasync def find_similar(query_vector: list[float]):\n    manager = connection_manager_factory.create()\n    store = manager.get_vector_store()\n\n    # 5. Search\n    results = await store.search(\n        collection_name=\"documents\",\n        query_vector=query_vector,\n        limit=5,\n        # Optional: Filters are passed as generic dicts and translated by the provider\n        filter_criteria={\"type\": \"knowledge_base\"}\n    )\n\n    return results\n</code></pre>"},{"location":"athomic/database/vector/#configuration","title":"Configuration","text":"<p>Vector stores are configured under the <code>[database.vector]</code> section in <code>settings.toml</code>. It follows the standard Connection Group pattern.</p>"},{"location":"athomic/database/vector/#qdrant-example","title":"Qdrant Example","text":"<pre><code>[default.database.vector]\nenabled = true\n# The name of the default connection to use\ndefault_connection_name = \"default_qdrant\"\n\n  # --- Connection Definition ---\n  [default.database.vector.connections.default_qdrant]\n  enabled = true\n  backend = \"qdrant\"\n  connection_name = \"default_qdrant\"\n\n    # --- Provider Specifics ---\n    [default.database.vector.connections.default_qdrant.provider]\n    location = \"http://localhost:6333\"\n    # Secret reference for production security\n    api_key = { path = \"database/qdrant\", key = \"api_key\" } # pragma: allowlist secret\n    prefer_grpc = true\n    timeout = 10.0\n\n    # --- Wrappers ---\n    # Automatically inject tenant_id into payloads/filters\n    [[default.database.vector.connections.default_qdrant.wrappers]]\n    name = \"context_aware\"\n    enabled = true\n</code></pre>"},{"location":"athomic/database/vector/#api-reference","title":"API Reference","text":""},{"location":"athomic/database/vector/#nala.athomic.database.vector.protocol.VectorStoreProtocol","title":"<code>nala.athomic.database.vector.protocol.VectorStoreProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol definition for Vector Database interactions. Enforces a standard API for Collection Management, Upsert, and Search.</p>"},{"location":"athomic/database/vector/#nala.athomic.database.vector.protocol.VectorStoreProtocol.create_collection","title":"<code>create_collection(collection_name, dimension, **kwargs)</code>  <code>async</code>","text":"<p>Creates a new collection or index if it does not exist.</p>"},{"location":"athomic/database/vector/#nala.athomic.database.vector.protocol.VectorStoreProtocol.delete","title":"<code>delete(collection_name, ids, **kwargs)</code>  <code>async</code>","text":"<p>Deletes specific records by their IDs.</p>"},{"location":"athomic/database/vector/#nala.athomic.database.vector.protocol.VectorStoreProtocol.delete_collection","title":"<code>delete_collection(collection_name)</code>  <code>async</code>","text":"<p>Deletes a collection and all associated data.</p>"},{"location":"athomic/database/vector/#nala.athomic.database.vector.protocol.VectorStoreProtocol.search","title":"<code>search(collection_name, query_vector, limit=10, score_threshold=0.0, filter_criteria=None, **kwargs)</code>  <code>async</code>","text":"<p>Performs semantic search using a query vector.</p>"},{"location":"athomic/database/vector/#nala.athomic.database.vector.protocol.VectorStoreProtocol.upsert","title":"<code>upsert(collection_name, records, **kwargs)</code>  <code>async</code>","text":"<p>Inserts or updates vector records in batch.</p>"},{"location":"athomic/database/vector/#nala.athomic.database.vector.manager.VectorManager","title":"<code>nala.athomic.database.vector.manager.VectorManager</code>","text":"<p>               Bases: <code>BaseManager[VectorStoreProtocol, VectorSettings]</code></p> <p>A specialized lifecycle manager for Vector Store connections.</p> <p>It orchestrates the initialization, connection, and graceful shutdown of vector database providers defined in the configuration.</p>"},{"location":"athomic/database/vector/#nala.athomic.database.vector.manager.VectorManager.__init__","title":"<code>__init__(settings)</code>","text":"<p>Initializes the VectorManager.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>DatabaseSettings</code> <p>The root database settings object containing the 'vector'       connection group configuration.</p> required"},{"location":"athomic/database/vector/#nala.athomic.database.vector.factory.VectorStoreFactory","title":"<code>nala.athomic.database.vector.factory.VectorStoreFactory</code>","text":"<p>               Bases: <code>FactoryProtocol</code></p> <p>Factory responsible for creating instances of the configured Vector Store provider. It applies wrappers dynamically based on the configuration defined in 'settings.wrappers'.</p>"},{"location":"athomic/database/vector/#nala.athomic.database.vector.factory.VectorStoreFactory.create","title":"<code>create(settings=None)</code>  <code>classmethod</code>","text":"<p>Creates and returns an instance of the Vector Store provider based on settings.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[VectorSettings]</code> <p>The specific connection settings. If None, loads from global config.</p> <code>None</code> <p>Returns:</p> Type Description <code>VectorStoreProtocol</code> <p>An initialized VectorStoreProtocol, potentially wrapped.</p>"},{"location":"athomic/database/vector/#nala.athomic.database.vector.types.VectorRecord","title":"<code>nala.athomic.database.vector.types.VectorRecord</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a single record to be stored in the vector database. Contains the vector embedding, original content, and metadata.</p>"},{"location":"athomic/database/vector/#nala.athomic.database.vector.providers.qdrant.QdrantVectorStore","title":"<code>nala.athomic.database.vector.providers.qdrant.QdrantVectorStore</code>","text":"<p>               Bases: <code>BaseVectorStore</code></p> <p>Qdrant implementation of the VectorStoreProtocol.</p> <p>Features: - Native Multi-Tenancy via Payload Filtering. - Automatic Payload Indexing for tenant_id. - Bulkhead concurrency control. - Robust API Adaptability.</p>"},{"location":"athomic/integration/config_providers/","title":"Dynamic Configuration Providers","text":""},{"location":"athomic/integration/config_providers/#overview","title":"Overview","text":"<p>The Dynamic Configuration Providers module contains the client-side logic for fetching configuration values from external, centralized sources like HashiCorp Consul's Key-Value (KV) store. This is the engine that powers the Live Configuration feature.</p>"},{"location":"athomic/integration/config_providers/#key-features","title":"Key Features","text":"<ul> <li>Provider-Based: Built on a protocol, allowing for different backends to be supported.</li> <li>Resilient Fallback: Can be configured with a chain of providers. If the primary provider (e.g., Consul) is unavailable, it can fall back to a secondary one.</li> <li>Long Polling: Efficiently watches for changes using long-polling HTTP requests, enabling near real-time configuration updates.</li> </ul>"},{"location":"athomic/integration/config_providers/#how-it-works","title":"How It Works","text":"<p>The <code>ConfigurationProviderFactory</code> creates a singleton instance of the configured provider. The primary implementation, <code>ConsulConfigProvider</code>, uses the shared <code>ConsulClient</code> to interact with the Consul KV API.</p> <p>This provider is primarily consumed by the <code>ConfigWatcherService</code>, which uses its <code>watch_prefix</code> method to monitor for changes and publish update events to the internal event bus.</p>"},{"location":"athomic/integration/config_providers/#api-reference","title":"API Reference","text":""},{"location":"athomic/integration/config_providers/#nala.athomic.integration.config_providers.protocol.ConfigProvidersProtocol","title":"<code>nala.athomic.integration.config_providers.protocol.ConfigProvidersProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the contract for any remote/dynamic configuration provider.</p> <p>Implementations will handle fetching configuration values from external sources like Consul, Vault, or other configuration services.</p>"},{"location":"athomic/integration/config_providers/#nala.athomic.integration.config_providers.protocol.ConfigProvidersProtocol.get","title":"<code>get(key, default=None)</code>  <code>async</code>","text":"<p>Fetches a configuration value for a given key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The identifier of the configuration value to fetch.</p> required <code>default</code> <code>Optional[Any]</code> <p>The default value to return if the key is not found      or if the provider is unavailable.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The configuration value, which could be a string, number, dict, or list.</p>"},{"location":"athomic/integration/config_providers/#nala.athomic.integration.config_providers.protocol.ConfigProvidersProtocol.watch","title":"<code>watch(key, last_known_index=0)</code>  <code>async</code>","text":"<p>Watches a key for changes using long polling.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to watch.</p> required <code>last_known_index</code> <code>int</code> <p>The last known 'ModifyIndex' from Consul for this key.</p> <code>0</code> <p>Returns:</p> Type Description <code>int</code> <p>A tuple containing (new_index, new_value).</p> <code>Any</code> <p>The call will block until there is a change or a timeout occurs.</p>"},{"location":"athomic/integration/config_providers/#nala.athomic.integration.config_providers.protocol.ConfigProvidersProtocol.watch_prefix","title":"<code>watch_prefix(prefix, last_known_index=0)</code>  <code>async</code>","text":"<p>Watches a key prefix for any changes recursively using long polling.</p> <p>Returns:</p> Type Description <code>int</code> <p>A tuple containing (new_index, list_of_changed_items).</p> <code>Optional[List[Dict[str, Any]]]</code> <p>Each item in the list is a dictionary like {'Key': ..., 'Value': ...}.</p>"},{"location":"athomic/integration/config_providers/#nala.athomic.integration.config_providers.factory.ConfigurationProviderFactory","title":"<code>nala.athomic.integration.config_providers.factory.ConfigurationProviderFactory</code>","text":"<p>Factory for the singleton instance of the dynamic configuration provider.</p> <p>This factory is responsible for building and providing a singleton instance of the <code>ConfigProvidersProtocol</code>. It intelligently assembles the provider based on the application's configuration. If a single provider is configured, it creates a direct instance. If multiple providers are configured, it creates a <code>FallbackConfigProvider</code> that wraps them in a resilient chain.</p>"},{"location":"athomic/integration/config_providers/#nala.athomic.integration.config_providers.factory.ConfigurationProviderFactory.clear","title":"<code>clear()</code>  <code>classmethod</code>","text":"<p>Clears the cached singleton instance. Useful for tests.</p>"},{"location":"athomic/integration/config_providers/#nala.athomic.integration.config_providers.factory.ConfigurationProviderFactory.create","title":"<code>create(settings=None)</code>  <code>classmethod</code>","text":"<p>Creates or retrieves the singleton instance of the configuration provider.</p> <p>This method orchestrates the creation of the dynamic configuration provider. It reads the settings, determines if a single provider or a fallback chain is needed, and uses the <code>config_provider_registry</code> to instantiate the appropriate provider class(es). The created instance is cached for subsequent calls.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[ConfigurationSettings]</code> <p>The configuration for the dynamic providers. If None, global settings are used.</p> <code>None</code> <p>Returns:</p> Type Description <code>ConfigProvidersProtocol</code> <p>A fully configured configuration provider instance, which may be</p> <code>ConfigProvidersProtocol</code> <p>a single provider or a <code>FallbackConfigProvider</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no providers are configured or if a configured backend name is not found in the registry.</p>"},{"location":"athomic/integration/config_providers/#nala.athomic.integration.config_providers.providers.consul_provider.ConsulConfigProvider","title":"<code>nala.athomic.integration.config_providers.providers.consul_provider.ConsulConfigProvider</code>","text":"<p>               Bases: <code>ConfigProvidersProtocol</code></p> <p>Fetches configuration values from Consul's Key-Value store.</p> <p>This class implements the <code>ConfigProvidersProtocol</code> to provide dynamic configuration values from a HashiCorp Consul KV backend. It uses the application's central, lifecycle-managed <code>ConsulClient</code> for all interactions with the Consul agent. It supports fetching single keys and long-polling for changes on keys or prefixes.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <code>ConsulSettings</code> <p>The configuration for connecting to the Consul agent.</p>"},{"location":"athomic/integration/config_providers/#nala.athomic.integration.config_providers.providers.consul_provider.ConsulConfigProvider.consul_client","title":"<code>consul_client</code>  <code>property</code>","text":"<p>Lazily resolves and returns the shared ConsulClient instance.</p> <p>This ensures that the client is only fetched when needed and that it's the same instance managed by the application's lifecycle.</p> <p>Returns:</p> Type Description <code>ConsulClient</code> <p>The shared, application-wide Consul client instance.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If a valid <code>ConsulClient</code> cannot be retrieved.</p>"},{"location":"athomic/integration/config_providers/#nala.athomic.integration.config_providers.providers.consul_provider.ConsulConfigProvider.__init__","title":"<code>__init__(settings=None, consul_client=None)</code>","text":"<p>Initializes the ConsulConfigProvider.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[ConsulSettings]</code> <p>The Consul connection settings.</p> <code>None</code> <code>consul_client</code> <code>Optional[ConsulClient]</code> <p>An optional, pre-initialized Consul client, primarily for testing and dependency injection.</p> <code>None</code>"},{"location":"athomic/integration/config_providers/#nala.athomic.integration.config_providers.providers.consul_provider.ConsulConfigProvider.get","title":"<code>get(key, default=None)</code>  <code>async</code>","text":"<p>Fetches a configuration value for a given key from Consul.</p> <p>It safely handles connection issues by returning the default value. It attempts to parse the retrieved value as JSON, falling back to a plain string if parsing fails.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The identifier of the configuration value to fetch.</p> required <code>default</code> <code>Optional[Any]</code> <p>The value to return if the key is not found or if the provider is unavailable.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The parsed configuration value or the provided default.</p>"},{"location":"athomic/integration/config_providers/#nala.athomic.integration.config_providers.providers.consul_provider.ConsulConfigProvider.watch","title":"<code>watch(key, last_known_index=0)</code>  <code>async</code>","text":"<p>Watches a single key in Consul for changes using long-polling.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to watch.</p> required <code>last_known_index</code> <code>int</code> <p>The last <code>ModifyIndex</code> returned by Consul for this key. The request will block until the index changes.</p> <code>0</code> <p>Returns:</p> Type Description <code>int</code> <p>A tuple containing the new index and the new value. The value is</p> <code>Any</code> <p>None if the key was deleted or not found.</p>"},{"location":"athomic/integration/config_providers/#nala.athomic.integration.config_providers.providers.consul_provider.ConsulConfigProvider.watch_prefix","title":"<code>watch_prefix(prefix, last_known_index=0)</code>  <code>async</code>","text":"<p>Watches a key prefix recursively in Consul for changes using long-polling.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>The key prefix (folder) to watch.</p> required <code>last_known_index</code> <code>int</code> <p>The last <code>ModifyIndex</code> returned by Consul for this prefix. The request will block until the index changes.</p> <code>0</code> <p>Returns:</p> Type Description <code>int</code> <p>A tuple containing the new index and a list of all key-value</p> <code>Optional[List[Dict[str, Any]]]</code> <p>data under the prefix.</p>"},{"location":"athomic/integration/consul/","title":"Consul Client","text":""},{"location":"athomic/integration/consul/#overview","title":"Overview","text":"<p>The Consul module provides a centralized, lifecycle-managed client for interacting with a HashiCorp Consul agent. This shared client is a fundamental dependency for several other modules that rely on Consul for their operation, including:</p> <ul> <li>Service Discovery</li> <li>Dynamic Configuration Providers</li> <li>Feature Flags (when using the Consul backend)</li> </ul>"},{"location":"athomic/integration/consul/#how-it-works","title":"How It Works","text":"<p>The <code>ConsulClient</code> is a <code>BaseService</code> that wraps the <code>consul.aio.Consul</code> library. The <code>ConsulClientFactory</code> creates a singleton instance of this client at startup, managing its connection and readiness state.</p> <p>By using a shared singleton, all other modules are guaranteed to use the same connection pool and configuration, making the system efficient and consistent. The client's lifecycle is managed by the main <code>LifecycleManager</code>, ensuring it connects at startup and disconnects gracefully.</p>"},{"location":"athomic/integration/consul/#api-reference","title":"API Reference","text":""},{"location":"athomic/integration/consul/#nala.athomic.integration.consul.client.ConsulClient","title":"<code>nala.athomic.integration.consul.client.ConsulClient</code>","text":"<p>               Bases: <code>BaseService</code></p> <p>An asynchronous, lifecycle-managed client for interacting with Consul.</p> <p>This class wraps the <code>consul.aio.Consul</code> client, integrating it into the Athomic service lifecycle. It handles connection setup, readiness checks by verifying a cluster leader, and graceful shutdown. This centralized client is intended to be used as a singleton by other Athomic components that need to interact with Consul, such as service discovery and dynamic configuration providers.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <code>ConsulSettings</code> <p>The configuration for connecting to the Consul agent.</p> <code>client</code> <code>Optional[Consul]</code> <p>The underlying <code>consul.aio.Consul</code> client instance, available after a successful connection.</p>"},{"location":"athomic/integration/consul/#nala.athomic.integration.consul.client.ConsulClient.__init__","title":"<code>__init__(settings)</code>","text":"<p>Initializes the ConsulClient.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>ConsulSettings</code> <p>The Pydantic model containing Consul connection details.</p> required"},{"location":"athomic/integration/consul/#nala.athomic.integration.consul.factory.ConsulClientFactory","title":"<code>nala.athomic.integration.consul.factory.ConsulClientFactory</code>","text":"<p>A singleton factory to create and provide the <code>ConsulClient</code> instance.</p> <p>This factory manages a single, shared instance of the <code>ConsulClient</code>. It ensures that the entire application uses the same client, preventing redundant connections and centralizing its creation logic.</p>"},{"location":"athomic/integration/consul/#nala.athomic.integration.consul.factory.ConsulClientFactory.clear","title":"<code>clear()</code>  <code>classmethod</code>","text":"<p>Clears the cached singleton instance.</p> <p>This method resets the factory, allowing a new <code>ConsulClient</code> instance to be created on the next <code>create()</code> call. This is essential for ensuring test isolation.</p>"},{"location":"athomic/integration/consul/#nala.athomic.integration.consul.factory.ConsulClientFactory.create","title":"<code>create(settings=None)</code>  <code>classmethod</code>","text":"<p>Creates or retrieves the singleton instance of <code>ConsulClient</code>.</p> <p>On the first call, this method instantiates the client using global or provided settings. Subsequent calls return the cached instance, ensuring a single connection manager is used throughout the application's lifecycle.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[ConsulSettings]</code> <p>Optional Consul settings to use for instantiation, primarily for dependency injection during testing. If None, global settings are used.</p> <code>None</code> <p>Returns:</p> Type Description <code>ConsulClient</code> <p>The singleton <code>ConsulClient</code> instance.</p>"},{"location":"athomic/integration/discovery/","title":"Service Discovery","text":""},{"location":"athomic/integration/discovery/#overview","title":"Overview","text":"<p>The Service Discovery module provides a mechanism for services to find and communicate with each other in a dynamic, distributed environment. Instead of hardcoding IP addresses or hostnames, services register themselves with a central registry (like HashiCorp Consul) and query it to discover the locations of other services.</p> <p>This is a fundamental pattern for building scalable and resilient microservice architectures. Athomic uses this module primarily for the Distributed Workload Sharding pattern.</p>"},{"location":"athomic/integration/discovery/#how-it-works","title":"How It Works","text":"<p>The <code>ServiceDiscoveryProtocol</code> defines the contract for any provider, with <code>register</code>, <code>deregister</code>, and <code>discover</code> methods. The <code>ConsulDiscoveryProvider</code> is the default implementation.</p> <p>At startup, a service instance calls <code>discovery_service.register_self()</code>, which registers its service name, instance ID, and address with Consul. On shutdown, it calls <code>deregister_self()</code>. Other services can then call <code>discovery_service.discover(\"service-name\")</code> to get a list of all healthy, active instances of that service.</p>"},{"location":"athomic/integration/discovery/#api-reference","title":"API Reference","text":""},{"location":"athomic/integration/discovery/#nala.athomic.integration.discovery.protocol.ServiceDiscoveryProtocol","title":"<code>nala.athomic.integration.discovery.protocol.ServiceDiscoveryProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the abstract contract for any service discovery provider.</p> <p>This protocol adheres to the Dependency Inversion Principle (DIP), decoupling service consumers (like the Sharding or HTTP Client modules) from the concrete implementation (e.g., Consul, ZooKeeper).</p>"},{"location":"athomic/integration/discovery/#nala.athomic.integration.discovery.protocol.ServiceDiscoveryProtocol.deregister","title":"<code>deregister(service_id)</code>  <code>async</code>","text":"<p>Removes this service instance from the discovery provider.</p> <p>Parameters:</p> Name Type Description Default <code>service_id</code> <code>str</code> <p>The unique ID of the service instance to deregister.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if deregistration was successful.</p>"},{"location":"athomic/integration/discovery/#nala.athomic.integration.discovery.protocol.ServiceDiscoveryProtocol.discover","title":"<code>discover(name)</code>  <code>async</code>","text":"<p>Discovers all healthy instances of a service by its logical name.</p> <p>The provider should only return services that have a passing health check.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The logical name of the service to discover.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries, where each dictionary                    represents a healthy service instance (containing address, port, etc.).</p>"},{"location":"athomic/integration/discovery/#nala.athomic.integration.discovery.protocol.ServiceDiscoveryProtocol.register","title":"<code>register(name, service_id, port, address=None, tags=None, health_check=None)</code>  <code>async</code>","text":"<p>Registers the current service instance with the discovery provider.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The logical name of the service (e.g., \"user-service\").</p> required <code>service_id</code> <code>str</code> <p>A unique ID for this specific instance of the service.</p> required <code>port</code> <code>int</code> <p>The port on which the service is running.</p> required <code>address</code> <code>Optional[str]</code> <p>The address of the service. If None, the agent's address is used.</p> <code>None</code> <code>tags</code> <code>Optional[List[str]]</code> <p>A list of tags to associate with the service.</p> <code>None</code> <code>health_check</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary defining a health check (e.g., HTTP, TCP, TTL).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if registration was successful, False otherwise.</p>"},{"location":"athomic/integration/discovery/#nala.athomic.integration.discovery.providers.consul_provider.ConsulDiscoveryProvider","title":"<code>nala.athomic.integration.discovery.providers.consul_provider.ConsulDiscoveryProvider</code>","text":"<p>               Bases: <code>ServiceDiscoveryProtocol</code></p> <p>Service Discovery implementation using HashiCorp Consul as the backend.</p> <p>This provider fulfills the <code>ServiceDiscoveryProtocol</code> contract by delegating I/O operations (register, deregister, discover) to the central, lifecycle-managed <code>ConsulClient</code> instance. All operations are wrapped for resilience via <code>@cancellable_operation</code>.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <code>DiscoverySettings</code> <p>The service discovery configuration.</p> <code>consul_client</code> <code>ConsulClient</code> <p>The shared, active Consul client instance.</p>"},{"location":"athomic/integration/discovery/#nala.athomic.integration.discovery.providers.consul_provider.ConsulDiscoveryProvider.__init__","title":"<code>__init__(settings=None, consul_client=None)</code>","text":"<p>Initializes the provider, resolving the shared ConsulClient dependency.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[DiscoverySettings]</code> <p>Configuration for the discovery module.</p> <code>None</code> <code>consul_client</code> <code>Optional[ConsulClient]</code> <p>Optional injected Consul client instance for testing.</p> <code>None</code>"},{"location":"athomic/integration/discovery/#nala.athomic.integration.discovery.providers.consul_provider.ConsulDiscoveryProvider.deregister","title":"<code>deregister(service_id)</code>  <code>async</code>","text":"<p>Removes the service instance from the Consul agent.</p> <p>Parameters:</p> Name Type Description Default <code>service_id</code> <code>str</code> <p>The unique ID of the service instance to deregister.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if deregistration was successful.</p>"},{"location":"athomic/integration/discovery/#nala.athomic.integration.discovery.providers.consul_provider.ConsulDiscoveryProvider.discover","title":"<code>discover(name)</code>  <code>async</code>","text":"<p>Discovers all currently healthy instances of a service by its logical name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The logical name of the service to discover.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries, each representing a healthy service instance.</p>"},{"location":"athomic/integration/discovery/#nala.athomic.integration.discovery.providers.consul_provider.ConsulDiscoveryProvider.register","title":"<code>register(name, service_id, port, address=None, tags=None, health_check=None)</code>  <code>async</code>","text":"<p>Registers the current service instance with the Consul agent.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The logical name of the service (e.g., \"user-service\").</p> required <code>service_id</code> <code>str</code> <p>A unique ID for this specific service instance.</p> required <code>port</code> <code>int</code> <p>The port on which the service is running.</p> required <code>address</code> <code>Optional[str]</code> <p>The routable address of the service.</p> <code>None</code> <code>tags</code> <code>Optional[List[str]]</code> <p>A list of tags for categorization.</p> <code>None</code> <code>health_check</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary defining a health check.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if registration was successful, False otherwise.</p>"},{"location":"athomic/integration/openlineage/","title":"OpenLineage Integration","text":""},{"location":"athomic/integration/openlineage/#overview","title":"Overview","text":"<p>The OpenLineage integration module provides a concrete storage backend for the Data Lineage system. It is responsible for sending structured lineage events to an OpenLineage-compatible HTTP API, such as Marquez.</p>"},{"location":"athomic/integration/openlineage/#how-it-works","title":"How It Works","text":"<p>The <code>OpenLineageStore</code> implements the <code>LineageStorageProtocol</code>. When the <code>LineageProcessor</code> has constructed a valid OpenLineage event, it passes it to this store's <code>save</code> method.</p> <p>The store then uses a pre-configured, resilient HTTP Client to send the event as a <code>POST</code> request to the configured OpenLineage collector endpoint. The operation is designed to be fail-safe; if the API call fails, the error is logged, but it will not interrupt the main application flow.</p>"},{"location":"athomic/integration/openlineage/#api-reference","title":"API Reference","text":""},{"location":"athomic/integration/openlineage/#nala.athomic.integration.openlineage.store.OpenLineageStore","title":"<code>nala.athomic.integration.openlineage.store.OpenLineageStore</code>","text":"<p>               Bases: <code>LineageStoreProtocol</code></p> <p>A concrete implementation of the LineageStorageProtocol that publishes lineage events to an external OpenLineage-compatible HTTP API (e.g., Marquez).</p> <p>It leverages Athomic's internal HttpClientFactory to ensure the connection inherits global network policies, resilience, and observability features.</p>"},{"location":"athomic/integration/openlineage/#nala.athomic.integration.openlineage.store.OpenLineageStore.__init__","title":"<code>__init__(settings)</code>","text":"<p>Initializes the store by fetching global client configuration and creating the underlying HTTP client.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>OpenLineageStoreSettings</code> <p>Instance-specific settings for the OpenLineage store,       including endpoint overrides.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the required global configuration block         ('integration.openlineage') is missing.</p>"},{"location":"athomic/integration/openlineage/#nala.athomic.integration.openlineage.store.OpenLineageStore.save","title":"<code>save(event_payload)</code>  <code>async</code>","text":"<p>Sends the pre-formatted lineage event payload to the configured OpenLineage API endpoint via an HTTP POST request.</p> <p>If the HTTP request fails, the error is logged but deliberately not re-raised (swallowed) to prevent lineage errors from blocking the main application flow.</p> <p>Parameters:</p> Name Type Description Default <code>event_payload</code> <code>Dict</code> <p>The fully structured lineage event data dictionary.</p> required"},{"location":"athomic/integration/outbox_publisher/","title":"Outbox Publisher","text":""},{"location":"athomic/integration/outbox_publisher/#overview","title":"Overview","text":"<p>The Outbox Publisher is the second half of the Transactional Outbox Pattern. It is a highly resilient, distributed background service responsible for polling the outbox database table, publishing the events to the message broker, and marking them as processed.</p>"},{"location":"athomic/integration/outbox_publisher/#key-features","title":"Key Features","text":"<ul> <li>Guaranteed Delivery: Continuously polls for new events to ensure they are eventually published.</li> <li>Horizontally Scalable: You can run multiple instances of the publisher service to increase throughput.</li> <li>Distributed Coordination: Leverages other Athomic resilience patterns to coordinate work between instances:<ul> <li>Sharding: To distribute the processing of different <code>aggregate_key</code>s across all active workers.</li> <li>Leasing: To ensure only one worker is processing events for a specific <code>aggregate_key</code> at a time, guaranteeing order.</li> <li>Backpressure: To temporarily stop polling for a key that is causing repeated publishing failures.</li> <li>Backoff: To reduce polling frequency when no events are found.</li> </ul> </li> </ul>"},{"location":"athomic/integration/outbox_publisher/#how-it-works","title":"How It Works","text":"<p>The <code>OutboxPublisher</code> is a <code>BaseService</code> managed by the application's <code>LifecycleManager</code>. In its main run loop, it performs the following steps: 1.  Queries the database for all distinct <code>aggregate_key</code>s with pending events. 2.  Uses the <code>ShardingService</code> to filter that list down to only the keys this specific instance is responsible for. 3.  Uses the <code>BackpressureManager</code> to further filter out any keys that are temporarily throttled. 4.  For each remaining key, it acquires a lease and then processes all pending events for that key in strict sequential order. 5.  It also polls for and processes any unordered (non-aggregate) events.</p>"},{"location":"athomic/integration/outbox_publisher/#api-reference","title":"API Reference","text":""},{"location":"athomic/integration/outbox_publisher/#nala.athomic.integration.outbox.publishers.base_publisher.OutboxPublisherBase","title":"<code>nala.athomic.integration.outbox.publishers.base_publisher.OutboxPublisherBase</code>","text":"<p>               Bases: <code>BaseService</code></p> <p>Base implementation for the Outbox Publisher (Poller) Service.</p> <p>This service is a high-resilience component that polls the Outbox storage, applies sharding, backpressure, and leasing mechanisms to ensure atomic, ordered, and distributed processing of events. It inherits lifecycle management from BaseService.</p>"},{"location":"athomic/integration/outbox_publisher/#nala.athomic.integration.outbox.publishers.base_publisher.OutboxPublisherBase.__init__","title":"<code>__init__(settings=None, storage=None, router=None, lease_manager=None, sharding_manager=None, backpressure_manager=None, connection_manager=None)</code>","text":"<p>Initializes the publisher, establishing all necessary resilience and data access dependencies, prioritizing explicit DI over factory creation.</p>"},{"location":"athomic/integration/tasks/","title":"Background Tasks","text":""},{"location":"athomic/integration/tasks/#overview","title":"Overview","text":"<p>The Tasks module provides a high-level abstraction for enqueuing and executing background tasks asynchronously. It decouples the function call from its execution, allowing you to offload long-running or non-critical operations to a separate pool of workers.</p>"},{"location":"athomic/integration/tasks/#key-features","title":"Key Features","text":"<ul> <li>Simple Decorator API: Convert any <code>async</code> function into a background task with the <code>@task</code> decorator.</li> <li>Multiple Backends: Supports <code>Taskiq</code> and <code>RQ</code> (Redis Queue) as distributed backends, as well as a local in-memory backend for testing.</li> <li>Automatic Context Propagation: The <code>ExecutionContext</code> (<code>trace_id</code>, <code>tenant_id</code>, etc.) from the caller is automatically captured and restored on the worker, ensuring seamless tracing and multi-tenancy.</li> <li>Resilient Fallback Chain: Configure a chain of providers (e.g., try <code>Taskiq</code> first, fall back to <code>RQ</code> if it's down) for high availability.</li> </ul>"},{"location":"athomic/integration/tasks/#how-to-define-and-enqueue-a-task","title":"How to Define and Enqueue a Task","text":"<pre><code># In your_app/tasks.py\nfrom nala.athomic.integration.tasks import task\n\n@task(queue=\"emails\")\nasync def send_welcome_email(user_id: str, email: str):\n    # This function will be executed by a worker process.\n    # The context from the original API request will be available here.\n    await email_service.send(to=email, ...)\n\n# In your API endpoint or service\nfrom your_app.tasks import send_welcome_email\n\nasync def create_user(user_data: dict):\n    # Instead of calling the function directly...\n    # send_welcome_email(user.id, user.email)\n\n    # ...you enqueue it for background execution using .delay()\n    await send_welcome_email.delay(user_id=user.id, email=user.email)\n</code></pre>"},{"location":"athomic/integration/tasks/#api-reference","title":"API Reference","text":""},{"location":"athomic/integration/tasks/#nala.athomic.integration.tasks.decorator.task","title":"<code>nala.athomic.integration.tasks.decorator.task(queue=None)</code>","text":"<p>Decorator used to register a standard asynchronous function as a background task.</p> <p>The decorated function is wrapped in a TaskWrapper, giving it the <code>.delay()</code> method for asynchronous execution scheduling.</p> <p>Parameters:</p> Name Type Description Default <code>queue</code> <code>Optional[str]</code> <p>Optional name of the queue where the task should be placed.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable[[Callable], TaskWrapper]</code> <p>The decorator function that returns a TaskWrapper instance.</p>"},{"location":"athomic/integration/tasks/#nala.athomic.integration.tasks.protocol.TaskBrokerProtocol","title":"<code>nala.athomic.integration.tasks.protocol.TaskBrokerProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the standard, agnostic interface (contract) for integrating with background task queue systems.</p> <p>This protocol serves as an abstraction layer (DIP) over concrete backends like RQ, Taskiq, or internal local executors, decoupling application logic from the task execution infrastructure.</p>"},{"location":"athomic/integration/tasks/#nala.athomic.integration.tasks.protocol.TaskBrokerProtocol.enqueue_task","title":"<code>enqueue_task(task_name, *args, **kwargs)</code>  <code>async</code>","text":"<p>Enqueues a task for eventual asynchronous execution by a dedicated worker process.</p> <p>Parameters:</p> Name Type Description Default <code>task_name</code> <code>str</code> <p>A string identifier for the task function        (e.g., 'send_welcome_email'). The format depends on the        specific task broker backend (e.g., function path, registered name).</p> required <code>*args</code> <code>Any</code> <p>Positional arguments to be passed to the task function.    These arguments must be safely serializable by the backend broker.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to be passed to the task function.       Must also be safely serializable.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>An optional string representing the unique ID assigned to the enqueued</p> <code>Optional[str]</code> <p>task by the backend (if available). Returns None if the backend</p> <code>Optional[str]</code> <p>does not provide an ID or enqueues silently.</p> <p>Raises:</p> Type Description <code>TaskEnqueueError</code> <p>If the system fails to communicate with the broker               or if a critical serialization failure prevents               the task from being successfully placed in the queue.</p>"},{"location":"athomic/integration/tasks/#nala.athomic.integration.tasks.worker.run_task_with_context","title":"<code>nala.athomic.integration.tasks.worker.run_task_with_context(func)</code>","text":"<p>A decorator wrapper applied to task functions on the worker side.</p> <p>This wrapper performs the critical function of: 1. Extracting the serialized execution context from task arguments. 2. Restoring the context (tracing IDs, tenant info, etc.) for the worker's thread/async loop. 3. Applying tracing and metrics before and after task execution. 4. Handling exceptions and logging status.</p>"},{"location":"athomic/integration/messaging/","title":"Messaging (Producers &amp; Consumers)","text":""},{"location":"athomic/integration/messaging/#overview","title":"Overview","text":"<p>The Messaging module provides the core abstractions and implementations for producing and consuming messages from external, distributed message brokers like Apache Kafka. It is designed for durable, reliable, and observable inter-service communication, forming the backbone of an event-driven microservices architecture.</p> <p>Reminder: For intra-service communication (within a single service), use the lightweight Internal Event Bus.</p>"},{"location":"athomic/integration/messaging/#key-features","title":"Key Features","text":"<ul> <li>Producer/Consumer Protocols: Built on clean <code>ProducerProtocol</code> and <code>ConsumerProtocol</code> interfaces, decoupling your business logic from the specific message broker technology.</li> <li>Decorator-Based Consumers: Define your message consumers declaratively and elegantly with a simple <code>@subscribe_to</code> decorator, keeping your code clean and focused.</li> <li>Extensible Payload Pipeline: Leverages the Payload Processing Pipeline for transparent serialization, encryption, and compression of messages.</li> <li>Built-in Resilience: Deep integration with resilience patterns like Retry/DLQ and the Transactional Outbox to ensure messages are not lost.</li> <li>Full Observability: Every message produced and consumed is automatically instrumented with distributed tracing, detailed Prometheus metrics, and data lineage events.</li> </ul>"},{"location":"athomic/integration/messaging/#how-it-works","title":"How It Works","text":"<ul> <li> <p>Producers: You obtain a singleton <code>Producer</code> instance from the <code>ProducerFactory</code>. When you call <code>producer.publish()</code>, the message passes through the configured payload processing pipeline (e.g., serialize, encrypt) before being sent to the broker by a backend-specific provider like <code>KafkaProducer</code>.</p> </li> <li> <p>Consumers: At application startup, the <code>ConsumerManager</code> discovers all functions that have been decorated with <code>@subscribe_to</code>. For each one, it uses a <code>ConsumerFactory</code> to build a complete consumer service, assembling all necessary components like the message processor, DLQ handler, and the backend-specific client (e.g., <code>KafkaConsumer</code>). These services are then managed by the application's main lifecycle.</p> </li> </ul>"},{"location":"athomic/integration/messaging/#deeper-dive","title":"Deeper Dive","text":"<p>Explore the specific components of the messaging system in more detail:</p> <ul> <li>Producer Guide: Learn how to publish messages and use advanced features like delayed publishing.</li> <li>Consumer Guide: Learn how to write consumer handlers using the <code>@subscribe_to</code> decorator.</li> <li>Retry &amp; DLQ: Understand the automatic retry mechanism and the Dead Letter Queue for handling failed messages.</li> <li>Delayed Messages: Learn how to publish messages that should only be processed after a delay.</li> <li>Context Propagation: See how tracing and multi-tenancy context flows seamlessly across the message bus.</li> </ul>"},{"location":"athomic/integration/messaging/#api-reference","title":"API Reference","text":""},{"location":"athomic/integration/messaging/#nala.athomic.integration.messaging.decorators.subscribe_to","title":"<code>nala.athomic.integration.messaging.decorators.subscribe_to(topic, *, target_type=None, group_id=None, priority=MessagePriority.NORMAL, concurrency=None, batch_size=None, fetch_timeout_ms=None, fetch_max_linger_ms=None, fetch_max_bytes=None, connection_name=None, idempotency_settings=None, idempotency_key_resolver=None, enable_lineage=True)</code>","text":"<p>This function collects the configuration metadata and registers it with the global <code>decorated_consumer_registry</code>.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>Union[str, List[str]]</code> <p>The topic name (str) or a list of topic names.</p> required <code>target_type</code> <code>Optional[Type[Any]]</code> <p>Optional Pydantic model or class used to deserialize.</p> <code>None</code> <code>group_id</code> <code>Optional[str]</code> <p>Optional consumer group identifier.</p> <code>None</code> <code>priority</code> <code>MessagePriority</code> <p>The priority queue this handler will consume from.</p> <code>NORMAL</code> <code>concurrency</code> <code>Optional[int]</code> <p>The maximum number of messages processed concurrently.</p> <code>None</code> <code>batch_size</code> <code>Optional[int]</code> <p>If &gt; 0, enables \"true batch\" processing mode.</p> <code>None</code> <code>fetch_timeout_ms</code> <code>Optional[int]</code> <p>Optional override for batch fetch timeout.</p> <code>None</code> <code>fetch_max_linger_ms</code> <code>Optional[int]</code> <p>Optional override for batch linger time.</p> <code>None</code> <code>fetch_max_bytes</code> <code>Optional[int]</code> <p>Optional override for batch size in bytes.</p> <code>None</code> <code>connection_name</code> <code>Optional[str]</code> <p>The specific messaging connection.</p> <code>None</code> <code>idempotency_settings</code> <code>Optional[IdempotencySettings]</code> <p>Optional configuration for idempotency.</p> <code>None</code> <code>idempotency_key_resolver</code> <code>Optional[Callable[..., str]]</code> <p>Optional function to resolve idempotency key.</p> <code>None</code> <code>enable_lineage</code> <code>bool</code> <p>If False, disables automatic lineage tracking for this consumer.             Use this for internal infrastructure consumers (e.g., Lineage Collector).</p> <code>True</code> <p>Returns:</p> Type Description <code>Callable</code> <p>The decorator function.</p>"},{"location":"athomic/integration/messaging/#nala.athomic.integration.messaging.producers.factory.ProducerFactory","title":"<code>nala.athomic.integration.messaging.producers.factory.ProducerFactory</code>","text":"<p>A Factory class that creates concrete Producer instances.</p> <p>This factory is responsible for the low-level assembly of a single producer instance. It resolves all necessary dependencies, such as the payload processor and publishing strategies, and injects them into the concrete producer class retrieved from the <code>messaging_producer_registry</code>.</p> <p>This factory is stateless and creates a new instance on every call.</p>"},{"location":"athomic/integration/messaging/#nala.athomic.integration.messaging.producers.factory.ProducerFactory.clear","title":"<code>clear()</code>  <code>classmethod</code>","text":"<p>Clears any potential class-level caches (if any were used). This method is kept for compatibility but does nothing as the factory is now stateless.</p>"},{"location":"athomic/integration/messaging/#nala.athomic.integration.messaging.producers.factory.ProducerFactory.create","title":"<code>create(settings)</code>  <code>staticmethod</code>","text":"<p>Creates a new producer instance for a specific connection.</p> <p>This is the primary public interface for obtaining a new producer. It handles dependency resolution and instance creation based on the provided settings. The <code>ProducerManager</code> (a BaseManager) uses this factory to build all producer instances.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>MessagingSettings</code> <p>The fully resolved messaging settings for a specific connection. This object must contain the <code>connection_name</code>.</p> required <p>Returns:</p> Type Description <code>BaseProducer</code> <p>A new, fully configured and initialized producer instance.</p>"},{"location":"athomic/integration/messaging/#nala.athomic.integration.messaging.consumers.factory.ConsumerFactory","title":"<code>nala.athomic.integration.messaging.consumers.factory.ConsumerFactory</code>","text":"<p>Builds a fully configured consumer instance by assembling components retrieved from registries based on configuration.</p> <p>This factory acts as the main assembler for the messaging consumer subsystem. It adheres to the Open/Closed Principle (OCP) by retrieving concrete classes (Consumer, Strategies) from registries rather than using explicit conditional logic (e.g., if-elif-else on backend type).</p>"},{"location":"athomic/integration/messaging/#nala.athomic.integration.messaging.consumers.factory.ConsumerFactory.create","title":"<code>create(settings, handler_config, connection_manager=None, serializer=None, dlq_handler=None)</code>  <code>staticmethod</code>","text":"<p>Assembles and returns a fully configured concrete consumer instance.</p> <p>The assembly process involves resolving the Consumer class, the DLQ Handler, the Serializer, the Consumption Strategy, and the Message Processor before injecting them into the final BaseConsumer implementation.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>MessagingSettings</code> <p>The global messaging configuration.</p> required <code>handler_config</code> <code>ConsumerHandlerConfig</code> <p>The configuration for the specific handler (topic, group, batch mode).</p> required <code>connection_manager</code> <code>Optional[ConnectionManager]</code> <p>Optional manager for databases/KV stores, required for ordering logic.</p> <code>None</code> <code>serializer</code> <code>Optional[SerializerProtocol]</code> <p>Optional pre-resolved serializer instance.</p> <code>None</code> <code>dlq_handler</code> <code>Optional[DLQHandler]</code> <p>Optional pre-resolved Dead Letter Queue handler.</p> <code>None</code> <p>Returns:</p> Type Description <code>BaseConsumer</code> <p>A concrete instance of <code>BaseConsumer</code> (e.g., KafkaConsumer).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If ordering is enabled but <code>connection_manager</code> is not provided or connected.</p> <code>KeyError</code> <p>If the specified backend or strategy creator is not found in the registries.</p>"},{"location":"athomic/integration/messaging/advanced/","title":"Advanced Consumer Configuration","text":"<p>Athomic provides granular controls to fine-tune how consumers process messages, allowing you to improve performance and ensure that critical data is handled with the necessary urgency.</p>"},{"location":"athomic/integration/messaging/advanced/#priority-queues","title":"Priority Queues","text":"<p>In many systems, not all messages carry the same weight. A \"password reset\" notification, for example, is far more critical than a weekly newsletter dispatch. Priority Queues allow you to ensure that high-priority messages are processed before low-priority ones, regardless of their arrival order.</p>"},{"location":"athomic/integration/messaging/advanced/#how-it-works","title":"How It Works","text":"<p>Under the hood, Athomic implements prioritization by publishing messages to distinct physical topics based on their priority. If you publish to a logical topic named <code>notifications</code>, a message with <code>HIGH</code> priority will go to a physical topic like <code>notifications.high</code>, while a <code>NORMAL</code> priority message will go to <code>notifications</code>. This allows you to scale resources (e.g., more pods/workers) specifically for higher-priority topics.</p>"},{"location":"athomic/integration/messaging/advanced/#usage","title":"Usage","text":"<p>You can set a consumer's priority using the <code>priority</code> parameter in the <code>@subscribe_to</code> or <code>@ordered_consumer</code> decorators.</p> <pre><code>from nala.athomic.integration.messaging import subscribe_to\nfrom nala.athomic.types.messaging import MessagePriority\n\n# This handler will consume from a high-priority topic (e.g., 'user-events.high')\n@subscribe_to(\n    topic=\"user-events\",\n    group_id=\"password-reset-workers\",\n    priority=MessagePriority.HIGH\n)\nasync def handle_password_reset(event: PasswordResetEvent, **kwargs):\n    # Immediate and critical processing\n    ...\n\n# This handler will consume from a low-priority topic (e.g., 'user-events.low')\n@subscribe_to(\n    topic=\"user-events\",\n    group_id=\"newsletter-workers\",\n    priority=MessagePriority.LOW\n)\nasync def handle_newsletter_signup(event: NewsletterEvent, **kwargs):\n    # Background, less critical processing\n    ...\n</code></pre> <p>To publish a message with a specific priority, use the <code>priority</code> parameter in the <code>publish</code> method:</p> <pre><code>await producer.publish(\n    topic=\"user-events\",\n    message=password_reset_data,\n    priority=MessagePriority.HIGH\n)\n</code></pre>"},{"location":"athomic/integration/messaging/advanced/#concurrency-control","title":"Concurrency Control","text":"<p>For resource-intensive tasks (e.g., heavy CPU, database connections), you might want to limit how many messages a single instance of your service processes concurrently. This helps prevent the service from becoming overloaded and ensures stable performance.</p> <p>The <code>concurrency</code> parameter sets a \"ceiling\" on parallel executions for a specific handler within a single process.</p>"},{"location":"athomic/integration/messaging/advanced/#usage_1","title":"Usage","text":"<p>Pass the <code>concurrency</code> parameter to the decorator.</p> <pre><code># This consumer instance will never execute more than 5 `handle_report` tasks at the same time.\n@subscribe_to(\n    topic=\"report-generation-tasks\",\n    group_id=\"report-workers\",\n    concurrency=5\n)\nasync def handle_report_generation(task: ReportTask, **kwargs):\n    await do_heavy_io_work()\n</code></pre> <p>!!! note \"Concurrency vs. Scalability\"     Concurrency control (<code>concurrency</code>) is a form of vertical scaling within a single instance (pod/process). It limits the resources used by that instance. Horizontal scaling is achieved by increasing the number of instances (replicas/pods) of your service. Both techniques can and should be used together for optimal performance.</p>"},{"location":"athomic/integration/messaging/advanced/#runtime-tuning-handler_tuning","title":"Runtime Tuning (<code>handler_tuning</code>)","text":"<p>One of Athomic's most powerful features is the ability to override decorator settings (<code>priority</code> and <code>concurrency</code>) at runtime through your <code>settings.toml</code> file. This allows SRE and DevOps teams to fine-tune application performance in production without requiring code changes or new deployments.</p>"},{"location":"athomic/integration/messaging/advanced/#how-it-works_1","title":"How It Works","text":"<p>You can add <code>[[integration.messaging.handler_tuning]]</code> sections to your settings file. Each entry is an \"array of tables\" that identifies a handler by its unique <code>handler_id</code> and defines the new values for its parameters.</p> <p>The <code>handler_id</code> is the function's fully qualified import path (e.g., <code>my_app.consumers.my_handler</code>).</p>"},{"location":"athomic/integration/messaging/advanced/#complete-example","title":"Complete Example","text":"<p>Imagine the following code in <code>my_app/consumers.py</code>:</p> <pre><code># my_app/consumers.py\nfrom nala.athomic.integration.messaging import subscribe_to\nfrom nala.athomic.types.messaging import MessagePriority\n\n@subscribe_to(\n    topic=\"user-events\",\n    group_id=\"password-reset-workers\",\n    priority=MessagePriority.NORMAL, # Default: NORMAL\n    concurrency=10                  # Default: 10\n)\nasync def handle_password_reset(event: PasswordResetEvent, **kwargs):\n    ...\n</code></pre> <p>Now, in a production environment, we want to treat password resets as high priority and increase the concurrency. We can do this in <code>settings.toml</code>:</p> <pre><code># settings.toml\n\n[[integration.messaging.handler_tuning]]\nhandler_id = \"my_app.consumers.handle_password_reset\"\npriority = \"high\"\nconcurrency = 25\n</code></pre> <p>With this configuration, even though the code defines the priority as <code>NORMAL</code> and concurrency as <code>10</code>, Athomic will start this consumer at runtime with <code>priority=HIGH</code> and <code>concurrency=25</code>.</p>"},{"location":"athomic/integration/messaging/consumer/","title":"Consuming Messages (Consumer)","text":""},{"location":"athomic/integration/messaging/consumer/#overview","title":"Overview","text":"<p>The Message Consumer is the component responsible for subscribing to topics on a message broker, receiving messages, and delegating them to your application's business logic for processing.</p> <p>In the Athomic Layer, defining a consumer is an extremely simple and declarative process thanks to the <code>@subscribe_to</code> decorator. This decorator handles all the complex boilerplate of connecting to the broker, managing the consumer lifecycle, and integrating with resilience and observability systems, allowing you to focus purely on your handler logic.</p>"},{"location":"athomic/integration/messaging/consumer/#defining-a-consumer","title":"Defining a Consumer","text":"<p>To create a message consumer, you simply decorate an <code>async</code> function with <code>@subscribe_to</code>. This function becomes your handler, which will be executed for each message received from the specified topic.</p>"},{"location":"athomic/integration/messaging/consumer/#basic-single-message-consumer","title":"Basic (Single Message) Consumer","text":"<p>By default, your handler will receive one message at a time. The handler should accept the deserialized message payload as its first argument.</p> <pre><code>from nala.athomic.integration.messaging import subscribe_to\nfrom your_app.schemas import UserCreatedEvent\n\n@subscribe_to(\n    topic=\"user.events.v1\",\n    group_id=\"user-event-processor-group\",\n    target_type=UserCreatedEvent # Pydantic model for deserialization and validation\n)\nasync def handle_user_created_event(message: UserCreatedEvent):\n    \"\"\"\n    This handler will be called for each message on the 'user.events.v1' topic.\n    The incoming JSON payload will be automatically deserialized and validated\n    into a `UserCreatedEvent` instance before this function is called.\n    \"\"\"\n    print(f\"Processing new user: {message.user_id}\")\n    await user_service.send_welcome_email(message.email)\n</code></pre>"},{"location":"athomic/integration/messaging/consumer/#batch-consumer","title":"Batch Consumer","text":"<p>For high-throughput scenarios, you can process messages in batches to improve efficiency. To enable batch mode, simply set the <code>batch_size</code> argument in the decorator. Your handler function will then receive a list of deserialized messages.</p> <pre><code>@subscribe_to(\n    topic=\"tracking.events.v1\",\n    group_id=\"tracking-batch-processor\",\n    target_type=TrackingEvent,\n    batch_size=100,\n    max_batch_linger_ms=5000\n)\nasync def handle_tracking_events_in_batch(messages: list[TrackingEvent]):\n    \"\"\"\n    This handler will receive a list of up to 100 TrackingEvent objects.\n    \"\"\"\n    await tracking_repo.save_many([event.data for event in messages])\n</code></pre>"},{"location":"athomic/integration/messaging/consumer/#advanced-ordered-consumption-fifo-with-ordered_consumer","title":"Advanced: Ordered Consumption (FIFO) with <code>@ordered_consumer</code>","text":"<p>This is an innovative feature of the Athomic Layer. The <code>@ordered_consumer</code> decorator guarantees that messages belonging to the same logical entity (e.g., all events for <code>order-123</code>) are processed in strict First-In, First-Out (FIFO) order, even in a distributed environment with multiple consumer instances.</p>"},{"location":"athomic/integration/messaging/consumer/#how-it-works","title":"How It Works","text":"<p>This strategy requires two fields in your message payload: -   An <code>aggregate_key_field</code>: The identifier for the entity (e.g., <code>\"order_id\"</code>). -   A <code>sequence_id_field</code>: A monotonically increasing number for that entity.</p> <p>The <code>OrderedOrchestrationStrategy</code> uses two KV stores behind the scenes: 1.  A State Store to track the last successfully processed <code>sequence_id</code> for each <code>aggregate_key</code>. 2.  A Waiting Room (a Redis Sorted Set) to temporarily buffer messages that arrive out of order.</p> <p>When a message arrives, the strategy checks its <code>sequence_id</code> against the last processed ID for its <code>aggregate_key</code>. If it's the next message in the sequence, it's processed. If it's a future message, it's placed in the waiting room. After processing a message, the strategy checks the waiting room to process any subsequent messages that have now come into order.</p>"},{"location":"athomic/integration/messaging/consumer/#usage-example","title":"Usage Example","text":"<pre><code>from nala.athomic.integration.messaging.decorators import ordered_consumer\nfrom your_app.schemas import OrderEvent\n\n@ordered_consumer(\n    topic=\"order.events.v1\",\n    group_id=\"ordered-order-processor\",\n    target_type=OrderEvent,\n    # Specify the fields in your message payload for ordering\n    aggregate_key_field=\"order_id\",\n    sequence_id_field=\"event_sequence\"\n)\nasync def handle_ordered_order_event(message: OrderEvent):\n    \"\"\"\n    This handler will process events for the same order_id in strict\n    sequence based on the event_sequence number.\n    \"\"\"\n    print(f\"Processing event #{message.event_sequence} for order {message.order_id}\")\n    await order_service.update_status(message.order_id, message.new_status)\n</code></pre>"},{"location":"athomic/integration/messaging/consumer/#disabling-lineage-for-specific-consumers","title":"Disabling Lineage for Specific Consumers","text":"<p>In some infrastructure cases (like the Lineage Collector itself), you may want to disable automatic lineage tracking to prevent recursion loops (a lineage consumer generating lineage events about consuming lineage events). You can do this via the <code>enable_lineage</code> parameter.</p> <pre><code>@subscribe_to(\n    topic=\"platform.lineage.events\",\n    group_id=\"nala-lineage-collector\",\n    target_type=LineageEvent,\n    enable_lineage=False  # Prevents generating lineage events for this handler\n)\nasync def handle_lineage_event(event: LineageEvent):\n    ...\n</code></pre>"},{"location":"athomic/integration/messaging/consumer/#api-reference","title":"API Reference","text":""},{"location":"athomic/integration/messaging/consumer/#nala.athomic.integration.messaging.decorators.subscribe_to","title":"<code>nala.athomic.integration.messaging.decorators.subscribe_to(topic, *, target_type=None, group_id=None, priority=MessagePriority.NORMAL, concurrency=None, batch_size=None, fetch_timeout_ms=None, fetch_max_linger_ms=None, fetch_max_bytes=None, connection_name=None, idempotency_settings=None, idempotency_key_resolver=None, enable_lineage=True)</code>","text":"<p>This function collects the configuration metadata and registers it with the global <code>decorated_consumer_registry</code>.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>Union[str, List[str]]</code> <p>The topic name (str) or a list of topic names.</p> required <code>target_type</code> <code>Optional[Type[Any]]</code> <p>Optional Pydantic model or class used to deserialize.</p> <code>None</code> <code>group_id</code> <code>Optional[str]</code> <p>Optional consumer group identifier.</p> <code>None</code> <code>priority</code> <code>MessagePriority</code> <p>The priority queue this handler will consume from.</p> <code>NORMAL</code> <code>concurrency</code> <code>Optional[int]</code> <p>The maximum number of messages processed concurrently.</p> <code>None</code> <code>batch_size</code> <code>Optional[int]</code> <p>If &gt; 0, enables \"true batch\" processing mode.</p> <code>None</code> <code>fetch_timeout_ms</code> <code>Optional[int]</code> <p>Optional override for batch fetch timeout.</p> <code>None</code> <code>fetch_max_linger_ms</code> <code>Optional[int]</code> <p>Optional override for batch linger time.</p> <code>None</code> <code>fetch_max_bytes</code> <code>Optional[int]</code> <p>Optional override for batch size in bytes.</p> <code>None</code> <code>connection_name</code> <code>Optional[str]</code> <p>The specific messaging connection.</p> <code>None</code> <code>idempotency_settings</code> <code>Optional[IdempotencySettings]</code> <p>Optional configuration for idempotency.</p> <code>None</code> <code>idempotency_key_resolver</code> <code>Optional[Callable[..., str]]</code> <p>Optional function to resolve idempotency key.</p> <code>None</code> <code>enable_lineage</code> <code>bool</code> <p>If False, disables automatic lineage tracking for this consumer.             Use this for internal infrastructure consumers (e.g., Lineage Collector).</p> <code>True</code> <p>Returns:</p> Type Description <code>Callable</code> <p>The decorator function.</p>"},{"location":"athomic/integration/messaging/consumer/#nala.athomic.integration.messaging.decorators.ordered_consumer.ordered_consumer","title":"<code>nala.athomic.integration.messaging.decorators.ordered_consumer.ordered_consumer(topic, *, group_id, aggregate_key_field, sequence_id_field, target_type=None, priority=MessagePriority.NORMAL, concurrency=None, fetch_size=None, idempotency_settings=None, idempotency_key_resolver=None)</code>","text":"<p>Registers an ordered consumer with optional idempotency.</p> <p>Enforces sequential processing. Idempotency is applied per-message if enabled.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>Union[str, List[str]]</code> <p>The topic name(s).</p> required <code>group_id</code> <code>str</code> <p>Consumer group ID.</p> required <code>aggregate_key_field</code> <code>str</code> <p>Field for ordering (partition key).</p> required <code>sequence_id_field</code> <code>str</code> <p>Field for sequencing.</p> required <code>target_type</code> <code>Optional[Type[Any]]</code> <p>Optional Pydantic model for deserialization.</p> <code>None</code> <code>priority</code> <code>MessagePriority</code> <p>Message priority level.</p> <code>NORMAL</code> <code>concurrency</code> <code>Optional[int]</code> <p>Limit on concurrent executions.</p> <code>None</code> <code>fetch_size</code> <code>Optional[int]</code> <p>Number of messages to pre-fetch from broker (IO optimization).</p> <code>None</code> <code>idempotency_settings</code> <code>Optional[IdempotencySettings]</code> <p>Resilience config (overrides global defaults).</p> <code>None</code> <code>idempotency_key_resolver</code> <code>Optional[Callable[..., str]]</code> <p>Strategy to extract ID.</p> <code>None</code>"},{"location":"athomic/integration/messaging/consumer/#nala.athomic.integration.messaging.consumers.protocol.ConsumerProtocol","title":"<code>nala.athomic.integration.messaging.consumers.protocol.ConsumerProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for consuming messages from a messaging system.</p> <p>Focuses on a subscription pattern where received messages are processed by an asynchronous callback function.</p>"},{"location":"athomic/integration/messaging/consumer/#nala.athomic.integration.messaging.consumers.protocol.ConsumerProtocol.consume","title":"<code>consume()</code>  <code>async</code>","text":"<p>Starts the continuous message consumption process.</p> <p>Subscribes to topics/queues and passes received messages to the provided callback function for processing. This function typically blocks execution (or runs in a background task) until <code>close()</code> is called or an unrecoverable error occurs.</p> <p>Raises:</p> Type Description <code>ConsumeError</code> <p>If an unrecoverable error occurs during consumption (e.g., persistent authentication failure, error in the consume loop).</p> <code>MessagingConnectionError</code> <p>If there are initial connection problems with the broker when trying to start consumption.</p> <code>ValueError</code> <p>If essential arguments (like <code>topics</code> for Kafka) are not provided.</p>"},{"location":"athomic/integration/messaging/consumer/#nala.athomic.integration.messaging.consumers.base.BaseConsumer","title":"<code>nala.athomic.integration.messaging.consumers.base.BaseConsumer</code>","text":"<p>               Bases: <code>BaseService</code>, <code>ABC</code>, <code>ConsumerProtocol</code></p> <p>Abstract base class for message consumers.</p> <p>This class serves as the core lifecycle manager for any message consumer implementation (e.g., Kafka, Redis, SQS). It enforces the required dependencies and delegates all message-specific logic to injected protocols, thereby adhering strictly to the Dependency Inversion Principle (DIP).</p> <p>Responsibilities include: - Managing the connection lifecycle to the message broker (via BaseService). - Holding the configuration and essential dependencies. - Delegating the message fetching loop to a <code>ConsumptionStrategy</code>. - Delegating message processing to a <code>Processor</code>.</p>"},{"location":"athomic/integration/messaging/consumer/#nala.athomic.integration.messaging.consumers.base.BaseConsumer.group_id","title":"<code>group_id</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>The consumer group identifier for this consumer instance.</p> <p>Returns:</p> Type Description <code>str</code> <p>The unique string identifying the consumer group.</p>"},{"location":"athomic/integration/messaging/consumer/#nala.athomic.integration.messaging.consumers.base.BaseConsumer.__init__","title":"<code>__init__(settings, handler_config, message_processor, consumption_strategy)</code>","text":"<p>Initializes the base consumer with all necessary dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>MessagingSettings</code> <p>The general messaging configuration settings.</p> required <code>handler_config</code> <code>ConsumerHandlerConfig</code> <p>The configuration metadata for the consumer handler, typically extracted from a decorator.</p> required <code>message_processor</code> <code>ProcessorProtocol</code> <p>The strategy responsible for the entire message processing pipeline (deserialization, execution, DLQ).</p> required <code>consumption_strategy</code> <code>ConsumptionStrategyProtocol</code> <p>The strategy defining how messages are fetched from the broker (e.g., individual or batch consumption).</p> required <p>Raises:</p> Type Description <code>InvalidConsumerConfigurationError</code> <p>If <code>handler_config</code> is not provided.</p>"},{"location":"athomic/integration/messaging/consumer/#nala.athomic.integration.messaging.consumers.base.BaseConsumer.get_batch","title":"<code>get_batch(max_records, timeout_ms)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Fetches a batch of raw messages from the broker in a single operation.</p> <p>This method must be implemented by concrete consumer providers to enable batch processing strategies.</p> <p>Parameters:</p> Name Type Description Default <code>max_records</code> <code>int</code> <p>The maximum number of messages to attempt to fetch.</p> required <code>timeout_ms</code> <code>int</code> <p>The maximum time (in milliseconds) to wait for records to arrive.</p> required <p>Returns:</p> Type Description <code>List[RawMessage]</code> <p>A list of <code>RawMessage</code> objects, which may be empty.</p>"},{"location":"athomic/integration/messaging/consumer/#nala.athomic.integration.messaging.consumers.base.BaseConsumer.get_message_iterator","title":"<code>get_message_iterator()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Returns an async generator that yields raw messages one by one from the broker.</p> <p>This method must be implemented by concrete consumer providers (e.g., Kafka, SQS) to handle the broker-specific low-level message fetching.</p> <p>Yields:</p> Name Type Description <code>RawMessage</code> <code>AsyncGenerator[RawMessage, None]</code> <p>The next raw message fetched from the broker.</p>"},{"location":"athomic/integration/messaging/context-propagation/","title":"Context Propagation in Messaging","text":""},{"location":"athomic/integration/messaging/context-propagation/#overview","title":"Overview","text":"<p>A fundamental challenge in distributed, event-driven architectures is maintaining a consistent \"context\" as a request flows from one service to another through a message broker. This context includes critical observability data like the <code>trace_id</code> for distributed tracing and application-level data like the <code>tenant_id</code> for multi-tenancy.</p> <p>The Athomic messaging module solves this problem with automatic context propagation. It ensures that when you consume a message in a background worker, you have access to the same execution context that existed in the service that originally published the message.</p>"},{"location":"athomic/integration/messaging/context-propagation/#how-it-works","title":"How It Works","text":"<p>The entire process is automated and built upon the Context Management module.</p>"},{"location":"athomic/integration/messaging/context-propagation/#injection-producer-side","title":"Injection (Producer Side)","text":"<ol> <li>When you call <code>producer.publish()</code>, the <code>BaseProducer</code> automatically invokes the <code>MessagingTelemetryAdapter</code>.</li> <li>This adapter calls the <code>capture_context()</code> function, which serializes all context variables marked for propagation (like <code>trace_id</code>, <code>tenant_id</code>, <code>request_id</code>, etc.) into a dictionary.</li> <li>The W3C Trace Context (<code>traceparent</code>, <code>tracestate</code>) is injected into this dictionary for OpenTelemetry.</li> <li>Finally, this context dictionary is injected as a special header into the message before it is sent to the broker.</li> </ol>"},{"location":"athomic/integration/messaging/context-propagation/#extraction-consumer-side","title":"Extraction (Consumer Side)","text":"<ol> <li>When a consumer receives a message, the <code>BaseMessageProcessor</code> receives the raw message with its headers.</li> <li>It uses the <code>MessagingTelemetryAdapter</code> to extract the W3C Trace Context, which creates and activates a new OpenTelemetry span that is correctly parented to the producer's span.</li> <li>The <code>@run_task_with_context</code> decorator (for background tasks) or internal consumer logic calls <code>restore_context()</code>, which takes the context dictionary from the message headers and applies its values to the <code>contextvars</code> of the current execution scope.</li> </ol> <p>The result is seamless. When your consumer handler function is executed, a call to <code>context_vars.get_tenant_id()</code> will return the same <code>tenant_id</code> from the original publisher, and your logs and traces will be perfectly correlated.</p>"},{"location":"athomic/integration/messaging/context-propagation/#what-is-propagated","title":"What is Propagated?","text":"<p>By default, the following context variables are automatically propagated in message headers:</p> <ul> <li><code>request_id</code></li> <li><code>trace_id</code></li> <li><code>span_id</code></li> <li><code>tenant_id</code></li> <li><code>user_id</code></li> <li><code>role</code></li> <li><code>locale</code></li> <li><code>correlation_id</code></li> <li><code>feature_flags</code></li> </ul> <p>This list is fully configurable in your <code>settings.toml</code> file under the <code>[context]</code> section.</p>"},{"location":"athomic/integration/messaging/context-propagation/#api-reference","title":"API Reference","text":""},{"location":"athomic/integration/messaging/context-propagation/#nala.athomic.context.propagation.capture_context","title":"<code>nala.athomic.context.propagation.capture_context()</code>","text":"<p>Captures the current values of all context variables marked for propagation.</p> <p>This function iterates through the centrally registered context variables in the <code>context_var_manager</code>, collects the values of those flagged with <code>propagate=True</code>, and returns them as a dictionary. This dictionary is suitable for serialization and passing to background tasks or events.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary containing the current context values to be propagated.</p>"},{"location":"athomic/integration/messaging/context-propagation/#nala.athomic.context.propagation.restore_context","title":"<code>nala.athomic.context.propagation.restore_context(context_dict)</code>","text":"<p>A context manager to temporarily set context variables from a dictionary.</p> <p>This is used on the worker side (e.g., in a background task or event handler) to re-establish the context that existed when the job was enqueued. It safely resets all variables to their previous state upon exiting the <code>with</code> block.</p> <p>Parameters:</p> Name Type Description Default <code>context_dict</code> <code>Optional[Dict[str, Any]]</code> <p>The dictionary of context values captured before the           task was enqueued.</p> required"},{"location":"athomic/integration/messaging/context-propagation/#nala.athomic.observability.telemetry.adapters.messaging_adapter.MessagingTelemetryAdapter","title":"<code>nala.athomic.observability.telemetry.adapters.messaging_adapter.MessagingTelemetryAdapter</code>","text":"<p>               Bases: <code>TelemetryAdapterProtocol</code></p> <p>A concrete Telemetry Adapter for messaging systems.</p> <p>This adapter handles the injection and extraction of W3C Trace Context into and from the Athomic MessageHeaders object, enabling end-to-end tracing across message broker boundaries.</p>"},{"location":"athomic/integration/messaging/context-propagation/#nala.athomic.observability.telemetry.adapters.messaging_adapter.MessagingTelemetryAdapter.activate_from_headers","title":"<code>activate_from_headers(headers)</code>","text":"<p>A context manager that extracts the trace context from incoming message headers and activates it for the duration of the message processing.</p> <p>This is the primary method used by consumer implementations to continue the distributed trace from the producer.</p>"},{"location":"athomic/integration/messaging/context-propagation/#nala.athomic.observability.telemetry.adapters.messaging_adapter.MessagingTelemetryAdapter.extract_context","title":"<code>extract_context(headers)</code>","text":"<p>Extracts the trace context from an incoming MessageHeaders object and returns the active OpenTelemetry Context.</p>"},{"location":"athomic/integration/messaging/context-propagation/#nala.athomic.observability.telemetry.adapters.messaging_adapter.MessagingTelemetryAdapter.inject_context","title":"<code>inject_context(headers)</code>","text":"<p>Injects the current trace context and a unique 'message_id' into a MessageHeaders object for an outgoing message.</p>"},{"location":"athomic/integration/messaging/delay/","title":"Delayed Messages","text":""},{"location":"athomic/integration/messaging/delay/#overview","title":"Overview","text":"<p>The Messaging module provides a robust mechanism for publishing messages that should only be processed after a specified delay. This is a common requirement in many applications, for example:</p> <ul> <li>Sending a follow-up email 24 hours after a user signs up.</li> <li>Retrying a failed operation with an exponential backoff delay.</li> <li>Scheduling a task to be executed at a specific time in the future.</li> </ul> <p>Athomic implements this using a Delay Topics Strategy, which is a reliable and scalable pattern for handling message delays in high-throughput systems like Kafka.</p>"},{"location":"athomic/integration/messaging/delay/#how-the-delay-topics-strategy-works","title":"How the Delay Topics Strategy Works","text":"<p>Instead of relying on a scheduler or timers within the application, this strategy uses a set of dedicated, intermediary topics in the message broker.</p> <ol> <li> <p>Configuration: You define a set of \"delay buckets\" in your configuration\u2014a mapping of a delay duration in seconds to a specific topic name.</p> <p>```toml</p> </li> <li> <p>Publishing with Delay: When you call <code>producer.publish(..., delay_seconds=X)</code>, the <code>DelayedPublishStrategy</code> is activated. It finds the smallest delay bucket that is greater than or equal to your requested delay.</p> </li> <li> <p>Envelope Wrapping: The producer wraps your original message (including its payload, headers, and final destination topic) inside a special \"envelope\" message.</p> </li> <li> <p>Publish to Delay Topic: This envelope is then published to the chosen delay topic (e.g., <code>platform.internal.delay-5m.v1</code>).</p> </li> <li> <p>The Republisher Service: Athomic runs a dedicated, internal consumer service (the \"republisher\"). This service is automatically configured to subscribe to all of your configured delay topics.</p> </li> <li> <p>Unwrap and Republish: When the republisher consumes a message from a delay topic, it unwraps the envelope, extracts the original message and its metadata, and publishes it to its final destination topic.</p> </li> </ol> <p>This approach is highly scalable and resilient because it leverages the underlying message broker's infrastructure for storing and delivering the delayed messages.</p>"},{"location":"athomic/integration/messaging/delay/#in-settingstoml","title":"In settings.toml","text":"<p>[default.integration.messaging.republisher.delay_topics] 300 = \"platform.internal.delay-5m.v1\"    # 5-minute bucket 900 = \"platform.internal.delay-15m.v1\"   # 15-minute bucket 3600 = \"platform.internal.delay-1h.v1\"   # 1-hour bucket ```</p>"},{"location":"athomic/integration/messaging/delay/#usage","title":"Usage","text":"<p>Using the delayed message feature is as simple as adding the <code>delay_seconds</code> parameter to a standard <code>publish</code> call.</p> <pre><code>from nala.athomic.integration.messaging import ProducerFactory\n\nproducer = ProducerFactory.create()\n\nasync def schedule_follow_up_email(user_id: str):\n    # This message will be processed in approximately 1 hour.\n    await producer.publish(\n        topic=\"user.follow-ups.v1\",\n        message={\"user_id\": user_id, \"email_type\": \"onboarding_d1\"},\n        key=user_id,\n        delay_seconds=3600 # 1 hour\n    )\n</code></pre>"},{"location":"athomic/integration/messaging/delay/#configuration","title":"Configuration","text":"<p>The delayed message system is configured under the <code>[integration.messaging.republisher]</code> section.</p> <pre><code>[default.integration.messaging.republisher]\nenabled = true\n\n# This must be set to use the delay topics strategy.\ndelay_strategy_backend = \"kafka_topic_delay_strategy\"\n\n  # The consumer and producer settings for the internal republisher service.\n  # This service needs its own configuration to connect to Kafka.\n  [default.integration.messaging.republisher.consumer]\n  bootstrap_servers = [\"localhost:9092\"]\n  group_id = \"nala-global-republisher-group\"\n\n  [default.integration.messaging.republisher.producer]\n  bootstrap_servers = [\"localhost:9092\"]\n\n  # The mapping of delay durations (in seconds) to topic names.\n  [default.integration.messaging.republisher.delay_topics]\n  60 = \"platform.internal.delay-1m.v1\"\n  300 = \"platform.internal.delay-5m.v1\"\n  3600 = \"platform.internal.delay-1h.v1\"\n</code></pre>"},{"location":"athomic/integration/messaging/delay/#api-reference","title":"API Reference","text":""},{"location":"athomic/integration/messaging/delay/#nala.athomic.integration.messaging.producers.strategies.delayed.DelayedPublishStrategy","title":"<code>nala.athomic.integration.messaging.producers.strategies.delayed.DelayedPublishStrategy</code>","text":"<p>               Bases: <code>BasePublishingStrategy</code>, <code>PublishingStrategyProtocol</code></p> <p>Implements the strategy for publishing a message with a delay.</p> <p>This strategy works by: 1. Finding the closest matching delay topic bucket based on <code>delay_seconds</code>. 2. Wrapping the original message and its metadata in an envelope. 3. Publishing the envelope to the intermediary delay topic.</p> <p>A separate consumer service (the republisher) will later unwrap and send the original message to the final destination after the delay.</p>"},{"location":"athomic/integration/messaging/delay/#nala.athomic.integration.messaging.producers.strategies.delayed.DelayedPublishStrategy.__init__","title":"<code>__init__(lineage_settings)</code>","text":"<p>Initializes the strategy with the specific lineage configuration.</p> <p>Parameters:</p> Name Type Description Default <code>lineage_settings</code> <code>Optional[LineageSettings]</code> <p>The application's global lineage configuration,               injected by the factory.</p> required"},{"location":"athomic/integration/messaging/delay/#nala.athomic.integration.messaging.producers.strategies.delayed.DelayedPublishStrategy.publish","title":"<code>publish(producer, message, topic, key, headers, delay_seconds)</code>  <code>async</code>","text":"<p>Processes the message for delayed delivery and dispatches it to the appropriate delay topic.</p> <p>Parameters:</p> Name Type Description Default <code>producer</code> <code>BaseProducer</code> <p>The producer instance used for transport.</p> required <code>message</code> <code>Any</code> <p>The original message payload.</p> required <code>topic</code> <code>str</code> <p>The final destination topic.</p> required <code>key</code> <code>Optional[Any]</code> <p>The message key.</p> required <code>headers</code> <code>Optional[MessageHeaders]</code> <p>The message headers.</p> required <code>delay_seconds</code> <code>Optional[int]</code> <p>The duration of the delay.</p> required <p>Returns:</p> Name Type Description <code>PublishingOutcome</code> <code>PublishingOutcome</code> <p>SUCCESS_DELAYED if the message was successfully published to the delay topic.</p>"},{"location":"athomic/integration/messaging/delay/#nala.athomic.integration.messaging.delay.republisher.DelayedMessageHandler","title":"<code>nala.athomic.integration.messaging.delay.republisher.DelayedMessageHandler</code>","text":"<p>A callable class that acts as a consumer callback for delayed messages.</p> <p>It unwraps the message envelope created by a delay strategy and republishes the original payload to its final destination topic using an injected producer.</p>"},{"location":"athomic/integration/messaging/delay/#nala.athomic.integration.messaging.delay.republisher.DelayedMessageHandler.__call__","title":"<code>__call__(message, message_key, raw_headers)</code>  <code>async</code>","text":"<p>Handles a message from a delay topic by republishing it to its final destination.</p> <p>This method deserializes the envelope, rehydrates the original headers, and uses the injected producer to send the message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Dict[str, Any]</code> <p>The message payload from the delay topic (the envelope).</p> required <code>message_key</code> <code>Optional[str]</code> <p>The message key (unused, as original key is in the envelope).</p> required <code>raw_headers</code> <code>MessageHeaders</code> <p>The headers from the delay topic message.</p> required <p>Returns:</p> Type Description <code>ProcessingOutcome</code> <p>ProcessingOutcome.ACK: If the message was successfully republished                    or if it was malformed and should be discarded.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Propagates exceptions from the producer (e.g., PublishError)        to trigger the consumer's retry/DLQ mechanism.</p>"},{"location":"athomic/integration/messaging/delay/#nala.athomic.integration.messaging.delay.republisher.DelayedMessageHandler.__init__","title":"<code>__init__(producer)</code>","text":"<p>Initializes the handler with its required producer dependency.</p> <p>Parameters:</p> Name Type Description Default <code>producer</code> <code>ProducerProtocol</code> <p>An active and connected producer instance that will be used       to publish the unwrapped message.</p> required"},{"location":"athomic/integration/messaging/internals/","title":"Messaging Internals &amp; Architecture","text":""},{"location":"athomic/integration/messaging/internals/#overview","title":"Overview","text":"<p>This page provides a deeper look into the internal components that orchestrate the message consumption lifecycle in the Athomic Layer. Understanding these components is useful for advanced debugging, customization, and contributing to the framework.</p>"},{"location":"athomic/integration/messaging/internals/#1-consumermanager","title":"1. <code>ConsumerManager</code>","text":"<p>This is the entry point for the entire consumer system. At application startup, its primary responsibilities are:</p> <ul> <li>Discovery: It inspects the application's code to find all functions that have been decorated with <code>@subscribe_to</code> or <code>@ordered_consumer</code>.</li> <li>Assembly: For each discovered handler, it uses the <code>ConsumerFactory</code> to build a complete, self-contained consumer service. This involves assembling all necessary parts: the correct consumption strategy, message processor, DLQ handler, and the underlying Kafka client.</li> <li>Lifecycle Management: It registers each of these fully assembled consumer services with the main Lifecycle Manager, which then controls their <code>start</code> and <code>stop</code> sequences along with the rest of the application.</li> </ul>"},{"location":"athomic/integration/messaging/internals/#2-the-message-processing-flow","title":"2. The Message Processing Flow","text":"<p>When a raw message is fetched from Kafka, it goes through a well-defined internal pipeline before your handler function is ever called.</p>"},{"location":"athomic/integration/messaging/internals/#rawmessage","title":"<code>RawMessage</code>","text":"<p>This is a simple, framework-agnostic data class that represents a message as it comes from the broker. It contains the raw, unprocessed payload (<code>bytes</code>), key, and headers. This abstraction decouples the core processing logic from any specific client library (like <code>aiokafka</code>).</p>"},{"location":"athomic/integration/messaging/internals/#headersorchestrator","title":"<code>HeadersOrchestrator</code>","text":"<p>This is the central component for managing context propagation. Before any other processing, the headers from the <code>RawMessage</code> are passed to the <code>HeadersOrchestrator</code>. It works with a series of specialized \"adapters\" to extract and prepare the context:</p> <ul> <li><code>MessagingTelemetryAdapter</code>: Extracts W3C Trace Context headers (<code>traceparent</code>) and starts a new OpenTelemetry span, correctly linking it to the producer's trace.</li> <li><code>LineageAdapter</code>: Extracts data lineage information.</li> <li><code>ContextAdapter</code>: Extracts the main application context (<code>tenant_id</code>, <code>user_id</code>, etc.).</li> </ul> <p>The extracted context is then applied to the current execution scope using <code>restore_context()</code>, so it becomes available throughout the rest of the processing pipeline and in your handler.</p>"},{"location":"athomic/integration/messaging/internals/#tasks-message-processors","title":"<code>Tasks</code> (Message Processors)","text":"<p>The \"task\" of processing a message is handled by a Message Processor (e.g., <code>SingleMessageProcessor</code> or <code>BatchMessageProcessor</code>). This component is responsible for: 1.  Receiving the <code>RawMessage</code>. 2.  Passing the raw payload through the Payload Processing Pipeline in reverse (decode) order (e.g., decompress -&gt; decrypt -&gt; deserialize). 3.  Invoking your actual handler function with the final, deserialized Pydantic model. 4.  Capturing the result of your handler.</p>"},{"location":"athomic/integration/messaging/internals/#outcomes","title":"<code>Outcomes</code>","text":"<p>After your handler function is executed, the Message Processor wraps the result in a standardized <code>ConsumptionOutcome</code> object.</p> <ul> <li><code>ConsumptionOutcome</code>: If the handler completes successfully, this object holds the result.</li> <li><code>FailureContext</code>: If the handler raises an exception, this object is created instead. It captures the original raw message, the exception details, and other metadata. This context object is then passed to the DLQ Handler to orchestrate the retry/DLQ logic.</li> </ul> <p>This structured flow ensures that every message is handled consistently and that all necessary context for observability and resilience is available at every step of the process.</p>"},{"location":"athomic/integration/messaging/producer/","title":"Publishing Messages (Producer)","text":""},{"location":"athomic/integration/messaging/producer/#overview","title":"Overview","text":"<p>The Message Producer is the component responsible for sending messages to the configured message broker (e.g., Kafka). It provides a high-level, abstract interface that handles complex underlying tasks transparently, allowing you to focus on the data you want to send.</p> <p>When you publish a message, the producer automatically orchestrates: -   Payload Processing: The message is passed through the configured Payload Processing Pipeline for serialization, encryption, and compression. -   Context Propagation: W3C Trace Context and other execution context variables are automatically injected into the message headers. -   Publishing Strategy: The producer intelligently selects the correct strategy for dispatching the message, supporting both immediate and delayed delivery.</p>"},{"location":"athomic/integration/messaging/producer/#getting-a-producer-instance","title":"Getting a Producer Instance","text":"<p>The <code>Producer</code> is managed as a singleton to ensure efficient use of resources like connection pools. The correct way to obtain the producer instance is through the <code>ProducerFactory</code>.</p> <pre><code>from nala.athomic.integration.messaging import ProducerFactory\n\n# The factory will create and cache a singleton instance based on your configuration.\nproducer = ProducerFactory.create()\n</code></pre>"},{"location":"athomic/integration/messaging/producer/#publishing-a-message","title":"Publishing a Message","text":"<p>The <code>producer.publish()</code> method is the single entry point for sending all messages.</p>"},{"location":"athomic/integration/messaging/producer/#basic-immediate-publishing","title":"Basic (Immediate) Publishing","text":"<p>To send a message for immediate delivery, you provide the topic and the message payload. The payload can be a Pydantic model, a dictionary, or any other data type your configured serializer can handle.</p> <pre><code>from your_app.schemas import UserCreatedEvent\n\n# The message can be a Pydantic model\nevent_payload = UserCreatedEvent(user_id=\"user-123\", email=\"test@example.com\")\n\nawait producer.publish(\n    topic=\"user.events.v1\",\n    message=event_payload,\n    key=\"user-123\"  # The key is used for partitioning in Kafka\n)\n</code></pre>"},{"location":"athomic/integration/messaging/producer/#publishing-with-a-delay","title":"Publishing with a Delay","text":"<p>This is a key feature of the Athomic producer. To send a message that should only be processed after a delay, simply add the <code>delay_seconds</code> argument to the <code>publish</code> call.</p> <pre><code># This message will be sent to an intermediary delay topic and only\n# republished to its final destination after 300 seconds (5 minutes).\nawait producer.publish(\n    topic=\"notifications.v1\",\n    message={\"email\": \"test@example.com\", \"template\": \"follow_up\"},\n    key=\"user-123\",\n    delay_seconds=300\n)\n</code></pre> <p>Behind the scenes, the <code>DelayedPublishStrategy</code> wraps your message in an \"envelope\" and sends it to a pre-configured delay topic. A separate background service, the republisher, is responsible for consuming from these delay topics and sending the message to its final destination when the time is up. See the Delayed Messages documentation for more details.</p>"},{"location":"athomic/integration/messaging/producer/#api-reference","title":"API Reference","text":""},{"location":"athomic/integration/messaging/producer/#nala.athomic.integration.messaging.producers.protocol.ProducerProtocol","title":"<code>nala.athomic.integration.messaging.producers.protocol.ProducerProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the contract for sending messages to a messaging system.</p>"},{"location":"athomic/integration/messaging/producer/#nala.athomic.integration.messaging.producers.protocol.ProducerProtocol.publish","title":"<code>publish(message=None, topic='default_topic', key=None, headers=None, delay_seconds=None, priority=MessagePriority.NORMAL)</code>  <code>async</code>","text":"<p>Publishes a message, deciding the strategy based on parameters.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Optional[Any]</code> <p>The content of the message to be sent.</p> <code>None</code> <code>topic</code> <code>str</code> <p>The target topic (or exchange).</p> <code>'default_topic'</code> <code>key</code> <code>Optional[str]</code> <p>The routing or partitioning key for the message.</p> <code>None</code> <code>headers</code> <code>Optional[MessageHeaders]</code> <p>A MessageHeaders object with metadata.</p> <code>None</code> <code>delay_seconds</code> <code>Optional[int]</code> <p>If provided and &gt; 0, the message will be sent            using a delayed publishing strategy.</p> <code>None</code> <code>priority</code> <code>MessagePriority</code> <p>The priority level for processing the message.</p> <code>NORMAL</code> <p>Returns:</p> Name Type Description <code>PublishingOutcome</code> <code>PublishingOutcome</code> <p>An enum indicating the result of the operation,                e.g., SUCCESS_DIRECT or SUCCESS_DELAYED.</p> <p>Raises:</p> Type Description <code>PublishError</code> <p>If publishing fails after all internal retries.</p> <code>MessagingConnectionError</code> <p>If there are connection problems with the broker.</p> <code>NotImplementedError</code> <p>If a requested strategy (e.g., 'delayed') is not configured.</p>"},{"location":"athomic/integration/messaging/producer/#nala.athomic.integration.messaging.producers.factory.ProducerFactory","title":"<code>nala.athomic.integration.messaging.producers.factory.ProducerFactory</code>","text":"<p>A Factory class that creates concrete Producer instances.</p> <p>This factory is responsible for the low-level assembly of a single producer instance. It resolves all necessary dependencies, such as the payload processor and publishing strategies, and injects them into the concrete producer class retrieved from the <code>messaging_producer_registry</code>.</p> <p>This factory is stateless and creates a new instance on every call.</p>"},{"location":"athomic/integration/messaging/producer/#nala.athomic.integration.messaging.producers.factory.ProducerFactory.clear","title":"<code>clear()</code>  <code>classmethod</code>","text":"<p>Clears any potential class-level caches (if any were used). This method is kept for compatibility but does nothing as the factory is now stateless.</p>"},{"location":"athomic/integration/messaging/producer/#nala.athomic.integration.messaging.producers.factory.ProducerFactory.create","title":"<code>create(settings)</code>  <code>staticmethod</code>","text":"<p>Creates a new producer instance for a specific connection.</p> <p>This is the primary public interface for obtaining a new producer. It handles dependency resolution and instance creation based on the provided settings. The <code>ProducerManager</code> (a BaseManager) uses this factory to build all producer instances.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>MessagingSettings</code> <p>The fully resolved messaging settings for a specific connection. This object must contain the <code>connection_name</code>.</p> required <p>Returns:</p> Type Description <code>BaseProducer</code> <p>A new, fully configured and initialized producer instance.</p>"},{"location":"athomic/integration/messaging/producer/#nala.athomic.integration.messaging.producers.strategies.direct.DirectPublishStrategy","title":"<code>nala.athomic.integration.messaging.producers.strategies.direct.DirectPublishStrategy</code>","text":"<p>               Bases: <code>BasePublishingStrategy</code>, <code>PublishingStrategyProtocol</code></p> <p>Implements the strategy for publishing a message directly to its final destination topic without any delay or intermediate processing route.</p> <p>This is the default, high-performance path for message delivery, focusing on immediate serialization, telemetry capture, and dispatch.</p>"},{"location":"athomic/integration/messaging/producer/#nala.athomic.integration.messaging.producers.strategies.direct.DirectPublishStrategy.__init__","title":"<code>__init__(lineage_settings=None)</code>","text":"<p>Initializes the strategy with the specific lineage configuration.</p> <p>Parameters:</p> Name Type Description Default <code>lineage_settings</code> <code>Optional[LineageSettings]</code> <p>The application's global lineage configuration,               injected by the factory.</p> <code>None</code>"},{"location":"athomic/integration/messaging/producer/#nala.athomic.integration.messaging.producers.strategies.direct.DirectPublishStrategy.publish","title":"<code>publish(producer, message, topic, key, headers, delay_seconds)</code>  <code>async</code>","text":"<p>Processes and publishes the message immediately to the broker. It handles the injection of tracing context, lineage headers, payload processing (serialization, compression), and metrics recording.</p> <p>Parameters:</p> Name Type Description Default <code>producer</code> <code>BaseProducer</code> <p>The producer instance used for transport and processing.</p> required <code>message</code> <code>Optional[Any]</code> <p>The message payload.</p> required <code>topic</code> <code>str</code> <p>The destination topic name.</p> required <code>key</code> <code>Optional[Any]</code> <p>The message key.</p> required <code>headers</code> <code>Optional[MessageHeaders]</code> <p>The message headers.</p> required <code>delay_seconds</code> <code>Optional[int]</code> <p>Ignored by this strategy.</p> required <p>Returns:</p> Name Type Description <code>PublishingOutcome</code> <code>PublishingOutcome</code> <p>SUCCESS_DIRECT if the message was successfully dispatched.</p>"},{"location":"athomic/integration/messaging/producer/#nala.athomic.integration.messaging.producers.strategies.delayed.DelayedPublishStrategy","title":"<code>nala.athomic.integration.messaging.producers.strategies.delayed.DelayedPublishStrategy</code>","text":"<p>               Bases: <code>BasePublishingStrategy</code>, <code>PublishingStrategyProtocol</code></p> <p>Implements the strategy for publishing a message with a delay.</p> <p>This strategy works by: 1. Finding the closest matching delay topic bucket based on <code>delay_seconds</code>. 2. Wrapping the original message and its metadata in an envelope. 3. Publishing the envelope to the intermediary delay topic.</p> <p>A separate consumer service (the republisher) will later unwrap and send the original message to the final destination after the delay.</p>"},{"location":"athomic/integration/messaging/producer/#nala.athomic.integration.messaging.producers.strategies.delayed.DelayedPublishStrategy.__init__","title":"<code>__init__(lineage_settings)</code>","text":"<p>Initializes the strategy with the specific lineage configuration.</p> <p>Parameters:</p> Name Type Description Default <code>lineage_settings</code> <code>Optional[LineageSettings]</code> <p>The application's global lineage configuration,               injected by the factory.</p> required"},{"location":"athomic/integration/messaging/producer/#nala.athomic.integration.messaging.producers.strategies.delayed.DelayedPublishStrategy.publish","title":"<code>publish(producer, message, topic, key, headers, delay_seconds)</code>  <code>async</code>","text":"<p>Processes the message for delayed delivery and dispatches it to the appropriate delay topic.</p> <p>Parameters:</p> Name Type Description Default <code>producer</code> <code>BaseProducer</code> <p>The producer instance used for transport.</p> required <code>message</code> <code>Any</code> <p>The original message payload.</p> required <code>topic</code> <code>str</code> <p>The final destination topic.</p> required <code>key</code> <code>Optional[Any]</code> <p>The message key.</p> required <code>headers</code> <code>Optional[MessageHeaders]</code> <p>The message headers.</p> required <code>delay_seconds</code> <code>Optional[int]</code> <p>The duration of the delay.</p> required <p>Returns:</p> Name Type Description <code>PublishingOutcome</code> <code>PublishingOutcome</code> <p>SUCCESS_DELAYED if the message was successfully published to the delay topic.</p>"},{"location":"athomic/integration/messaging/retry-dlq/","title":"Retry &amp; Dead Letter Queue (DLQ)","text":""},{"location":"athomic/integration/messaging/retry-dlq/#overview","title":"Overview","text":"<p>The Retry and Dead Letter Queue (DLQ) mechanism is a critical resilience pattern that prevents message loss when a consumer fails to process a message. It provides an automated, configurable way to handle transient (temporary) and permanent processing failures gracefully.</p> <ul> <li>Retry: When a consumer handler fails with a transient error, the message is automatically republished to the original topic to be processed again. This is handled by the <code>DLQHandler</code>.</li> <li>Dead Letter Queue (DLQ): If a message fails to be processed after a configured number of retry attempts, it is considered a \"poison pill\" and is moved to a separate, dedicated topic called the Dead Letter Queue. This removes the failing message from the main processing queue, preventing it from blocking other messages.</li> </ul>"},{"location":"athomic/integration/messaging/retry-dlq/#the-failure-handling-flow","title":"The Failure Handling Flow","text":"<p>When a decorated consumer handler (<code>@subscribe_to</code>) raises an exception, the following automated flow is triggered:</p> <ol> <li>Failure Detection: The <code>BaseMessageProcessor</code> catches the exception.</li> <li>DLQ Handler Invocation: It packages all information about the failure (the original message, headers, exception details) into a <code>FailureContext</code> object and passes it to the <code>DLQHandler</code>.</li> <li>Retry Check: The <code>DLQHandler</code> inspects the message's headers to check the current retry attempt count (using the <code>x-retry-attempts</code> header).</li> <li>Decision:<ul> <li>If the attempt count is less than <code>max_attempts</code> (from your configuration), the handler republishes the message back to its original topic with an incremented retry header.</li> <li>If the attempt count has reached <code>max_attempts</code>, the handler constructs a detailed \"DLQ envelope\" (a JSON object with the original message and error details) and publishes it to the configured DLQ topic.</li> </ul> </li> </ol>"},{"location":"athomic/integration/messaging/retry-dlq/#the-dlq-processor","title":"The DLQ Processor","text":"<p>Once a message is in the DLQ topic, it needs to be handled. Athomic provides a dedicated background service, the <code>DeadLetterProcessorService</code>, which automatically subscribes to your DLQ topic.</p> <p>This service consumes the DLQ envelopes and passes them to a configured Dead Letter Strategy for final disposition.</p>"},{"location":"athomic/integration/messaging/retry-dlq/#available-strategies","title":"Available Strategies","text":"<ul> <li><code>logging_only</code> (Default): This strategy simply logs the full details of the failed message envelope as a structured <code>ERROR</code> log. This is the safest default, ensuring that failing messages are recorded for manual inspection without taking further automated action.</li> <li><code>republish_to_parking_lot</code>: This strategy republishes the failed message to yet another topic, a \"parking lot\", which can be used for long-term storage or offline analysis.</li> </ul>"},{"location":"athomic/integration/messaging/retry-dlq/#configuration","title":"Configuration","text":"<p>The retry and DLQ mechanism is configured under the <code>[integration.messaging.dlq]</code> section in your <code>settings.toml</code>.</p> <pre><code>[default.integration.messaging.dlq]\n# A master switch to enable or disable the entire retry/DLQ flow.\nenabled = true\n\n# The maximum number of times to retry processing a message before sending it to the DLQ.\nmax_attempts = 3\n\n# The name of the Dead Letter Queue topic where failed messages will be sent.\ntopic = \"platform.dead-letter-queue.v1\"\n\n# The strategy for handling messages that arrive in the DLQ.\n# Can be \"logging_only\" or \"republish_to_parking_lot\".\nstrategy = \"logging_only\"\n\n# (Optional) The topic to use if the 'republish_to_parking_lot' strategy is chosen.\n# parking_lot_topic = \"platform.parking-lot.v1\"\n</code></pre>"},{"location":"athomic/integration/messaging/retry-dlq/#api-reference","title":"API Reference","text":""},{"location":"athomic/integration/messaging/retry-dlq/#nala.athomic.integration.messaging.retry.handler.DLQHandler","title":"<code>nala.athomic.integration.messaging.retry.handler.DLQHandler</code>","text":"<p>Orchestrates the handling of consumption failures, implementing the retry mechanism (republishing) and the final dead-letter queue (DLQ) publishing.</p> <p>This class serves as the decision point: - If attempts &lt; max_attempts and it's a single message: Republish for retry. - If attempts &gt;= max_attempts or it's a batch failure: Send to DLQ.</p>"},{"location":"athomic/integration/messaging/retry-dlq/#nala.athomic.integration.messaging.retry.handler.DLQHandler.__init__","title":"<code>__init__(producer, retry_policy, serializer, topic, max_attempts)</code>","text":"<p>Initializes the DLQ handler with all necessary dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>producer</code> <code>ProducerProtocol</code> <p>The producer used to send messages (for retry or DLQ).</p> required <code>retry_policy</code> <code>MaxAttemptsPolicy</code> <p>The policy defining how retry headers and counts are managed.</p> required <code>serializer</code> <code>SerializerProtocol</code> <p>The serializer used for general data conversion.</p> required <code>topic</code> <code>str</code> <p>The destination topic name for the Dead Letter Queue.</p> required <code>max_attempts</code> <code>int</code> <p>The maximum number of times to retry a message before sending to DLQ.</p> required"},{"location":"athomic/integration/messaging/retry-dlq/#nala.athomic.integration.messaging.retry.handler.DLQHandler.handle_failure","title":"<code>handle_failure(context)</code>  <code>async</code>","text":"<p>Orchestrates failure handling using an explicit context object. - Single messages will be retried up to <code>max_attempts</code>. - Batch messages will be sent directly to the DLQ without retry attempts.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>FailureContext</code> <p>The <code>FailureContext</code> object containing details of the consumption failure.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the message was handled (retried or sent to DLQ), False otherwise.</p>"},{"location":"athomic/integration/messaging/retry-dlq/#nala.athomic.integration.messaging.retry.processor_service.DeadLetterProcessorService","title":"<code>nala.athomic.integration.messaging.retry.processor_service.DeadLetterProcessorService</code>","text":"<p>               Bases: <code>BaseService</code></p> <p>A dedicated background service that manages the lifecycle of a consumer subscribed to the Dead Letter Queue (DLQ).</p> <p>This service is backend-agnostic as it receives a pre-configured consumer instance via dependency injection (DIP). Its purpose is purely to ensure the DLQ consumer is started and stopped gracefully with the application.</p>"},{"location":"athomic/integration/messaging/retry-dlq/#nala.athomic.integration.messaging.retry.processor_service.DeadLetterProcessorService.__init__","title":"<code>__init__(consumer)</code>","text":"<p>Initializes the service with a pre-configured DLQ consumer.</p> <p>Parameters:</p> Name Type Description Default <code>consumer</code> <code>BaseConsumer</code> <p>A consumer instance that is already configured to subscribe       to the DLQ topic and process its messages. This consumer       is typically created by a Factory and injected here.</p> required"},{"location":"athomic/integration/messaging/retry-dlq/#nala.athomic.integration.messaging.retry.strategies.protocol.DeadLetterStrategyProtocol","title":"<code>nala.athomic.integration.messaging.retry.strategies.protocol.DeadLetterStrategyProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the contract for strategies that manage and handle messages that have permanently failed processing (Dead Letter Queue - DLQ).</p> <p>Any class implementing this protocol must provide a concrete implementation for the asynchronous 'handle' method.</p>"},{"location":"athomic/integration/messaging/retry-dlq/#nala.athomic.integration.messaging.retry.strategies.protocol.DeadLetterStrategyProtocol.handle","title":"<code>handle(failure_details)</code>  <code>async</code>","text":"<p>Processes a message that has permanently failed and requires a final disposition (e.g., logging, manual queue, or permanent storage).</p> <p>Parameters:</p> Name Type Description Default <code>failure_details</code> <code>Dict[str, Any]</code> <p>A dictionary containing all relevant information              about the failed message, including payload,              exception details, and retry history.</p> required <p>Returns:</p> Name Type Description <code>ProcessingOutcome</code> <code>ProcessingOutcome</code> <p>An enum indicating the result of the DLQ handling process.                - ACK: Indicates the handling strategy was successful                       and the message can be removed from the DLQ.                - REQUEUE: Indicates a temporary failure in the handling                           process, and the message should be requeued                           to the DLQ for another attempt.</p>"},{"location":"athomic/observability/health/","title":"Health &amp; Readiness Checks","text":""},{"location":"athomic/observability/health/#overview","title":"Overview","text":"<p>The Health &amp; Readiness module provides a standardized and extensible system for determining if the application is healthy and ready to handle traffic. This is a critical feature for running in orchestrated environments like Kubernetes, which rely on readiness probes to know when to add a service instance to the load balancer.</p> <p>The framework exposes an HTTP endpoint, typically <code>/readyz</code>, which runs a series of checks against all critical dependencies (databases, message brokers, external APIs) and reports their status.</p>"},{"location":"athomic/observability/health/#how-it-works","title":"How It Works","text":"<p>The system is built around a few core components that promote decoupling and extensibility:</p> <ol> <li> <p><code>ReadinessCheck</code> Protocol: A simple contract that any readiness check must follow. It requires a unique <code>name</code>, an <code>enabled()</code> method, and an asynchronous <code>check()</code> method that returns <code>True</code> for healthy or <code>False</code> for unhealthy.</p> </li> <li> <p><code>ReadinessRegistry</code>: A singleton registry where all readiness check instances are registered during application startup.</p> </li> <li> <p><code>ServiceReadinessCheck</code>: A generic and powerful implementation that can check the status of any Athomic <code>BaseService</code>. It automatically integrates with the service lifecycle, so a readiness check for the Kafka consumer, for example, simply queries <code>kafka_consumer.is_ready()</code>.</p> </li> <li> <p><code>/readyz</code> Endpoint: An internal API route that, when called, executes the <code>run_all()</code> method on the <code>ReadinessRegistry</code>. This runs all registered checks concurrently and aggregates their results into a single JSON response. The overall HTTP status will be <code>200 OK</code> only if all enabled checks pass.</p> </li> </ol>"},{"location":"athomic/observability/health/#how-to-add-a-custom-readiness-check","title":"How to Add a Custom Readiness Check","text":"<p>You can easily add your own application-specific readiness checks. For example, you might want to check the status of a critical third-party API that your service depends on.</p>"},{"location":"athomic/observability/health/#1-create-the-check-class","title":"1. Create the Check Class","text":"<p>Create a class that implements the <code>ReadinessCheck</code> protocol.</p> <pre><code># In your_app/health_checks.py\nfrom nala.athomic.http import HttpClientFactory\nfrom nala.athomic.observability.health import ReadinessCheck\n\nclass ExternalApiServiceCheck(ReadinessCheck):\n    name = \"external_api_status\"\n\n    def __init__(self):\n        # Get a pre-configured HTTP client from the factory\n        self.http_client = HttpClientFactory.create(\"my_external_api_client\")\n\n    def enabled(self) -&gt; bool:\n        # The check is enabled if the client itself is enabled in the config\n        return self.http_client.is_enabled()\n\n    async def check(self) -&gt; bool:\n        try:\n            # Perform a lightweight check, like a HEAD request or a health endpoint call\n            response = await self.http_client.get(\"/_health\")\n            return response.status_code == 200\n        except Exception:\n            return False\n</code></pre>"},{"location":"athomic/observability/health/#2-register-the-check","title":"2. Register the Check","text":"<p>In your application's startup sequence (e.g., <code>domain_initializers.py</code>), instantiate your check and register it.</p> <pre><code># In your_app/startup/domain_initializers.py\nfrom nala.athomic.observability.health import readiness_registry\nfrom your_app.health_checks import ExternalApiServiceCheck\n\ndef register_domain_services():\n    # ... other registrations ...\n\n    # Register your custom health check\n    readiness_registry.register(ExternalApiServiceCheck())\n</code></pre> <p>Your custom check will now be automatically executed and reported by the <code>/readyz</code> endpoint.</p>"},{"location":"athomic/observability/health/#example-response","title":"Example Response","text":"<p>A call to the <code>/readyz</code> endpoint will return a JSON response detailing the status of each check.</p> <pre><code>{\n  \"status\": \"unhealthy\",\n  \"checks\": {\n    \"consul_client\": \"ok\",\n    \"database_connection_manager\": \"ok\",\n    \"kafka_consumer_my_app.events.v1\": \"ok\",\n    \"external_api_status\": \"fail\"\n  }\n}\n</code></pre>"},{"location":"athomic/observability/health/#api-reference","title":"API Reference","text":""},{"location":"athomic/observability/health/#nala.athomic.observability.health.protocol.ReadinessCheck","title":"<code>nala.athomic.observability.health.protocol.ReadinessCheck</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the contract for an individual readiness check implementation.</p> <p>Any class that implements this protocol can be registered with the ReadinessRegistry to contribute to the overall application readiness state.</p>"},{"location":"athomic/observability/health/#nala.athomic.observability.health.protocol.ReadinessCheck.name","title":"<code>name</code>  <code>instance-attribute</code>","text":"<p>A unique, descriptive name for the check (e.g., 'database_connection').</p>"},{"location":"athomic/observability/health/#nala.athomic.observability.health.protocol.ReadinessCheck.check","title":"<code>check()</code>  <code>async</code>","text":"<p>Performs the asynchronous check of the dependency or resource.</p> <p>This method must be lightweight and fast to avoid delaying the readiness probe.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the resource is healthy (ready), False otherwise.</p>"},{"location":"athomic/observability/health/#nala.athomic.observability.health.protocol.ReadinessCheck.enabled","title":"<code>enabled()</code>","text":"<p>Determines if the check should be executed based on configuration or runtime environment.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the check should run, False otherwise.</p>"},{"location":"athomic/observability/health/#nala.athomic.observability.health.registry.ReadinessRegistry","title":"<code>nala.athomic.observability.health.registry.ReadinessRegistry</code>","text":"<p>A registry responsible for collecting and orchestrating all application readiness checks.</p> <p>This acts as a centralized source of truth for determining if the application and its core dependencies (databases, message brokers, external services) are fully initialized and ready to handle live traffic.</p>"},{"location":"athomic/observability/health/#nala.athomic.observability.health.registry.ReadinessRegistry.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the internal dictionary to store readiness checks, mapped by name.</p>"},{"location":"athomic/observability/health/#nala.athomic.observability.health.registry.ReadinessRegistry.register","title":"<code>register(check)</code>","text":"<p>Registers a new ReadinessCheck implementation with the registry.</p> <p>Parameters:</p> Name Type Description Default <code>check</code> <code>ReadinessCheck</code> <p>An instance of a ReadinessCheck protocol implementation.</p> required"},{"location":"athomic/observability/health/#nala.athomic.observability.health.registry.ReadinessRegistry.run_all","title":"<code>run_all()</code>  <code>async</code>","text":"<p>Executes all registered readiness checks asynchronously.</p> <p>It respects the <code>enabled()</code> status of each check and handles exceptions during execution by marking the check as failed.</p> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: A dictionary containing the name of each check             and its resulting status: 'ok', 'fail', or 'skipped'.</p>"},{"location":"athomic/observability/health/#nala.athomic.observability.health.checks.service_check.ServiceReadinessCheck","title":"<code>nala.athomic.observability.health.checks.service_check.ServiceReadinessCheck</code>","text":"<p>               Bases: <code>ReadinessCheck</code></p> <p>A generic readiness check implementation that verifies the health and readiness state of any core Athomic service implementing the BaseServiceProtocol.</p> <p>This check is a crucial part of the Dependency Inversion Principle, allowing the health system to query service status without knowing the service's internal implementation details.</p>"},{"location":"athomic/observability/health/#nala.athomic.observability.health.checks.service_check.ServiceReadinessCheck.__init__","title":"<code>__init__(service)</code>","text":"<p>Initializes the check by injecting the service instance to be monitored.</p> <p>Parameters:</p> Name Type Description Default <code>service</code> <code>BaseServiceProtocol</code> <p>The service instance (e.g., OutboxPublisher, HttpClient)      whose readiness state will be checked.</p> required"},{"location":"athomic/observability/health/#nala.athomic.observability.health.checks.service_check.ServiceReadinessCheck.check","title":"<code>check()</code>  <code>async</code>","text":"<p>Checks if the service is ready to operate (e.g., connected to its dependencies and initialized).</p> <p>Delegates the call directly to the service's <code>is_ready()</code> method.</p>"},{"location":"athomic/observability/health/#nala.athomic.observability.health.checks.service_check.ServiceReadinessCheck.enabled","title":"<code>enabled()</code>","text":"<p>Checks if the underlying service is enabled based on its configuration.</p> <p>Delegates the call directly to the service's <code>is_enabled()</code> method.</p>"},{"location":"athomic/observability/log/","title":"Structured Logging","text":""},{"location":"athomic/observability/log/#overview","title":"Overview","text":"<p>The logging module provides a powerful, structured, and secure logging system built upon the <code>Loguru</code> library. It is designed to be both highly performant and developer-friendly, with two primary goals:</p> <ol> <li>Rich, Structured Logs: Automatically enriches log records with contextual information from the <code>Context</code> module (e.g., <code>request_id</code>, <code>tenant_id</code>, <code>trace_id</code>). It can output logs as JSON, making them easy to parse, index, and query in modern log management systems.</li> <li>Security by Default: Includes a robust and extensible sensitive data masking engine that automatically redacts PII (Personally Identifiable Information) and secrets from log messages before they are written to any sink.</li> </ol>"},{"location":"athomic/observability/log/#how-to-use","title":"How to Use","text":"<p>Getting a logger instance is simple. The <code>get_logger()</code> function returns a pre-configured singleton instance of the Loguru logger.</p> <pre><code>from nala.athomic.observability import get_logger\n\n# It's recommended to get a logger scoped to your module\nlogger = get_logger(__name__)\n\ndef process_user_data(user_id: str):\n    # This log record will be automatically enriched with any\n    # context variables that are currently set.\n    logger.info(f\"Processing data for user {user_id}\")\n</code></pre>"},{"location":"athomic/observability/log/#sensitive-data-masking","title":"Sensitive Data Masking","text":"<p>This is a core security feature of the Athomic Layer. The <code>SensitiveDataFilter</code> automatically finds and redacts sensitive information from log messages before they are written.</p>"},{"location":"athomic/observability/log/#out-of-the-box-patterns","title":"Out-of-the-Box Patterns","text":"<p>The filter comes with a wide range of built-in patterns for common PII and secrets, including:</p> <ul> <li>JWTs: Replaces JSON Web Tokens with <code>***REDACTED_JWT***</code>.</li> <li>Credit Card Numbers: Performs PCI-compliant masking, showing only the last four digits (e.g., <code>****-****-****-1234</code>).</li> <li>Brazilian CPFs: Performs smart masking, showing only the verification digits (e.g., <code>***.***.***-56</code>).</li> <li>Phone Numbers: Performs smart masking, preserving the area code and last two digits (e.g., <code>(11) *****-**78</code>).</li> <li>Email Addresses: Replaces emails with <code>***@***.***</code>.</li> <li>Common Secret Keys: Redacts values for keys like <code>\"password\"</code>, <code>\"api_key\"</code>, and <code>\"token\"</code> in JSON-like strings.</li> <li>Authorization Headers: Redacts the token from <code>Authorization: Bearer ...</code> headers.</li> </ul>"},{"location":"athomic/observability/log/#pattern-scoring-system","title":"Pattern Scoring System","text":"<p>To ensure accuracy, the filter applies masking patterns in order of specificity. A highly specific pattern for a JWT will run before a more generic pattern for an email address, preventing incorrect redactions.</p>"},{"location":"athomic/observability/log/#adding-custom-masking-patterns","title":"Adding Custom Masking Patterns","text":"<p>You can easily extend the masker with your own regex patterns directly in your configuration file.</p> <pre><code># In settings.toml\n[default.logging]\n# ... other logging settings ...\n\n[[default.logging.sensitive_patterns]]\n# This will find any string matching \"secret-project-key-...\" and redact it.\nregex = \"secret-project-key-[a-zA-Z0-9]+\"\nreplacement = \"REDACTED_PROJECT_KEY\"\n\n[[default.logging.sensitive_patterns]]\n# You can also use regex capture groups.\nregex = \"user_phone_number=(d+)\"\nreplacement = \"user_phone_number=REDACTED\"\n</code></pre>"},{"location":"athomic/observability/log/#json-logging","title":"JSON Logging","text":"<p>For production environments, it is highly recommended to enable JSON logging. This formats every log entry as a structured JSON object, which can be easily ingested and indexed by platforms like Elasticsearch (ELK Stack), Datadog, Splunk, or Google Cloud Logging.</p> <p>To enable it, simply set <code>serialize = true</code> in your configuration.</p>"},{"location":"athomic/observability/log/#configuration","title":"Configuration","text":"<p>The logging system is configured under the <code>[logging]</code> section in your <code>settings.toml</code>.</p> <pre><code>[default.logging]\n# The minimum log level to process (e.g., \"DEBUG\", \"INFO\", \"WARNING\").\nlevel = \"INFO\"\n\n# If true, log records are formatted as JSON strings.\nserialize = true\n\n# If true, logs are written to a file instead of the console.\nlog_to_file = false\nlog_file_path = \".logs/app.log\"\nrotation = \"100 MB\"\nretention = \"7 days\"\n\n# A list of custom patterns for the sensitive data filter.\n[[default.logging.sensitive_patterns]]\nregex = \"my-custom-secret-([a-z0-9-]+)\"\nreplacement = \"my-custom-secret-REDACTED\"\n</code></pre>"},{"location":"athomic/observability/log/#api-reference","title":"API Reference","text":""},{"location":"athomic/observability/log/#nala.athomic.observability.get_logger","title":"<code>nala.athomic.observability.get_logger(name=None)</code>","text":"<p>Retrieves a logger instance, binding the component name if provided. This is the primary way for application modules to obtain a logger instance.</p>"},{"location":"athomic/observability/log/#nala.athomic.observability.log.filters.sensitive_data_filter.SensitiveDataFilter","title":"<code>nala.athomic.observability.log.filters.sensitive_data_filter.SensitiveDataFilter</code>","text":"<p>A callable filter designed to sanitize log messages by redacting or masking sensitive data (PII, secrets, tokens) before they are written to any sink.</p> <p>This filter is the core enforcement mechanism for log compliance and security.</p>"},{"location":"athomic/observability/log/#nala.athomic.observability.log.filters.sensitive_data_filter.SensitiveDataFilter.__call__","title":"<code>__call__(message)</code>","text":"<p>Makes the filter instance callable, which is the contract required by Loguru.</p>"},{"location":"athomic/observability/log/#nala.athomic.observability.log.filters.sensitive_data_filter.SensitiveDataFilter.__init__","title":"<code>__init__(patterns, repl='***REDACTED***')</code>","text":"<p>Initializes the filter by compiling all registered and configured patterns.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>List[tuple[Union[str, Pattern], Union[str, Callable[[Match], str]]]]</code> <p>A list of maskers/patterns, potentially unsorted.</p> required <code>repl</code> <code>str</code> <p>Default replacement string (not currently used in internal logic).</p> <code>'***REDACTED***'</code>"},{"location":"athomic/observability/log/#nala.athomic.observability.log.maskers.base_masker.BaseMasker","title":"<code>nala.athomic.observability.log.maskers.base_masker.BaseMasker</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract Base Class (ABC) for all sensitive data maskers within the Athomic framework.</p> <p>This class enforces the contract required by the SensitiveDataFilter, ensuring that every masker provides both a unique regex pattern and a specific masking implementation.</p>"},{"location":"athomic/observability/log/#nala.athomic.observability.log.maskers.base_masker.BaseMasker.mask","title":"<code>mask(match)</code>  <code>abstractmethod</code>","text":"<p>Receives the regex match object for the sensitive data and returns the masked (redacted) version of the data.</p> <p>This method should contain the core logic for intelligent masking.</p> <p>Parameters:</p> Name Type Description Default <code>match</code> <code>Match</code> <p>The regex match object containing the captured sensitive data.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The masked replacement string.</p>"},{"location":"athomic/observability/log/#nala.athomic.observability.log.maskers.base_masker.BaseMasker.pattern","title":"<code>pattern()</code>  <code>abstractmethod</code>","text":"<p>Returns the compiled regex pattern object that this masker is responsible for detecting.</p> <p>Returns:</p> Type Description <code>Pattern</code> <p>re.Pattern: The compiled regular expression.</p>"},{"location":"athomic/observability/metrics/","title":"Metrics","text":""},{"location":"athomic/observability/metrics/#overview","title":"Overview","text":"<p>The metrics module provides deep, out-of-the-box application monitoring using the Prometheus format, the de-facto standard for cloud-native observability. Nearly every component in the Athomic Layer is instrumented with detailed metrics, offering invaluable insights into performance, throughput, error rates, and overall system health.</p> <p>This allows you to create powerful dashboards and alerts to monitor your application in real-time.</p>"},{"location":"athomic/observability/metrics/#how-it-works","title":"How It Works","text":"<ol> <li>Metric Registration: Throughout the Athomic codebase, Prometheus metrics (<code>Counter</code>, <code>Histogram</code>, <code>Gauge</code>) are defined to track specific events and states.</li> <li>Instrumentation: Core components automatically update these metrics during their operation. For example, the <code>HttpClient</code> updates a histogram with request latency and a counter for successes or failures.</li> <li>Exposure: At startup, the application automatically starts a lightweight HTTP server that exposes a <code>/metrics</code> endpoint. This endpoint serves all registered metrics in the text-based Prometheus format.</li> <li>Scraping: A Prometheus server is then configured to periodically \"scrape\" (fetch) the data from this <code>/metrics</code> endpoint, storing it as a time series.</li> </ol>"},{"location":"athomic/observability/metrics/#the-metricscheduler","title":"The <code>MetricScheduler</code>","text":"<p>For metrics that cannot be updated on-event (like the current number of pending messages), Athomic includes a <code>MetricScheduler</code>. This is a background service that periodically runs \"Probes\" (<code>MetricProbe</code> implementations) to collect and update gauge-based metrics, such as Kafka consumer lag.</p>"},{"location":"athomic/observability/metrics/#built-in-metrics","title":"Built-in Metrics","text":"<p>Athomic provides a comprehensive set of built-in metrics. Below is a high-level summary of what is available out-of-the-box. For a full list, you can inspect the <code>/metrics</code> endpoint of a running application.</p> <ul> <li>API Layer:<ul> <li>Request rate, error rate, and latency (per route, method, and status code).</li> <li>Number of in-progress requests.</li> </ul> </li> <li>Database &amp; Cache:<ul> <li>Operation latency and total counts for each KV store and document database operation (<code>get</code>, <code>set</code>, <code>find_by_id</code>, etc.).</li> <li>Cache hit and miss ratios.</li> </ul> </li> <li>Messaging (Kafka):<ul> <li>Number of messages published and consumed.</li> <li>Message publishing latency.</li> <li>Consumer Lag per topic and partition.</li> <li>Total messages sent to the Dead Letter Queue (DLQ).</li> </ul> </li> <li>Resilience Patterns:<ul> <li>Circuit Breaker: State changes, failures recorded, and calls blocked.</li> <li>Retry: Total retry attempts and permanent failures.</li> <li>Rate Limiter: Total requests allowed and blocked per policy.</li> <li>Bulkhead: Concurrently executing requests and rejections.</li> </ul> </li> <li>Transactional Outbox:<ul> <li>Number of events processed (success/failure).</li> <li>Event processing lag (time between creation and publication).</li> <li>Pending messages for the \"hottest\" aggregate keys.</li> </ul> </li> </ul>"},{"location":"athomic/observability/metrics/#adding-custom-metrics","title":"Adding Custom Metrics","text":"<p>You can easily define and use your own custom metrics within your application's business logic using the <code>prometheus-client</code> library.</p> <pre><code>from prometheus_client import Counter\nfrom nala.athomic.observability import get_logger\n\nlogger = get_logger(__name__)\n\n# 1. Define your metric at the module level.\nORDERS_PROCESSED_TOTAL = Counter(\n    \"orders_processed_total\",\n    \"Total number of orders processed.\",\n    [\"order_type\", \"status\"]\n)\n\nclass OrderService:\n    async def process_order(self, order: Order):\n        try:\n            # ... your business logic ...\n\n            # 2. Increment the counter on success.\n            ORDERS_PROCESSED_TOTAL.labels(order_type=order.type, status=\"success\").inc()\n            logger.info(\"Order processed successfully.\")\n\n        except Exception:\n            # 3. Increment the counter with a different label on failure.\n            ORDERS_PROCESSED_TOTAL.labels(order_type=order.type, status=\"failure\").inc()\n            logger.error(\"Failed to process order.\")\n            raise\n</code></pre>"},{"location":"athomic/observability/metrics/#configuration","title":"Configuration","text":"<p>Metrics are configured under the <code>[observability]</code> and <code>[observability.metrics]</code> sections in your <code>settings.toml</code>.</p> <pre><code>[default.observability]\nenabled = true\n\n# If true, starts an HTTP server to expose Prometheus metrics.\nexporter_enabled = true\n\n# The port on which the Prometheus metrics server will listen.\nexporter_port = 9100\n\n  [default.observability.metrics]\n  enabled = true\n\n  # The interval in seconds at which the MetricScheduler runs its probes (e.g., for Kafka lag).\n  collection_interval_seconds = 60\n\n  # If true, access to the /metrics endpoint is restricted to the IPs below.\n  metrics_protection_enabled = false\n  allow_metrics_ips = [\"127.0.0.1\", \"10.0.0.5\"]\n</code></pre>"},{"location":"athomic/observability/metrics/#api-reference","title":"API Reference","text":""},{"location":"athomic/observability/metrics/#nala.athomic.observability.metrics.exporter.start_metrics_server","title":"<code>nala.athomic.observability.metrics.exporter.start_metrics_server()</code>","text":"<p>Initializes and starts the HTTP server to expose Prometheus metrics for scraping by external monitoring systems.</p> <p>This operation is conditional and runs only if observability is enabled in the application settings.</p>"},{"location":"athomic/observability/metrics/#nala.athomic.observability.metrics.metric_scheduler.MetricScheduler","title":"<code>nala.athomic.observability.metrics.metric_scheduler.MetricScheduler</code>","text":"<p>               Bases: <code>BaseService</code></p> <p>Manages a background task responsible for the periodic execution of all registered MetricProbes in the central ProbeRegistry.</p> <p>This service ensures continuous and asynchronous collection of application and infrastructure telemetry.</p>"},{"location":"athomic/observability/metrics/#nala.athomic.observability.metrics.metric_scheduler.MetricScheduler.__init__","title":"<code>__init__(collection_interval_seconds)</code>","text":"<p>Initializes the scheduler with the specified collection interval.</p> <p>Parameters:</p> Name Type Description Default <code>collection_interval_seconds</code> <code>int</code> <p>The frequency (in seconds) at which                          the probes should be executed.</p> required"},{"location":"athomic/observability/metrics/#nala.athomic.observability.metrics.protocol.MetricProbe","title":"<code>nala.athomic.observability.metrics.protocol.MetricProbe</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the contract for a component that periodically collects and updates specific application metrics (Probes).</p> <p>Any class implementing this protocol can be scheduled for asynchronous execution by a MetricScheduler service.</p>"},{"location":"athomic/observability/metrics/#nala.athomic.observability.metrics.protocol.MetricProbe.name","title":"<code>name</code>  <code>instance-attribute</code>","text":"<p>A unique, human-readable name identifying the metric probe (e.g., 'cpu_usage_probe').</p>"},{"location":"athomic/observability/metrics/#nala.athomic.observability.metrics.protocol.MetricProbe.update","title":"<code>update()</code>  <code>async</code>","text":"<p>Executes the metric collection logic (e.g., reads a value, calculates a delta) and updates the corresponding Prometheus collector.</p> <p>This method must be non-blocking (asynchronous).</p>"},{"location":"athomic/observability/tracing/","title":"Distributed Tracing","text":""},{"location":"athomic/observability/tracing/#overview","title":"Overview","text":"<p>The distributed tracing module provides end-to-end request tracing capabilities built on the OpenTelemetry standard, the industry benchmark for observability. This feature is invaluable for debugging performance bottlenecks and understanding complex, asynchronous workflows in a microservices architecture.</p> <p>It allows you to visualize the entire journey of a request as it travels through different services and components\u2014from the initial API call, through message brokers, to database queries and back.</p>"},{"location":"athomic/observability/tracing/#key-features","title":"Key Features","text":"<ul> <li>OpenTelemetry Native: Fully compliant with the OpenTelemetry standard, allowing integration with any OTLP-compatible backend (e.g., Jaeger, Datadog, Honeycomb).</li> <li>Auto-Instrumentation: Automatically creates trace spans for common I/O operations, including outgoing HTTP requests (<code>httpx</code>), database queries (<code>pymongo</code>), and cache interactions (<code>redis</code>).</li> <li>Effortless Manual Instrumentation: A simple <code>@with_observability</code> decorator allows you to add detailed tracing to your business logic with a single line of code.</li> <li>Automatic Context Propagation: The trace context (e.g., <code>trace_id</code>, <code>span_id</code>) is automatically propagated across service boundaries, including HTTP calls and messages sent via a broker like Kafka.</li> </ul>"},{"location":"athomic/observability/tracing/#how-it-works","title":"How It Works","text":""},{"location":"athomic/observability/tracing/#setup","title":"Setup","text":"<p>During application startup, the <code>setup_tracing()</code> function is automatically called. It configures the global OpenTelemetry SDK, initializes an OTLP exporter to send trace data to a collector, and applies auto-instrumentation patches to key libraries.</p>"},{"location":"athomic/observability/tracing/#instrumentation","title":"Instrumentation","text":"<ul> <li>Automatic: For libraries like <code>httpx</code> and <code>pymongo</code>, you get tracing for free. Every database query or external API call will appear as a span in your trace without you writing any extra code.</li> <li>Manual: For your own business logic, you should use the provided decorators to create spans that represent meaningful units of work.</li> </ul>"},{"location":"athomic/observability/tracing/#usage-the-with_observability-decorator","title":"Usage: The <code>@with_observability</code> Decorator","text":"<p>The easiest and recommended way to add tracing to your code is with the <code>@with_observability</code> decorator. It's a powerful tool that automatically handles the entire span lifecycle.</p> <p>When you decorate a function, it will: 1.  Start a new span when the function is called. 2.  Automatically add useful attributes to the span, such as the function's arguments. 3.  Measure the execution duration. 4.  Record any exceptions that occur. 5.  Set the span's final status (<code>OK</code> or <code>ERROR</code>).</p>"},{"location":"athomic/observability/tracing/#example","title":"Example","text":"<pre><code>from nala.athomic.observability.decorators import with_observability\n\nclass UserProfileService:\n    @with_observability(\n        name=\"service.get_user_profile\",\n        attributes_from_args={\"user_id\": \"user.id\"}\n    )\n    async def get_profile(self, user_id: str) -&gt; dict:\n        # ... your business logic ...\n        profile_data = await self.repository.find_by_id(user_id)\n        # ... more logic ...\n        return profile_data\n</code></pre> <p>In this example, every call to <code>get_profile</code> will generate a trace span named <code>\"service.get_user_profile\"</code>, and it will automatically include an attribute <code>user.id</code> with the value of the <code>user_id</code> argument.</p>"},{"location":"athomic/observability/tracing/#context-propagation","title":"Context Propagation","text":"<p>Athomic handles trace context propagation automatically across service boundaries:</p> <ul> <li>HTTP Requests: The <code>HttpClient</code> automatically injects W3C Trace Context headers into outgoing requests. The API middleware automatically extracts them from incoming requests, ensuring the trace continues seamlessly.</li> <li>Messaging: When a message is published, the <code>Producer</code> injects the trace context into the message headers. The <code>Consumer</code> on the other side extracts it, allowing a single trace to span from an API request to a background worker processing a resulting message.</li> </ul>"},{"location":"athomic/observability/tracing/#configuration","title":"Configuration","text":"<p>Tracing is configured under the <code>[observability]</code> section in your <code>settings.toml</code>.</p> <pre><code>[default.observability]\n# A master switch for all observability features (metrics and tracing).\nenabled = true\n\n# A master switch specifically for distributed tracing.\ntracing_enabled = true\n\n# The gRPC or HTTP endpoint of the OpenTelemetry Collector.\n# Spans will be sent here.\n# Example for a local Jaeger setup:\notlp_endpoint = \"http://localhost:4317\"\n\n# The sampling rate for traces (1.0 = 100%, 0.5 = 50%).\nsampling_rate = 1.0\n\n# Optional: Override the service name that appears in your tracing backend.\n# If not set, it defaults to the `app_name`.\nservice_name_override = \"my-awesome-service\"\n</code></pre>"},{"location":"athomic/observability/tracing/#api-reference","title":"API Reference","text":""},{"location":"athomic/observability/tracing/#nala.athomic.observability.tracing.setup_tracing","title":"<code>nala.athomic.observability.tracing.setup_tracing(settings=None)</code>","text":"<p>Initializes the OpenTelemetry Tracing SDK, configures the OTLP exporter, and instruments core libraries.</p> <p>This function is idempotent and serves as the central configuration point for distributed tracing.</p>"},{"location":"athomic/observability/tracing/#nala.athomic.observability.decorators.with_observability","title":"<code>nala.athomic.observability.decorators.with_observability</code>","text":""},{"location":"athomic/observability/tracing/#nala.athomic.observability.decorators.with_observability.build_span_attributes","title":"<code>build_span_attributes(bound_args, static_attributes, attributes_from_args)</code>","text":"<p>Constructs OpenTelemetry span attributes from context and function arguments.</p>"},{"location":"athomic/observability/tracing/#nala.athomic.observability.decorators.with_observability.log_call","title":"<code>log_call(logger, func_name, args, kwargs)</code>","text":"<p>Logs the function call, including arguments and execution context.</p>"},{"location":"athomic/observability/tracing/#nala.athomic.observability.decorators.with_observability.log_error","title":"<code>log_error(logger, func_name, error)</code>","text":"<p>Logs detailed exception information upon function failure.</p>"},{"location":"athomic/observability/tracing/#nala.athomic.observability.decorators.with_observability.log_result","title":"<code>log_result(logger, func_name, result)</code>","text":"<p>Logs the function return value upon successful completion.</p>"},{"location":"athomic/observability/tracing/#nala.athomic.observability.decorators.with_observability.with_observability","title":"<code>with_observability(name=None, kind=SpanKind.INTERNAL, attributes_from_args=None, static_attributes=None, should_log_result=True, log_args=True)</code>","text":"<p>A unified decorator for Observability that instruments a function with OpenTelemetry tracing and structured logging (call, result, error).</p> <p>This combines tracing and logging boilerplate into a single, declarative wrapper, adhering to Aspect-Oriented Programming (AOP).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>Optional name for the span. Defaults to the decorated function's name.</p> <code>None</code> <code>kind</code> <code>SpanKind</code> <p>The SpanKind (e.g., SERVER, CLIENT, CONSUMER, PRODUCER). Defaults to INTERNAL.</p> <code>INTERNAL</code> <code>attributes_from_args</code> <code>Optional[Dict[str, str]]</code> <p>A mapping of {function_arg_name: span_attribute_key}                   to dynamically extract values from the call signature.</p> <code>None</code> <code>static_attributes</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary of static key/value pairs to add to the span.</p> <code>None</code> <code>should_log_result</code> <code>bool</code> <p>If True, logs the function's return value. Defaults to True.</p> <code>True</code> <code>log_args</code> <code>bool</code> <p>If True, logs the function's arguments upon call. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable[..., Any]</code> <p>The decorator function that returns the wrapped function.</p>"},{"location":"athomic/observability/tracing/#nala.athomic.observability.get_tracer","title":"<code>nala.athomic.observability.get_tracer(name=None)</code>","text":"<p>Retrieves the global OpenTelemetry Tracer instance, optionally scoped by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>The name used to scope the tracer instance. Defaults to the   configured service name.</p> <code>None</code>"},{"location":"athomic/observability/tracing/#nala.athomic.observability.telemetry.adapters.messaging_adapter.MessagingTelemetryAdapter","title":"<code>nala.athomic.observability.telemetry.adapters.messaging_adapter.MessagingTelemetryAdapter</code>","text":"<p>               Bases: <code>TelemetryAdapterProtocol</code></p> <p>A concrete Telemetry Adapter for messaging systems.</p> <p>This adapter handles the injection and extraction of W3C Trace Context into and from the Athomic MessageHeaders object, enabling end-to-end tracing across message broker boundaries.</p>"},{"location":"athomic/observability/tracing/#nala.athomic.observability.telemetry.adapters.messaging_adapter.MessagingTelemetryAdapter.activate_from_headers","title":"<code>activate_from_headers(headers)</code>","text":"<p>A context manager that extracts the trace context from incoming message headers and activates it for the duration of the message processing.</p> <p>This is the primary method used by consumer implementations to continue the distributed trace from the producer.</p>"},{"location":"athomic/observability/tracing/#nala.athomic.observability.telemetry.adapters.messaging_adapter.MessagingTelemetryAdapter.extract_context","title":"<code>extract_context(headers)</code>","text":"<p>Extracts the trace context from an incoming MessageHeaders object and returns the active OpenTelemetry Context.</p>"},{"location":"athomic/observability/tracing/#nala.athomic.observability.telemetry.adapters.messaging_adapter.MessagingTelemetryAdapter.inject_context","title":"<code>inject_context(headers)</code>","text":"<p>Injects the current trace context and a unique 'message_id' into a MessageHeaders object for an outgoing message.</p>"},{"location":"athomic/performance/bootstrap/","title":"Performance Bootstrap (uvloop)","text":""},{"location":"athomic/performance/bootstrap/#overview","title":"Overview","text":"<p>The <code>athomic.performance.bootstrap</code> module contains utilities that should be run at the very beginning of the application's lifecycle to apply performance optimizations.</p>"},{"location":"athomic/performance/bootstrap/#install_uvloop_if_available","title":"<code>install_uvloop_if_available()</code>","text":"<p>The primary utility is <code>install_uvloop_if_available()</code>. uvloop is a high-performance, drop-in replacement for Python's default <code>asyncio</code> event loop. It is implemented in Cython and built on top of <code>libuv</code>, the same library that powers Node.js.</p> <p>By using uvloop, your application can achieve significant performance improvements for I/O-bound operations, often seeing a 2-4x increase in throughput.</p>"},{"location":"athomic/performance/bootstrap/#how-it-works","title":"How It Works","text":"<p>This function is called in your application's entrypoint (e.g., <code>main.py</code>) before any other code runs. It attempts to import <code>uvloop</code>. -   If the <code>uvloop</code> package is installed in the environment, it sets it as the global asyncio event loop policy. -   If it's not installed, it does nothing, and the application gracefully falls back to using the standard asyncio event loop.</p> <p>This allows <code>uvloop</code> to be an optional, production-only dependency.</p>"},{"location":"athomic/performance/cache/","title":"Caching","text":""},{"location":"athomic/performance/cache/#overview","title":"Overview","text":"<p>The Caching module provides a powerful, decorator-based system for caching the results of asynchronous functions. It is designed to be highly resilient and performant, implementing several advanced caching strategies out-of-the-box.</p>"},{"location":"athomic/performance/cache/#key-features","title":"Key Features","text":"<ul> <li><code>@cache</code> Decorator: The primary interface for caching function results.</li> <li><code>@invalidate_cache</code> Decorator: For declaratively invalidating cache keys.</li> <li>Resilient Fallback: Can be configured with a fallback cache (e.g., in-memory) that is used if the primary cache (e.g., Redis) is unavailable.</li> <li>Single-Flight Caching: Uses a distributed lock to prevent the \"thundering herd\" problem, where multiple concurrent requests for a missed key all trigger the expensive computation.</li> <li>Refresh-Ahead (Stale-While-Revalidate): Can serve stale data while a background task refreshes the cache, minimizing latency for users.</li> </ul>"},{"location":"athomic/performance/cache/#usage","title":"Usage","text":"<pre><code>from nala.athomic.performance import cache, invalidate_cache\n\nclass ProductService:\n    @cache(ttl=300, key_prefix=\"products\") # Cache results for 5 minutes\n    async def get_product_details(self, product_id: str) -&gt; dict:\n        # Expensive database call\n        return await db.fetch_product(product_id)\n\n    @invalidate_cache(key_prefix=\"products\", key_resolver=lambda result, **kwargs: f\"products:{kwargs['product_id']}\")\n    async def update_product_details(self, product_id: str, data: dict):\n        # Update the product in the database\n        # The cache for this product will be automatically invalidated.\n        return await db.update_product(product_id, data)\n</code></pre> <p>For more details on the resilient provider, see the Fallback documentation.</p>"},{"location":"athomic/performance/cache/#api-reference","title":"API Reference","text":""},{"location":"athomic/performance/cache/#nala.athomic.performance.cache.decorators.cache","title":"<code>nala.athomic.performance.cache.decorators.cache(ttl=60, key_prefix=None, key_resolver=None, use_jitter=False, use_lock=False, lock_timeout=30, refresh_ahead=False, refresh_threshold=None, provider=None, ttl_key=None)</code>","text":"<p>Decorator to cache the result of an asynchronous function using the Cache-Aside, Single-Flight, and Refresh-Ahead strategies.</p> <p>It collects all configuration parameters and delegates the complex execution logic to the CacheHandler.</p> <p>Parameters:</p> Name Type Description Default <code>ttl</code> <code>int</code> <p>Time to live (in seconds) for the cached item. Can be overridden by ttl_key.</p> <code>60</code> <code>key_prefix</code> <code>Optional[str]</code> <p>Static prefix for the cache key (e.g., 'user_service:').</p> <code>None</code> <code>key_resolver</code> <code>Optional[ContextualKeyResolverType]</code> <p>Custom function, string, or list to generate contextual keys.</p> <code>None</code> <code>use_jitter</code> <code>Optional[bool]</code> <p>If True, adds random variance to the TTL to prevent cache stampedes.</p> <code>False</code> <code>use_lock</code> <code>Optional[bool]</code> <p>If True, enables distributed locking (Single-Flight Caching).</p> <code>False</code> <code>lock_timeout</code> <code>Optional[int]</code> <p>Timeout in seconds for acquiring the distributed lock.</p> <code>30</code> <code>refresh_ahead</code> <code>Optional[bool]</code> <p>If True, enables the background refresh strategy (stale hit).</p> <code>False</code> <code>refresh_threshold</code> <code>Optional[float]</code> <p>Percentage of TTL (0.0 to 1.0) when the item is considered stale                and a background refresh should be triggered.</p> <code>None</code> <code>provider</code> <code>Optional[CacheProtocol]</code> <p>Optional explicit CacheProtocol instance (for testing). Defaults to       CacheFallbackFactory.create().</p> <code>None</code> <code>ttl_key</code> <code>Optional[str]</code> <p>Key name in Live Config to dynamically override the TTL.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable[..., Any]</code> <p>The decorator function.</p>"},{"location":"athomic/performance/cache/#nala.athomic.performance.cache.decorators.invalidate_cache","title":"<code>nala.athomic.performance.cache.decorators.invalidate_cache(key_prefix=None, key_resolver=None, provider=None)</code>","text":"<p>Decorator to invalidate cache after the decorated function is called. This decorator will call the <code>invalidate</code> method of the provided cache provider with the specified key prefix and context. Args:     key_prefix: An optional prefix to use for the cache keys.     key_resolver: An optional function to generate cache keys based on the function's context.     provider: An optional cache provider to use for invalidation.</p>"},{"location":"athomic/performance/compression_middleware/","title":"HTTP Compression Middleware","text":""},{"location":"athomic/performance/compression_middleware/#overview","title":"Overview","text":"<p>This module provides factories for creating ASGI compression middleware, which can automatically compress API responses to reduce payload size and improve network latency.</p>"},{"location":"athomic/performance/compression_middleware/#how-it-works","title":"How It Works","text":"<p>The <code>CompressionMiddlewareFactory</code> reads the application configuration and, if enabled, returns the configured middleware class (e.g., <code>GZipMiddleware</code> from Starlette) and its settings. This is then added to the main FastAPI/Starlette application's middleware stack.</p>"},{"location":"athomic/performance/compression_middleware/#configuration","title":"Configuration","text":"<pre><code>[default.performance.compression]\nenabled = true\n# Can be \"gzip\" or \"brotli\" (if brotli-asgi is installed)\nbackend = \"gzip\"\n# Don't compress small responses\nminimum_size = 500 # bytes\n</code></pre>"},{"location":"athomic/performance/compression_middleware/#api-reference","title":"API Reference","text":""},{"location":"athomic/performance/compression_middleware/#nala.athomic.performance.compression.factory.CompressionMiddlewareFactory","title":"<code>nala.athomic.performance.compression.factory.CompressionMiddlewareFactory</code>","text":"<p>Factory to create the configured compression middleware.</p>"},{"location":"athomic/resilience/adaptive_throttling/","title":"Adaptive Throttling","text":""},{"location":"athomic/resilience/adaptive_throttling/#overview","title":"Overview","text":"<p>Adaptive Throttling is a sophisticated, closed-loop resilience pattern that dynamically adjusts rate limits based on the real-time health of downstream services. While a standard rate limiter uses static, pre-configured limits, an adaptive throttler reacts to changing conditions to proactively prevent cascading failures.</p> <p>For example, if the P99 latency of a downstream service suddenly spikes, or its error rate increases, the adaptive throttling engine will automatically reduce the rate limit of calls to that service, giving it a chance to recover. Once the service's health metrics return to normal, the throttler will gradually relax the limit back to its configured maximum.</p> <p>This creates a self-regulating system that is far more resilient to partial outages and performance degradation than static rate limiting alone.</p>"},{"location":"athomic/resilience/adaptive_throttling/#how-it-works-the-feedback-loop","title":"How It Works: The Feedback Loop","text":"<p>The system is orchestrated by the <code>AdaptiveThrottlingService</code>, a background service that runs a continuous feedback loop:</p> <ol> <li> <p>Monitor: A <code>MetricsFetcher</code> periodically queries a monitoring system (like Prometheus) for key health indicators of downstream services. These are defined by you as PromQL queries in the configuration.</p> </li> <li> <p>Decide: The fetched metrics (e.g., <code>latency_p99</code>, <code>error_rate_percent</code>) are passed to a <code>DecisionAlgorithm</code>. The algorithm compares these real-time values against healthy thresholds defined in your configuration.</p> </li> <li> <p>Adjust:</p> <ul> <li>If a threshold is breached, the algorithm calculates a new, more restrictive rate limit (e.g., reducing the current limit by 20%).</li> <li>If the system is healthy, the algorithm gradually increases the rate limit back towards the statically configured maximum.</li> </ul> </li> <li> <p>Store: The newly calculated dynamic limit is stored in a distributed <code>AdaptiveStateStore</code> (e.g., Redis) with a Time-To-Live (TTL).</p> </li> <li> <p>Enforce: The <code>AdaptiveRateLimiterProvider</code> is configured to wrap the standard rate limiter. When a request is made, it first checks the <code>AdaptiveStateStore</code> for a dynamic limit. If one exists, it is enforced. Otherwise, the static limit from the configuration is used.</p> </li> </ol> <p>This cycle repeats continuously, allowing the system to autonomously adapt to the real-time health of its dependencies.</p>"},{"location":"athomic/resilience/adaptive_throttling/#configuration","title":"Configuration","text":"<p>Adaptive Throttling is a powerful feature that requires careful configuration of its two main parts: the rate limiter itself, and the adaptive engine that controls it.</p>"},{"location":"athomic/resilience/adaptive_throttling/#1-enable-the-adaptive-rate-limiter-provider","title":"1. Enable the Adaptive Rate Limiter Provider","text":"<p>First, in your <code>[resilience.rate_limiter]</code> section, you must set the <code>backend</code> to <code>\"adaptive\"</code>. This tells the <code>RateLimiterFactory</code> to create the <code>AdaptiveRateLimiterProvider</code>, which wraps your primary enforcement provider (like <code>limits</code>).</p> <pre><code>[default.resilience.rate_limiter]\n# Enable the adaptive provider as the main backend\nbackend = \"adaptive\"\n\n  # The adaptive provider wraps another provider. Configure the base provider here.\n  [default.resilience.rate_limiter.provider]\n  backend = \"limits\"\n  storage_backend = \"redis\"\n  redis_storage_uri = \"redis://localhost:6379/4\"\n  strategy = \"moving-window\"\n\n  # Your static policies still act as the MAXIMUM ceiling for the adaptive limits.\n  [default.resilience.rate_limiter.policies]\n  external_api = \"100/minute\"\n</code></pre>"},{"location":"athomic/resilience/adaptive_throttling/#2-configure-the-adaptive-throttling-engine","title":"2. Configure the Adaptive Throttling Engine","text":"<p>Next, configure the feedback loop engine in the <code>[resilience.adaptive_throttling]</code> section.</p> <pre><code>[default.resilience.adaptive_throttling]\nenabled = true\ncheck_interval_seconds = 15 # Run the feedback loop every 15 seconds.\n\n# Tell the engine which rate limit policies it should dynamically adapt.\npolicies_to_adapt = [\"external_api\"]\n\n  # --- State Store (where dynamic limits are stored) ---\n  state_store_backend = \"redis\"\n  state_store_uri = \"redis://localhost:6379/5\"\n  state_store_ttl_seconds = 300 # Dynamic limits expire after 5 minutes.\n\n  # --- Metrics Fetcher (where to get health data from) ---\n  metrics_fetcher_type = \"prometheus\"\n  metrics_fetcher_url = \"http://prometheus:9090\"\n\n    # Map internal metric names to your actual PromQL queries.\n    [default.resilience.adaptive_throttling.prometheus_queries]\n    latency_p99 = \"histogram_quantile(0.99, sum(rate(http_client_request_duration_seconds_bucket{service_name='http_external_api'}[1m])) by (le))\"\n    error_rate_percent = \"(sum(rate(http_client_requests_total{service_name='http_external_api', status='failure'}[1m])) / sum(rate(http_client_requests_total{service_name='http_external_api'}[1m]))) * 100\"\n\n  # --- Decision Algorithm Parameters ---\n  decision_algorithm = \"threshold\"\n    [default.resilience.adaptive_throttling.algorithm_params]\n    # Reduce limit if latency is over 2000ms\n    latency_threshold_ms = 2000.0\n    # Reduce limit if error rate is over 10%\n    error_threshold_percent = 10.0\n    # When breached, reduce the current limit by 25%\n    reduction_factor = 0.75\n    # When healthy, increase the current limit by 10%\n    increase_factor = 1.1\n</code></pre>"},{"location":"athomic/resilience/adaptive_throttling/#api-reference","title":"API Reference","text":""},{"location":"athomic/resilience/adaptive_throttling/#nala.athomic.resilience.adaptive_throttling.service.AdaptiveThrottlingService","title":"<code>nala.athomic.resilience.adaptive_throttling.service.AdaptiveThrottlingService</code>","text":"<p>               Bases: <code>BaseService</code></p> <p>Manages the lifecycle and core logic of the adaptive throttling engine.</p> <p>This background service periodically fetches system metrics, calculates optimal rate limits based on predefined algorithms and thresholds, and stores the dynamic limits for enforcement by the <code>AdaptiveRateLimiterProvider</code>. It inherits lifecycle management from <code>BaseService</code>.</p> <p>Attributes:</p> Name Type Description <code>adaptive_settings</code> <code>AdaptiveThrottlingSettings</code> <p>Specific configuration for the engine.</p> <code>rate_limit_settings</code> <code>RateLimiterSettings</code> <p>Global rate limiter settings (used as reference).</p> <code>state_store</code> <code>AdaptiveStateStore</code> <p>Component for storing and fetching dynamic limits.</p> <code>metrics_fetcher</code> <code>MetricsFetcher</code> <p>Component for gathering system metrics (e.g., from Prometheus).</p> <code>decision_algorithm</code> <code>DecisionAlgorithm</code> <p>Algorithm for calculating new limits.</p>"},{"location":"athomic/resilience/adaptive_throttling/#nala.athomic.resilience.adaptive_throttling.service.AdaptiveThrottlingService.__init__","title":"<code>__init__(settings=None)</code>","text":"<p>Initializes the AdaptiveThrottlingService.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[AdaptiveThrottlingSettings]</code> <p>Configuration settings. If None, loads from global settings.</p> <code>None</code>"},{"location":"athomic/resilience/adaptive_throttling/#nala.athomic.resilience.adaptive_throttling.service.AdaptiveThrottlingService.after_stop","title":"<code>after_stop()</code>  <code>async</code>","text":"<p>Hook called after the run loop is stopped to close background components.</p>"},{"location":"athomic/resilience/adaptive_throttling/#nala.athomic.resilience.adaptive_throttling.protocols.DecisionAlgorithm","title":"<code>nala.athomic.resilience.adaptive_throttling.protocols.DecisionAlgorithm</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Interface for algorithms that decide the new adaptive rate limit based on current conditions and metrics.</p>"},{"location":"athomic/resilience/adaptive_throttling/#nala.athomic.resilience.adaptive_throttling.protocols.DecisionAlgorithm.calculate_new_limit","title":"<code>calculate_new_limit(policy_name, current_configured_limit, current_dynamic_limit, metrics)</code>","text":"<p>Calculates the new adaptive limit based on provided context and metrics.</p> <p>Parameters:</p> Name Type Description Default <code>policy_name</code> <code>str</code> <p>The name of the policy being adjusted (e.g., \"default\", \"premium\").</p> required <code>current_configured_limit</code> <code>str</code> <p>The rate limit string defined in the static                       configuration (RateLimiterSettings) for this policy                       (or the default). Acts as a ceiling/reference.</p> required <code>current_dynamic_limit</code> <code>Optional[str]</code> <p>The currently active dynamic limit string retrieved                    from the AdaptiveStateStore, if any.</p> required <code>metrics</code> <code>Dict[str, Any]</code> <p>The dictionary of metrics retrieved by the MetricsFetcher.</p> required <p>Returns:</p> Name Type Description <code>AdaptiveDecision</code> <code>AdaptiveDecision</code> <p>An object detailing the calculated decision (action and new limit).</p>"},{"location":"athomic/resilience/adaptive_throttling/#nala.athomic.resilience.adaptive_throttling.providers.adaptive_provider.AdaptiveRateLimiterProvider","title":"<code>nala.athomic.resilience.adaptive_throttling.providers.adaptive_provider.AdaptiveRateLimiterProvider</code>","text":"<p>               Bases: <code>RateLimiterProtocol</code></p> <p>A Rate Limiter Provider that dynamically adjusts limits based on system health.</p> <p>This provider acts as a decorator, wrapping a base rate limiter implementation. Before enforcing a limit, it queries an <code>AdaptiveStateStore</code> for a potentially more restrictive dynamic limit calculated by a separate Decision Engine. It then applies the effective limit using the base provider.</p> <p>Attributes:</p> Name Type Description <code>base_provider</code> <code>RateLimiterProtocol</code> <p>The underlying implementation for limit enforcement.</p> <code>config</code> <code>RateLimiterSettings</code> <p>The application's rate limiter configuration.</p> <code>state_store</code> <code>AdaptiveStateStore</code> <p>The store used to fetch current dynamic limits.</p>"},{"location":"athomic/resilience/adaptive_throttling/#nala.athomic.resilience.adaptive_throttling.providers.adaptive_provider.AdaptiveRateLimiterProvider.__init__","title":"<code>__init__(config, base_provider, state_store)</code>","text":"<p>Initializes the AdaptiveRateLimiterProvider.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>RateLimiterSettings</code> <p>The application's RateLimiterSettings object.</p> required <code>base_provider</code> <code>RateLimiterProtocol</code> <p>The underlying implementation                                  responsible for actual limit enforcement.</p> required <code>state_store</code> <code>AdaptiveStateStore</code> <p>The store used to fetch current dynamic limits.</p> required"},{"location":"athomic/resilience/adaptive_throttling/#nala.athomic.resilience.adaptive_throttling.providers.adaptive_provider.AdaptiveRateLimiterProvider.allow","title":"<code>allow(key, rate, policy=None)</code>  <code>async</code>","text":"<p>Checks if the request is allowed based on the dynamically adjusted limit.</p> <p>The method determines the effective limit (dynamic or configured) and delegates the final enforcement check to the base provider.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The identifier being rate limited.</p> required <code>rate</code> <code>str</code> <p>The configured rate limit string (default/maximum limit).</p> required <code>policy</code> <code>Optional[str]</code> <p>The policy name used to look up the dynamic limit.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if allowed, False otherwise.</p>"},{"location":"athomic/resilience/adaptive_throttling/#nala.athomic.resilience.adaptive_throttling.providers.adaptive_provider.AdaptiveRateLimiterProvider.clear","title":"<code>clear(key, rate)</code>  <code>async</code>","text":"<p>Clears rate limit counters in the base provider for the specific key and rate.</p> <p>Note: This operation only clears the counter state managed by the base provider's storage. Clearing the dynamic limit itself is the responsibility of the Decision Engine during the recovery cycle.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The identifier whose counters should be cleared.</p> required <code>rate</code> <code>str</code> <p>The rate limit rule associated with the key.</p> required"},{"location":"athomic/resilience/adaptive_throttling/#nala.athomic.resilience.adaptive_throttling.providers.adaptive_provider.AdaptiveRateLimiterProvider.get_current_usage","title":"<code>get_current_usage(key, rate)</code>  <code>async</code>","text":"<p>Gets the current usage count from the base provider based on the provided rate string.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The identifier being checked.</p> required <code>rate</code> <code>str</code> <p>The rate limit string rule to check usage against.</p> required <p>Returns:</p> Type Description <code>Optional[int]</code> <p>Optional[int]: The current usage count, or None if the operation fails.</p>"},{"location":"athomic/resilience/adaptive_throttling/#nala.athomic.resilience.adaptive_throttling.providers.adaptive_provider.AdaptiveRateLimiterProvider.reset","title":"<code>reset()</code>  <code>async</code>","text":"<p>Resets ALL rate limit counters in the base provider's storage.</p> <p>WARNING: This operation is potentially global and does NOT automatically clear dynamic limits in the <code>AdaptiveStateStore</code>.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Propagates any error from the base provider's reset attempt.</p>"},{"location":"athomic/resilience/backoff/","title":"Exponential Backoff","text":""},{"location":"athomic/resilience/backoff/#overview","title":"Overview","text":"<p>Exponential Backoff is a standard resilience strategy used to gradually increase the delay between consecutive actions. This is useful in two primary scenarios:</p> <ol> <li>Polling Idle Resources: When a background service is polling for work (like the <code>OutboxPublisher</code> checking for new events) and finds none, it's inefficient to poll again immediately. Exponential backoff increases the wait time between polls, reducing CPU and network usage during idle periods.</li> <li>Retrying Failed Operations: When retrying a failed call to a downstream service, applying an increasing delay between attempts gives the struggling service time to recover.</li> </ol> <p>The Athomic implementation provides a stateful <code>BackoffHandler</code> that manages this logic based on configurable, named policies.</p>"},{"location":"athomic/resilience/backoff/#key-features","title":"Key Features","text":"<ul> <li>Policy-Based: Define multiple named backoff policies with different timings (<code>min_delay</code>, <code>max_delay</code>, <code>factor</code>) for various use cases.</li> <li>Stateful Handler: The <code>BackoffHandler</code> automatically manages the current delay state, increasing it after each wait and resetting it when work is found.</li> <li>Live Configuration: Backoff policies can be tuned in real-time without an application restart.</li> </ul>"},{"location":"athomic/resilience/backoff/#how-it-works","title":"How It Works","text":"<p>The system is composed of three main components:</p> <ol> <li> <p><code>BackoffPolicy</code>: A simple data object that holds the rules for a backoff strategy:</p> <ul> <li><code>min_delay</code>: The initial and minimum wait time.</li> <li><code>max_delay</code>: The maximum time to wait, which caps the exponential growth.</li> <li><code>factor</code>: The multiplier used to increase the delay after each wait (e.g., a factor of <code>1.5</code> will increase the wait time by 50% on each step).</li> </ul> </li> <li> <p><code>BackoffHandler</code>: The stateful object that orchestrates the backoff logic. Its key methods are:</p> <ul> <li><code>wait()</code>: Asynchronously sleeps for the <code>current_delay</code> period and then calculates the next, longer delay.</li> <li><code>reset()</code>: Resets the <code>current_delay</code> back to the policy's <code>min_delay</code>.</li> </ul> </li> <li> <p><code>BackoffFactory</code>: A factory used to create configured <code>BackoffHandler</code> instances based on named policies from your <code>settings.toml</code>.</p> </li> </ol>"},{"location":"athomic/resilience/backoff/#use-case-the-outboxpublisher-polling-loop","title":"Use Case: The <code>OutboxPublisher</code> Polling Loop","text":"<p>The <code>OutboxPublisher</code> is a perfect example of this pattern in action:</p> <ul> <li>In its main loop, it polls the database for new events.</li> <li>If no events are found, it calls <code>backoff_handler.wait()</code>. The first time, it might wait 1 second. The next, 1.5 seconds, then 2.25, and so on, up to a configured maximum. This makes the service highly efficient when idle.</li> <li>As soon as it finds and processes an event, it immediately calls <code>backoff_handler.reset()</code>. This resets the delay to the minimum, ensuring the service becomes highly responsive as soon as there is work to do.</li> </ul>"},{"location":"athomic/resilience/backoff/#usage-example","title":"Usage Example","text":"<p>You can use the <code>BackoffHandler</code> in any custom polling loop.</p> <pre><code>import asyncio\nfrom nala.athomic.resilience.backoff import BackoffFactory\n\n# Get a handler configured with the \"my_worker_policy\" from settings.toml\nbackoff_factory = BackoffFactory()\nbackoff_handler = backoff_factory.create_handler(policy_name=\"my_worker_policy\")\n\nasync def my_polling_worker():\n    while True:\n        work_done = await poll_for_work()\n\n        if work_done:\n            # We found work, so reset the delay to be responsive\n            backoff_handler.reset()\n        else:\n            # No work found, wait with an increasing delay\n            print(\"No work found, backing off...\")\n            await backoff_handler.wait()\n</code></pre>"},{"location":"athomic/resilience/backoff/#configuration","title":"Configuration","text":"<p>You define backoff policies in your <code>settings.toml</code> under the <code>[resilience.backoff]</code> section.</p> <pre><code>[default.resilience.backoff]\nenabled = true\n\n  # A default policy if no specific one is requested.\n  [default.resilience.backoff.default_policy]\n  min_delay_seconds = 1.0\n  max_delay_seconds = 30.0\n  factor = 1.5\n\n  # A dictionary of named, reusable policies.\n  [default.resilience.backoff.policies]\n\n    # A policy for an aggressive, fast-polling worker.\n    [default.resilience.backoff.policies.outbox_publisher_polling]\n    min_delay_seconds = 0.1\n    max_delay_seconds = 5.0\n    factor = 1.2\n\n    # A policy for a slow, infrequent background job.\n    [default.resilience.backoff.policies.daily_cleanup_job]\n    min_delay_seconds = 60.0\n    max_delay_seconds = 3600.0 # 1 hour\n    factor = 2.0\n</code></pre>"},{"location":"athomic/resilience/backoff/#live-configuration","title":"Live Configuration","text":"<p>Because <code>BackoffSettings</code> is a <code>LiveConfigModel</code>, you can change any of these policy values in your live configuration source (e.g., Consul), and the changes will be reflected in the <code>BackoffHandler</code> instances without requiring a restart.</p>"},{"location":"athomic/resilience/backoff/#api-reference","title":"API Reference","text":""},{"location":"athomic/resilience/backoff/#nala.athomic.resilience.backoff.handler.BackoffHandler","title":"<code>nala.athomic.resilience.backoff.handler.BackoffHandler</code>","text":"<p>Manages the state and logic for an exponential backoff strategy.</p> <p>This handler keeps track of the current delay, dynamically increasing it based on the configured policy parameters (min_delay, max_delay, factor) and providing hooks for error handling.</p>"},{"location":"athomic/resilience/backoff/#nala.athomic.resilience.backoff.handler.BackoffHandler.__init__","title":"<code>__init__(policy, operation_name='unknown', on_error=None)</code>","text":"<p>Initializes the BackoffHandler.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>BackoffPolicy</code> <p>The immutable policy defining the backoff rules.</p> required <code>operation_name</code> <code>str</code> <p>A descriptive name for the operation being managed, used for dedicated logging. Defaults to \"unknown\".</p> <code>'unknown'</code> <code>on_error</code> <code>Optional[ErrorCallback]</code> <p>An asynchronous callback executed immediately after an error occurs but before sleeping.</p> <code>None</code>"},{"location":"athomic/resilience/backoff/#nala.athomic.resilience.backoff.handler.BackoffHandler.reset","title":"<code>reset()</code>","text":"<p>Resets the internal delay counter back to the minimum policy value.</p> <p>This should be called after a successful operation to immediately resume high-frequency polling or operation attempts.</p>"},{"location":"athomic/resilience/backoff/#nala.athomic.resilience.backoff.handler.BackoffHandler.wait","title":"<code>wait()</code>  <code>async</code>","text":"<p>Waits for the current delay period (typical usage for an idle/polling state) and then increases the delay for the next cycle.</p>"},{"location":"athomic/resilience/backoff/#nala.athomic.resilience.backoff.handler.BackoffHandler.wait_after_error","title":"<code>wait_after_error(exc)</code>  <code>async</code>","text":"<p>Executes the <code>on_error</code> callback (if configured) and then waits for the current delay period, increasing it for the next cycle.</p> <p>Parameters:</p> Name Type Description Default <code>exc</code> <code>Exception</code> <p>The exception that triggered the delay.</p> required"},{"location":"athomic/resilience/backoff/#nala.athomic.resilience.backoff.policy.BackoffPolicy","title":"<code>nala.athomic.resilience.backoff.policy.BackoffPolicy</code>  <code>dataclass</code>","text":"<p>A Value Object holding the configuration for an exponential backoff strategy.</p> <p>This policy defines the parameters used by the <code>BackoffHandler</code> to control the delay between consecutive polling cycles or retry attempts, ensuring the system waits increasingly longer after failures or during idle periods.</p> <p>Attributes:</p> Name Type Description <code>min_delay</code> <code>float</code> <p>The initial and minimum delay in seconds.</p> <code>max_delay</code> <code>float</code> <p>The maximum delay in seconds, capping the exponential increase.</p> <code>factor</code> <code>float</code> <p>The multiplier used to increase the delay after each step.</p>"},{"location":"athomic/resilience/backoff/#nala.athomic.resilience.backoff.factory.BackoffFactory","title":"<code>nala.athomic.resilience.backoff.factory.BackoffFactory</code>","text":"<p>Factory class responsible for creating configured instances of <code>BackoffHandler</code>.</p> <p>It resolves configuration policies (default or named override) and constructs a runnable handler ready for use in polling loops or retry mechanisms.</p>"},{"location":"athomic/resilience/backoff/#nala.athomic.resilience.backoff.factory.BackoffFactory.__init__","title":"<code>__init__(settings=None)</code>","text":"<p>Initializes the factory with resolved application settings for backoff.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[BackoffSettings]</code> <p>Explicit settings instance. If None, loads from global application settings.</p> <code>None</code>"},{"location":"athomic/resilience/backoff/#nala.athomic.resilience.backoff.factory.BackoffFactory.create_handler","title":"<code>create_handler(policy_name=None, operation_name='default', on_error=None)</code>","text":"<p>Creates a new <code>BackoffHandler</code> instance configured with the specified policy.</p> <p>The method resolves the correct configuration: prioritizing a named policy if provided and found, or falling back to the default policy otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>policy_name</code> <code>Optional[str]</code> <p>The name of a policy defined in settings (e.g., 'high_contention').</p> <code>None</code> <code>operation_name</code> <code>str</code> <p>A descriptive name for the operation using this handler, used for logging and metrics. Defaults to \"default\".</p> <code>'default'</code> <code>on_error</code> <code>Optional[ErrorCallback]</code> <p>An asynchronous callable hook executed before sleeping after a failure.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>BackoffHandler</code> <code>BackoffHandler</code> <p>A fully configured handler instance.</p>"},{"location":"athomic/resilience/backpressure/","title":"Backpressure","text":""},{"location":"athomic/resilience/backpressure/#overview","title":"Overview","text":"<p>Backpressure is a resilience pattern used to prevent a system from being overwhelmed by temporarily stopping the flow of work towards a component that is known to be failing or overloaded. It's a form of flow control that helps prevent cascading failures.</p> <p>In the Athomic Layer, the Backpressure mechanism is primarily used by background processing services, like the <code>OutboxPublisher</code>, to avoid repeatedly attempting to process a task or resource that is consistently causing errors.</p>"},{"location":"athomic/resilience/backpressure/#key-features","title":"Key Features","text":"<ul> <li>Temporary Throttling: Pauses work on a specific resource (e.g., a message <code>aggregate_key</code>) for a configurable duration after a failure.</li> <li>Distributed State: Uses a distributed Key-Value store (like Redis) to share the backpressure state across all instances of a service.</li> <li>Automatic Recovery: Since the backpressure flag is set with a Time-To-Live (TTL), the system automatically attempts to process the resource again after the embargo period expires.</li> </ul>"},{"location":"athomic/resilience/backpressure/#how-it-works","title":"How It Works","text":"<p>The system is orchestrated by the <code>BackpressureManager</code>.</p> <ol> <li> <p>Failure Detection: A service (like the <code>OutboxPublisher</code>) detects a failure associated with a specific resource. For example, it fails to acquire a lease for <code>order-123</code>, or it fails to publish a message for <code>order-123</code>.</p> </li> <li> <p>Applying Backpressure: The service calls <code>backpressure_manager.apply_backpressure(resource_id=\"order-123\", ...)</code>. This creates a key (e.g., <code>athomic:backpressure:order-123</code>) in the configured KV store with a specific TTL (e.g., 30 seconds). The existence of this key acts as a \"throttling\" flag.</p> </li> <li> <p>Checking Before Processing: In its next processing cycle, before attempting to work on any resources, the service first calls <code>backpressure_manager.filter_throttled([\"order-123\", \"order-456\", ...])</code>. This method checks the KV store for any backpressure flags.</p> </li> <li> <p>Skipping Work: The manager returns a filtered list containing only the resources that are not currently throttled. The service then proceeds to work only on this \"safe\" list, skipping <code>order-123</code> for this cycle.</p> </li> <li> <p>Automatic Expiration: After 30 seconds, the key <code>athomic:backpressure:order-123</code> automatically expires in Redis. In the next cycle, the <code>filter_throttled</code> method will no longer filter out <code>order-123</code>, and the service will attempt to process it again.</p> </li> </ol>"},{"location":"athomic/resilience/backpressure/#usage-example","title":"Usage Example","text":"<p>While primarily used internally by Athomic services, you could use the <code>BackpressureManager</code> in a custom background worker.</p> <pre><code>from nala.athomic.resilience.backpressure import BackpressureFactory\n\n# Get the singleton manager instance\nbackpressure_manager = BackpressureFactory.create()\n\nasync def process_batch_of_items(all_item_ids: list[str]):\n    # First, filter out any items that are currently throttled\n    safe_to_process_ids = await backpressure_manager.filter_throttled(all_item_ids)\n\n    for item_id in safe_to_process_ids:\n        try:\n            await do_work_on(item_id)\n        except Exception as e:\n            # If work fails, apply backpressure to this item for 60 seconds\n            print(f\"Work failed for {item_id}. Applying backpressure.\")\n            await backpressure_manager.apply_backpressure(\n                resource_id=item_id,\n                reason=\"processing_failed\",\n                ttl_seconds=60\n            )\n</code></pre>"},{"location":"athomic/resilience/backpressure/#configuration","title":"Configuration","text":"<p>The backpressure system is configured under the <code>[resilience.backpressure]</code> section in your <code>settings.toml</code>. It requires a KV store connection to manage its state.</p> <pre><code>[default.resilience.backpressure]\nenabled = true\n\n# The default duration in seconds to apply backpressure if not specified.\ndefault_ttl_seconds = 30\n\n# The prefix for all backpressure keys stored in the KV store.\nkey_prefix = \"athomic:backpressure\"\n\n# The name of the KVStore connection (from [database.kvstore]) to use.\nkv_store_connection_name = \"default_redis\"\n</code></pre>"},{"location":"athomic/resilience/backpressure/#api-reference","title":"API Reference","text":""},{"location":"athomic/resilience/backpressure/#nala.athomic.resilience.backpressure.manager.BackpressureManager","title":"<code>nala.athomic.resilience.backpressure.manager.BackpressureManager</code>","text":"<p>Manages the state of throttled resources in a distributed Key-Value (KV) store.</p> <p>This class implements the core logic of the Backpressure pattern by using time-to-live (TTL) keys in a shared store to mark resources that are temporarily overloaded or failing. This prevents downstream services from being overwhelmed.</p> <p>Attributes:</p> Name Type Description <code>storage</code> <code>KVStoreProtocol</code> <p>The KV store client used for state persistence.</p> <code>key_prefix</code> <code>str</code> <p>The static prefix for all keys in the store.</p> <code>default_ttl</code> <code>int</code> <p>The default expiration time in seconds for a backpressure flag.</p>"},{"location":"athomic/resilience/backpressure/#nala.athomic.resilience.backpressure.manager.BackpressureManager.__init__","title":"<code>__init__(storage, key_prefix, default_ttl)</code>","text":"<p>Initializes the BackpressureManager.</p> <p>Parameters:</p> Name Type Description Default <code>storage</code> <code>KVStoreProtocol</code> <p>The KV store implementation (e.g., Redis).</p> required <code>key_prefix</code> <code>str</code> <p>The key prefix used for all backpressure flags.</p> required <code>default_ttl</code> <code>int</code> <p>The default duration for a backpressure embargo in seconds.</p> required"},{"location":"athomic/resilience/backpressure/#nala.athomic.resilience.backpressure.manager.BackpressureManager.apply_backpressure","title":"<code>apply_backpressure(resource_id, reason, ttl_seconds=None)</code>  <code>async</code>","text":"<p>Marks a resource as throttled by setting a time-limited flag in the KV store.</p> <p>Parameters:</p> Name Type Description Default <code>resource_id</code> <code>str</code> <p>The unique ID of the resource (e.g., aggregate key).</p> required <code>reason</code> <code>str</code> <p>The cause of the backpressure (e.g., 'publish_error', 'lease_conflict').</p> required <code>ttl_seconds</code> <code>Optional[int]</code> <p>The duration of the embargo. If None, uses the default TTL.</p> <code>None</code>"},{"location":"athomic/resilience/backpressure/#nala.athomic.resilience.backpressure.manager.BackpressureManager.filter_throttled","title":"<code>filter_throttled(resource_ids)</code>  <code>async</code>","text":"<p>Filters a list of resource IDs concurrently, returning only those that are NOT throttled.</p> <p>Parameters:</p> Name Type Description Default <code>resource_ids</code> <code>List[str]</code> <p>The input list of resource identifiers.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A new list containing only the resource IDs that are currently operational.</p>"},{"location":"athomic/resilience/backpressure/#nala.athomic.resilience.backpressure.manager.BackpressureManager.is_throttled","title":"<code>is_throttled(resource_id)</code>  <code>async</code>","text":"<p>Checks if a resource is currently under a backpressure embargo.</p> <p>Parameters:</p> Name Type Description Default <code>resource_id</code> <code>str</code> <p>The unique ID of the resource.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the resource is throttled (key exists), False otherwise.</p>"},{"location":"athomic/resilience/backpressure/#nala.athomic.resilience.backpressure.factory.BackpressureFactory","title":"<code>nala.athomic.resilience.backpressure.factory.BackpressureFactory</code>","text":"<p>Factory responsible for creating the singleton instance of <code>BackpressureManager</code>.</p> <p>This factory resolves the necessary dependencies\u2014the configuration settings and the configured Key-Value (KV) store client\u2014to instantiate the manager. It performs a runtime check to ensure the backpressure mechanism is enabled.</p>"},{"location":"athomic/resilience/backpressure/#nala.athomic.resilience.backpressure.factory.BackpressureFactory.create","title":"<code>create(connection_manager=None)</code>  <code>classmethod</code>","text":"<p>Creates and returns a configured instance of <code>BackpressureManager</code>.</p> <p>The process involves: 1. Checking if the feature is enabled in application settings. 2. Resolving the designated KV store client via the injected ConnectionManager. 3. Instantiating the manager with the KV store, key prefix, and default TTL.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the backpressure mechanism is disabled in the settings.</p> <p>Returns:</p> Name Type Description <code>BackpressureManager</code> <code>BackpressureManager</code> <p>A fully initialized instance ready for use.</p>"},{"location":"athomic/resilience/bulkhead/","title":"Bulkhead","text":""},{"location":"athomic/resilience/bulkhead/#overview","title":"Overview","text":"<p>The Bulkhead pattern is a resilience mechanism designed to isolate failures in one part of an application from affecting others. It works by limiting the number of concurrent executions for a specific operation.</p> <p>Imagine a ship's hull, which is divided into isolated compartments (bulkheads). If one compartment is breached and floods, the bulkheads prevent the water from sinking the entire ship. Similarly, in a software system, if a downstream service becomes slow, the bulkhead pattern prevents that slowness from consuming all available application resources (like worker threads or connections), which would otherwise cause a cascading failure across your entire service.</p> <p>The Athomic implementation uses <code>asyncio.Semaphore</code> to enforce these concurrency limits on any asynchronous function via the <code>@bulkhead</code> decorator.</p>"},{"location":"athomic/resilience/bulkhead/#key-features","title":"Key Features","text":"<ul> <li>Concurrency Limiting: Restrict how many instances of a function can run simultaneously.</li> <li>Policy-Based: Define named policies with different concurrency limits for different operations in your configuration.</li> <li>Fail-Fast Mechanism: When a bulkhead's limit is reached, new calls don't wait in a queue. They are rejected immediately, raising a <code>BulkheadRejectedError</code>. This \"fail-fast\" behavior is crucial for shedding load and protecting system stability.</li> <li>Full Observability: All bulkhead activity is instrumented with Prometheus metrics, tracking concurrent requests, accepted calls, and rejections for each policy.</li> </ul>"},{"location":"athomic/resilience/bulkhead/#how-it-works","title":"How It Works","text":"<ol> <li>Policies &amp; Semaphores: You define named policies in your configuration, each with a specific concurrency limit (e.g., <code>payment_api: 5</code>). The singleton <code>BulkheadService</code> manages an <code>asyncio.Semaphore</code> for each policy.</li> <li><code>@bulkhead</code> Decorator: You apply the <code>@bulkhead(policy=\"...\")</code> decorator to an <code>async</code> function.</li> <li>Acquisition Attempt: When the decorated function is called, it attempts to acquire a \"slot\" from the semaphore associated with its policy. This is a non-blocking check.</li> <li>Execution or Rejection:<ul> <li>If a slot is available, the function executes normally. When it completes (or fails), the slot is released.</li> <li>If no slots are available (the limit is reached), the acquisition fails immediately, and a <code>BulkheadRejectedError</code> is raised without executing the function.</li> </ul> </li> </ol>"},{"location":"athomic/resilience/bulkhead/#usage-example","title":"Usage Example","text":"<p>Let's say you have two functions: one is a highly resource-intensive video processing task, and the other is a fast metadata lookup. You can use separate bulkheads to ensure the slow video task can't block the fast metadata lookups.</p> <pre><code>from nala.athomic.resilience.bulkhead import bulkhead, BulkheadRejectedError\n\n# This policy is defined in settings.toml with a low limit (e.g., 2)\n@bulkhead(policy=\"video_processing\")\nasync def generate_video_thumbnail(video_id: str):\n    # Very slow and resource-intensive I/O operation\n    await process_video(video_id)\n\n# This policy has a higher limit (e.g., 50)\n@bulkhead(policy=\"metadata_lookup\")\nasync def fetch_user_metadata(user_id: str):\n    # Fast database call\n    await db.fetch_user(user_id)\n\nasync def handle_request(video_id: str):\n    try:\n        # If 2 video tasks are already running, this call will fail immediately\n        await generate_video_thumbnail(video_id)\n    except BulkheadRejectedError:\n        # You can now handle the rejection gracefully, e.g., by returning a 429 Too Many Requests\n        print(\"The system is currently busy processing other videos. Please try again later.\")\n</code></pre>"},{"location":"athomic/resilience/bulkhead/#configuration","title":"Configuration","text":"<p>You configure your bulkhead policies in <code>settings.toml</code> under the <code>[resilience.bulkhead]</code> section.</p> <pre><code>[default.resilience.bulkhead]\nenabled = true\n\n# The default concurrency limit to apply if a policy is not found.\ndefault_limit = 20\n\n  # A dictionary of named policies and their specific concurrency limits.\n  [default.resilience.bulkhead.policies]\n  video_processing = 2\n  metadata_lookup = 50\n  external_api_calls = 10\n</code></pre>"},{"location":"athomic/resilience/bulkhead/#api-reference","title":"API Reference","text":""},{"location":"athomic/resilience/bulkhead/#nala.athomic.resilience.bulkhead.decorator.bulkhead","title":"<code>nala.athomic.resilience.bulkhead.decorator.bulkhead(policy)</code>","text":"<p>Decorator to protect an asynchronous function with a concurrency limit (Bulkhead).</p> <p>This pattern isolates resource usage by limiting the number of concurrent executions for the decorated function, preventing cascading failures. It uses the <code>BulkheadService</code> to acquire a slot before execution.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>str</code> <p>The unique name of the bulkhead policy to apply,           as defined in the application configuration.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable[..., Any]</code> <p>The decorator function.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the decorated function is not asynchronous.</p>"},{"location":"athomic/resilience/bulkhead/#nala.athomic.resilience.bulkhead.service.BulkheadService","title":"<code>nala.athomic.resilience.bulkhead.service.BulkheadService</code>","text":"<p>Manages all bulkhead policies and their corresponding semaphores.</p> <p>This service enforces concurrency limits on asynchronous operations based on predefined policies, preventing cascading failures by isolating resource usage. It lazily creates an <code>asyncio.Semaphore</code> for each unique policy name.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <code>BulkheadSettings</code> <p>The bulkhead configuration settings.</p> <code>_semaphores</code> <code>Dict[str, Semaphore]</code> <p>Cache of active semaphores, keyed by policy name.</p>"},{"location":"athomic/resilience/bulkhead/#nala.athomic.resilience.bulkhead.service.BulkheadService.__init__","title":"<code>__init__(settings)</code>","text":"<p>Initializes the BulkheadService.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>BulkheadSettings</code> <p>The application's bulkhead configuration.</p> required"},{"location":"athomic/resilience/bulkhead/#nala.athomic.resilience.bulkhead.service.BulkheadService.acquire","title":"<code>acquire(policy)</code>  <code>async</code>","text":"<p>Acquires a slot from the bulkhead for a given policy, using an async context manager.</p> <p>If the bulkhead is full, it raises <code>BulkheadRejectedError</code> immediately (non-blocking acquisition attempt with a minimal timeout). Observability metrics are updated for both accepted and rejected calls.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>str</code> <p>The name of the bulkhead policy to enforce.</p> required <p>Raises:</p> Type Description <code>BulkheadRejectedError</code> <p>If the acquisition times out (meaning the bulkhead is full).</p>"},{"location":"athomic/resilience/bulkhead/#nala.athomic.resilience.bulkhead.exceptions.BulkheadRejectedError","title":"<code>nala.athomic.resilience.bulkhead.exceptions.BulkheadRejectedError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when a call is rejected because the bulkhead is full.</p>"},{"location":"athomic/resilience/circuit_breaker/","title":"Circuit Breaker","text":""},{"location":"athomic/resilience/circuit_breaker/#overview","title":"Overview","text":"<p>The Circuit Breaker is a stateful resilience pattern designed to prevent an application from repeatedly attempting to execute an operation that is likely to fail. When a downstream service is struggling, the circuit breaker \"opens\" to stop sending requests to it, allowing the failing service time to recover and preventing the upstream service from wasting resources.</p> <p>The Athomic implementation is built on the <code>aiobreaker</code> library and is managed by a central <code>CircuitBreakerService</code>.</p>"},{"location":"athomic/resilience/circuit_breaker/#the-three-states","title":"The Three States","text":"<p>A circuit breaker operates as a state machine with three states:</p> <ol> <li><code>CLOSED</code>: This is the normal, healthy state. All calls are allowed to pass through to the protected function. The breaker counts failures, and if the count exceeds a configured threshold (<code>fail_max</code>), it transitions to <code>OPEN</code>.</li> <li><code>OPEN</code>: In this state, all calls to the protected function are blocked immediately without execution, raising a <code>CircuitBreakerError</code>. The breaker remains <code>OPEN</code> for a configured duration (<code>reset_timeout</code>). After the timeout expires, it transitions to <code>HALF_OPEN</code>.</li> <li><code>HALF_OPEN</code>: In this state, the breaker allows a single \"probe\" call to pass through.<ul> <li>If this single call succeeds, the breaker transitions back to <code>CLOSED</code>.</li> <li>If it fails, the breaker transitions back to <code>OPEN</code>, restarting the reset timeout.</li> </ul> </li> </ol>"},{"location":"athomic/resilience/circuit_breaker/#how-it-works","title":"How It Works","text":""},{"location":"athomic/resilience/circuit_breaker/#circuit_breaker-decorator","title":"<code>@circuit_breaker</code> Decorator","text":"<p>The primary way to use the pattern is by applying the <code>@circuit_breaker</code> decorator to any asynchronous function that performs a potentially failing operation (like an external API call). You must give each circuit a unique <code>name</code>.</p> <pre><code>from nala.athomic.resilience import circuit_breaker\n\n@circuit_breaker(name=\"payment_service_api\")\nasync def call_payment_service(payment_data: dict):\n    # This call is now protected by the 'payment_service_api' circuit.\n    response = await http_client.post(\"/v1/payments\", json=payment_data)\n    return response\n</code></pre>"},{"location":"athomic/resilience/circuit_breaker/#circuitbreakerservice","title":"<code>CircuitBreakerService</code>","text":"<p>A singleton <code>CircuitBreakerService</code> manages all named circuit breakers in the application. It lazily creates and caches a breaker instance for each unique name, configured according to your settings.</p>"},{"location":"athomic/resilience/circuit_breaker/#distributed-state-storage","title":"Distributed State Storage","text":"<p>The state of each circuit breaker (its current state, failure count) must be stored somewhere. Athomic supports two backends:</p> <ul> <li><code>local</code>: In-memory storage. The state is not shared between service instances and is lost on restart. Ideal for local development and testing.</li> <li><code>redis</code>: (Recommended for production). Stores the state in Redis. This allows all instances of your service to share the same circuit state, so if one instance detects a failure, all other instances will also open the circuit for that service.</li> </ul>"},{"location":"athomic/resilience/circuit_breaker/#configuration","title":"Configuration","text":"<p>You configure the circuit breaker module under the <code>[resilience.circuit_breaker]</code> section in your <code>settings.toml</code>. You can define global defaults and then override them for specific, named circuits.</p> <pre><code>[default.resilience.circuit_breaker]\nenabled = true\nnamespace = \"cb\" # A prefix for all keys in the storage backend.\n\n# --- Default settings for all circuits ---\ndefault_fail_max = 5 # Open the circuit after 5 consecutive failures.\ndefault_reset_timeout_sec = 30.0 # Keep the circuit open for 30 seconds.\n\n  # --- Storage Provider ---\n  # Use Redis for distributed state in production.\n  [default.resilience.circuit_breaker.provider]\n  backend = \"redis\"\n    [default.resilience.circuit_breaker.provider.redis]\n    # Reuses a KVStore connection configuration.\n    uri = \"redis://localhost:6379/3\"\n\n  # --- Specific Overrides for Named Circuits ---\n  # This section is a dictionary of named circuit configurations.\n  [default.resilience.circuit_breaker.circuits]\n    # Override settings for the circuit named \"payment_service_api\".\n    [default.resilience.circuit_breaker.circuits.payment_service_api]\n    fail_max = 3 # More sensitive: open after only 3 failures.\n    reset_timeout_sec = 60.0 # Keep open for 60 seconds.\n</code></pre>"},{"location":"athomic/resilience/circuit_breaker/#live-configuration","title":"Live Configuration","text":"<p>The settings for individual circuits (under <code>[resilience.circuit_breaker.circuits]</code>) can be updated at runtime without an application restart if you are using a live configuration provider like Consul. This allows you to tune the sensitivity of your circuit breakers in response to production incidents.</p>"},{"location":"athomic/resilience/circuit_breaker/#api-reference","title":"API Reference","text":""},{"location":"athomic/resilience/circuit_breaker/#nala.athomic.resilience.circuit_breaker.decorator.circuit_breaker","title":"<code>nala.athomic.resilience.circuit_breaker.decorator.circuit_breaker(name=None)</code>","text":"<p>Decorator to protect an asynchronous function with a Circuit Breaker pattern.</p> <p>This decorator ensures that calls to the decorated function are monitored for failures. If the failure rate exceeds a threshold, the circuit opens, and subsequent calls are blocked, failing fast to allow the system to recover. All execution and state logic are delegated to the central <code>CircuitBreakerService</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>The unique name for this circuit. If None, the                   fully qualified name of the decorated function (<code>func.__qualname__</code>) is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable[..., Any]</code> <p>The decorator function.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the decorated function is not asynchronous.</p>"},{"location":"athomic/resilience/circuit_breaker/#nala.athomic.resilience.circuit_breaker.service.CircuitBreakerService","title":"<code>nala.athomic.resilience.circuit_breaker.service.CircuitBreakerService</code>","text":"<p>Manages and executes operations protected by the Circuit Breaker pattern.</p> <p>This service acts as the central factory and orchestrator for <code>aiobreaker.CircuitBreaker</code> instances. It lazily creates and caches a breaker for each unique circuit name, resolving its configuration (fail_max, reset_timeout) and distributed storage based on application settings.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <code>CircuitBreakerSettings</code> <p>The configuration for the circuit breaker module.</p> <code>_breakers</code> <code>Dict[str, CircuitBreaker]</code> <p>Cache of active circuit breaker instances.</p>"},{"location":"athomic/resilience/circuit_breaker/#nala.athomic.resilience.circuit_breaker.service.CircuitBreakerService.__init__","title":"<code>__init__(settings)</code>","text":"<p>Initializes the CircuitBreakerService.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>CircuitBreakerSettings</code> <p>Configuration for the circuit breaker module.</p> required"},{"location":"athomic/resilience/circuit_breaker/#nala.athomic.resilience.circuit_breaker.service.CircuitBreakerService.execute","title":"<code>execute(circuit_name, func, *args, **kwargs)</code>  <code>async</code>","text":"<p>Executes an asynchronous function protected by the specified circuit breaker.</p> <p>It automatically wraps the execution with an OpenTelemetry span and handles the <code>CircuitBreakerError</code> case for proper observability.</p> <p>Parameters:</p> Name Type Description Default <code>circuit_name</code> <code>str</code> <p>The name of the circuit to use.</p> required <code>func</code> <code>Callable</code> <p>The asynchronous function to execute.</p> required <code>*args</code> <code>Any</code> <p>Positional arguments for the function.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments for the function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the executed function.</p> <p>Raises:</p> Type Description <code>CircuitBreakerError</code> <p>If the circuit is open and the call is blocked.</p>"},{"location":"athomic/resilience/circuit_breaker/#nala.athomic.resilience.circuit_breaker.exceptions.CircuitBreakerError","title":"<code>nala.athomic.resilience.circuit_breaker.exceptions.CircuitBreakerError = aiobreaker.CircuitBreakerError</code>  <code>module-attribute</code>","text":""},{"location":"athomic/resilience/fallback/","title":"Fallback","text":""},{"location":"athomic/resilience/fallback/#overview","title":"Overview","text":"<p>The Fallback pattern provides an alternative execution path for an operation when it fails. Instead of allowing a failure to cascade through the system, a fallback can return a default value, data from an alternative source, or trigger a different logic path. This allows the application to degrade gracefully rather than failing completely.</p> <p>Athomic provides two distinct implementations of this pattern:</p> <ol> <li>A generic <code>@fallback_handler</code> decorator for protecting any function.</li> <li>A specialized <code>FallbackKVProvider</code> for creating highly resilient, multi-layer caching strategies.</li> </ol>"},{"location":"athomic/resilience/fallback/#1-generic-fallback-with-fallback_handler","title":"1. Generic Fallback with <code>@fallback_handler</code>","text":"<p>This decorator allows you to wrap any asynchronous function with one or more fallback functions. If the primary function raises an exception, the fallback functions are executed in order until one succeeds.</p>"},{"location":"athomic/resilience/fallback/#how-it-works","title":"How It Works","text":"<p>The <code>FallbackLogicHandler</code> executes your primary function. If an exception occurs, it catches it and begins iterating through the list of fallback functions you provided, calling them with the same arguments as the original function. The result of the first successful fallback is returned. If all fallbacks also fail, a single <code>FallbackError</code> containing all the underlying exceptions is raised.</p>"},{"location":"athomic/resilience/fallback/#usage-example","title":"Usage Example","text":"<pre><code>from nala.athomic.resilience.fallback import fallback_handler\n\n# A fallback function that returns a default static value\nasync def get_default_user_config(*args, **kwargs):\n    return {\"theme\": \"dark\", \"notifications\": \"enabled\"}\n\n# Another fallback that could try a different source\nasync def get_user_config_from_legacy_system(user_id: str):\n    # ... logic to call a legacy API ...\n    pass\n\n@fallback_handler(fallbacks=[get_user_config_from_legacy_system, get_default_user_config])\nasync def get_user_config_from_primary(user_id: str) -&gt; dict:\n    \"\"\"\n    Attempts to fetch user config from the primary Redis cache.\n    If Redis is down, it tries the legacy system.\n    If that also fails, it returns a hardcoded default.\n    \"\"\"\n    # This call might fail if Redis is unavailable\n    return await redis_cache.get(f\"user-config:{user_id}\")\n</code></pre>"},{"location":"athomic/resilience/fallback/#2-the-fallbackkvprovider-for-resilient-caching","title":"2. The <code>FallbackKVProvider</code> (for Resilient Caching)","text":"<p>This is a specialized, high-performance implementation of the fallback pattern designed specifically for caching. It's a <code>KVStoreProtocol</code> provider that wraps a chain of other KV store providers (e.g., a primary Redis cache and a secondary in-memory cache).</p>"},{"location":"athomic/resilience/fallback/#key-features","title":"Key Features","text":""},{"location":"athomic/resilience/fallback/#read-strategies","title":"Read Strategies","text":"<p>You can configure when the fallback is triggered: -   <code>on_error</code>: The fallback is used only if the primary cache is down (e.g., Redis connection fails). A normal cache miss on the primary will not trigger the fallback. -   <code>on_miss_or_error</code>: (Default) The fallback is used if the primary cache is down OR if the key is not found (a cache miss).</p>"},{"location":"athomic/resilience/fallback/#write-strategies","title":"Write Strategies","text":"<p>You can configure how writes are propagated through the cache layers: -   <code>write_around</code>: (Default) Writes are only sent to the primary cache. The fallback caches are not updated. -   <code>write_through</code>: Writes are sent to the primary cache and then propagated to all fallback caches concurrently.</p>"},{"location":"athomic/resilience/fallback/#self-healing","title":"Self-Healing","text":"<p>This is a powerful feature of the <code>FallbackKVProvider</code>. If a value is successfully retrieved from a fallback cache (e.g., the in-memory cache), a background task is automatically spawned to write that value back into the primary cache (and any other preceding caches that failed). This \"heals\" the primary cache, so the next request for the same key will be a hit on the fastest layer.</p>"},{"location":"athomic/resilience/fallback/#configuration","title":"Configuration","text":"<p>The <code>FallbackKVProvider</code> is typically configured as part of your main cache setup in <code>settings.toml</code>.</p> <pre><code>[default.performance.cache]\nenabled = true\n# The primary provider for the cache is a connection named \"main_redis\"\nkv_store_connection_name = \"main_redis\"\n\n  # --- Fallback Configuration ---\n  [default.performance.cache.fallback]\n  enabled = true\n  # Define the read strategy\n  read_strategy = \"on_miss_or_error\"\n\n  # An ordered list of KVStore connection names to use as fallbacks.\n  # Here, it will try \"local_memory_cache\" if \"main_redis\" misses or fails.\n  provider_connection_names = [\"local_memory_cache\"]\n\n# You must also define the KV store connections themselves:\n[default.database.kvstore]\n  [default.database.kvstore.main_redis]\n  # ... redis config ...\n\n  [default.database.kvstore.local_memory_cache]\n  provider.backend = \"local\"\n</code></pre>"},{"location":"athomic/resilience/fallback/#api-reference","title":"API Reference","text":""},{"location":"athomic/resilience/fallback/#nala.athomic.resilience.fallback.decorator.fallback_handler","title":"<code>nala.athomic.resilience.fallback.decorator.fallback_handler(fallbacks)</code>","text":"<p>Decorator that applies the Fallback resilience pattern to a function.</p> <p>It wraps the decorated primary function with an execution chain: if the primary function fails for any reason, the configured list of fallback functions is executed sequentially until one succeeds.</p> <p>Parameters:</p> Name Type Description Default <code>fallbacks</code> <code>Union[Callable, List[Callable]]</code> <p>A single callable or an ordered list of callables (sync or async) to be tried as alternatives if the decorated function raises an exception.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable[[Callable[..., T]], Callable[..., T]]</code> <p>The decorator function.</p>"},{"location":"athomic/resilience/fallback/#nala.athomic.resilience.fallback.providers.fallback_kv_provider.FallbackKVProvider","title":"<code>nala.athomic.resilience.fallback.providers.fallback_kv_provider.FallbackKVProvider</code>","text":"<p>               Bases: <code>BaseService</code>, <code>KVStoreProtocol</code></p> <p>A Key-Value (KV) provider that implements a resilient fallback and self-healing strategy.</p> <p>It orchestrates a chain of underlying KV providers: trying the primary first, and then sequentially trying fallbacks in case of failure or cache miss (depending on the configured read strategy).</p> <p>Attributes:</p> Name Type Description <code>primary</code> <code>KVStoreProtocol</code> <p>The main, preferred KV store provider.</p> <code>fallbacks</code> <code>List[KVStoreProtocol]</code> <p>Ordered list of secondary providers.</p> <code>write_strategy</code> <code>WriteStrategyProtocol</code> <p>Strategy defining how write operations are handled across providers.</p> <code>read_strategy</code> <code>ReadStrategyProtocol</code> <p>Strategy defining when the fallback chain is triggered.</p> <code>enable_self_healing</code> <code>bool</code> <p>If True, successful reads from a fallback provider                         will asynchronously write the value back to the failed/missing providers.</p>"},{"location":"athomic/resilience/fallback/#nala.athomic.resilience.fallback.providers.fallback_kv_provider.FallbackKVProvider.__init__","title":"<code>__init__(primary_provider, fallback_providers, read_strategy_type=FallbackReadStrategyType.ON_MISS_OR_ERROR, write_strategy_type=FallbackWriteStrategyType.WRITE_AROUND, enable_self_healing=True)</code>","text":"<p>Initializes the FallbackKVProvider.</p> <p>Parameters:</p> Name Type Description Default <code>primary_provider</code> <code>KVStoreProtocol</code> <p>The primary KV store instance.</p> required <code>fallback_providers</code> <code>List[KVStoreProtocol]</code> <p>A list of secondary providers,                                         ordered by preference.</p> required <code>read_strategy_type</code> <code>FallbackReadStrategyType</code> <p>Defines the trigger for the fallback read chain.</p> <code>ON_MISS_OR_ERROR</code> <code>write_strategy_type</code> <code>FallbackWriteStrategyType</code> <p>Defines how write operations propagate.</p> <code>WRITE_AROUND</code> <code>enable_self_healing</code> <code>bool</code> <p>Enables background healing of failed providers on successful reads.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>primary_provider</code> is None.</p>"},{"location":"athomic/resilience/fallback/#nala.athomic.resilience.fallback.providers.fallback_kv_provider.FallbackKVProvider.clear","title":"<code>clear()</code>  <code>async</code>","text":"<p>Clears all keys in the provider(s) based on the configured write strategy.</p>"},{"location":"athomic/resilience/fallback/#nala.athomic.resilience.fallback.providers.fallback_kv_provider.FallbackKVProvider.delete","title":"<code>delete(key)</code>  <code>async</code>","text":"<p>Deletes a key based on the configured write strategy.</p>"},{"location":"athomic/resilience/fallback/#nala.athomic.resilience.fallback.providers.fallback_kv_provider.FallbackKVProvider.exists","title":"<code>exists(key)</code>  <code>async</code>","text":"<p>Checks for the existence of a key by delegating the check/fallback logic to the configured read strategy.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the key exists in any available provider, False otherwise.</p>"},{"location":"athomic/resilience/fallback/#nala.athomic.resilience.fallback.providers.fallback_kv_provider.FallbackKVProvider.get","title":"<code>get(key)</code>  <code>async</code>","text":"<p>Retrieves a value by delegating the complex read/fallback logic to the configured read strategy.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to retrieve.</p> required <p>Returns:</p> Type Description <code>Optional[Any]</code> <p>Optional[Any]: The value from the first successful provider, or None.</p>"},{"location":"athomic/resilience/fallback/#nala.athomic.resilience.fallback.providers.fallback_kv_provider.FallbackKVProvider.get_final_client","title":"<code>get_final_client()</code>  <code>async</code>","text":"<p>Returns the raw client of the primary provider for direct access.</p>"},{"location":"athomic/resilience/fallback/#nala.athomic.resilience.fallback.providers.fallback_kv_provider.FallbackKVProvider.get_sync_client","title":"<code>get_sync_client()</code>","text":"<p>Returns the synchronous client of the primary provider for direct access.</p>"},{"location":"athomic/resilience/fallback/#nala.athomic.resilience.fallback.providers.fallback_kv_provider.FallbackKVProvider.heal","title":"<code>heal(key, value, providers_to_heal)</code>  <code>async</code>","text":"<p>Asynchronously attempts to heal failed or missed providers by writing the successfully retrieved value back to them.</p> <p>This method is non-blocking and is executed as a fire-and-forget background task.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key that was successfully retrieved.</p> required <code>value</code> <code>Any</code> <p>The value associated with the key.</p> required <code>providers_to_heal</code> <code>List[KVStoreProtocol]</code> <p>The chain of providers (including the primary)                                        that failed or missed the key.</p> required"},{"location":"athomic/resilience/fallback/#nala.athomic.resilience.fallback.providers.fallback_kv_provider.FallbackKVProvider.is_available","title":"<code>is_available()</code>","text":"<p>Returns True if AT LEAST ONE provider is connected and ready.</p>"},{"location":"athomic/resilience/fallback/#nala.athomic.resilience.fallback.providers.fallback_kv_provider.FallbackKVProvider.set","title":"<code>set(key, value, ttl=None, nx=False)</code>  <code>async</code>","text":"<p>Sets a key-value pair based on the configured write strategy.</p>"},{"location":"athomic/resilience/fallback/#nala.athomic.resilience.fallback.exceptions.FallbackError","title":"<code>nala.athomic.resilience.fallback.exceptions.FallbackError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when all fallback attempts fail.</p>"},{"location":"athomic/resilience/idempotency/","title":"Idempotency","text":""},{"location":"athomic/resilience/idempotency/#overview","title":"Overview","text":"<p>Idempotency is a critical property of distributed systems that ensures an operation can be performed multiple times with the same result as if it were performed only once. This is essential for building reliable APIs and message handlers, as it allows clients to safely retry requests (e.g., after a network failure) without causing duplicate side effects, such as creating two orders or charging a credit card twice.</p> <p>The Athomic Layer provides a powerful <code>@idempotent</code> decorator that makes any asynchronous function idempotent by storing its result and using a distributed lock to handle concurrent requests.</p>"},{"location":"athomic/resilience/idempotency/#key-features","title":"Key Features","text":"<ul> <li>Safe Retries: Protects <code>POST</code>, <code>PUT</code>, and other state-changing operations from creating duplicate resources.</li> <li>Single-Flight Execution: Uses a distributed lock to ensure that for a given idempotency key, the business logic is executed only once, even under high concurrency.</li> <li>Cache-Aside Result Storage: The result of the first successful operation is stored in a Key-Value store. Subsequent requests with the same key receive this stored result immediately.</li> <li>Declarative Usage: Enforce idempotency with a simple and clean decorator.</li> </ul>"},{"location":"athomic/resilience/idempotency/#how-it-works-single-flight-cache-aside","title":"How It Works: Single-Flight Cache-Aside","text":"<p>The <code>@idempotent</code> decorator orchestrates a sophisticated, race-condition-free workflow:</p> <ol> <li> <p>Key Resolution: When the decorated function is called, a unique idempotency key is generated for that specific operation. This is typically resolved from a request header (e.g., <code>Idempotency-Key</code>) or a field in a message payload.</p> </li> <li> <p>Cache Check (Attempt 1): The system first checks the configured KV store (e.g., Redis) to see if a result for this key has already been stored. If a result is found, it is returned immediately.</p> </li> <li> <p>Distributed Lock: If no result is found (a cache miss), the system attempts to acquire a distributed lock for that idempotency key. This is the crucial \"Single-Flight\" step:</p> <ul> <li>Only the first request to arrive will acquire the lock.</li> <li>Any other concurrent requests for the same key will wait for the lock to be released.</li> </ul> </li> <li> <p>Double-Checked Locking: After acquiring the lock, the system checks the cache a second time. This handles the case where another request finished computing the result while the current request was waiting for the lock. If a result is now found, it is returned, and the original function is not executed.</p> </li> <li> <p>Execution &amp; Storage: If the cache is still empty, the original decorated function (your business logic) is executed. Its result is then stored in the KV store with a configured TTL, and finally, the lock is released.</p> </li> <li> <p>Conflict: If a request cannot acquire the lock within its timeout, it means the operation is already in progress. In this case, an <code>IdempotencyConflictError</code> is raised, which can be translated to an <code>HTTP 409 Conflict</code> response.</p> </li> </ol>"},{"location":"athomic/resilience/idempotency/#usage-example","title":"Usage Example","text":"<p>Here is how you would protect a FastAPI endpoint that creates a new resource. The client is expected to provide a unique <code>Idempotency-Key</code> header.</p> <pre><code>import uuid\nfrom fastapi import Header, Request\nfrom nala.athomic.resilience.idempotency import idempotent, IdempotencyConflictError\n\n@router.post(\"/orders\")\n@idempotent(\n    # The key resolver is a lambda that extracts the key from the request kwargs.\n    key=lambda request, **kwargs: request.headers.get(\"Idempotency-Key\"),\n    lock_timeout=10 # Wait up to 10s for a concurrent request to finish.\n)\nasync def create_order(request: Request, order_data: dict):\n    \"\"\"\n    Creates a new order. This operation is idempotent.\n    \"\"\"\n    # This logic will only be executed ONCE for a given Idempotency-Key.\n    order_id = await order_service.create(order_data)\n    return {\"status\": \"created\", \"order_id\": order_id}\n\n# You would typically have an exception handler to catch the conflict error.\n@app.exception_handler(IdempotencyConflictError)\nasync def idempotency_conflict_handler(request: Request, exc: IdempotencyConflictError):\n    return JSONResponse(\n        status_code=409,\n        content={\"message\": \"Request conflict: An identical request is already being processed.\"},\n    )\n</code></pre>"},{"location":"athomic/resilience/idempotency/#configuration","title":"Configuration","text":"<p>The idempotency module is configured under the <code>[resilience.idempotency]</code> section in your <code>settings.toml</code>. It requires a KV store for storing results and managing locks.</p> <pre><code>[default.resilience.idempotency]\nenabled = true\n\n# The default Time-To-Live in seconds for stored idempotency results.\ndefault_ttl_seconds = 86400 # 24 hours\n\n# The default time in seconds to wait for a distributed lock.\ndefault_lock_timeout_seconds = 10\n\n  # The KVStore configuration for storing results and locks.\n  [default.resilience.idempotency.kvstore]\n  namespace = \"idempotency\"\n    [default.resilience.idempotency.kvstore.provider]\n    backend = \"redis\"\n    uri = \"redis://localhost:6379/6\"\n</code></pre>"},{"location":"athomic/resilience/idempotency/#api-reference","title":"API Reference","text":""},{"location":"athomic/resilience/idempotency/#nala.athomic.resilience.idempotency.decorator.idempotent","title":"<code>nala.athomic.resilience.idempotency.decorator.idempotent(key, ttl=None, lock_timeout=None, storage=None, locker=None)</code>","text":"<p>Decorator to make an asynchronous function idempotent.</p> <p>The primary goal is to ensure that a function is executed logically only once for a given idempotency key, returning the cached result on all subsequent calls within the key's TTL.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>ContextualKeyResolverType</code> <p>A resolver mechanism to generate the unique key from function arguments. Can be a callable (function/lambda) or an f-string template referencing kwargs (e.g., \"order:{order_id}\").</p> required <code>ttl</code> <code>Optional[int]</code> <p>Time-to-live (in seconds) for the stored result. Uses configured default if None.</p> <code>None</code> <code>lock_timeout</code> <code>Optional[int]</code> <p>Time (in seconds) to wait for the distributed lock                           during cache miss before raising a conflict error.                           Uses configured default if None.</p> <code>None</code> <code>storage</code> <code>Optional[KVStoreProtocol]</code> <p>Optional storage provider instance (for DI/testing).</p> <code>None</code> <code>locker</code> <code>Optional[LockingProtocol]</code> <p>Optional distributed locking provider instance (for DI/testing).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable[..., Any]</code> <p>The decorator function.</p>"},{"location":"athomic/resilience/idempotency/#nala.athomic.resilience.idempotency.exceptions.IdempotencyConflictError","title":"<code>nala.athomic.resilience.idempotency.exceptions.IdempotencyConflictError</code>","text":"<p>               Bases: <code>IdempotencyError</code></p> <p>Raised when an idempotent operation is attempted for a key that is already being processed by a concurrent request.</p>"},{"location":"athomic/resilience/idempotency/#nala.athomic.resilience.idempotency.handler.IdempotencyHandler","title":"<code>nala.athomic.resilience.idempotency.handler.IdempotencyHandler</code>","text":"<p>Orchestrates the entire idempotency check and execution logic.</p> <p>Supports two modes: 1. Cache-Aside (Single-Flight): For RPC/HTTP where the result is computed,    locked, and cached. (Via <code>execute</code> method). 2. Batch Filtering: For message consumers where we lock/filter duplicates    before processing using atomic batch operations. (Via <code>acquire_batch</code> method).</p>"},{"location":"athomic/resilience/idempotency/#nala.athomic.resilience.idempotency.handler.IdempotencyHandler.__init__","title":"<code>__init__(func=None, args=None, kwargs=None, key_resolver=None, ttl=None, lock_timeout=None, storage=None, locker=None, settings=None)</code>","text":"<p>Initializes the IdempotencyHandler.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Optional[Callable]</code> <p>The function being decorated. Optional for Batch mode.</p> <code>None</code> <code>args</code> <code>Optional[Tuple]</code> <p>Positional arguments. Optional for Batch mode.</p> <code>None</code> <code>kwargs</code> <code>Optional[Dict]</code> <p>Keyword arguments. Optional for Batch mode.</p> <code>None</code> <code>key_resolver</code> <code>Optional[ContextualKeyResolverType]</code> <p>Mechanism to generate the unique key.</p> <code>None</code> <code>ttl</code> <code>Optional[int]</code> <p>Time-to-live for the stored result/lock.</p> <code>None</code> <code>lock_timeout</code> <code>Optional[int]</code> <p>Time to wait to acquire the distributed lock.</p> <code>None</code> <code>storage</code> <code>Optional[KVIdempotencyStorage]</code> <p>Storage provider instance.</p> <code>None</code> <code>locker</code> <code>Optional[LockingProtocol]</code> <p>Distributed locking provider instance.</p> <code>None</code> <code>settings</code> <code>Optional[IdempotencySettings]</code> <p>Settings instance.</p> <code>None</code>"},{"location":"athomic/resilience/idempotency/#nala.athomic.resilience.idempotency.handler.IdempotencyHandler.acquire_batch","title":"<code>acquire_batch(keys, ttl)</code>  <code>async</code>","text":"<p>Attempts to acquire locks for multiple keys in a single atomic batch operation. Used for 'Filter &amp; Forward' patterns in consumers.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>List[str]</code> <p>List of LOGICAL keys from the message.</p> required <code>ttl</code> <code>int</code> <p>Time to live for the lock.</p> required <p>Returns:</p> Type Description <code>Dict[str, bool]</code> <p>Dict[str, bool]: Map of {logical_key: success}.</p>"},{"location":"athomic/resilience/idempotency/#nala.athomic.resilience.idempotency.handler.IdempotencyHandler.confirm_batch","title":"<code>confirm_batch(keys, ttl)</code>  <code>async</code>","text":"<p>Marks a batch of keys as COMPLETED. Updates the state in storage to prevent re-processing.</p>"},{"location":"athomic/resilience/idempotency/#nala.athomic.resilience.idempotency.handler.IdempotencyHandler.execute","title":"<code>execute()</code>  <code>async</code>","text":"<p>Executes the full idempotency flow (Cache-Aside + Single Flight). Used primarily for decorators on single methods.</p>"},{"location":"athomic/resilience/idempotency/#nala.athomic.resilience.idempotency.handler.IdempotencyHandler.release_batch","title":"<code>release_batch(keys)</code>  <code>async</code>","text":"<p>Releases locks for a batch of keys (Rollback). Used when the batch processing fails to allow immediate retry.</p>"},{"location":"athomic/resilience/leasing/","title":"Distributed Leasing","text":""},{"location":"athomic/resilience/leasing/#overview","title":"Overview","text":"<p>Distributed Leasing is an advanced resilience pattern used for leader election or to ensure a single service instance has exclusive ownership of a long-running task or resource. It is a more sophisticated form of distributed locking.</p> <p>The key difference between a lease and a lock is that a lease is time-based and requires the holder to actively maintain ownership by sending periodic \"heartbeats\" to renew it. If the leaseholder crashes or becomes unresponsive, its lease automatically expires after a configured duration, allowing another healthy instance to acquire it and take over the task.</p> <p>This pattern is critical for building fault-tolerant, active-passive systems. A primary use case within the Athomic Layer is in the <code>OutboxPublisher</code>, where leasing ensures that only one publisher instance is processing events for a specific <code>aggregate_key</code> at any given time.</p>"},{"location":"athomic/resilience/leasing/#key-features","title":"Key Features","text":"<ul> <li>Exclusive Ownership: Guarantees that only one worker in a cluster can operate on a leased resource.</li> <li>Fault Tolerance: Leases automatically expire, allowing a new worker to take over if the current owner fails.</li> <li>Automatic Renewal: An active lease is automatically renewed in a background task (heartbeat) as long as the worker is healthy.</li> <li>Simple Interface: Implemented via a clean <code>async with</code> context manager.</li> </ul>"},{"location":"athomic/resilience/leasing/#how-it-works","title":"How It Works","text":"<ol> <li> <p>Acquisition: A worker requests a lease on a specific resource by calling <code>lease_manager.acquire(\"resource_id\")</code> inside an <code>async with</code> block. The manager attempts to atomically acquire the lease from the backend (Redis) using a <code>SET NX EX</code> command.</p> </li> <li> <p>Heartbeat: If the lease is acquired successfully, the <code>LeaseManager</code> yields a <code>LeaseHolder</code> object. This object immediately starts a background <code>asyncio.Task</code> which periodically sends a heartbeat to the backend, renewing the lease before it expires.</p> </li> <li> <p>Execution: The code inside the <code>async with</code> block executes. The worker can now safely perform its task, confident that it has exclusive ownership of the resource.</p> </li> <li> <p>Release: When the <code>async with</code> block is exited (either by completing successfully or by raising an exception), the heartbeat task is automatically stopped, and the lease is explicitly released from the backend.</p> </li> </ol>"},{"location":"athomic/resilience/leasing/#usage-example","title":"Usage Example","text":"<p>Here is a conceptual example of how a background worker could use leasing to ensure only one instance is processing a specific job at a time.</p> <pre><code>from nala.athomic.resilience.leasing import LeaseFactory, LeaseAcquisitionError\n\nlease_manager = LeaseFactory.create()\njob_id = \"process-daily-reports\"\n\nasync def run_daily_report_job():\n    try:\n        # Try to acquire an exclusive lease for this job\n        async with lease_manager.acquire(resource_id=job_id):\n            # This code will only be executed by the single worker\n            # that successfully acquired the lease.\n            print(\"Lease acquired! Running the daily report job...\")\n            await generate_reports()\n            print(\"Job finished, releasing lease.\")\n\n    except LeaseAcquisitionError:\n        # This will be raised if another worker already holds the lease.\n        print(\"Could not acquire lease. Another worker is already running the job.\")\n    except Exception as e:\n        print(f\"An error occurred during the job: {e}\")\n</code></pre>"},{"location":"athomic/resilience/leasing/#configuration","title":"Configuration","text":"<p>The leasing system is configured under the <code>[resilience.leasing]</code> section in your <code>settings.toml</code>. It requires a KV store connection (Redis) to manage the lease state.</p> <pre><code>[default.resilience.leasing]\nenabled = true\n\n# The name of the KVStore connection (from [database.kvstore]) to use.\nkv_store_connection_name = \"default_redis\"\n\n# The duration in seconds a lease is held before it expires if not renewed.\nduration_seconds = 60\n\n# How often the background heartbeat task should renew the lease.\n# This MUST be shorter than duration_seconds.\nrenewal_interval_seconds = 20\n\n# The default time in seconds a process will wait to acquire a lease.\ndefault_acquire_timeout_seconds = 5\n</code></pre>"},{"location":"athomic/resilience/leasing/#api-reference","title":"API Reference","text":""},{"location":"athomic/resilience/leasing/#nala.athomic.resilience.leasing.manager.LeaseManager","title":"<code>nala.athomic.resilience.leasing.manager.LeaseManager</code>","text":"<p>Orchestrates the acquisition, heartbeat renewal, and release of distributed leases.</p> <p>This class provides a high-level <code>async context manager</code> interface for the leasing mechanism, handling the complex distributed locking logic, continuous renewal in the background, and graceful cleanup.</p> <p>Attributes:</p> Name Type Description <code>provider</code> <code>LeaseProtocol</code> <p>The underlying, concrete distributed lease provider (e.g., Redis).</p> <code>settings</code> <code>LeasingSettings</code> <p>The configuration for lease duration and timeouts.</p> <code>owner_id</code> <code>str</code> <p>A unique identifier for this service instance, used to claim ownership of leases.</p>"},{"location":"athomic/resilience/leasing/#nala.athomic.resilience.leasing.manager.LeaseManager.__init__","title":"<code>__init__(provider, settings)</code>","text":"<p>Initializes the LeaseManager.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>LeaseProtocol</code> <p>The underlying distributed lease provider.</p> required <code>settings</code> <code>LeasingSettings</code> <p>The configuration settings.</p> required"},{"location":"athomic/resilience/leasing/#nala.athomic.resilience.leasing.manager.LeaseManager.acquire","title":"<code>acquire(resource_id)</code>  <code>async</code>","text":"<p>Attempts to acquire an exclusive lease on a specified resource.</p> <p>This is a non-blocking operation managed by a timeout. If successful, it yields a <code>LeaseHolder</code> that automatically renews the lease in a background task (heartbeat).</p> <p>Parameters:</p> Name Type Description Default <code>resource_id</code> <code>str</code> <p>The unique identifier of the resource to lock (e.g., a message aggregate key).</p> required <p>Yields:</p> Name Type Description <code>LeaseHolder</code> <code>AsyncGenerator[LeaseHolder, None]</code> <p>An object representing the active lease, providing renewal management.</p> <p>Raises:</p> Type Description <code>LeaseAcquisitionError</code> <p>If the lease cannot be acquired within the configured acquisition timeout.</p>"},{"location":"athomic/resilience/leasing/#nala.athomic.resilience.leasing.holder.LeaseHolder","title":"<code>nala.athomic.resilience.leasing.holder.LeaseHolder</code>","text":"<p>Represents an acquired lease and manages its renewal heartbeat.</p> <p>An instance of this class is yielded by the <code>LeaseManager</code>'s context manager, encapsulating the active lease and the background task responsible for periodically renewing its expiration time.</p>"},{"location":"athomic/resilience/leasing/#nala.athomic.resilience.leasing.holder.LeaseHolder.is_acquired","title":"<code>is_acquired</code>  <code>property</code>","text":"<p>Returns True if the lease was successfully acquired.</p>"},{"location":"athomic/resilience/leasing/#nala.athomic.resilience.leasing.holder.LeaseHolder.__init__","title":"<code>__init__(lease, provider, settings)</code>","text":"<p>Initializes the LeaseHolder.</p> <p>Parameters:</p> Name Type Description Default <code>lease</code> <code>Optional[Lease]</code> <p>The acquired lease object. None if acquisition failed.</p> required <code>provider</code> <code>LeaseProtocol</code> <p>The concrete lease provider used for renewal/release operations.</p> required <code>settings</code> <code>LeasingSettings</code> <p>The configuration defining renewal intervals and durations.</p> required"},{"location":"athomic/resilience/leasing/#nala.athomic.resilience.leasing.holder.LeaseHolder.start_heartbeat","title":"<code>start_heartbeat()</code>","text":"<p>Starts the background renewal task if a lease was acquired.</p>"},{"location":"athomic/resilience/leasing/#nala.athomic.resilience.leasing.holder.LeaseHolder.stop_heartbeat","title":"<code>stop_heartbeat()</code>  <code>async</code>","text":"<p>Stops the background renewal task gracefully and waits for it to finish.</p>"},{"location":"athomic/resilience/leasing/#nala.athomic.resilience.leasing.exceptions.LeaseAcquisitionError","title":"<code>nala.athomic.resilience.leasing.exceptions.LeaseAcquisitionError</code>","text":"<p>               Bases: <code>LeaseError</code></p> <p>Raised when a worker fails to acquire a lease on a resource.</p>"},{"location":"athomic/resilience/locking/","title":"Distributed Locking","text":""},{"location":"athomic/resilience/locking/#overview","title":"Overview","text":"<p>Distributed Locking is a resilience pattern that provides a mechanism for mutual exclusion across multiple processes or service instances. It ensures that only one process can execute a critical section of code at a time for a specific, shared resource.</p> <p>This is essential for preventing race conditions in distributed systems. For example, if two requests try to update a user's account balance at the same time, a distributed lock can ensure that these operations happen sequentially, not concurrently, thus preventing data corruption.</p> <p>The Athomic implementation provides a simple yet powerful <code>@distributed_lock</code> decorator to protect any asynchronous function.</p>"},{"location":"athomic/resilience/locking/#key-features","title":"Key Features","text":"<ul> <li>Declarative Use: Protect critical sections of code with a simple decorator.</li> <li>Dynamic Key Resolution: Lock keys can be dynamically generated from the arguments of the decorated function.</li> <li>Multiple Backends: Supports a distributed Redis backend for production and a local in-memory backend for testing.</li> <li>Deadlock Prevention: Locks are configured with a timeout, ensuring they are automatically released even if a process crashes.</li> </ul>"},{"location":"athomic/resilience/locking/#how-it-works","title":"How It Works","text":"<ol> <li>Decorator: You apply the <code>@distributed_lock(key=\"...\", timeout=...)</code> decorator to an <code>async</code> function.</li> <li>Key Resolution: When the decorated function is called, the <code>key</code> template string is formatted using the function's arguments. For example, a key of <code>\"user-balance:{user_id}\"</code> for a function call with <code>user_id=123</code> will resolve to <code>\"user-balance:123\"</code>.</li> <li>Acquisition Attempt: The system then attempts to acquire a lock for this resolved key from the configured provider (e.g., Redis). It will wait for up to the specified <code>timeout</code>.</li> <li>Execution: If the lock is acquired, the original function is executed. Once the function completes (either by returning or raising an exception), the lock is always released automatically.</li> <li>Failure: If the lock cannot be acquired within the timeout (because another process holds it), a <code>LockAcquisitionError</code> is raised immediately, and the function is not executed.</li> </ol>"},{"location":"athomic/resilience/locking/#available-providers","title":"Available Providers","text":"<ul> <li><code>RedisLockProvider</code>: The recommended provider for production. It uses Redis's atomic operations to implement a reliable, distributed lock. It reuses the application's main <code>KVStore</code> client for the connection.</li> <li><code>LocalLockProvider</code>: An in-memory lock provider that uses <code>asyncio.Lock</code>. It is suitable for single-process applications or for running tests without external dependencies. It is not distributed.</li> </ul>"},{"location":"athomic/resilience/locking/#usage-example","title":"Usage Example","text":"<p>Imagine a function that needs to safely deduct a value from a user's balance.</p> <pre><code>from nala.athomic.resilience.locking import distributed_lock, LockAcquisitionError\n\nclass BalanceService:\n    @distributed_lock(key=\"balance:{user_id}\", timeout=10)\n    async def deduct_from_balance(self, user_id: str, amount: float):\n        \"\"\"\n        Safely deducts from a user's balance. Only one process can\n        execute this for the same user_id at a time.\n        \"\"\"\n        current_balance = await db.get_balance(user_id)\n        if current_balance &lt; amount:\n            raise ValueError(\"Insufficient funds.\")\n\n        await db.set_balance(user_id, current_balance - amount)\n\nasync def handle_payment(user_id: str, amount: float):\n    try:\n        await balance_service.deduct_from_balance(user_id, amount)\n    except LockAcquisitionError:\n        # This occurs if another request for the same user is already processing.\n        # You can ask the client to retry the request.\n        print(\"Could not process payment at this time, please try again.\")\n</code></pre>"},{"location":"athomic/resilience/locking/#configuration","title":"Configuration","text":"<p>The locking provider is configured under the <code>[resilience.locking]</code> section in your <code>settings.toml</code>.</p> <pre><code>[default.resilience.locking]\nenabled = true\n\n# The default time in seconds a lock is held before it automatically expires.\nlock_timeout_sec = 30\n\n  # Configure the backend provider\n  [default.resilience.locking.provider]\n  backend = \"redis\"\n\n    # The redis provider reuses a KVStore connection configuration.\n    [default.resilience.locking.provider.kvstore]\n    # The namespace and other wrappers will apply to the lock keys\n    namespace = \"locks\"\n      [default.resilience.locking.provider.kvstore.provider]\n      backend = \"redis\"\n      uri = \"redis://localhost:6379/5\"\n</code></pre>"},{"location":"athomic/resilience/locking/#api-reference","title":"API Reference","text":""},{"location":"athomic/resilience/locking/#nala.athomic.resilience.locking.decorator.distributed_lock","title":"<code>nala.athomic.resilience.locking.decorator.distributed_lock(key, timeout=30)</code>","text":"<p>Decorator that enforces mutual exclusion for an asynchronous function using a distributed lock.</p> <p>The decorator ensures that only one call runs at a time for a given unique key, preventing race conditions in a distributed environment.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>A template for the lock key. Can use arguments from the        decorated function (e.g., \"payment:{payment_id}\").</p> required <code>timeout</code> <code>int</code> <p>Time (in seconds) to wait to acquire the lock before failing.            Defaults to 30 seconds.</p> <code>30</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>The decorator function.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the decorated function is not asynchronous.</p>"},{"location":"athomic/resilience/locking/#nala.athomic.resilience.locking.protocol.LockingProtocol","title":"<code>nala.athomic.resilience.locking.protocol.LockingProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the contract for a locking provider, abstracting the mechanism used to ensure mutual exclusion for shared resources.</p> <p>Implementations can be either local (in-memory) for single-process use or distributed (e.g., Redis) for microservices in a clustered environment.</p>"},{"location":"athomic/resilience/locking/#nala.athomic.resilience.locking.protocol.LockingProtocol.acquire","title":"<code>acquire(key, timeout=30)</code>  <code>async</code>","text":"<p>Acquires a lock for a specific key, intended for use with the <code>async with</code> statement.</p> <p>The operation blocks until the lock is acquired or the timeout is reached.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The unique resource identifier to be locked (e.g., \"user:123\").</p> required <code>timeout</code> <code>int</code> <p>The maximum time (in seconds) to wait to acquire the lock.            Defaults to 30 seconds.</p> <code>30</code> <p>Yields:</p> Name Type Description <code>None</code> <code>AsyncGenerator[None, None]</code> <p>Execution continues inside the block only if the lock is successfully acquired.</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If the lock cannot be acquired within the specified timeout.</p>"},{"location":"athomic/resilience/locking/#nala.athomic.resilience.locking.exceptions.LockAcquisitionError","title":"<code>nala.athomic.resilience.locking.exceptions.LockAcquisitionError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when a distributed lock cannot be acquired within the specified timeout.</p>"},{"location":"athomic/resilience/rate_limiter/","title":"Rate Limiter","text":""},{"location":"athomic/resilience/rate_limiter/#overview","title":"Overview","text":"<p>The Rate Limiter is a resilience pattern used to control the frequency of operations, such as API requests or function calls, over a period of time. It is essential for:</p> <ul> <li>Protecting downstream services from being overwhelmed.</li> <li>Preventing resource starvation and ensuring fair usage.</li> <li>Enforcing API usage quotas for different clients or user tiers.</li> </ul> <p>The Athomic rate limiter is a flexible, policy-based system that allows you to declaratively apply rate limits to any asynchronous function using the <code>@rate_limited</code> decorator.</p>"},{"location":"athomic/resilience/rate_limiter/#key-features","title":"Key Features","text":"<ul> <li>Policy-Based: Define named, reusable rate limit policies (e.g., <code>\"100/hour\"</code>, <code>\"10/second\"</code>) in your configuration.</li> <li>Multiple Backends &amp; Strategies: Supports multiple provider backends, including a highly flexible one based on the <code>limits</code> library that allows for different strategies (e.g., <code>fixed-window</code>, <code>moving-window</code>).</li> <li>Context-Aware: Automatically generates unique keys based on the current context, enabling per-user or per-tenant rate limiting.</li> <li>Live Configuration: Rate limit policies can be adjusted at runtime without restarting the application, allowing you to respond to traffic spikes dynamically.</li> <li>Adaptive Throttling: Can integrate with an advanced Adaptive Throttling engine to automatically adjust limits based on real-time system health metrics.</li> </ul>"},{"location":"athomic/resilience/rate_limiter/#how-it-works","title":"How It Works","text":"<ol> <li>Decorator: You apply the <code>@rate_limited(policy=\"...\")</code> decorator to an <code>async</code> function.</li> <li><code>RateLimiterService</code>: When the function is called, the decorator invokes the central <code>RateLimiterService</code>.</li> <li>Key Generation: The service uses the <code>ContextKeyGenerator</code> to create a unique key for the operation based on the policy name, function name, and the current execution context (like <code>tenant_id</code> or <code>user_id</code>).</li> <li>Provider Check: The service then asks the configured provider (e.g., the <code>limits</code> provider) if a request for that key is allowed under the specified rate limit string. If the request is denied, a <code>RateLimitExceeded</code> exception is raised.</li> </ol>"},{"location":"athomic/resilience/rate_limiter/#available-providers","title":"Available Providers","text":"<ul> <li><code>limits</code> Provider: The default and most flexible provider. It is an adapter for the powerful <code>limits</code> library and can be configured to use different storage backends (in-memory for testing, Redis for distributed state) and different algorithms (fixed-window, moving-window).</li> <li><code>redis</code> Provider: A lightweight, custom implementation that uses native Redis commands to enforce a simple fixed-window algorithm. It reuses the application's main <code>KVStore</code> client.</li> </ul>"},{"location":"athomic/resilience/rate_limiter/#usage-example","title":"Usage Example","text":"<p>Applying a rate limit to a function is as simple as adding the decorator.</p> <pre><code>from nala.athomic.resilience.rate_limiter import rate_limited, RateLimitExceeded\n\n@rate_limited(policy=\"api_heavy_usage\")\nasync def generate_large_report(user_id: str):\n    # This function can now only be called according to the\n    # \"api_heavy_usage\" policy defined in the settings.\n    # ...\n    pass\n\nasync def handle_request(user_id: str):\n    try:\n        await generate_large_report(user_id)\n    except RateLimitExceeded:\n        # Handle the case where the user has exceeded their quota\n        print(\"Too many requests, please try again later.\")\n</code></pre>"},{"location":"athomic/resilience/rate_limiter/#configuration","title":"Configuration","text":"<p>You define your rate limiting backend and policies in <code>settings.toml</code> under the <code>[resilience.rate_limiter]</code> section.</p> <pre><code>[default.resilience.rate_limiter]\nenabled = true\nbackend = \"limits\" # Use the 'limits' library provider\n\n  # --- Default settings for the 'limits' provider ---\n  [default.resilience.rate_limiter.provider]\n  backend = \"limits\"\n  # Use Redis for distributed state\n  storage_backend = \"redis\"\n  redis_storage_uri = \"redis://localhost:6379/4\"\n  # Use a more accurate strategy\n  strategy = \"moving-window\"\n\n  # --- Policy Definitions ---\n  # A default limit to apply if no specific policy is requested\n  default_policy_limit = \"1000/hour\"\n\n  # A dictionary of named, reusable policies\n  [default.resilience.rate_limiter.policies]\n  api_light_usage = \"100/minute\"\n  api_heavy_usage = \"10/minute\"\n  login_attempts = \"5/hour\"\n</code></pre>"},{"location":"athomic/resilience/rate_limiter/#live-configuration","title":"Live Configuration","text":"<p>Because <code>RateLimiterSettings</code> is a <code>LiveConfigModel</code>, you can change the limit strings for any policy in your live configuration source (e.g., Consul) and the changes will take effect immediately without a restart. This is extremely useful for mitigating traffic spikes or abuse in real-time.</p>"},{"location":"athomic/resilience/rate_limiter/#api-reference","title":"API Reference","text":""},{"location":"athomic/resilience/rate_limiter/#nala.athomic.resilience.rate_limiter.decorators.rate_limited","title":"<code>nala.athomic.resilience.rate_limiter.decorators.rate_limited</code>","text":""},{"location":"athomic/resilience/rate_limiter/#nala.athomic.resilience.rate_limiter.decorators.rate_limited.rate_limited","title":"<code>rate_limited(policy)</code>","text":"<p>Decorator that applies the Rate Limiting pattern to an asynchronous function.</p> <p>It delegates the entire enforcement logic to the central <code>RateLimiterService</code>. If the limit is exceeded, it raises a specific exception instead of executing the decorated function.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>str</code> <p>The name of the rate limit policy to apply (e.g., 'default', 'heavy_users'),           as defined in the configuration.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable[..., Awaitable[Any]]</code> <p>The decorator function.</p> <p>Raises:</p> Type Description <code>RateLimitExceeded</code> <p>If the request is blocked by the rate limiter.</p>"},{"location":"athomic/resilience/rate_limiter/#nala.athomic.resilience.rate_limiter.service.RateLimiterService","title":"<code>nala.athomic.resilience.rate_limiter.service.RateLimiterService</code>","text":"<p>               Bases: <code>BaseService</code></p> <p>Orchestrates rate limiting logic, integrating provider, configuration, key generation, and detailed observability.</p>"},{"location":"athomic/resilience/rate_limiter/#nala.athomic.resilience.rate_limiter.service.RateLimiterService.__init__","title":"<code>__init__(settings=None, provider=None)</code>","text":"<p>Initializes the RateLimiterService.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>Optional[RateLimiterProtocol]</code> <p>Allows injecting a specific provider, primarily for testing.       If None, the default provider is created via the factory.</p> <code>None</code>"},{"location":"athomic/resilience/rate_limiter/#nala.athomic.resilience.rate_limiter.service.RateLimiterService.check","title":"<code>check(policy, *key_parts)</code>  <code>async</code>","text":"<p>Performs a rate limit check with integrated, detailed observability.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>str</code> <p>The name of the rate limit policy to apply.</p> required <code>*key_parts</code> <code>str</code> <p>Logical parts of the key (e.g., function name, resource ID).</p> <code>()</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the request is allowed, False if it is blocked.</p>"},{"location":"athomic/resilience/rate_limiter/#nala.athomic.resilience.rate_limiter.protocol.RateLimiterProtocol","title":"<code>nala.athomic.resilience.rate_limiter.protocol.RateLimiterProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the interface for a context-aware rate limiter. All implementations must respect the allow/clear/reset contract.</p>"},{"location":"athomic/resilience/rate_limiter/#nala.athomic.resilience.rate_limiter.protocol.RateLimiterProtocol.allow","title":"<code>allow(key, rate, policy=None)</code>  <code>async</code>","text":"<p>Checks if the given key is allowed under the specified rate limit and consumes a token if allowed.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The identifier for the entity being rate limited (e.g., user ID, IP).</p> required <code>rate</code> <code>str</code> <p>The rate limit string (e.g., \"5/second\", \"100/minute\").</p> required <code>policy</code> <code>Optional[str]</code> <p>An optional policy identifier to apply specific rate limits.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the request is allowed, False if the rate limit has been exceeded.</p>"},{"location":"athomic/resilience/rate_limiter/#nala.athomic.resilience.rate_limiter.protocol.RateLimiterProtocol.clear","title":"<code>clear(key, rate)</code>  <code>async</code>","text":"<p>Resets the rate limit counters specifically for the given key and rate limit rule.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The identifier whose rate limit counters to reset.</p> required <code>rate</code> <code>str</code> <p>The rate limit string rule associated with the key to clear         (needed to identify the correct counter(s)).</p> required"},{"location":"athomic/resilience/rate_limiter/#nala.athomic.resilience.rate_limiter.protocol.RateLimiterProtocol.get_current_usage","title":"<code>get_current_usage(key, rate)</code>  <code>async</code>","text":"<p>Optional: Returns the current usage count for the given key under a specific rate. May not be supported by all implementations or strategies.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The identifier to query.</p> required <code>rate</code> <code>str</code> <p>The rate limit string rule to check usage against.</p> required <p>Returns:</p> Type Description <code>Optional[int]</code> <p>Optional[int]: Number of requests used in the window, if supported.</p>"},{"location":"athomic/resilience/rate_limiter/#nala.athomic.resilience.rate_limiter.protocol.RateLimiterProtocol.reset","title":"<code>reset()</code>  <code>async</code>","text":"<p>Resets all rate limit counters managed by this storage instance. WARNING: Use with extreme caution, especially with shared storage like Redis,          as this will affect all keys managed by this limiter instance.</p>"},{"location":"athomic/resilience/rate_limiter/#nala.athomic.resilience.rate_limiter.exceptions.RateLimitExceeded","title":"<code>nala.athomic.resilience.rate_limiter.exceptions.RateLimitExceeded</code>","text":"<p>               Bases: <code>Exception</code></p>"},{"location":"athomic/resilience/retry/","title":"Retry","text":""},{"location":"athomic/resilience/retry/#overview","title":"Overview","text":"<p>The Retry pattern is a fundamental resilience mechanism that allows an application to automatically re-execute an operation that has failed due to a transient error, such as a temporary network glitch or a brief service unavailability. This prevents temporary issues from escalating into hard failures, significantly improving the stability and reliability of your service.</p> <p>The Athomic retry module is built on the robust and battle-tested <code>tenacity</code> library. It provides a highly configurable, policy-based approach to retries through a simple <code>@retry</code> decorator.</p>"},{"location":"athomic/resilience/retry/#key-features","title":"Key Features","text":"<ul> <li>Declarative Use: Apply complex retry logic with a single decorator.</li> <li>Policy-Based Configuration: Centrally define multiple named retry policies (e.g., \"fast_retry\", \"slow_retry\") in your configuration files.</li> <li>Exponential Backoff &amp; Jitter: Built-in support for exponential backoff with random jitter to prevent \"thundering herd\" problems.</li> <li>Live Configuration: Retry policies can be tuned and updated at runtime without restarting the application.</li> <li>Full Observability: Every retry attempt is logged and instrumented with traces and metrics.</li> </ul>"},{"location":"athomic/resilience/retry/#how-it-works","title":"How It Works","text":"<p>The retry mechanism is centered around Retry Policies. A policy is a set of rules that defines the retry behavior:</p> <ul> <li><code>attempts</code>: The maximum number of times to retry the operation.</li> <li><code>wait_min_seconds</code> / <code>wait_max_seconds</code>: The minimum and maximum delay between retries.</li> <li><code>backoff</code>: A multiplier for the delay, enabling exponential backoff.</li> <li><code>jitter</code>: A random factor added to the delay to de-synchronize retries from multiple clients.</li> <li><code>exceptions</code>: A list of specific exception types that should trigger a retry.</li> </ul> <p>The <code>@retry</code> decorator applies a policy to a function. When the decorated function is called, a <code>RetryHandler</code> manages the execution. If the function raises one of the configured exceptions, the handler waits for the calculated delay and then re-executes the function, up to the maximum number of attempts.</p>"},{"location":"athomic/resilience/retry/#usage-example","title":"Usage Example","text":"<p>You can apply a named retry policy directly to any asynchronous function.</p> <pre><code>from nala.athomic.resilience import retry, RetryFactory\n\n# In a real app, the factory would be a singleton.\nretry_factory = RetryFactory()\n# Get a pre-configured policy from your settings.toml\nfast_retry_policy = retry_factory.create_policy(name=\"fast_retry\")\n\n@retry(policy=fast_retry_policy)\nasync def call_flaky_service(request_data: dict) -&gt; dict:\n    \"\"\"\n    This function will be retried up to 3 times with a short,\n    exponential backoff if it fails with an HTTPException.\n    \"\"\"\n    # This call might fail temporarily\n    response = await http_client.post(\"/external-api\", json=request_data)\n    response.raise_for_status()\n    return response.json()\n</code></pre>"},{"location":"athomic/resilience/retry/#configuration","title":"Configuration","text":"<p>You define all your retry policies in <code>settings.toml</code> under the <code>[resilience.retry]</code> section. You can specify a <code>default_policy</code> and a dictionary of named <code>policies</code> for different use cases.</p> <pre><code>[default.resilience.retry]\nenabled = true\n\n  # This policy will be used if no specific policy is requested.\n  [default.resilience.retry.default_policy]\n  attempts = 3\n  wait_min_seconds = 1.0\n  wait_max_seconds = 10.0\n  backoff = 2.0 # Wait time will be ~1s, 2s, 4s...\n  exceptions = [\"HTTPRequestError\", \"HTTPTimeoutError\"]\n\n  # A dictionary of named, reusable policies.\n  [default.resilience.retry.policies]\n\n    # A policy for quick, internal retries.\n    [default.resilience.retry.policies.fast_retry]\n    attempts = 3\n    wait_min_seconds = 0.1\n    wait_max_seconds = 1.0\n    backoff = 1.5\n    exceptions = [\"HTTPException\"]\n\n    # A policy for long-running background tasks.\n    [default.resilience.retry.policies.long_retry]\n    attempts = 10\n    wait_min_seconds = 5.0\n    wait_max_seconds = 300.0 # 5 minutes\n    backoff = 2.0\n    exceptions = [\"ConnectionError\"]\n</code></pre>"},{"location":"athomic/resilience/retry/#live-configuration","title":"Live Configuration","text":"<p>Because the <code>RetrySettings</code> model is a <code>LiveConfigModel</code>, you can change any of these values (e.g., <code>attempts</code>, <code>wait_max_seconds</code>) in your live configuration source (like Consul), and the <code>RetryFactory</code> will use the new values for all subsequent operations without requiring an application restart.</p>"},{"location":"athomic/resilience/retry/#api-reference","title":"API Reference","text":""},{"location":"athomic/resilience/retry/#nala.athomic.resilience.retry.decorator.retry","title":"<code>nala.athomic.resilience.retry.decorator.retry(*, policy=None, operation_name=None, on_retry=None, on_fail=None, circuit_breaker_hook=None, logger=None, tracer=None)</code>","text":"<p>A decorator to add retry logic to synchronous or asynchronous functions, with optional hooks for retry, failure, circuit breaker, logging, and tracing. Args:     policy (Optional[RetryPolicy]): The retry policy to use. If None, a default policy may be applied.     operation_name (Optional[str]): An optional name for the operation, used for logging or tracing.     on_retry (Optional[Callable]): Optional callback invoked on each retry attempt.     on_fail (Optional[Callable]): Optional callback invoked when all retry attempts fail.     circuit_breaker_hook (Optional[Callable[[BaseException], None]]): Optional callback invoked when a circuit breaker event occurs.     logger (Optional[Callable]): Optional logger function for logging retry events.     tracer (Optional[Callable]): Optional tracer function for tracing retry events. Returns:     Callable: A decorator that wraps the target function with retry logic, supporting both sync and async functions. Usage:     @retry(policy=my_policy, on_retry=my_on_retry)     def my_function(...):         ...</p>"},{"location":"athomic/resilience/retry/#nala.athomic.resilience.retry.policy.RetryPolicy","title":"<code>nala.athomic.resilience.retry.policy.RetryPolicy</code>","text":"<p>Define the retry behavior: exceptions, attempts, delays, backoff, jitter, etc.</p>"},{"location":"athomic/resilience/retry/#nala.athomic.resilience.retry.factory.RetryFactory","title":"<code>nala.athomic.resilience.retry.factory.RetryFactory</code>","text":"<p>Factory class for creating retry policies and decorators for resilient operations.</p>"},{"location":"athomic/resilience/retry/#nala.athomic.resilience.retry.factory.RetryFactory.create_policy","title":"<code>create_policy(name=None)</code>","text":"<p>Creates a RetryPolicy instance based on a named policy from settings.</p> <p>In MESH mode, this returns a NoOpRetryPolicy (single attempt).</p>"},{"location":"athomic/resilience/retry/#nala.athomic.resilience.retry.factory.RetryFactory.create_retry_decorator","title":"<code>create_retry_decorator(*, policy=None, operation_name=None, logger=None, tracer=None, on_retry=None, on_fail=None, circuit_breaker_hook=None, **policy_overrides)</code>","text":"<p>Creates the retry decorator.</p>"},{"location":"athomic/resilience/retry/#nala.athomic.resilience.retry.factory.RetryFactory.create_retry_handler","title":"<code>create_retry_handler(policy_name=None, operation_name=None, **kwargs)</code>","text":"<p>Creates a RetryHandler instance configured with a specific named policy.</p>"},{"location":"athomic/resilience/retry/#nala.athomic.resilience.retry.handler.RetryHandler","title":"<code>nala.athomic.resilience.retry.handler.RetryHandler</code>","text":"<p>Manages the retry logic for synchronous and asynchronous functions based on a defined policy.</p> <p>This class encapsulates the retry behavior, using the <code>tenacity</code> library, and integrates framework features such as exponential backoff, observability (logging, tracing, metrics), and circuit breaker integration.</p>"},{"location":"athomic/resilience/retry/#nala.athomic.resilience.retry.handler.RetryHandler.__init__","title":"<code>__init__(policy, operation_name=None, on_retry=None, on_fail=None, circuit_breaker_hook=None, tracer=None, logger=None)</code>","text":"<p>Initializes the RetryHandler.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>RetryPolicy</code> <p>The retry policy defining behavior (attempts, delays, exceptions).</p> required <code>operation_name</code> <code>Optional[str]</code> <p>A descriptive name for the operation, used in logs and metrics.</p> <code>None</code> <code>on_retry</code> <code>Optional[RetryCallback]</code> <p>Hook executed before sleep on each failed retry attempt.</p> <code>None</code> <code>on_fail</code> <code>Optional[FailCallback]</code> <p>Hook executed when all retry attempts fail permanently.</p> <code>None</code> <code>circuit_breaker_hook</code> <code>Optional[Callable[[], bool]]</code> <p>Hook function that returns True if the circuit is open, aborting retries.</p> <code>None</code> <code>tracer</code> <code>Optional[Tracer]</code> <p>OpenTelemetry tracer instance.</p> <code>None</code> <code>logger</code> <code>Optional[Logger]</code> <p>Logger instance for instrumentation.</p> <code>None</code>"},{"location":"athomic/resilience/retry/#nala.athomic.resilience.retry.handler.RetryHandler.arun","title":"<code>arun(fn, *args, **kwargs)</code>  <code>async</code>","text":"<p>Executes an asynchronous function with the configured retry policy.</p>"},{"location":"athomic/resilience/retry/#nala.athomic.resilience.retry.handler.RetryHandler.run","title":"<code>run(fn, *args, **kwargs)</code>","text":"<p>Executes a synchronous function with the configured retry policy.</p>"},{"location":"athomic/resilience/sagas/","title":"Sagas for Distributed Transactions","text":""},{"location":"athomic/resilience/sagas/#overview","title":"Overview","text":"<p>The Saga pattern is an advanced resilience mechanism for managing data consistency across multiple services in a distributed transaction. Unlike traditional (ACID) transactions, which are not feasible in a microservices architecture, a saga is a sequence of local transactions where each step is coordinated with the next.</p> <p>If any step in the sequence fails, the saga executes a series of compensating actions in reverse order to undo the work that was already completed, thus maintaining overall data consistency.</p> <p>Athomic provides a complete framework for defining, executing, and recovering sagas, supporting two distinct execution models: Orchestration and Choreography.</p>"},{"location":"athomic/resilience/sagas/#core-concepts","title":"Core Concepts","text":"<ul> <li>Saga Definition: A static blueprint of the entire business transaction, created using the <code>SagaBuilder</code>.</li> <li>Step: A single, atomic operation within the saga. Each step consists of an Action and a Compensation.</li> <li>Action: A forward-moving operation that performs a task (e.g., <code>reserve_inventory</code>).</li> <li>Compensation: An operation that semantically undoes an Action (e.g., <code>release_inventory</code>).</li> <li>Saga State: The dynamic, persisted state of a single running saga instance. It tracks the current step, the business payload, and the history of executed steps.</li> </ul>"},{"location":"athomic/resilience/sagas/#execution-models","title":"Execution Models","text":""},{"location":"athomic/resilience/sagas/#1-orchestration-command-based","title":"1. Orchestration (Command-Based)","text":"<p>In this model, a central <code>OrchestrationSagaExecutor</code> manages the entire flow. It directly calls the action handlers for each step in sequence. If any action fails, the orchestrator is responsible for calling the corresponding compensation handlers in reverse order.</p> <ul> <li>Pros: Simple to understand, centralized logic, explicit control flow.</li> <li>Cons: Can lead to a \"god object\" orchestrator if the saga is very complex.</li> </ul>"},{"location":"athomic/resilience/sagas/#2-choreography-event-based","title":"2. Choreography (Event-Based)","text":"<p>In this model, there is no central orchestrator. The <code>ChoreographySagaExecutor</code> simply starts the saga by publishing the first action as a command/event on the message bus.</p> <p>Each step is then handled by an independent service (a \"participant\") that subscribes to its relevant event. After completing its local transaction, the participant uses the injected <code>SagaContext</code> to publish the command/event for the next step. Compensation also flows via events.</p> <ul> <li>Pros: Highly decoupled, services have no direct knowledge of each other.</li> <li>Cons: Can be harder to debug and visualize the end-to-end flow.</li> </ul>"},{"location":"athomic/resilience/sagas/#1-how-to-define-a-saga","title":"1. How to Define a Saga","text":"<p>Regardless of the execution model, you define a saga using the fluent <code>SagaBuilder</code>.</p> <pre><code># In your_app/sagas.py\nfrom nala.athomic.resilience.sagas import SagaBuilder, saga_definition_registry\n\n# Define the blueprint for an \"create_order\" saga\ncreate_order_saga = SagaBuilder(name=\"create_order_saga\") \n    .add_step(\n        name=\"reserve_inventory\",\n        action=\"tasks.inventory.reserve_stock\",\n        compensation=\"tasks.inventory.release_stock\"\n    ) \n    .add_step(\n        name=\"process_payment\",\n        action=\"tasks.payment.charge_card\",\n        compensation=\"tasks.payment.refund_charge\"\n    ) \n    .add_step(\n        name=\"update_order_status\",\n        action=\"tasks.orders.set_order_approved\",\n        compensation=\"tasks.orders.set_order_failed\"\n    ) \n    .build()\n\n# Register the definition so the SagaManager can find it\nsaga_definition_registry.register(create_order_saga)\n</code></pre> <p>The <code>action</code> and <code>compensation</code> strings are identifiers that the executor will resolve into callable functions or tasks.</p>"},{"location":"athomic/resilience/sagas/#2-how-to-execute-a-saga","title":"2. How to Execute a Saga","text":"<p>You start a new saga instance using the <code>SagaManager</code>.</p> <pre><code>from nala.athomic.resilience.sagas import SagaManager\n\nasync def handle_create_order_request(order_details: dict):\n    saga_manager = SagaManager()\n\n    initial_payload = {\n        \"order_id\": order_details[\"id\"],\n        \"customer_id\": order_details[\"customer_id\"],\n        \"amount\": order_details[\"total_price\"]\n    }\n\n    # This call will block until the saga completes or fails\n    final_state = await saga_manager.execute(\n        saga_name=\"create_order_saga\",\n        initial_payload=initial_payload\n    )\n\n    return final_state\n</code></pre>"},{"location":"athomic/resilience/sagas/#3-implementing-participants-for-choreography","title":"3. Implementing Participants (for Choreography)","text":"<p>If you are using the Choreography model, each action and compensation handler is a message consumer decorated with <code>@saga_participant</code>. The decorator injects a <code>SagaContext</code> object, which provides methods to safely advance or compensate the saga.</p> <pre><code># In your inventory service's consumers\nfrom nala.athomic.resilience.sagas import saga_participant, SagaContext\n\n@subscribe_to(\"tasks.inventory.reserve_stock\")\n@saga_participant()\nasync def reserve_stock_handler(context: SagaContext):\n    try:\n        inventory = await inventory_service.reserve(context.payload)\n\n        # Action succeeded: update the payload and complete the step.\n        # This will automatically publish the next event (\"tasks.payment.charge_card\").\n        await context.complete_step(updated_payload={\"inventory_info\": inventory})\n\n    except Exception as e:\n        # Action failed: initiate the compensation flow.\n        # This will publish the first compensation event (\"tasks.inventory.release_stock\").\n        await context.fail_and_compensate(error_details=str(e))\n</code></pre>"},{"location":"athomic/resilience/sagas/#configuration","title":"Configuration","text":"<p>The saga system is configured under the <code>[resilience.sagas]</code> section in your <code>settings.toml</code>.</p> <pre><code>[default.resilience.sagas]\nenabled = true\n\n# Choose the execution model: \"orchestration\" or \"choreography\"\nexecutor_type = \"orchestration\"\n\n# The name of the KVStore connection used for persisting saga state.\nkv_store_connection_name = \"default_redis\"\n\n  # Configuration for the \"Reaper\" service, which recovers stalled sagas.\n  [default.resilience.sagas.reaper]\n  enabled = true\n  poll_interval_seconds = 300 # Check every 5 minutes\n  stalled_threshold_seconds = 900 # A saga is stalled if not updated for 15 minutes\n</code></pre>"},{"location":"athomic/resilience/sagas/#api-reference","title":"API Reference","text":""},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.builder.SagaBuilder","title":"<code>nala.athomic.resilience.sagas.builder.SagaBuilder</code>","text":"<p>A fluent builder for creating immutable SagaDefinition objects.</p> <p>This class provides a declarative, chainable API for defining the sequence of steps, actions, and compensations that constitute a distributed transaction.</p>"},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.builder.SagaBuilder.__init__","title":"<code>__init__(name)</code>","text":"<p>Initializes the builder for a new saga definition.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The unique name for the saga, used for identification and logging.</p> required"},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.builder.SagaBuilder.add_step","title":"<code>add_step(name, action, compensation)</code>","text":"<p>Adds a new step to the saga sequence.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>A human-readable name for the step.</p> required <code>action</code> <code>str</code> <p>An identifier for the function/task that performs the action.</p> required <code>compensation</code> <code>str</code> <p>An identifier for the function/task that compensates the action.</p> required <p>Returns:</p> Type Description <code>SagaBuilder</code> <p>The builder instance to allow for method chaining.</p>"},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.builder.SagaBuilder.build","title":"<code>build()</code>","text":"<p>Constructs and returns the final, immutable SagaDefinition object.</p> <p>Once built, the definition should not be altered.</p> <p>Returns:</p> Type Description <code>SagaDefinition</code> <p>A configured and validated SagaDefinition instance.</p>"},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.manager.SagaManager","title":"<code>nala.athomic.resilience.sagas.manager.SagaManager</code>","text":"<p>The main entry point and public interface for the Saga distributed transaction framework.</p> <p>This manager is responsible for initiating a saga run by resolving the static <code>SagaDefinition</code> and delegating the entire execution lifecycle (forward actions and compensation) to the configured <code>SagaExecutorProtocol</code>.</p> <p>Attributes:</p> Name Type Description <code>executor</code> <code>SagaExecutorProtocol</code> <p>The saga executor responsible for running the process.</p> <code>registry</code> <p>The repository holding registered saga definitions.</p>"},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.manager.SagaManager.__init__","title":"<code>__init__(executor)</code>","text":"<p>Initializes the SagaManager.</p> <p>Parameters:</p> Name Type Description Default <code>executor</code> <code>SagaExecutorProtocol</code> <p>A pre-configured saga executor instance responsible for the entire saga lifecycle.</p> required"},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.manager.SagaManager.execute","title":"<code>execute(saga_name, initial_payload)</code>  <code>async</code>","text":"<p>Initiates a new instance of a registered saga.</p> <p>Parameters:</p> Name Type Description Default <code>saga_name</code> <code>str</code> <p>The unique name of the saga definition to execute.</p> required <code>initial_payload</code> <code>Dict[str, Any]</code> <p>The business data required to start the transaction.</p> required <p>Returns:</p> Name Type Description <code>SagaState</code> <code>SagaState</code> <p>The final state object of the saga upon completion or failure.</p> <p>Raises:</p> Type Description <code>SagaError</code> <p>If the saga definition cannot be found in the registry.</p>"},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.participant.saga_participant","title":"<code>nala.athomic.resilience.sagas.participant.saga_participant()</code>","text":"<p>Decorator for a message consumer function to designate it as a participant in a choreographed saga.</p> <p>This decorator handles the necessary boilerplate for saga participation: 1. Extracts the <code>saga_id</code> from the incoming message. 2. Loads the current <code>SagaState</code> from the repository. 3. Validates that the saga is in a runnable state (RUNNING or COMPENSATING). 4. Injects a rich <code>SagaContext</code> object into the decorated function, which    allows the handler to safely advance or compensate the saga.</p>"},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.context.SagaContext","title":"<code>nala.athomic.resilience.sagas.context.SagaContext</code>","text":"<p>An active context object injected into saga participant handlers.</p> <p>It holds the current, mutable state of the saga and provides high-level methods to safely transition the saga's state (advance or fail/compensate), abstracting away the underlying repository and messaging infrastructure.</p>"},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.context.SagaContext.payload","title":"<code>payload</code>  <code>property</code>","text":"<p>Returns the mutable business payload associated with the saga.</p>"},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.context.SagaContext.saga_id","title":"<code>saga_id</code>  <code>property</code>","text":"<p>Returns the unique ID of the current saga instance.</p>"},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.context.SagaContext.__init__","title":"<code>__init__(state, definition, repository, producer)</code>","text":"<p>Initializes the SagaContext.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>SagaState</code> <p>The current, mutable state of the running saga instance.</p> required <code>definition</code> <code>SagaDefinition</code> <p>The static blueprint defining the steps of the saga.</p> required <code>repository</code> <code>SagaStateRepositoryProtocol</code> <p>The persistence layer for state updates.</p> required <code>producer</code> <code>ProducerProtocol</code> <p>The messaging component used to publish events (commands).</p> required"},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.context.SagaContext.complete_step","title":"<code>complete_step(updated_payload=None)</code>  <code>async</code>","text":"<p>Marks the current step as succeeded and triggers the next action in the saga chain.</p> <p>If this is the final step, the saga status is set to <code>COMPLETED</code>. Otherwise, the next action is published as an event via the producer.</p> <p>Parameters:</p> Name Type Description Default <code>updated_payload</code> <code>Optional[Dict[str, Any]]</code> <p>Optional dictionary to update                                         or merge into the saga's persistent payload.</p> <code>None</code>"},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.context.SagaContext.fail_and_compensate","title":"<code>fail_and_compensate(error_details='')</code>  <code>async</code>","text":"<p>Marks the current step as failed, initiates the compensation flow, and publishes the first compensation event.</p> <p>Parameters:</p> Name Type Description Default <code>error_details</code> <code>str</code> <p>A message detailing the reason for the failure.</p> <code>''</code>"},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.models.SagaState","title":"<code>nala.athomic.resilience.sagas.models.SagaState</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the dynamic, persisted state of a single, running saga instance.</p> <p>This is the core model managed by the <code>SagaStateRepository</code>, tracking the saga's progress, status, and accumulated payload.</p> <p>Attributes:</p> Name Type Description <code>saga_id</code> <code>UUID</code> <p>The unique instance ID of the running saga.</p> <code>saga_name</code> <code>str</code> <p>The name of the saga definition used to start this instance.</p> <code>status</code> <code>SagaStatus</code> <p>The current state of the saga (RUNNING, COMPLETED, etc.).</p> <code>current_step</code> <code>int</code> <p>The index of the last or currently executing step in the definition.</p> <code>payload</code> <code>Dict[str, Any]</code> <p>The mutable business data for the transaction, updated by steps.</p> <code>context</code> <code>Dict[str, Any]</code> <p>The propagated <code>ExecutionContext</code> (e.g., <code>trace_id</code>, <code>tenant_id</code>) captured at initiation.</p> <code>step_history</code> <code>List[SagaStepHistory]</code> <p>An ordered log of execution attempts for each step.</p> <code>created_at</code> <code>datetime</code> <p>The UTC time when the saga instance was created.</p> <code>updated_at</code> <code>datetime</code> <p>The UTC time of the last state update (used by the reaper service).</p>"},{"location":"athomic/resilience/sharding/","title":"Distributed Workload Sharding","text":""},{"location":"athomic/resilience/sharding/#overview","title":"Overview","text":"<p>Distributed Workload Sharding (also known as partitioning) is a pattern for dynamically distributing a set of tasks or data items across a cluster of active worker instances. It is a fundamental technique for achieving horizontal scalability and high availability for background processing services.</p> <p>The Athomic implementation uses Consistent Hashing to ensure that the workload is distributed in a stable and predictable way.</p>"},{"location":"athomic/resilience/sharding/#core-use-case","title":"Core Use Case","text":"<p>The primary use case for sharding in the Athomic Layer is the <code>OutboxPublisher</code>. When you have multiple instances of your service running, sharding ensures that all instances can share the work of publishing outbox events without duplicating effort or creating conflicts. Each publisher instance will claim and process only a specific subset of the pending events.</p>"},{"location":"athomic/resilience/sharding/#core-technology-consistent-hashing","title":"Core Technology: Consistent Hashing","text":"<p>Traditional sharding (e.g., using a modulo operator) can be inefficient. If a worker is added or removed, the entire distribution of work changes, causing massive reshuffling.</p> <p>Athomic uses Consistent Hashing (via the <code>uhashring</code> library) to solve this. With consistent hashing, when a worker joins or leaves the cluster, only a small, necessary fraction of the workload is redistributed. This minimizes disruption and ensures the system remains stable during scaling events or deployments.</p>"},{"location":"athomic/resilience/sharding/#how-it-works-integration-with-service-discovery","title":"How It Works (Integration with Service Discovery)","text":"<p>The sharding mechanism is tightly integrated with a Service Discovery backend (like Consul).</p> <ol> <li> <p>Registration: At startup, each worker instance that participates in a sharding group (e.g., each <code>OutboxPublisher</code> service) registers itself with a specific <code>group_name</code> (e.g., <code>\"outbox-publishers\"</code>) in the service discovery backend. Each instance gets a unique ID.</p> </li> <li> <p>Discovery: When it's time to process work, the <code>ShardingService</code> queries the service discovery backend to get a list of all currently healthy workers registered for its group.</p> </li> <li> <p>Hashing &amp; Filtering: A consistent hash ring is built using the IDs of the active workers. The service then takes the total list of work items (e.g., all pending <code>aggregate_key</code>s from the outbox) and passes them through the hash ring. It claims and processes only the items that hash to its own unique instance ID.</p> </li> <li> <p>Deregistration: On graceful shutdown, the worker deregisters itself. On the next processing cycle, the hash ring will be rebuilt by the remaining workers without the departed instance, and its workload will be automatically and stably redistributed.</p> </li> </ol>"},{"location":"athomic/resilience/sharding/#usage-example","title":"Usage Example","text":"<p>While the <code>OutboxPublisher</code> uses this internally, you could use the <code>ShardingService</code> for any custom distributed worker.</p> <pre><code>from nala.athomic.resilience.sharding import ShardingFactory\n\n# Get the sharding service for a specific policy defined in settings.toml\nsharding_service = ShardingFactory.create(policy_name=\"my_custom_workers\")\n\nasync def process_all_tenants():\n    # 1. Get the complete list of work from a central source\n    all_tenant_ids = await db.get_all_tenant_ids() # e.g., [\"tenant-a\", \"tenant-b\", ...]\n\n    # 2. Filter the list to get only the items this instance is responsible for\n    my_tenant_ids = await sharding_service.filter_owned_items(all_tenant_ids)\n\n    print(f\"This worker owns {len(my_tenant_ids)} tenants.\")\n\n    # 3. Process only the assigned workload\n    for tenant_id in my_tenant_ids:\n        await process_data_for(tenant_id)\n</code></pre>"},{"location":"athomic/resilience/sharding/#configuration","title":"Configuration","text":"<p>You configure sharding policies in your <code>settings.toml</code> under the <code>[resilience.sharding]</code> section. A policy simply defines a <code>group_name</code>, which must correspond to the name used for service discovery.</p> <pre><code>[default.resilience.sharding]\nenabled = true\n\n  # A dictionary of named sharding policies\n  [default.resilience.sharding.policies]\n    # This policy is used by the OutboxPublisher\n    [default.resilience.sharding.policies.outbox_publishers]\n    # This name must match the service name registered in your discovery backend (e.g., Consul)\n    group_name = \"outbox-publisher-service\"\n\n    # You can define other policies for other worker types\n    [default.resilience.sharding.policies.my_custom_workers]\n    group_name = \"custom-worker-service\"\n</code></pre>"},{"location":"athomic/resilience/sharding/#api-reference","title":"API Reference","text":""},{"location":"athomic/resilience/sharding/#nala.athomic.resilience.sharding.protocol.ShardingProtocol","title":"<code>nala.athomic.resilience.sharding.protocol.ShardingProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the contract for a generic service that distributes items among a dynamic group of workers.</p> <p>This protocol abstracts the mechanism for load balancing workloads (like processing message aggregates or tasks) using consistent hashing across multiple service instances.</p>"},{"location":"athomic/resilience/sharding/#nala.athomic.resilience.sharding.protocol.ShardingProtocol.deregister_self","title":"<code>deregister_self()</code>  <code>async</code>","text":"<p>Deregisters the current service instance from the sharding group, allowing its workload to be redistributed to remaining active workers.</p> <p>This must be called during graceful service shutdown.</p>"},{"location":"athomic/resilience/sharding/#nala.athomic.resilience.sharding.protocol.ShardingProtocol.filter_owned_items","title":"<code>filter_owned_items(items)</code>  <code>async</code>","text":"<p>Given a list of all potential items (e.g., aggregate keys), returns the subset that belongs to the current worker instance based on a consistent hashing algorithm and the current set of active workers.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>List[str]</code> <p>A list of all item identifiers.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The subset of items assigned to this worker instance.</p>"},{"location":"athomic/resilience/sharding/#nala.athomic.resilience.sharding.protocol.ShardingProtocol.register_self","title":"<code>register_self()</code>  <code>async</code>","text":"<p>Registers the current service instance as an active worker in the sharding group via a service discovery mechanism (e.g., Consul).</p> <p>This must be called during service startup.</p>"},{"location":"athomic/resilience/sharding/#nala.athomic.resilience.sharding.service.ShardingService","title":"<code>nala.athomic.resilience.sharding.service.ShardingService</code>","text":"<p>               Bases: <code>ShardingProtocol</code></p> <p>Orchestrates distributed workload assignment for a group of workers using consistent hashing and a Service Discovery backend.</p> <p>This service ensures that a set of items (e.g., message aggregate keys or tasks) is distributed stably among currently active service instances (workers).</p> <p>Attributes:</p> Name Type Description <code>policy_settings</code> <code>ShardingPolicySettings</code> <p>Configuration defining the target worker group name.</p> <code>server_settings</code> <code>ServerSettings</code> <p>Configuration defining the host and port for registration.</p> <code>group_name</code> <code>str</code> <p>The name used for service discovery (e.g., 'outbox-publishers').</p> <code>instance_id</code> <code>str</code> <p>A unique identifier for this specific worker instance.</p> <code>discovery</code> <code>ServiceDiscoveryProtocol</code> <p>The client for service discovery operations.</p>"},{"location":"athomic/resilience/sharding/#nala.athomic.resilience.sharding.service.ShardingService.__init__","title":"<code>__init__(policy_settings, server_settings)</code>","text":"<p>Initializes the ShardingService with policy and network configuration.</p> <p>Parameters:</p> Name Type Description Default <code>policy_settings</code> <code>ShardingPolicySettings</code> <p>The sharding policy configuration.</p> required <code>server_settings</code> <code>ServerSettings</code> <p>The server host and port configuration.</p> required"},{"location":"athomic/resilience/sharding/#nala.athomic.resilience.sharding.service.ShardingService.deregister_self","title":"<code>deregister_self()</code>  <code>async</code>","text":"<p>Deregisters the current service instance from the sharding group during graceful shutdown.</p>"},{"location":"athomic/resilience/sharding/#nala.athomic.resilience.sharding.service.ShardingService.filter_owned_items","title":"<code>filter_owned_items(items)</code>  <code>async</code>","text":"<p>Filters a list of items, returning only those mathematically owned by this worker instance based on the consistent hash ring.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>List[str]</code> <p>The list of all item identifiers (e.g., aggregate keys).</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The subset of items assigned to this worker.</p>"},{"location":"athomic/resilience/sharding/#nala.athomic.resilience.sharding.service.ShardingService.register_self","title":"<code>register_self()</code>  <code>async</code>","text":"<p>Registers the current service instance as an active worker in the group via the Service Discovery provider.</p>"},{"location":"athomic/resilience/sharding/#nala.athomic.resilience.sharding.factory.ShardingFactory","title":"<code>nala.athomic.resilience.sharding.factory.ShardingFactory</code>","text":"<p>Manages singleton instances of ShardingService, one per configured policy.</p>"},{"location":"athomic/resilience/sharding/#nala.athomic.resilience.sharding.factory.ShardingFactory.clear","title":"<code>clear()</code>  <code>classmethod</code>","text":"<p>Clears all cached instances. Used for testing.</p>"},{"location":"athomic/resilience/sharding/#nala.athomic.resilience.sharding.factory.ShardingFactory.create","title":"<code>create(policy_name)</code>  <code>classmethod</code>","text":"<p>Creates and returns a singleton instance of the ShardingService for a specific, named policy defined in the configuration.</p>"},{"location":"athomic/resilience/timeout/","title":"Timeouts &amp; Cancellation","text":""},{"location":"athomic/resilience/timeout/#overview","title":"Overview","text":"<p>The Timeout module provides a robust and flexible way to enforce time limits on operations. This is a critical resilience pattern that prevents requests from hanging indefinitely, which could otherwise consume and exhaust system resources.</p> <p>The key feature of Athomic's implementation is Contextual Timeout Cancellation. This allows a single, top-level timeout to be respected across a deep call stack of multiple, nested asynchronous operations.</p>"},{"location":"athomic/resilience/timeout/#how-it-works-contextual-timeout-cancellation","title":"How It Works: Contextual Timeout Cancellation","text":"<p>The system works through the coordinated use of two decorators and the <code>Context</code> module.</p> <ol> <li> <p>Setting the Deadline with <code>@timeout</code>: You apply the main <code>@timeout(seconds=...)</code> decorator to a high-level function, like an API route handler. When this function is called, the decorator records an absolute <code>deadline</code> timestamp (current time + timeout seconds) in the current <code>ExecutionContext</code>.</p> </li> <li> <p>Enforcing the Deadline with <code>@cancellable_operation</code>: You apply the <code>@cancellable_operation</code> decorator to lower-level, internal functions that are called within the initial <code>@timeout</code> block (e.g., service methods, repository calls).</p> </li> <li> <p>Dynamic Cancellation: Each <code>@cancellable_operation</code> decorator reads the <code>deadline</code> from the context, calculates the remaining time, and applies that remaining time as its own timeout for the function it wraps.</p> </li> </ol> <p>This creates a powerful effect: if an API endpoint has a 30-second timeout, and the first 20 seconds are spent in one service call, a subsequent cancellable operation will automatically have a timeout of less than 10 seconds to ensure the total deadline is met. If the deadline has already passed, it will be cancelled immediately.</p>"},{"location":"athomic/resilience/timeout/#usage","title":"Usage","text":""},{"location":"athomic/resilience/timeout/#1-the-timeout-decorator","title":"1. The <code>@timeout</code> Decorator","text":"<p>Use this decorator on top-level functions that represent a complete unit of work, like an API endpoint handler. This sets the overall deadline for the entire operation.</p> <pre><code>from nala.athomic.resilience.timeout import timeout, TimeoutException\n\n@router.post(\"/process-data\")\n@timeout(seconds=15.0)\nasync def process_data_endpoint(data: dict):\n    \"\"\"\n    This entire request, including all downstream calls,\n    must complete within 15 seconds.\n    \"\"\"\n    try:\n        # This service call is decorated with @cancellable_operation\n        result = await data_service.process(data)\n        return {\"status\": \"success\", \"result\": result}\n    except TimeoutException:\n        return {\"status\": \"error\", \"message\": \"Processing timed out.\"}\n</code></pre>"},{"location":"athomic/resilience/timeout/#2-the-cancellable_operation-decorator","title":"2. The <code>@cancellable_operation</code> Decorator","text":"<p>Use this decorator on internal functions that are part of a larger workflow initiated by a function with a <code>@timeout</code>.</p> <pre><code>from nala.athomic.resilience.timeout import cancellable_operation\n\nclass DataService:\n    @cancellable_operation\n    async def process(self, data: dict):\n        # This operation will respect the deadline set by the\n        # @timeout decorator on the API endpoint.\n        await self.repository.save(data)\n        await self.external_api.notify(data)\n</code></pre>"},{"location":"athomic/resilience/timeout/#3-the-dynamic_timeout-decorator","title":"3. The <code>@dynamic_timeout</code> Decorator","text":"<p>As an alternative to <code>@timeout</code>, you can use <code>@dynamic_timeout</code> if you need the timeout duration to be specified at call time rather than at decoration time.</p> <pre><code>from nala.athomic.resilience.timeout import dynamic_timeout\n\n@dynamic_timeout(default=10.0)\nasync def fetch_data(url: str):\n    # ...\n    pass\n\n# This call will use a 5-second timeout\nawait fetch_data(\"[http://example.com](http://example.com)\", seconds=5.0)\n\n# This call will use the 10-second default\nawait fetch_data(\"[http://another-example.com](http://another-example.com)\")\n</code></pre>"},{"location":"athomic/resilience/timeout/#configuration","title":"Configuration","text":"<p>This module does not require any configuration in <code>settings.toml</code>. All timeout behavior is defined directly in the code via decorator arguments.</p>"},{"location":"athomic/resilience/timeout/#api-reference","title":"API Reference","text":""},{"location":"athomic/resilience/timeout/#nala.athomic.resilience.timeout.decorators.timeout","title":"<code>nala.athomic.resilience.timeout.decorators.timeout(seconds, fallback=None, *, preserve_context=False)</code>","text":"<p>Decorator to apply a maximum execution time limit (timeout) to synchronous or asynchronous functions.</p> <p>The timeout uses the application's internal mechanism (<code>run_with_timeout</code>) to handle both concurrency types and ensures consistent context propagation and cleanup.</p> <p>Parameters:</p> Name Type Description Default <code>seconds</code> <code>float</code> <p>The total timeout duration in seconds.</p> required <code>fallback</code> <code>Optional[Callable]</code> <p>An optional zero-argument callable (sync or async)                            to execute if the primary function times out.</p> <code>None</code> <code>preserve_context</code> <code>bool</code> <p>Internal flag. If True, skips cleaning the timeout deadline                      from the context upon exiting the wrapper (primarily for testing purposes).</p> <code>False</code> <p>Returns:</p> Type Description <code>Callable[[F], F]</code> <p>Callable[[F], F]: The wrapped function with timeout behavior.</p>"},{"location":"athomic/resilience/timeout/#nala.athomic.resilience.timeout.cancellation.cancellable_operation","title":"<code>nala.athomic.resilience.timeout.cancellation.cancellable_operation(func)</code>","text":"<p>Decorator that wraps an asynchronous operation to respect a global timeout deadline set earlier in the execution context by the <code>@timeout</code> decorator.</p> <p>This mechanism enforces dynamic timeout cancellation: if the remaining time until the context deadline is less than the function's execution time, <code>asyncio.wait_for</code> is used to enforce the remaining duration.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Awaitable[Any]]</code> <p>The asynchronous function to wrap.</p> required <p>Returns:</p> Type Description <code>Callable[..., Awaitable[Any]]</code> <p>Callable[..., Awaitable[Any]]: The wrapped function with cancellation logic.</p> <p>Raises:</p> Type Description <code>CancelledException</code> <p>If the deadline has already passed before the function starts.</p> <code>TimeoutException</code> <p>If the function exceeds the remaining time before the deadline.</p>"},{"location":"athomic/resilience/timeout/#nala.athomic.resilience.timeout.exceptions.TimeoutException","title":"<code>nala.athomic.resilience.timeout.exceptions.TimeoutException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the execution of a function exceeds the defined timeout.</p>"},{"location":"athomic/resilience/timeout/#nala.athomic.resilience.timeout.exceptions.CancelledException","title":"<code>nala.athomic.resilience.timeout.exceptions.CancelledException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when execution is cancelled gracefully before timeout.</p>"},{"location":"athomic/security/auth/","title":"Authentication &amp; Authorization","text":""},{"location":"athomic/security/auth/#overview","title":"Overview","text":"<p>The Authentication and Authorization module provides a flexible, policy-based system for securing API endpoints. It is designed to clearly separate the concepts of Authentication (verifying who a user or service is) and Authorization (determining what they are allowed to do).</p>"},{"location":"athomic/security/auth/#key-features","title":"Key Features","text":"<ul> <li>Policy-Based: Access control is defined by simple, named policies (e.g., \"public\", \"private\", \"internal\").</li> <li>Multiple Mechanisms: Out-of-the-box support for common authentication mechanisms like JWT Bearer tokens and API Keys (both shared and per-client).</li> <li>Decoupled Architecture: A clean separation between Policies (Executors), which define what access is required, and Authentication Providers, which handle how credentials are validated.</li> <li>Extensible: New authentication providers or authorization policies can be added by implementing simple protocols and registering them.</li> </ul>"},{"location":"athomic/security/auth/#core-concepts","title":"Core Concepts","text":""},{"location":"athomic/security/auth/#policies-executors","title":"Policies (Executors)","text":"<p>An Authorization Policy is a named rule that defines the security requirements for an endpoint. For example, a \"private\" policy might require a valid JWT, while an \"internal\" policy might require a specific IP address.</p> <p>In Athomic, policies are implemented by classes that follow the <code>AuthExecutorProtocol</code>. The <code>AuthService</code> uses a central registry to look up the correct executor for a given policy name.</p>"},{"location":"athomic/security/auth/#authentication-providers","title":"Authentication Providers","text":"<p>An Authentication Provider is a component that performs the actual credential validation. For example, the <code>JWTAuth</code> provider knows how to decode a JWT, verify its signature, and check its expiration. The <code>APIKeyDBProvider</code> knows how to look up an API key's hash in a database.</p> <p>All providers implement the <code>AuthProviderProtocol</code>, allowing the executors to be decoupled from the specific validation logic.</p>"},{"location":"athomic/security/auth/#authservice","title":"<code>AuthService</code>","text":"<p>The <code>AuthService</code> is the central orchestrator that ties everything together. When an endpoint needs to be secured, you ask the <code>AuthService</code> to authorize the request against a specific policy. The service finds the correct executor for that policy and runs it, which in turn uses the appropriate authentication provider.</p>"},{"location":"athomic/security/auth/#available-policies-executors","title":"Available Policies (Executors)","text":"<p>Athomic comes with a set of pre-configured policies:</p> <ul> <li><code>public</code>: Allows anonymous, unrestricted access.</li> <li><code>private</code>: Requires a valid JWT Bearer token in the <code>Authorization</code> header.</li> <li><code>shared_api_key</code>: Requires a valid, globally shared API key in the configured header.</li> <li><code>db_api_key</code>: Requires a valid, per-client API key that is looked up in the database.</li> <li><code>internal</code>: Requires the request's IP address to be on a configured allowlist.</li> </ul>"},{"location":"athomic/security/auth/#how-to-secure-an-endpoint","title":"How to Secure an Endpoint","text":"<p>Securing an endpoint is typically done at the web framework layer, for example, using a FastAPI dependency. The dependency would be responsible for extracting credentials from the request, building a <code>SecurityRequestContext</code>, and calling the <code>AuthService</code>.</p>"},{"location":"athomic/security/auth/#conceptual-fastapi-example","title":"Conceptual FastAPI Example","text":"<pre><code>from fastapi import Depends, HTTPException, Request\nfrom nala.athomic.security.auth import AuthService, SecurityRequestContext\nfrom nala.athomic.security.exceptions import AthomicAuthError, AthomicForbiddenError\n\nauth_service = AuthService() # Singleton instance\n\nasync def require_private_access(request: Request):\n    \"\"\"A FastAPI dependency that enforces the 'private' policy.\"\"\"\n\n    context = SecurityRequestContext(\n        client_host=request.client.host,\n        headers=dict(request.headers)\n    )\n\n    try:\n        # Ask the AuthService to authorize the request against the \"private\" policy\n        authenticated_user = await auth_service.authorize(context, \"private\")\n        return authenticated_user\n    except AthomicAuthError as e:\n        raise HTTPException(status_code=401, detail=str(e))\n    except AthomicForbiddenError as e:\n        raise HTTPException(status_code=403, detail=str(e))\n\n@router.get(\"/profile\")\nasync def get_user_profile(user: AuthenticatedUser = Depends(require_private_access)):\n    # This endpoint is now protected. The 'user' object contains the\n    # validated claims from the JWT.\n    return {\"user_id\": user.sub, \"scopes\": user.scopes}\n</code></pre>"},{"location":"athomic/security/auth/#configuration","title":"Configuration","text":"<p>Authentication is configured under the <code>[security.auth]</code> section in your <code>settings.toml</code>.</p> <pre><code>[default.security.auth]\n# A master switch to enable or disable authentication.\nenable = true\n\n# The default backend to use. Not always necessary as policies are explicit.\nbackend = \"jwt\"\n\n  # Configuration for JWT-based authentication.\n  [default.security.auth.jwt]\n  # The secret key for signing tokens. Best practice is to use a secret reference.\n  secret_key = { path = \"auth/jwt\", key = \"secret\" }\n  algorithm = \"HS256\"\n  expiration_minutes = 60\n\n  # Configuration for API Key-based authentication.\n  [default.security.auth.api_key]\n  enabled = true\n  header_name = \"X-API-KEY\"\n\n  # The secret for the \"shared_api_key\" policy.\n  secret_key = { path = \"auth/api-key\", key = \"shared-secret\" }\n\n  # The database connection for the \"db_api_key\" policy.\n  document_db_connection_name = \"default_mongo\"\n</code></pre>"},{"location":"athomic/security/auth/#api-reference","title":"API Reference","text":""},{"location":"athomic/security/auth/#nala.athomic.security.auth.service.AuthService","title":"<code>nala.athomic.security.auth.service.AuthService</code>","text":"<p>The central authorization service.</p> <p>This class acts as an Orchestrator, delegating the specific policy execution logic to the appropriate <code>AuthExecutorProtocol</code> instance, which is retrieved through a standardized registry. It integrates metrics for all policy check attempts.</p>"},{"location":"athomic/security/auth/#nala.athomic.security.auth.service.AuthService.__init__","title":"<code>__init__(registry=None)</code>","text":"<p>Initializes the service with a specific executor registry.</p> <p>Parameters:</p> Name Type Description Default <code>registry</code> <code>Optional[AuthExecutorRegistry]</code> <p>An optional registry instance                                       for dependency injection (DI).                                       Defaults to the global singleton.</p> <code>None</code>"},{"location":"athomic/security/auth/#nala.athomic.security.auth.service.AuthService.authorize","title":"<code>authorize(context, policy_name)</code>  <code>async</code>","text":"<p>Finds the correct executor in the registry and executes the authorization check.</p> <p>This method encapsulates the authorization flow: it records the attempt, retrieves the executor, executes the check, and logs the final success or failure.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>SecurityRequestContext</code> <p>The request context containing headers and client information.</p> required <code>policy_name</code> <code>str</code> <p>The name of the authorization policy to enforce (e.g., 'private', 'db_api_key').</p> required <p>Returns:</p> Name Type Description <code>AuthenticatedUser</code> <code>AuthenticatedUser</code> <p>A user object representing the authenticated identity.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no executor is registered for the specified <code>policy_name</code>.</p> <code>Exception</code> <p>Propagates the exception raised by the executor upon authorization failure.</p>"},{"location":"athomic/security/auth/#nala.athomic.security.auth.executors.protocols.AuthExecutorProtocol","title":"<code>nala.athomic.security.auth.executors.protocols.AuthExecutorProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the contract for an authorization execution strategy.</p> <p>Each executor is responsible for enforcing a single, specific form of validation (e.g., JWT token check, shared API key validation, IP allowlist check), adhering to the Single Responsibility Principle (SRP).</p>"},{"location":"athomic/security/auth/#nala.athomic.security.auth.executors.protocols.AuthExecutorProtocol.execute","title":"<code>execute(context)</code>  <code>async</code>","text":"<p>Executes the authorization logic against the provided request context.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>SecurityRequestContext</code> <p>The request context containing client host                             and headers necessary for validation.</p> required <p>Returns:</p> Name Type Description <code>AuthenticatedUser</code> <code>AuthenticatedUser</code> <p>An object representing the authenticated identity and its claims.</p> <p>Raises:</p> Type Description <code>AthomicAuthError</code> <p>If authentication fails (e.g., invalid token, expired, revoked).</p> <code>AthomicForbiddenError</code> <p>If access is denied (e.g., IP not allowed, insufficient scope).</p>"},{"location":"athomic/security/auth/#nala.athomic.security.auth.protocol.AuthProviderProtocol","title":"<code>nala.athomic.security.auth.protocol.AuthProviderProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p>"},{"location":"athomic/security/auth/#nala.athomic.security.auth.protocol.AuthProviderProtocol.check","title":"<code>check(token)</code>  <code>async</code>","text":"<p>Verify a token and return its decoded payload if valid.</p> <p>:param token: The token to verify. :return: A dictionary with the decoded payload. :raises: AuthenticationError or other relevant exception if invalid.</p>"},{"location":"athomic/security/auth/#nala.athomic.security.auth.protocol.AuthProviderProtocol.create_token","title":"<code>create_token(subject, scope=None)</code>","text":"<p>Create a signed token for the given subject.</p> <p>:param subject: Unique identifier for the token owner (e.g., user ID). :param scope: Optional scope string for authorization. :return: A JWT or compatible token as a string.</p>"},{"location":"athomic/security/auth/#nala.athomic.security.auth.models.auth_user.AuthenticatedUser","title":"<code>nala.athomic.security.auth.models.auth_user.AuthenticatedUser</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents an authenticated identity extracted from a validated token payload (e.g., JWT).</p> <p>This model provides a normalized view of the authenticated user's claims, used throughout the application for authorization decisions.</p> <p>Attributes:</p> Name Type Description <code>sub</code> <code>str</code> <p>The unique identifier for the user or entity (Subject) that the token represents.</p> <code>scopes</code> <code>List[str]</code> <p>A list of normalized (lowercase, split) user permission scopes.</p> <code>roles</code> <code>List[str]</code> <p>A list of roles assigned to the user.</p> <code>jti</code> <code>str</code> <p>The unique ID of the token (JWT ID), crucial for token denylist/revocation checks.</p> <code>exp</code> <code>int</code> <p>The timestamp (UNIX epoch) when the token expires.</p> <code>raw_scope</code> <code>str</code> <p>The original, unmodified scope string from the token payload.</p>"},{"location":"athomic/security/auth/#nala.athomic.security.auth.models.auth_user.AuthenticatedUser.from_payload","title":"<code>from_payload(payload)</code>  <code>classmethod</code>","text":"<p>Creates an <code>AuthenticatedUser</code> instance from a raw decoded token payload dictionary.</p> <p>This factory method handles normalization, such as splitting the raw scope string into a list of lowercase scopes.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>Dict[str, Any]</code> <p>The decoded token payload (dictionary).</p> required <p>Returns:</p> Name Type Description <code>AuthenticatedUser</code> <code>AuthenticatedUser</code> <p>A validated instance of the user model.</p>"},{"location":"athomic/security/crypto/","title":"Cryptography","text":""},{"location":"athomic/security/crypto/#overview","title":"Overview","text":"<p>The Cryptography module provides a simple, high-level abstraction for symmetric (secret key) encryption and decryption. It is built to be secure by default, leveraging the robust and widely trusted Fernet implementation from the <code>cryptography</code> library.</p> <p>Its primary use case is to provide an end-to-end encryption layer for message payloads as part of the <code>Payload Processing Pipeline</code>, but it can also be used for any general-purpose encryption needs within your application.</p>"},{"location":"athomic/security/crypto/#key-features","title":"Key Features","text":"<ul> <li>Strong Encryption: Uses Fernet (AES-128 in CBC mode with PKCS7 padding, signed with HMAC using SHA256).</li> <li>Secure Key Handling: The encryption key (<code>fernet_key</code>) is configured as a secret and resolved at runtime via the Secrets Management module.</li> <li>Lazy Initialization: The Fernet client is only initialized on its first use, preventing unnecessary key handling at startup.</li> <li>Pipeline Integration: Seamlessly integrates into the messaging payload pipeline to automatically encrypt and decrypt messages.</li> </ul>"},{"location":"athomic/security/crypto/#how-it-works","title":"How It Works","text":"<p>The module is centered around the <code>CryptoProviderProtocol</code>, which defines a simple <code>encrypt_bytes</code> and <code>decrypt_bytes</code> interface. The default implementation is the <code>FernetCryptoProvider</code>.</p> <p>The <code>CryptoProviderFactory</code> is a singleton factory used to obtain a configured instance of the provider. It ensures that the same key and client are used throughout the application.</p>"},{"location":"athomic/security/crypto/#integration-with-the-messaging-pipeline","title":"Integration with the Messaging Pipeline","text":"<p>The most common way to use the crypto module is to add it as a step in the messaging payload pipeline. This automatically encrypts the entire message payload after serialization and before it is sent to the broker. On the consumer side, it automatically decrypts the payload before deserialization.</p> <p>To enable this, simply add a <code>\"crypto\"</code> step to your pipeline configuration.</p> <pre><code># In settings.toml\n[default.integration.messaging.payload]\n\n# The pipeline is executed in order for encoding, and in reverse for decoding.\n# Flow: Pydantic Model -&gt; [Serializer] -&gt; Bytes -&gt; [Crypto] -&gt; Encrypted Bytes\n[[default.integration.messaging.payload.pipeline]]\nstep = \"serializer\"\n\n[[default.integration.messaging.payload.pipeline]]\nstep = \"crypto\"\n</code></pre> <p>With this configuration, all messages will be transparently encrypted and decrypted as they are produced and consumed.</p>"},{"location":"athomic/security/crypto/#direct-usage-example","title":"Direct Usage Example","text":"<p>You can also use the crypto provider directly for general-purpose encryption tasks.</p> <pre><code>from nala.athomic.security.crypto import CryptoProviderFactory\n\nasync def encrypt_sensitive_data(data: str) -&gt; bytes:\n    crypto_provider = CryptoProviderFactory.create()\n\n    data_bytes = data.encode('utf-8')\n    encrypted_bytes = await crypto_provider.encrypt_bytes(data_bytes)\n\n    return encrypted_bytes\n\nasync def decrypt_sensitive_data(encrypted_data: bytes) -&gt; str:\n    crypto_provider = CryptoProviderFactory.create()\n\n    decrypted_bytes = await crypto_provider.decrypt_bytes(encrypted_data)\n\n    return decrypted_bytes.decode('utf-8')\n</code></pre>"},{"location":"athomic/security/crypto/#configuration","title":"Configuration","text":"<p>The cryptography module requires a <code>fernet_key</code> to be configured under the <code>[security.crypto]</code> section in your <code>settings.toml</code>. For production, this must be a reference to a secret stored in a secure backend like Vault.</p> <pre><code>[default.security.crypto]\n# The Fernet key must be 32 URL-safe base64-encoded bytes.\n# In production, this should always be a secret reference.\nfernet_key = { path = \"global/crypto\", key = \"fernet-key\" }\n</code></pre> <p>You can generate a new Fernet key using the <code>cryptography</code> library:</p> <pre><code>from cryptography.fernet import Fernet\nkey = Fernet.generate_key()\nprint(key.decode())\n</code></pre>"},{"location":"athomic/security/crypto/#api-reference","title":"API Reference","text":""},{"location":"athomic/security/crypto/#nala.athomic.security.crypto.protocol.CryptoProviderProtocol","title":"<code>nala.athomic.security.crypto.protocol.CryptoProviderProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the contract for a cryptographic provider that operates on raw bytes.</p> <p>This protocol ensures that any concrete implementation (e.g., Fernet, AES) conforms to a standardized asynchronous interface for encryption and decryption, adhering to the Dependency Inversion Principle (DIP).</p>"},{"location":"athomic/security/crypto/#nala.athomic.security.crypto.protocol.CryptoProviderProtocol.decrypt_bytes","title":"<code>decrypt_bytes(encrypted_data)</code>  <code>async</code>","text":"<p>Decrypts an encrypted byte string.</p> <p>Parameters:</p> Name Type Description Default <code>encrypted_data</code> <code>bytes</code> <p>The encrypted byte string.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The original, decrypted byte string.</p> <p>Raises:</p> Type Description <code>CryptoOperationError</code> <p>If decryption fails (e.g., invalid key, invalid token).</p>"},{"location":"athomic/security/crypto/#nala.athomic.security.crypto.protocol.CryptoProviderProtocol.encrypt_bytes","title":"<code>encrypt_bytes(data)</code>  <code>async</code>","text":"<p>Encrypts a raw byte string.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>The raw, unencrypted byte string.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The encrypted byte string.</p>"},{"location":"athomic/security/crypto/#nala.athomic.security.crypto.factory.CryptoProviderFactory","title":"<code>nala.athomic.security.crypto.factory.CryptoProviderFactory</code>","text":"<p>Manages the singleton instance of the CryptoProvider.</p>"},{"location":"athomic/security/crypto/#nala.athomic.security.crypto.fernet_provider.FernetCryptoProvider","title":"<code>nala.athomic.security.crypto.fernet_provider.FernetCryptoProvider</code>","text":"<p>               Bases: <code>CryptoProviderProtocol</code>, <code>CredentialResolve</code></p> <p>A cryptographic provider implementation using the Fernet symmetric encryption standard.</p> <p>This class adheres to the CryptoProviderProtocol and manages the entire lifecycle and security of the Fernet key: 1. Lazy Initialization: The key is resolved and the Fernet instance is created    only on the first call to <code>encrypt_bytes</code> or <code>decrypt_bytes</code>. 2. Asynchronous Credential Resolution: It uses the <code>CredentialResolve</code> mixin    to securely fetch the encryption key from an external source (e.g., Vault). 3. Integrated Observability: All encryption and decryption attempts are    instrumented with distributed tracing and Prometheus metrics.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <code>CryptoSettings</code> <p>The configuration for the crypto module.</p> <code>key_source</code> <p>The source object (e.g., <code>SecretStr</code>, <code>CredentialProxy</code>) for the Fernet key.</p> <code>fernet</code> <code>Optional[Fernet]</code> <p>The underlying Fernet encryption instance.</p> <code>_init_lock</code> <code>Lock</code> <p>A lock to ensure thread-safe, single-time initialization.</p> <code>tracer</code> <p>An OpenTelemetry tracer instance.</p> <code>provider_name</code> <code>str</code> <p>The name used for metric and span labeling.</p>"},{"location":"athomic/security/crypto/#nala.athomic.security.crypto.fernet_provider.FernetCryptoProvider.__init__","title":"<code>__init__(settings=None)</code>","text":"<p>Initializes the provider, but delays Fernet instance creation.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[CryptoSettings]</code> <p>The configuration settings for the provider.</p> <code>None</code>"},{"location":"athomic/security/crypto/#nala.athomic.security.crypto.fernet_provider.FernetCryptoProvider.decrypt_bytes","title":"<code>decrypt_bytes(encrypted_data)</code>  <code>async</code>","text":"<p>Decrypts an encrypted byte string, wrapped with tracing and metrics.</p> <p>Parameters:</p> Name Type Description Default <code>encrypted_data</code> <code>bytes</code> <p>The encrypted byte string.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The original, decrypted byte string.</p> <p>Raises:</p> Type Description <code>CryptoOperationError</code> <p>If decryption fails due to an invalid token                   or any other internal error.</p>"},{"location":"athomic/security/crypto/#nala.athomic.security.crypto.fernet_provider.FernetCryptoProvider.encrypt_bytes","title":"<code>encrypt_bytes(data)</code>  <code>async</code>","text":"<p>Encrypts a raw byte string, wrapped with tracing and metrics.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>The raw, unencrypted byte string.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The encrypted byte string.</p> <p>Raises:</p> Type Description <code>CryptoOperationError</code> <p>If encryption fails for any reason (e.g., internal error).</p>"},{"location":"athomic/security/crypto/#nala.athomic.security.crypto.adapters.CryptoStepAdapter","title":"<code>nala.athomic.security.crypto.adapters.CryptoStepAdapter</code>","text":"<p>               Bases: <code>ProcessingStepProtocol</code></p> <p>An adapter that integrates a CryptoProviderProtocol into the payload processing pipeline.</p> <p>This class acts as the Adapter for the encryption step (part of the Pipes and Filters pattern), mapping the generic <code>encode</code>/<code>decode</code> operations required by the <code>PayloadProcessor</code> to the concrete <code>encrypt_bytes</code>/<code>decrypt_bytes</code> methods of the underlying <code>CryptoProviderProtocol</code>. This adheres to the SRP by ensuring the <code>CryptoProvider</code> focuses only on the cryptographic operation, while the adapter focuses only on the interface transformation.</p> <p>Attributes:</p> Name Type Description <code>crypto</code> <code>CryptoProviderProtocol</code> <p>The underlying provider instance that performs                              the actual encryption and decryption.</p>"},{"location":"athomic/security/crypto/#nala.athomic.security.crypto.adapters.CryptoStepAdapter.__init__","title":"<code>__init__(crypto_provider)</code>","text":"<p>Initializes the adapter with the core cryptographic provider.</p> <p>Parameters:</p> Name Type Description Default <code>crypto_provider</code> <code>CryptoProviderProtocol</code> <p>An instance of the concrete                                      cryptographic provider (e.g., <code>FernetCryptoProvider</code>).</p> required"},{"location":"athomic/security/crypto/#nala.athomic.security.crypto.adapters.CryptoStepAdapter.decode","title":"<code>decode(data, **kwargs)</code>  <code>async</code>","text":"<p>Decrypts the data (inbound pipeline flow).</p> <p>Maps the pipeline's <code>decode</code> call to the provider's <code>decrypt_bytes</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The encrypted payload to be decrypted, expected as bytes.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments (ignored by this step).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The decrypted, original data as bytes.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input <code>data</code> is not of type bytes.</p> <code>CryptoOperationError</code> <p>Propagated from the underlying provider if decryption fails.</p>"},{"location":"athomic/security/crypto/#nala.athomic.security.crypto.adapters.CryptoStepAdapter.encode","title":"<code>encode(data, **kwargs)</code>  <code>async</code>","text":"<p>Encrypts the data (outbound pipeline flow).</p> <p>Maps the pipeline's <code>encode</code> call to the provider's <code>encrypt_bytes</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The payload to be encrypted, expected as bytes.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments (ignored by this step).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The encrypted data as bytes.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input <code>data</code> is not of type bytes.</p>"},{"location":"athomic/security/secrets/","title":"Secrets Management","text":""},{"location":"athomic/security/secrets/#overview","title":"Overview","text":"<p>The Secrets Management module provides a secure and centralized system for handling sensitive data like API keys, database passwords, and authentication tokens. Instead of hardcoding secrets in configuration files or environment variables, Athomic treats them as references to be resolved at runtime from a secure, external backend.</p> <p>This approach significantly improves the security posture of the application and is a best practice for modern cloud-native development.</p>"},{"location":"athomic/security/secrets/#key-features","title":"Key Features","text":"<ul> <li>Multiple Backends: Out-of-the-box support for fetching secrets from HashiCorp Vault, environment variables, or local files.</li> <li>Runtime Resolution: Secrets are fetched \"just-in-time\" only when they are first needed, not at application startup. This reduces the time they are held in memory and supports dynamic secret rotation.</li> <li>Extensible Provider Model: The system is built on a protocol, making it easy to add new secret providers in the future.</li> <li>Resilient and Observable: All interactions with the secrets backend are fully instrumented with tracing and metrics.</li> </ul>"},{"location":"athomic/security/secrets/#the-just-in-time-resolution-flow","title":"The Just-in-Time Resolution Flow","text":"<p>The core of the secrets module is its just-in-time (JIT) resolution mechanism. Here is how it works:</p> <ol> <li> <p>Configure a Reference: In your <code>settings.toml</code>, you define a reference to a secret instead of the secret itself. This is done using a <code>path</code> and <code>key</code> structure.</p> <p>```toml</p> </li> <li> <p>Startup &amp; Proxy Replacement: When the application starts, the <code>SecretsManager</code> service traverses the entire configuration object. When it finds a secret reference, it replaces it in-memory with a lightweight <code>CredentialProxy</code> object. This proxy knows how to fetch the secret but doesn't do it yet.</p> </li> <li> <p>First Access &amp; Resolution: Later, when a component (like a database connector) needs the password, it tries to access the value. This triggers the <code>CredentialProxy</code>, which then makes a live, asynchronous call to the configured secrets provider (e.g., Vault) to fetch the actual secret value.</p> </li> </ol> <p>This lazy-loading approach is highly secure and efficient.</p>"},{"location":"athomic/security/secrets/#instead-of-this","title":"Instead of this:","text":""},{"location":"athomic/security/secrets/#password-my-secret-password-pragma-allowlist-secret","title":"password = \"my-secret-password\" # pragma: allowlist secret","text":""},{"location":"athomic/security/secrets/#you-do-this","title":"You do this:","text":"<p>password = { path = \"database/mongo/prod\", key = \"password\" } ```</p>"},{"location":"athomic/security/secrets/#available-providers","title":"Available Providers","text":""},{"location":"athomic/security/secrets/#vaultsecretsprovider","title":"<code>VaultSecretsProvider</code>","text":"<p>The recommended provider for production environments. It connects to a HashiCorp Vault server and supports both Token and AppRole authentication methods. It also includes a background task for automatic, continuous renewal of the Vault token, ensuring long-running services don't lose access.</p>"},{"location":"athomic/security/secrets/#envsecretsprovider","title":"<code>EnvSecretsProvider</code>","text":"<p>This provider reads secrets from environment variables. It constructs the variable name to look for based on the reference's path and key. For example, a reference with <code>path=\"database/mongo\"</code> and <code>key=\"password\"</code> would resolve to the environment variable <code>NALA_SECRET_DATABASE_MONGO_PASSWORD</code>.</p>"},{"location":"athomic/security/secrets/#filesecretsprovider","title":"<code>FileSecretsProvider</code>","text":"<p>This provider reads secrets from files on the local filesystem. This is particularly useful for containerized environments like Docker and Kubernetes, where secrets are often mounted as files into the container (e.g., <code>/var/run/secrets/db-password</code>).</p>"},{"location":"athomic/security/secrets/#configuration","title":"Configuration","text":"<p>The secrets provider is configured under the <code>[security.secrets]</code> section in your <code>settings.toml</code>.</p>"},{"location":"athomic/security/secrets/#vault-example","title":"Vault Example","text":"<pre><code>[default.security.secrets]\nbackend = \"vault\"\n\n  [default.security.secrets.provider]\n  addr = \"http://localhost:8200\"\n  auth_method = \"token\"\n  token = \"my-root-token\" # Use a secret reference in production!\n</code></pre>"},{"location":"athomic/security/secrets/#environment-variable-example","title":"Environment Variable Example","text":"<pre><code>[default.security.secrets]\nbackend = \"env\"\n</code></pre>"},{"location":"athomic/security/secrets/#file-example","title":"File Example","text":"<pre><code>[default.security.secrets]\nbackend = \"file\"\n  [default.security.secrets.provider]\n  base_path = \"/var/run/secrets\" # Base directory for secret files\n</code></pre>"},{"location":"athomic/security/secrets/#api-reference","title":"API Reference","text":""},{"location":"athomic/security/secrets/#nala.athomic.security.secrets.manager.SecretsManager","title":"<code>nala.athomic.security.secrets.manager.SecretsManager</code>","text":"<p>Orchestrates the resolution of external secret references within application settings.</p> <p>This manager is responsible for traversing the entire <code>AppSettings</code> structure and replacing static <code>SecretValue</code> objects (references to external backends like Vault) with lazy, asynchronous <code>CredentialProxy</code> objects. This ensures that actual secret values are only fetched just-in-time when first accessed by a consuming component, supporting secure, runtime secret rotation.</p> <p>Attributes:</p> Name Type Description <code>app_settings</code> <code>AppSettings</code> <p>The validated application configuration object.</p> <code>secrets_provider</code> <code>SecretsProtocol</code> <p>The concrete provider (e.g., Vault, Env)                                 used to fetch the raw secret data.</p>"},{"location":"athomic/security/secrets/#nala.athomic.security.secrets.manager.SecretsManager.__init__","title":"<code>__init__(settings, secrets_provider=None)</code>","text":"<p>Initializes the SecretsManager.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>AppSettings</code> <p>The application's root configuration object.</p> required <code>secrets_provider</code> <code>SecretsProtocol</code> <p>The concrete provider (e.g., Vault, Env)                             used to fetch the raw secret data.</p> <code>None</code>"},{"location":"athomic/security/secrets/#nala.athomic.security.secrets.manager.SecretsManager.resolve_settings","title":"<code>resolve_settings()</code>  <code>async</code>","text":"<p>Traverses the entire AppSettings object and replaces all SecretValue instances with CredentialProxy instances that fetch the secret just-in-time.</p> <p>This method is typically called early in the application's startup sequence after configuration is loaded but before services connect.</p>"},{"location":"athomic/security/secrets/#nala.athomic.security.secrets.protocol.SecretsProtocol","title":"<code>nala.athomic.security.secrets.protocol.SecretsProtocol</code>","text":"<p>               Bases: <code>BaseServiceProtocol</code>, <code>Protocol</code></p> <p>Defines the core contract for a secrets provider implementation.</p> <p>This protocol adheres to the Dependency Inversion Principle (DIP), decoupling the <code>SecretsManager</code> and consumers of credentials from the concrete source (e.g., HashiCorp Vault, environment variables). By inheriting from <code>BaseServiceProtocol</code>, all providers gain automatic lifecycle management (<code>connect</code>/<code>close</code>) by the framework.</p>"},{"location":"athomic/security/secrets/#nala.athomic.security.secrets.protocol.SecretsProtocol.get_secret","title":"<code>get_secret(path, key)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Retrieves a specific secret value as raw bytes from the configured backend.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The logical path or mount point where the secret resides         (e.g., 'database/production').</p> required <code>key</code> <code>str</code> <p>The specific key within the path that holds the secret value        (e.g., 'password').</p> required <p>Returns:</p> Type Description <code>Optional[bytes]</code> <p>Optional[bytes]: The resolved secret value as raw bytes, or None if              the secret is not found.</p> <p>Raises:</p> Type Description <code>SecretNotFoundError</code> <p>If the path or key does not exist.</p> <code>SecretAccessError</code> <p>For permission or other access-related failures.</p>"},{"location":"athomic/security/secrets/#nala.athomic.credentials.proxy.CredentialProxy","title":"<code>nala.athomic.credentials.proxy.CredentialProxy</code>","text":"<p>A lazy-loading proxy for a secret value.</p> <p>It holds a reference to the secrets provider and the secret's location (path/key). It fetches the secret's actual value \"just-in-time\" only when its <code>get()</code> method is explicitly awaited. This ensures that the application always uses the most up-to-date credential, supporting secret rotation.</p>"},{"location":"athomic/security/secrets/#nala.athomic.credentials.proxy.CredentialProxy.get","title":"<code>get()</code>  <code>async</code>","text":"<p>Fetches the secret's value just-in-time from the secrets provider.</p> <p>Returns:</p> Type Description <code>Optional[Any]</code> <p>A Resolved secret value, or None if not found.</p>"},{"location":"athomic/security/secrets/#nala.athomic.security.secrets.providers.vault_secrets_provider.VaultSecretsProvider","title":"<code>nala.athomic.security.secrets.providers.vault_secrets_provider.VaultSecretsProvider</code>","text":"<p>               Bases: <code>SecretsBase</code></p> <p>A secrets provider that fetches secrets from HashiCorp Vault.</p> <p>This provider implements the <code>SecretsProtocol</code> using the synchronous <code>hvac</code> client, wrapping all blocking operations in <code>asyncio.to_thread</code>. It handles: 1. Authentication: Supports AppRole and Token methods. 2. Token Renewal: Runs a background loop (<code>_run_loop</code>) to automatically    renew the token before expiration. 3. Lifecycle: Manages authentication (<code>_connect</code>) and token revocation (<code>_close</code>).</p> <p>Attributes:</p> Name Type Description <code>settings</code> <p>The Vault-specific configuration settings.</p> <code>client</code> <p>The underlying synchronous <code>hvac.Client</code> instance.</p>"},{"location":"athomic/security/secrets/#nala.athomic.security.secrets.providers.vault_secrets_provider.VaultSecretsProvider.__init__","title":"<code>__init__(settings)</code>","text":"<p>Initializes the VaultSecretsProvider.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>SecretsSettings</code> <p>The configuration for the secrets module.</p> required"},{"location":"guides/ai/ai_guide/","title":"AI &amp; Semantic Memory Usage Guide","text":""},{"location":"guides/ai/ai_guide/#overview","title":"Overview","text":"<p>This guide provides practical recipes for leveraging the <code>athomic.ai</code> module. It covers common scenarios such as generating text with LLMs, extracting structured data, building RAG (Retrieval-Augmented Generation) pipelines using Semantic Memory, and enforcing governance policies.</p>"},{"location":"guides/ai/ai_guide/#recipe-1-chat-text-generation","title":"Recipe 1: Chat &amp; Text Generation","text":"<p>This recipe demonstrates the standard way to invoke an LLM (like OpenAI GPT-4 or Google Gemini) for text generation.</p> <pre><code>from nala.athomic.ai.llm.factory import LLMFactory\nfrom nala.athomic.config import get_settings\n\nasync def generate_marketing_copy(product_name: str):\n    # 1. Load configuration\n    # Assumes 'default' is configured in settings.toml\n    settings = get_settings().ai.llm.connections[\"default\"]\n\n    # 2. Create the provider instance via Factory\n    # This automatically handles authentication and connection setup\n    llm = LLMFactory.create(settings)\n\n    # 3. Generate content\n    # The 'system_message' sets the behavior of the model\n    response = await llm.generate_content(\n        prompt=f\"Write a catchy tagline for {product_name}\",\n        system_message=\"You are a creative marketing expert.\",\n        temperature=0.9 # Higher creativity\n    )\n\n    return response\n</code></pre>"},{"location":"guides/ai/ai_guide/#key-concepts","title":"Key Concepts","text":"<ul> <li><code>LLMFactory</code>: Decouples your code from the specific provider class (OpenAI vs Vertex).</li> <li><code>generate_content</code>: The unified method for unstructured text, automatically traced and measured.</li> </ul>"},{"location":"guides/ai/ai_guide/#recipe-2-structured-data-extraction","title":"Recipe 2: Structured Data Extraction","text":"<p>Extracting reliable JSON from an LLM is a common requirement. Athomic standardizes this using Pydantic models.</p> <pre><code>from pydantic import BaseModel, Field\nfrom nala.athomic.ai.llm.factory import LLMFactory\nfrom nala.athomic.config import get_settings\n\n# 1. Define the desired output schema\nclass SupportTicketAnalysis(BaseModel):\n    sentiment: str = Field(..., description=\"Customer sentiment: positive, neutral, negative\")\n    urgency: int = Field(..., description=\"Urgency level from 1 to 5\")\n    category: str = Field(..., description=\"Category: billing, technical, feature_request\")\n\nasync def analyze_ticket(ticket_body: str):\n    settings = get_settings().ai.llm.connections[\"default\"]\n    llm = LLMFactory.create(settings)\n\n    # 2. Call generate_structured\n    # This guarantees the return type is SupportTicketAnalysis or raises StructuredOutputError\n    analysis = await llm.generate_structured(\n        prompt=f\"Analyze this ticket:n{ticket_body}\",\n        response_model=SupportTicketAnalysis,\n        max_retries=2\n    )\n\n    if analysis.urgency &gt;= 4:\n        print(f\"High urgency ticket detected! Category: {analysis.category}\")\n</code></pre>"},{"location":"guides/ai/ai_guide/#recipe-3-implementing-semantic-memory-rag","title":"Recipe 3: Implementing Semantic Memory (RAG)","text":"<p>The <code>SemanticMemoryService</code> orchestrates the entire RAG flow: embedding text, storing it in the vector DB, and retrieving relevant context.</p>"},{"location":"guides/ai/ai_guide/#storing-information-indexing","title":"Storing Information (Indexing)","text":"<pre><code>from nala.athomic.ai.memory.factory import SemanticMemoryFactory\n\nasync def learn_user_preferences(user_id: str, notes: list[str]):\n    # 1. Initialize the service\n    # 'user_context' is the logical collection name in the Vector DB\n    memory_service = SemanticMemoryFactory.create(collection_name=\"user_context\")\n\n    # 2. Add memories in batch for efficiency\n    # The service automatically handles embedding generation (Document Mode)\n    await memory_service.add_batch(\n        contents=notes,\n        metadatas=[{\"user_id\": user_id, \"source\": \"onboarding\"}] * len(notes)\n    )\n    print(\"Preferences stored.\")\n</code></pre>"},{"location":"guides/ai/ai_guide/#recalling-information-retrieval","title":"Recalling Information (Retrieval)","text":"<pre><code>async def chat_with_context(user_id: str, user_question: str):\n    memory_service = SemanticMemoryFactory.create(collection_name=\"user_context\")\n    llm = LLMFactory.create(get_settings().ai.llm.connections[\"default\"])\n\n    # 1. Recall relevant memories\n    # The service automatically embeds the query (Query Mode) and searches the DB\n    results = await memory_service.recall(\n        query=user_question,\n        limit=3,\n        filters={\"user_id\": user_id} # Tenancy isolation\n    )\n\n    # 2. Construct the context block\n    context_str = \"n\".join([f\"- {res.memory.content}\" for res in results])\n\n    # 3. Generate answer using context\n    prompt = f\"\"\"\n    Context information is below:\n    {context_str}\n    ---------------------\n    Given the context, answer the question: {user_question}\n    \"\"\"\n\n    return await llm.generate_content(prompt)\n</code></pre>"},{"location":"guides/ai/ai_guide/#recipe-4-enforcing-ai-governance-rate-limits","title":"Recipe 4: Enforcing AI Governance (Rate Limits)","text":"<p>To prevent cost overruns or abuse, you can attach governance guards to your LLM provider.</p> <pre><code>from nala.athomic.ai.llm.factory import LLMFactory\nfrom nala.athomic.ai.governance.guards import RateLimitGuard\nfrom nala.athomic.config import get_settings\n\nasync def protected_generation(user_id: str, prompt: str):\n    settings = get_settings().ai.llm.connections[\"default\"]\n\n    # 1. Define the guard\n    # 'ai_standard_user' must be a policy defined in your RateLimiter configuration\n    rate_limiter = RateLimitGuard(policy_name=\"ai_standard_user\")\n\n    # 2. Initialize LLM with the guard\n    llm = LLMFactory.create(settings)\n    llm.guards.append(rate_limiter)\n\n    try:\n        # 3. The check is executed automatically before generation\n        # Pass 'user_id' in kwargs so the guard knows the scope\n        return await llm.generate_content(prompt, user_id=user_id)\n\n    except Exception as e:\n        # Captures QuotaExceededError if limit is hit\n        print(f\"Request blocked by governance: {e}\")\n</code></pre>"},{"location":"guides/ai/ai_guide/#best-practices","title":"Best Practices","text":""},{"location":"guides/ai/ai_guide/#1-asymmetric-embeddings","title":"1. Asymmetric Embeddings","text":"<p>When working with embeddings directly (via <code>EmbeddingFactory</code>), always distinguish between documents and queries. * Use <code>embed_documents()</code> for text you are storing. * Use <code>embed_query()</code> for the search term. * This distinction is critical for providers like Vertex AI (<code>text-embedding-gecko</code>) to produce high-quality vectors.</p>"},{"location":"guides/ai/ai_guide/#2-error-handling","title":"2. Error Handling","text":"<p>Always wrap AI calls in try/except blocks to handle standard Athomic exceptions: * <code>ContextWindowExceededError</code>: Use a fallback strategy (e.g., summarize context). * <code>StructuredOutputError</code>: The model failed to produce JSON. You might want to retry or return a default value.</p>"},{"location":"guides/ai/ai_guide/#3-tracing","title":"3. Tracing","text":"<p>The AI module is fully instrumented with OpenTelemetry. Use the <code>trace_id</code> from your application logs to visualize the full chain: <code>Request -&gt; Controller -&gt; Memory Recall -&gt; Vector Search -&gt; LLM Generation</code>.</p>"},{"location":"guides/database/graph/","title":"Graph Database (Neo4j)","text":"<p>The <code>athomic.database.graph</code> module provides a high-performance, asynchronous interface for interacting with Graph Databases. Currently, it ships with a robust Neo4j implementation.</p> <p>It is designed to support complex relationship mapping, such as Data Lineage, Impact Analysis, and Knowledge Graphs.</p>"},{"location":"guides/database/graph/#features","title":"Features","text":"<ul> <li>Async-First: Built on top of the official <code>neo4j</code> asynchronous driver.</li> <li>Observability: Automatic OpenTelemetry tracing (spans) and Prometheus metrics for every query executed.</li> <li>Connection Groups: Supports multiple named connections (e.g., <code>default</code>, <code>analytics</code>) via <code>ConnectionManager</code>.</li> <li>Resilience: Integrated connection pooling and automatic retry mechanisms managed by the driver.</li> </ul>"},{"location":"guides/database/graph/#configuration","title":"Configuration","text":"<p>Configure the graph connection in your <code>settings.toml</code>. The module uses the standard <code>ConnectionGroup</code> pattern found in other database modules.</p> <pre><code>[default.database.graph]\n# The default connection to use when none is specified\ndefault_connection_name = \"default_graph\"\n\n[default.database.graph.connections.default_graph]\nenabled = true\nbackend = \"neo4j\"\n\n[default.database.graph.connections.default_graph.provider]\nbackend = \"neo4j\"\n# Connection URI (supports bolt://, neo4j://, neo4j+s://)\nuri = \"bolt://localhost:7687\"\nuser = \"neo4j\"\npassword = \"password\" # pragma: allowlist secret\nmax_connection_pool_size = 50\nmax_connection_lifetime = 3600\n</code></pre>"},{"location":"guides/database/graph/#usage","title":"Usage","text":""},{"location":"guides/database/graph/#dependency-injection-recommended","title":"Dependency Injection (Recommended)","text":"<p>The best way to interact with the Graph DB is by injecting the <code>GraphDatabaseProtocol</code> via the <code>ConnectionManager</code> or the specific Factory.</p> <pre><code>from nala.athomic.database.graph.protocol import GraphDatabaseProtocol\nfrom nala.athomic.database.factory import connection_manager_factory\n\nasync def get_service_dependencies():\n    # 1. Get the manager\n    connection_manager = connection_manager_factory.create()\n\n    # 2. Get the client (uses 'default_connection_name' if name is omitted)\n    graph_client: GraphDatabaseProtocol = connection_manager.get_graph_db()\n\n    # 3. Execute a Cypher query\n    query = \"\"\"\n    MATCH (s:Service)-[:DEPENDS_ON]-&gt;(d:Database)\n    WHERE s.name = $service_name\n    RETURN d.name as database, d.type as type\n    \"\"\"\n\n    results = await graph_client.execute_query(\n        query, \n        parameters={\"service_name\": \"payment-service\"}\n    )\n\n    for record in results:\n        print(f\"Database: {record['database']}, Type: {record['type']}\")\n</code></pre>"},{"location":"guides/database/graph/#multiple-connections","title":"Multiple Connections","text":"<p>If you have configured multiple connections (e.g., one for Lineage, one for Fraud Detection), you can request them by name:</p> <pre><code># Accessing the 'fraud_graph' connection configured in settings.toml\nfraud_graph = connection_manager.get_graph_db(\"fraud_graph\")\n</code></pre>"},{"location":"guides/database/graph/#architecture-patterns","title":"Architecture &amp; Patterns","text":""},{"location":"guides/database/graph/#the-protocol","title":"The Protocol","text":"<p>The module exposes <code>GraphDatabaseProtocol</code>. Your domain logic should depend on this protocol, not on <code>Neo4jProvider</code> directly. This ensures strict Dependency Inversion (DIP).</p>"},{"location":"guides/database/graph/#observability","title":"Observability","text":"<p>You do not need to add manual logging or timing code. The <code>BaseGraphStore</code> wrapper automatically:</p> <ol> <li>Starts a Trace Span named <code>graph_db.execute_query</code>.</li> <li>Records Prometheus Metrics:<ul> <li><code>nala_graph_db_operations_total{status=\"success|failure\"}</code></li> <li><code>nala_graph_db_operation_duration_seconds</code></li> </ul> </li> </ol>"},{"location":"guides/database/graph/#error-handling","title":"Error Handling","text":"<p>Driver-specific exceptions (like <code>neo4j.exceptions.Neo4jError</code>) are logged and propagated. In infrastructure layers, we wrap critical connection errors in <code>ServiceConnectionError</code> to standardize health checks.</p>"},{"location":"guides/database/guide/","title":"Vector Store Usage Guide","text":""},{"location":"guides/database/guide/#overview","title":"Overview","text":"<p>This guide provides practical recipes and patterns for using the Vector Store module in real-world scenarios, such as implementing Retrieval-Augmented Generation (RAG), semantic search with filtering, and multi-tenant data isolation.</p>"},{"location":"guides/database/guide/#recipe-1-indexing-documents-for-rag","title":"Recipe 1: Indexing Documents for RAG","text":"<p>This recipe demonstrates how to chunk a text document, generate embeddings using an embedding model, and store them with metadata for later retrieval.</p> <p>```python import uuid from typing import List</p> <p>from nala.athomic.ai.embeddings import EmbeddingModelProtocol from nala.athomic.database.factory import connection_manager_factory from nala.athomic.database.vector.types import VectorRecord</p> <p>async def index_content(     content: str,      metadata: dict,      embedding_model: EmbeddingModelProtocol ):     # 1. Get the Vector Store client     store = connection_manager_factory.create().get_vector_store()</p> <pre><code># 2. Chunk the content (Simple overlap strategy)\nchunk_size = 1000\noverlap = 100\nchunks = [\n    content[i : i + chunk_size] \n    for i in range(0, len(content), chunk_size - overlap)\n]\n\n# 3. Generate Embeddings in Batch\n# It is much more efficient to embed multiple texts at once\nvectors = await embedding_model.embed_documents(chunks)\n\n# 4. Create Vector Records\nrecords = []\nfor chunk, vector in zip(chunks, vectors):\n    record = VectorRecord(\n        id=str(uuid.uuid4()),\n        vector=vector,\n        payload={\n            \"content\": chunk,\n            \"source\": metadata.get(\"source\"),\n            \"author\": metadata.get(\"author\"),\n            \"created_at\": metadata.get(\"created_at\")\n        }\n    )\n    records.append(record)\n\n# 5. Upsert to the 'knowledge_base' collection\n# Ensure the collection exists first (idempotent operation)\nawait store.create_collection(\"knowledge_base\", dimension=1536)\n\nawait store.upsert(\"knowledge_base\", records)\nprint(f\"Indexed {len(records)} chunks.\")\n</code></pre> <p>```</p>"},{"location":"guides/database/guide/#recipe-2-semantic-search-with-metadata-filtering","title":"Recipe 2: Semantic Search with Metadata Filtering","text":"<p>This recipe shows how to perform a semantic search while restricting results to specific criteria (e.g., only documents from a specific author).</p> <p>```python async def search_knowledge_base(query: str, author_filter: str = None):     store = connection_manager_factory.create().get_vector_store()     embedding_model = ... # Get your embedding model</p> <pre><code># 1. Convert the user query into a vector\nquery_vector = await embedding_model.embed_query(query)\n\n# 2. Build filters (if applicable)\n# The dictionary format allows providers to translate to their native query language\nfilters = {}\nif author_filter:\n    filters[\"author\"] = author_filter\n\n# 3. Execute Search\nresults = await store.search(\n    collection=\"knowledge_base\",\n    vector=query_vector,\n    limit=5,\n    score_threshold=0.8, # Only relevant matches\n    filter=filters\n)\n\n# 4. Process Results\nfor hit in results:\n    print(f\"Score: {hit.score:.4f} | Content: {hit.payload['content'][:100]}...\")\n</code></pre> <p>```</p>"},{"location":"guides/database/guide/#recipe-3-implementing-multi-tenancy","title":"Recipe 3: Implementing Multi-Tenancy","text":"<p>One of the strongest features of the Athomic Vector Store is its seamless support for multi-tenancy via the <code>ContextAwareVectorStore</code> wrapper.</p>"},{"location":"guides/database/guide/#automatic-injection","title":"Automatic Injection","text":"<p>If you have the <code>context_aware</code> wrapper enabled in your configuration, you do not need to pass the tenant ID manually. The system extracts it from the current execution context.</p> <p>```python</p>"},{"location":"guides/database/guide/#in-a-fastapi-request-handler-or-task","title":"In a FastAPI request handler or Task","text":"<p>from nala.athomic.context import context_vars</p> <p>async def handle_request(user_query: str):     # Assume context middleware has already set this     # context_vars.set_tenant_id(\"tenant-123\")</p> <pre><code>store = connection_manager_factory.create().get_vector_store()\n\n# This search is AUTOMATICALLY scoped to \"tenant-123\".\n# Data from other tenants will strictly NOT be visible.\nresults = await store.search(\"documents\", vector=...)\n</code></pre> <p>```</p>"},{"location":"guides/database/guide/#manual-override-admin-mode","title":"Manual Override (Admin Mode)","text":"<p>If you are writing an admin tool or a background job that needs to access a specific tenant's data explicitly, you can bypass the context:</p> <p>```python async def admin_audit_tenant(target_tenant_id: str):     store = connection_manager_factory.create().get_vector_store()</p> <pre><code># Explicitly passing 'tenant' overrides the context variable\nresults = await store.search(\n    \"documents\", \n    vector=..., \n    tenant=target_tenant_id\n)\n</code></pre> <p>```</p>"},{"location":"guides/database/guide/#best-practices","title":"Best Practices","text":""},{"location":"guides/database/guide/#1-collection-management","title":"1. Collection Management","text":"<ul> <li>Dimension Matching: Always ensure the <code>dimension</code> passed to <code>create_collection</code> matches exactly the output dimension of your embedding model (e.g., OpenAI <code>text-embedding-3-small</code> is 1536).</li> <li>Naming Convention: Use snake_case for collection names. Avoid using tenant IDs in collection names; use the <code>tenant</code> partitioning feature instead for better scalability.</li> </ul>"},{"location":"guides/database/guide/#2-error-handling","title":"2. Error Handling","text":"<p>The Vector Store methods can raise specific exceptions that you should handle: * <code>CollectionNotFoundError</code>: If you try to search in a collection that hasn't been created. * <code>ServiceConnectionError</code>: If the vector database is down.</p> <p>```python from nala.athomic.services.exceptions import ServiceConnectionError</p> <p>try:     await store.search(...) except ServiceConnectionError:     # Implement fallback logic (e.g., basic keyword search in DB)     return await keyword_search(...) ```</p>"},{"location":"guides/database/guide/#3-batching-upserts","title":"3. Batching Upserts","text":"<p>Avoid calling <code>upsert</code> in a loop for single records. Always batch your records into lists (e.g., 100-500 records per call) to reduce network overhead and improve throughput.</p>"},{"location":"guides/workflows/getting-started/","title":"Getting Started with Durable Workflows","text":"<p>The <code>nala.athomic.workflows</code> module provides an agnostic way to orchestrate long-running business processes. It allows you to write workflow logic once and execute it either on a production-grade engine (Temporal.io) or in-memory for lightning-fast tests (Local).</p>"},{"location":"guides/workflows/getting-started/#1-defining-a-workflow","title":"1. Defining a Workflow","text":"<p>Workflows are defined using the standard Temporal Python SDK decorators. This ensures compatibility with the production engine.</p> <pre><code>from datetime import timedelta\nfrom temporalio import activity, workflow\n\n# 1. Define Activities (The \"Side Effects\")\n# Activities are where you perform I/O (DB calls, HTTP requests).\n@activity.defn\nasync def charge_user(user_id: str, amount: float) -&gt; str:\n    # Logic to call payment gateway...\n    return \"transaction_id_123\"\n\n@activity.defn\nasync def send_receipt(email: str) -&gt; None:\n    # Logic to send email...\n    pass\n\n# 2. Define the Workflow (The \"Orchestrator\")\n# Workflows must be deterministic (no random, no system time, no direct I/O).\n@workflow.defn(name=\"PaymentProcessing\")\nclass PaymentWorkflow:\n    @workflow.run\n    async def run(self, user_id: str, email: str, amount: float) -&gt; dict:\n        workflow.logger.info(f\"Starting payment for {user_id}\")\n\n        # Execute Activity 1: Charge User\n        # Automatic retries are handled by the engine here.\n        tx_id = await workflow.execute_activity(\n            charge_user,\n            args=[user_id, amount],\n            start_to_close_timeout=timedelta(seconds=10),\n        )\n\n        # Execute Activity 2: Send Receipt\n        await workflow.execute_activity(\n            send_receipt,\n            email,\n            start_to_close_timeout=timedelta(seconds=5),\n        )\n\n        return {\"status\": \"COMPLETED\", \"transaction_id\": tx_id}\n</code></pre>"},{"location":"guides/workflows/getting-started/#2-registering-for-local-execution","title":"2. Registering for Local Execution","text":"<p>When running in Production (Temporal), the Worker automatically scans and registers workflows. However, when running in Local/Test Mode (In-Memory), the <code>LocalWorkflowClient</code> does not scan code. You must explicitly register your workflow function in the in-memory registry.</p> <p>This is typically done in your <code>conftest.py</code> or application startup logic for dev environments.</p> <pre><code>from nala.athomic.workflows.providers.local.registry import local_workflow_registry\nfrom my_app.workflows.payment import PaymentWorkflow\n\n# Register the workflow name pointing to the class method\n# This allows client.start_workflow(\"PaymentProcessing\", ...) to work locally.\nlocal_workflow_registry.register(\"PaymentProcessing\", PaymentWorkflow().run)\n</code></pre>"},{"location":"guides/workflows/getting-started/#3-executing-a-workflow","title":"3. Executing a Workflow","text":"<p>To start a workflow, use the agnostic <code>WorkflowClientFactory</code>. This keeps your business logic decoupled from the specific backend (Temporal vs. Local) configured in your <code>settings.toml</code>.</p> <pre><code>import asyncio\nfrom nala.athomic.workflows.factory import WorkflowClientFactory\nfrom nala.athomic.workflows.models import WorkflowOptions\n\nasync def trigger_process():\n    # 1. Create the client (Backend is determined by settings.WORKFLOWS.PROVIDER.BACKEND)\n    client = WorkflowClientFactory.create()\n\n    # 2. Configure Execution Options\n    # Note: 'execution_timeout_seconds' accepts strings like \"1h\", \"30m\"\n    options = WorkflowOptions(\n        workflow_id=f\"pay-req-{uuid.uuid4()}\",\n        task_queue=\"payments-queue\",\n        execution_timeout_seconds=\"1h\" \n    )\n\n    # 3. Start Execution\n    # The payload (arguments) is automatically serialized, compressed, \n    # and encrypted by the Nala Payload Pipeline before leaving the app.\n    execution = await client.start_workflow(\n        \"PaymentProcessing\", # Matches the name in @workflow.defn\n        \"user_123\",          # Arg 1\n        \"user@example.com\",  # Arg 2\n        99.90,               # Arg 3\n        options=options\n    )\n\n    print(f\"Workflow started with RunID: {execution.run_id}\")\n\n    # 4. (Optional) Wait for result\n    result = await client.get_result(execution.workflow_id, run_id=execution.run_id)\n    print(f\"Result: {result.result}\")\n</code></pre>"}]}