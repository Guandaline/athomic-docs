{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Athomic Docs","text":"<p>Athomic Docs is an opinionated, production-ready chassis for building resilient, observable, and scalable microservices in Python. It provides a robust foundation\u2014the Athomic Layer\u2014that handles cross-cutting concerns, allowing developers to focus purely on business logic.</p> <p>Our philosophy is grounded in battle-tested software engineering principles like SOLID, Single Responsibility (SRP), and Dependency Injection (DI), ensuring that applications built on this chassis are maintainable, scalable, and easy to test.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Follow these steps to get a local development environment up and running.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker &amp; Docker Compose</li> <li>Python 3.11+</li> <li>Poetry</li> </ul>"},{"location":"#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone [https://github.com/guandaline/athomic-docs.git](https://github.com/guandaline/athomic-docs.git)\ncd athomic-docs\n</code></pre>"},{"location":"#2-start-infrastructure-services","title":"2. Start Infrastructure Services","text":"<p>This command will start all necessary services defined in <code>docker-compose.yml</code>, such as MongoDB, Redis, and Vault.</p> <pre><code>docker-compose up -d\n</code></pre>"},{"location":"#3-install-dependencies","title":"3. Install Dependencies","text":"<p>Install the project's Python dependencies using Poetry.</p> <pre><code>poetry install\n</code></pre>"},{"location":"#4-run-the-application","title":"4. Run the Application","text":"<p>Execute the application using <code>uvicorn</code>. The server will start on <code>http://127.0.0.1:8000</code>.</p> <pre><code>poetry run uvicorn nala.api.main:app --reload\n</code></pre> <p>You can now access the API documentation at <code>http://127.0.0.1:8000/docs</code>.</p>"},{"location":"#explore-the-documentation","title":"Explore the Documentation","text":"<ul> <li>Architecture Overview: Dive deep into the layered architecture and design principles.</li> <li>Dependency Injection: Understand how services are managed.</li> </ul>"},{"location":"#athomic-layer-modules","title":"Athomic Layer Modules","text":""},{"location":"#core-lifecycle","title":"Core &amp; Lifecycle","text":"<ul> <li>Services: The base service lifecycle.</li> <li>Lifecycle Management: How services are started and stopped in order.</li> <li>Plugins: The extensible plugin system.</li> </ul>"},{"location":"#configuration-context","title":"Configuration &amp; Context","text":"<ul> <li>Configuration Management: The <code>Dynaconf</code> and <code>Pydantic</code> based configuration system.</li> <li>Context Management: Handling request-scoped context for tracing and multi-tenancy.</li> </ul>"},{"location":"#data-persistence","title":"Data &amp; Persistence","text":"<ul> <li>Connection Management: The central manager for all data store connections.</li> <li>Document Stores: Abstraction for document databases like MongoDB.</li> <li>Key-Value Stores: Abstraction for KV stores like Redis, with a powerful wrapper system.</li> <li>Transactional Outbox: For guaranteeing at-least-once event delivery.</li> <li>Database Migrations: Version-controlled schema management.</li> <li>File Storage: Abstraction for object storage like GCS or local files.</li> </ul>"},{"location":"#security","title":"Security","text":"<ul> <li>Secrets Management: Securely resolve secrets at runtime from backends like Vault.</li> <li>Authentication &amp; Authorization: Policy-based security for endpoints using JWT or API Keys.</li> <li>Cryptography: High-level abstraction for symmetric encryption.</li> </ul>"},{"location":"#observability","title":"Observability","text":"<ul> <li>Structured Logging: <code>Loguru</code>-based logging with automatic sensitive data masking.</li> <li>Distributed Tracing: End-to-end tracing with OpenTelemetry.</li> <li>Metrics: Out-of-the-box instrumentation with Prometheus.</li> <li>Health &amp; Readiness: Extensible readiness probes for Kubernetes.</li> </ul>"},{"location":"#cross-cutting-concerns","title":"Cross-Cutting Concerns","text":"<ul> <li>Serializer: Pluggable serializers (JSON, Protobuf) for data conversion.</li> <li>Payload Processing Pipeline: A \"Pipes and Filters\" system for composing transformations like encryption and compression.</li> <li>Notifications: Resiliently send emails via SMTP or other providers.</li> <li>Internal Event Bus: In-process Pub/Sub for decoupling internal components.</li> <li>Resilient HTTP Client: A factory for creating pre-configured, resilient clients.</li> </ul>"},{"location":"#resilience-patterns","title":"Resilience Patterns","text":"<ul> <li>Retry</li> <li>Circuit Breaker</li> <li>Rate Limiter</li> <li>Timeout &amp; Cancellation</li> <li>Bulkhead</li> <li>Fallback</li> <li>Distributed Locking</li> <li>Idempotency</li> <li>Distributed Leasing</li> <li>Workload Sharding</li> <li>Backpressure</li> <li>Exponential Backoff</li> <li>Sagas</li> <li>Adaptive Throttling</li> </ul>"},{"location":"README-onboarding/","title":"\ud83d\ude80 Guia de Onboarding \u2014 athomic-docs","text":"<p>Este guia ajuda novos desenvolvedores a configurar rapidamente o ambiente de desenvolvimento para o projeto <code>athomic-docs</code>.</p>"},{"location":"README-onboarding/#setup-local","title":"\u2699\ufe0f Setup Local","text":"<ol> <li>D\u00ea permiss\u00e3o de execu\u00e7\u00e3o ao script de setup:</li> </ol> <pre><code>chmod +x bin/setup.sh\n</code></pre> <ol> <li>Execute o script com Make:</li> </ol> <pre><code>make setup\n</code></pre> <p>Esse comando ir\u00e1: - Criar um ambiente virtual <code>.venv</code> local com Poetry - Ativar o ambiente virtual (via <code>source</code>) - Instalar depend\u00eancias do projeto - Criar o <code>.env</code> a partir de <code>.env.example</code> (se necess\u00e1rio) - Carregar a vari\u00e1vel <code>GH_TOKEN</code> ou <code>CI_JOB_TOKEN</code> - Iniciar a aplica\u00e7\u00e3o com <code>uvicorn</code></p> <p>\u2139\ufe0f Se preferir rodar manualmente: <code>bash source bin/setup.sh</code></p>"},{"location":"README-onboarding/#versionamento-com-semantic-release","title":"\ud83d\udce6 Versionamento com Semantic Release","text":"<p>Este projeto utiliza <code>python-semantic-release</code> para controle automatizado de vers\u00e3o.</p>"},{"location":"README-onboarding/#rodar-release-manualmente","title":"Rodar release manualmente:","text":"<pre><code>make release\n</code></pre> <p>Ou:</p> <pre><code>poetry run dotenv run -- poetry run semantic-release version\n</code></pre>"},{"location":"README-onboarding/#configurando-o-gh_token-no-github","title":"\ud83d\udd10 Configurando o GH_TOKEN no GitHub","text":"<ol> <li>V\u00e1 para: <code>Settings &gt; Secrets and variables &gt; Actions</code></li> <li>Clique em New repository secret</li> <li>Nome: <code>GH_TOKEN</code></li> <li>Valor: seu token pessoal com escopos <code>repo</code>, <code>workflow</code>, <code>write:packages</code></li> </ol>"},{"location":"README-onboarding/#opcional-configurar-gh_token-via-github-cli","title":"\ud83d\udcbb (Opcional) Configurar GH_TOKEN via GitHub CLI","text":"<pre><code>gh auth login\ngh secret set GH_TOKEN -b\"seu_token_aqui\"\n</code></pre>"},{"location":"README-onboarding/#integracao-com-gitlab-cicd-preparado-para-o-futuro","title":"\ud83d\udee0\ufe0f Integra\u00e7\u00e3o com GitLab CI/CD (preparado para o futuro)","text":"<p>No GitLab CI, o token <code>CI_JOB_TOKEN</code> ser\u00e1 usado automaticamente como <code>GH_TOKEN</code>.</p>"},{"location":"README-onboarding/#comandos-uteis-com-make","title":"\ud83e\uddf0 Comandos \u00fateis com Make","text":""},{"location":"README-onboarding/#setup-local_1","title":"\ud83d\udd27 Setup local","text":"<pre><code>make setup\n</code></pre>"},{"location":"api-reference/","title":"File: docs/api_reference.md","text":""},{"location":"api-reference/#api-reference","title":"API Reference","text":"<p>Aqui est\u00e1 a documenta\u00e7\u00e3o gerada automaticamente do nosso c\u00f3digo.</p>"},{"location":"api-reference/#modulo-principal","title":"M\u00f3dulo Principal","text":""},{"location":"api-reference/#nala.athomic","title":"<code>nala.athomic</code>","text":""},{"location":"api-reference/#nala.athomic.Athomic","title":"<code>Athomic</code>","text":"<p>A Fa\u00e7ade that provides a single, simple entry point to all of Athomic's services.</p> <p>This class acts as the main interaction point for an application utilizing the Athomic framework. It manages the initialization, dependency injection, and lifecycle of all infrastructure components, such as secrets management, plugins, and background services.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <code>AppSettings</code> <p>The validated application settings.</p> <code>secrets_manager</code> <code>SecretsManager</code> <p>The manager responsible for resolving secrets.</p> <code>lifecycle_manager</code> <code>LifecycleManager</code> <p>The orchestrator for the service lifecycle.</p> <code>plugin_manager</code> <code>PluginManager</code> <p>The manager for the plugin system.</p> <code>tracer</code> <code>Tracer</code> <p>The OpenTelemetry Tracer instance for this component.</p> <code>observability</code> <p>A namespace for observability components like loggers.</p>"},{"location":"api-reference/#nala.athomic.Athomic.__init__","title":"<code>__init__(domain_initializers_registrar=None, settings=None)</code>","text":"<p>Initializes the Athomic layer.</p> <p>Parameters:</p> Name Type Description Default <code>domain_initializers_registrar</code> <code>Optional[Callable[[], None]]</code> <p>A function from the application layer (e.g., API) that registers all business domain-specific initializers.</p> <code>None</code> <code>settings</code> <code>Optional[AppSettings]</code> <p>An instance of application settings. If not provided, it will be loaded globally. Ideal for dependency injection in tests.</p> <code>None</code>"},{"location":"api-reference/#nala.athomic.Athomic.shutdown","title":"<code>shutdown()</code>  <code>async</code>","text":"<p>Runs the graceful shutdown sequence for all services.</p>"},{"location":"api-reference/#nala.athomic.Athomic.startup","title":"<code>startup()</code>  <code>async</code>","text":"<p>Runs the complete, ordered startup sequence for the application's infrastructure.</p> <p>This method orchestrates the startup of services in the correct dependency order: 1. Resolves all secret references within the configuration. 2. Discovers and loads all available plugins. 3. Calls the 'on_athomic_startup' hook, allowing plugins to initialize. 4. Starts all registered services (e.g., database, messaging).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If any critical step in the startup sequence fails.</p>"},{"location":"api-reference/#nala.athomic.get_settings","title":"<code>get_settings()</code>  <code>cached</code>","text":"<p>Loads, validates, and returns the application settings as a singleton.</p> <p>This is the primary entry point for accessing application configuration. It uses <code>@lru_cache</code> to ensure that the potentially expensive process of loading and validating settings from files and environment variables happens only once.</p> <p>Returns:</p> Name Type Description <code>AppSettings</code> <code>AppSettings</code> <p>A validated Pydantic model containing the entire</p> <code>AppSettings</code> <p>application configuration.</p>"},{"location":"api-reference/#classe-main-api","title":"Classe Main API","text":""},{"location":"api-reference/#nala.api.main","title":"<code>nala.api.main</code>","text":""},{"location":"adr/ADR-000-sample-doc/","title":"ADR-XXX: [Short Decision Title]","text":"<ul> <li>Status: [Proposed | Accepted | Deprecated | Superseded by ADR-YYY]</li> <li>Date: [YYYY-MM-DD]</li> </ul>"},{"location":"adr/ADR-000-sample-doc/#context","title":"Context","text":"<p>[Describe here the problem or architectural force being addressed. What was the situation? What alternatives were considered? What were the constraints (technical, time, business)?]</p>"},{"location":"adr/ADR-000-sample-doc/#decision","title":"Decision","text":"<p>[Describe the chosen solution clearly and concisely. What decision was made? How will it be implemented?]</p>"},{"location":"adr/ADR-000-sample-doc/#consequences","title":"Consequences","text":"<p>[List the expected or observed results of making this decision. It is important to be honest about the pros and cons.]</p> <ul> <li>Positive:<ul> <li>[Benefit 1]</li> <li>[Benefit 2]</li> </ul> </li> <li>Negative:<ul> <li>[Disadvantage or trade-off 1]</li> <li>[Risk or limitation 1]</li> </ul> </li> <li>Neutral/Other:<ul> <li>[Implication or future consideration]</li> </ul> </li> </ul>"},{"location":"adr/ADR-001-dynaconf-with-pydantic/","title":"ADR-001: Use of Dynaconf + Pydantic for Configuration Management","text":"<ul> <li>Status: Accepted (Implemented)</li> <li>Date: 2025-04-13 (Note: Ideally, use the date the decision was made)</li> </ul>"},{"location":"adr/ADR-001-dynaconf-with-pydantic/#context","title":"Context","text":"<ul> <li>Problem: Modern applications need to manage complex configurations (database, cache, external services, feature flags) that vary across environments (development, testing, production). Using only environment variables or simple files (<code>.ini</code>, <code>.json</code>) becomes difficult to manage, validate, and ensure correct typing.</li> <li>Needs:<ul> <li>Load configurations from multiple sources (files like <code>.toml</code>, <code>.env</code>, environment variables, and potentially Vault in the future [source: 9]).</li> <li>Separate configurations by environment (<code>default</code>, <code>development</code>, <code>production</code>). [source: 18, 20, 21, 22]</li> <li>Rigorously validate configuration values and structure at startup to prevent runtime errors. [source: 235]</li> <li>Provide type-safe access to configurations within the application code.</li> </ul> </li> <li>Alternatives Considered:<ul> <li>Pure Environment Variables: Simple, but impractical for nested structures and lacks intrinsic validation/typing.</li> <li>Pydantic <code>BaseSettings</code>: Excellent for validation and typing, but more limited in loading sources (focuses on env vars and <code>.env</code>).</li> <li><code>.ini</code>/<code>.json</code> files + <code>ConfigParser</code>/<code>json</code>: Less expressive than TOML for complex structures, lack strong integrated validation.</li> <li>Python Modules (<code>config.py</code>): Difficult to override externally (via env vars) and can violate the separation of code and configuration.</li> </ul> </li> </ul>"},{"location":"adr/ADR-001-dynaconf-with-pydantic/#decision","title":"Decision","text":"<ul> <li>Adopt the <code>Dynaconf</code> library as the primary configuration loader, leveraging its ability to read and merge multiple sources and environments. [source: 4, 775-792]</li> <li>Utilize <code>.toml</code> files to define default configurations and environment-specific overrides (<code>settings/settings.toml</code>, <code>settings/development.toml</code>, <code>settings/production.toml</code>). [source: 18, 20-22]</li> <li>Allow the use of <code>.env</code> files for local overrides or secrets during development. [source: 17, 784]</li> <li>Allow environment variables (prefixed with <code>NALA_</code> [source: 775]) to override any values defined in the files.</li> <li>Define the entire expected configuration structure using <code>Pydantic</code> models (starting with <code>AppSettings</code> [source: 817] and including nested models like <code>CacheSettings</code> [source: 845], <code>SecretsSettings</code> [source: 820], <code>DatabaseSettings</code> [source: 846], etc.). [source: 767]</li> <li>Implement a centralized access point (<code>nala.athomic.config.settings.get_settings</code>) [source: 768] that:<ol> <li>Uses <code>DynaconfLoader</code> [source: 769] to load raw configuration data into a dictionary.</li> <li>Validates this dictionary against the <code>AppSettings</code> Pydantic model. [source: 771-774]</li> <li>Returns the validated, type-safe instance of <code>AppSettings</code>.</li> <li>Utilizes <code>@lru_cache</code> [source: 712, 768] to ensure loading and validation occur only once.</li> </ol> </li> </ul>"},{"location":"adr/ADR-001-dynaconf-with-pydantic/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Flexibility: Configuration loading from diverse sources with clear precedence (env var &gt; .env &gt; environment file &gt; default file).</li> <li>Early Validation: Configuration errors (wrong types, missing values) are caught at application startup, not at runtime.</li> <li>Type Safety: Accessing configurations in code (e.g., <code>settings.cache.ttl</code>) is type-safe, aided by type hints and IDE auto-completion.</li> <li>Clear Structure: The configuration structure is explicitly defined and documented by the Pydantic models.</li> <li>Maintainability: Facilitates refactoring and understanding of available configurations.</li> <li>Extensibility: Prepared for adding new sources (like Vault) in the future, integrating them with Dynaconf or as additional providers validated by Pydantic.</li> </ul> </li> <li>Negative:<ul> <li>Dependencies: Adds two significant dependencies (<code>Dynaconf</code>, <code>Pydantic</code>) to the project.</li> <li>Learning Curve: Requires familiarity with Dynaconf's loading rules and Pydantic model definition/validation.</li> <li>Dual Definition: It's necessary to maintain the structure in both the <code>.toml</code> files and the Pydantic models (although aliases help [source: 818, 824]).</li> </ul> </li> <li>Neutral/Other:<ul> <li>Configuration is loaded and validated once at the start (due to <code>@lru_cache</code>), making subsequent access very fast but requiring an application restart to reflect changes (unless a live-reload mechanism is implemented).</li> </ul> </li> </ul>"},{"location":"adr/ADR-002-abstract-layer/","title":"ADR-002: Creation of the <code>athomic</code> Abstraction Layer","text":"<ul> <li>Status: Accepted (Implemented)</li> <li>Date: 2025-04-13 (Note: Ideally, use the date the decision was made)</li> </ul>"},{"location":"adr/ADR-002-abstract-layer/#context","title":"Context","text":"<ul> <li>Problem: Modern APIs rely heavily on cross-cutting infrastructure concerns like caching, configuration management, secrets handling, rate limiting, logging, observability, and resilience patterns (retry, fallback). Mixing this logic directly with the API endpoint handlers (e.g., in FastAPI routes) or business logic leads to high coupling, code duplication, difficulty in testing, and hinders reusability across different projects or API frameworks.</li> <li>Goal: Create a reusable, framework-agnostic core library (<code>athomic-docs</code>) that encapsulates these infrastructure best practices, allowing application developers to focus on business logic. [source: 7, 15, 16]</li> <li>Alternatives Considered:<ul> <li>No Abstraction: Implement infrastructure logic directly within the API framework (e.g., using FastAPI dependencies and middleware). Drawback: High coupling to the framework, poor reusability, harder testing.</li> <li>Utility Functions: Place infrastructure logic in simple utility functions. Drawback: Less organized, harder to enforce consistency, doesn't easily support stateful components or different providers.</li> <li>Multiple Smaller Libraries: Create separate libraries for cache, secrets, etc. Drawback: Increases dependency management overhead, harder to ensure cohesive integration and consistent patterns across libraries.</li> </ul> </li> </ul>"},{"location":"adr/ADR-002-abstract-layer/#decision","title":"Decision","text":"<ul> <li>Create a distinct top-level Python package named <code>nala.athomic</code> within the <code>src/nala/</code> directory. [source: 8]</li> <li>This <code>athomic</code> layer will house all core, reusable infrastructure components, designed to be as independent as possible from specific web frameworks (like FastAPI).</li> <li>Sub-modules within <code>athomic</code> will be organized by concern: <code>cache</code>, <code>config</code>, <code>db</code>, <code>log</code> (safelogger), <code>observability</code>, <code>rate_limiter</code>, <code>security</code>, <code>service_discovery</code>, <code>utils</code>. [source: 8-10, 689-1127]</li> <li>Components within <code>athomic</code> will primarily expose functionality through well-defined interfaces (Protocols/ABCs) [source: 443, 689, 855, 918, 1000, 1065] and decorators [source: 445, 726-735, 942-959, 962, 1092].</li> <li>The <code>nala.api</code> layer (containing FastAPI specifics like routes, middleware, request/response models) will depend on and consume the services provided by the <code>nala.athomic</code> layer.</li> </ul>"},{"location":"adr/ADR-002-abstract-layer/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>High Reusability: <code>athomic</code> components can potentially be reused in other Python applications or services, not just FastAPI APIs. [source: 14, 16]</li> <li>Improved Testability: Core infrastructure logic in <code>athomic</code> can be tested independently of the web framework, often with simpler unit tests.</li> <li>Clear Separation of Concerns: Enforces a clean architecture, separating infrastructure logic from API handling and business rules.</li> <li>Enhanced Maintainability: Changes to infrastructure (e.g., switching cache providers) are localized within <code>athomic</code>, minimizing impact on the API layer.</li> <li>Scalability: Promotes building specialized, robust core components.</li> </ul> </li> <li>Negative:<ul> <li>Increased Indirection: Adds a layer of abstraction, which might slightly increase complexity for simple use cases or for developers unfamiliar with the structure.</li> <li>Potential for Over-Engineering: Need to be mindful not to abstract prematurely or create overly complex interfaces if the benefit isn't clear.</li> </ul> </li> <li>Neutral/Other:<ul> <li>Requires clear documentation explaining the purpose and usage of the <code>athomic</code> layer and its components. [source: 7-16, 175-616]</li> <li>Establishes a strong architectural pattern that should guide future development within the <code>athomic-docs</code> ecosystem.</li> </ul> </li> </ul>"},{"location":"adr/ADR-003-provider-registry-for-extension/","title":"ADR-003: Provider/Registry/Factory Pattern for Extensibility","text":"<ul> <li>Status: Accepted (Implemented in several modules)</li> <li>Date: 2025-04-13 (Note: Ideally, use the date the decision was made)</li> </ul>"},{"location":"adr/ADR-003-provider-registry-for-extension/#context","title":"Context","text":"<ul> <li>Problem: The <code>athomic-docs</code> needs to interact with various external systems or support multiple strategies for core functionalities (e.g., different cache backends like Redis/Memory [source: 715, 716]; different secrets managers like Vault/AWS/Local [source: 873, 898]; different auth methods; different rate limiting storages [source: 1075]; different databases [source: 1007]). Hardcoding specific implementations would make the system rigid, difficult to test, and hard to extend or adapt to different deployment environments or future technologies.</li> <li>Goal: Allow easy swapping and addition of implementations for key infrastructure components without modifying the core consuming logic. Enable configuration-driven selection of implementations. Promote testability by allowing mock providers to be injected. [source: 14, 16]</li> <li>Alternatives Considered:<ul> <li>Conditional Logic (if/else): Directly checking configuration values in the code and executing different logic paths. Drawback: Violates Open/Closed Principle, leads to complex conditional blocks, hard to add new options.</li> <li>Strategy Pattern (Simple): Using classes for strategies but without a central registry or factory. Drawback: Requires manual instantiation and passing of the correct strategy class throughout the application, configuration logic might be scattered.</li> <li>Dependency Injection Framework: Using a full DI framework (like <code>python-dependency-injector</code>). Drawback: Adds a potentially heavier dependency, might be overkill if the primary need is swappable providers based on simple configuration keys.</li> </ul> </li> </ul>"},{"location":"adr/ADR-003-provider-registry-for-extension/#decision","title":"Decision","text":"<ul> <li>Implement a consistent \"Provider/Registry/Factory\" pattern for key extensible components within the <code>athomic</code> layer (Cache, Secrets, Auth, Rate Limiting, DB Repositories, potentially Config loading itself).</li> <li>Provider (Interface): Define a clear interface (using <code>abc.ABC</code> or <code>typing.Protocol</code>) for each component, specifying the required methods and their signatures (e.g., <code>CacheProtocol</code>, <code>SecretsProvider</code>, <code>AuthProvider</code>, <code>AbstractRateLimiter</code>, <code>IRepository</code>). [source: 443, 689, 855, 918, 1000, 1065]</li> <li>Provider (Implementations): Create concrete classes that implement these interfaces for specific technologies or strategies (e.g., <code>RedisCacheProvider</code> [source: 739], <code>VaultSecretsProvider</code> [source: 898], <code>JWTAuthProvider</code> [source: 929], <code>LimitsRateLimiter</code> [source: 1099], <code>MongoUserRepository</code> [source: 1030]).</li> <li>Registry (Optional but common): Maintain a mapping (often a simple dictionary) from a configuration key (e.g., backend name like \"redis\", \"vault\") to the corresponding Provider class (e.g., <code>SecretsRegistry</code> [source: 857], <code>STORAGE_REGISTRY</code> in rate limiter [source: 1075]). This centralizes the knowledge of available implementations.</li> <li>Factory (often Singleton): Create a factory function (e.g., <code>get_cache</code> [source: 712], <code>get_secrets_provider</code> [source: 859], <code>get_repository</code> [source: 1009], <code>get_auth_provider</code> [source: 938]) responsible for:<ol> <li>Reading the relevant configuration (via <code>get_settings</code>).</li> <li>Determining the desired provider implementation based on the configuration key (using the Registry if applicable).</li> <li>Instantiating the chosen Provider class, passing necessary configuration.</li> <li>Often using <code>@lru_cache</code> to return a singleton instance of the configured provider, ensuring efficiency and consistent state.</li> </ol> </li> </ul>"},{"location":"adr/ADR-003-provider-registry-for-extension/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>High Extensibility: Adding support for a new backend (e.g., a new cache provider) involves creating a new class implementing the interface and registering it, with minimal changes to consuming code.</li> <li>Configurability: The active provider is determined by configuration settings, allowing easy adaptation to different environments (e.g., use <code>MemoryCache</code> in tests, <code>RedisCache</code> in production).</li> <li>Improved Testability: Consuming code depends on the interface, making it easy to inject mock providers during unit testing.</li> <li>Decoupling: Core logic interacts with the interface, unaware of the specific implementation details.</li> <li>Consistency: Provides a standard way to handle pluggable components across the framework.</li> </ul> </li> <li>Negative:<ul> <li>Boilerplate: Introduces some boilerplate code (interfaces, registries, factories) for each extensible component.</li> <li>Indirection: Adds a layer of indirection compared to direct instantiation.</li> </ul> </li> <li>Neutral/Other:<ul> <li>Relies heavily on the configuration system (ADR-001) being robust.</li> <li>Requires developers to understand the pattern and where to register new providers.</li> </ul> </li> </ul>"},{"location":"adr/ADR-004-safelogger-with-masking/","title":"ADR-003: Provider/Registry/Factory Pattern for Extensibility","text":"<ul> <li>Status: Accepted (Implemented in several modules)</li> <li>Date: 2025-04-13 (Note: Ideally, use the date the decision was made)</li> </ul>"},{"location":"adr/ADR-004-safelogger-with-masking/#context","title":"Context","text":"<ul> <li>Problem: The <code>athomic-docs</code> needs to interact with various external systems or support multiple strategies for core functionalities (e.g., different cache backends like Redis/Memory [source: 715, 716]; different secrets managers like Vault/AWS/Local [source: 873, 898]; different auth methods; different rate limiting storages [source: 1075]; different databases [source: 1007]). Hardcoding specific implementations would make the system rigid, difficult to test, and hard to extend or adapt to different deployment environments or future technologies.</li> <li>Goal: Allow easy swapping and addition of implementations for key infrastructure components without modifying the core consuming logic. Enable configuration-driven selection of implementations. Promote testability by allowing mock providers to be injected. [source: 14, 16]</li> <li>Alternatives Considered:<ul> <li>Conditional Logic (if/else): Directly checking configuration values in the code and executing different logic paths. Drawback: Violates Open/Closed Principle, leads to complex conditional blocks, hard to add new options.</li> <li>Strategy Pattern (Simple): Using classes for strategies but without a central registry or factory. Drawback: Requires manual instantiation and passing of the correct strategy class throughout the application, configuration logic might be scattered.</li> <li>Dependency Injection Framework: Using a full DI framework (like <code>python-dependency-injector</code>). Drawback: Adds a potentially heavier dependency, might be overkill if the primary need is swappable providers based on simple configuration keys.</li> </ul> </li> </ul>"},{"location":"adr/ADR-004-safelogger-with-masking/#decision","title":"Decision","text":"<ul> <li>Implement a consistent \"Provider/Registry/Factory\" pattern for key extensible components within the <code>athomic</code> layer (Cache, Secrets, Auth, Rate Limiting, DB Repositories, potentially Config loading itself).</li> <li>Provider (Interface): Define a clear interface (using <code>abc.ABC</code> or <code>typing.Protocol</code>) for each component, specifying the required methods and their signatures (e.g., <code>CacheProtocol</code>, <code>SecretsProvider</code>, <code>AuthProvider</code>, <code>AbstractRateLimiter</code>, <code>IRepository</code>). [source: 443, 689, 855, 918, 1000, 1065]</li> <li>Provider (Implementations): Create concrete classes that implement these interfaces for specific technologies or strategies (e.g., <code>RedisCacheProvider</code> [source: 739], <code>VaultSecretsProvider</code> [source: 898], <code>JWTAuthProvider</code> [source: 929], <code>LimitsRateLimiter</code> [source: 1099], <code>MongoUserRepository</code> [source: 1030]).</li> <li>Registry (Optional but common): Maintain a mapping (often a simple dictionary) from a configuration key (e.g., backend name like \"redis\", \"vault\") to the corresponding Provider class (e.g., <code>SecretsRegistry</code> [source: 857], <code>STORAGE_REGISTRY</code> in rate limiter [source: 1075]). This centralizes the knowledge of available implementations.</li> <li>Factory (often Singleton): Create a factory function (e.g., <code>get_cache</code> [source: 712], <code>get_secrets_provider</code> [source: 859], <code>get_repository</code> [source: 1009], <code>get_auth_provider</code> [source: 938]) responsible for:<ol> <li>Reading the relevant configuration (via <code>get_settings</code>).</li> <li>Determining the desired provider implementation based on the configuration key (using the Registry if applicable).</li> <li>Instantiating the chosen Provider class, passing necessary configuration.</li> <li>Often using <code>@lru_cache</code> to return a singleton instance of the configured provider, ensuring efficiency and consistent state.</li> </ol> </li> </ul>"},{"location":"adr/ADR-004-safelogger-with-masking/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>High Extensibility: Adding support for a new backend (e.g., a new cache provider) involves creating a new class implementing the interface and registering it, with minimal changes to consuming code.</li> <li>Configurability: The active provider is determined by configuration settings, allowing easy adaptation to different environments (e.g., use <code>MemoryCache</code> in tests, <code>RedisCache</code> in production).</li> <li>Improved Testability: Consuming code depends on the interface, making it easy to inject mock providers during unit testing.</li> <li>Decoupling: Core logic interacts with the interface, unaware of the specific implementation details.</li> <li>Consistency: Provides a standard way to handle pluggable components across the framework.</li> </ul> </li> <li>Negative:<ul> <li>Boilerplate: Introduces some boilerplate code (interfaces, registries, factories) for each extensible component.</li> <li>Indirection: Adds a layer of indirection compared to direct instantiation.</li> </ul> </li> <li>Neutral/Other:<ul> <li>Relies heavily on the configuration system (ADR-001) being robust.</li> <li>Requires developers to understand the pattern and where to register new providers.</li> </ul> </li> </ul>"},{"location":"adr/ADR-005-cache-strategy/","title":"ADR-005: Advanced Caching Strategy with Decorators and Multiple Backends","text":"<ul> <li>Status: Accepted (Implemented)</li> <li>Date: 2025-04-13 (Note: Using current date as placeholder)</li> </ul>"},{"location":"adr/ADR-005-cache-strategy/#context","title":"Context","text":"<ul> <li>Problem: In distributed systems and high-load APIs, frequent access to slow data sources (databases, external service calls) can become a significant performance and availability bottleneck. Simple caching helps, but doesn't address common large-scale caching issues, such as:<ul> <li>Cache Stampede (Thundering Herd): Multiple concurrent requests/processes attempting to recalculate an expired cache value simultaneously, overwhelming the underlying data source.</li> <li>Expiration Latency: When a cached item expires, the next request needing it must wait for the recalculation time, increasing perceived latency.</li> <li>Need for Multiple Backends &amp; Fallback: The requirement to use different cache types (e.g., local in-memory for ultra-fast access of small/frequent data, Redis for distributed/persistent cache) and having a fallback strategy if the primary cache fails. </li> <li>Code Clutter: Manually implementing cache checking, storage, and invalidation logic within business functions pollutes the core logic, making it repetitive and error-prone.</li> </ul> </li> <li>Goal: Implement a robust, flexible, and easy-to-use caching system for <code>athomic-docs</code> that mitigates the problems above, offers high performance and resilience, and integrates cleanly into the codebase, preferably via decorators.</li> <li>Alternatives Considered:<ul> <li>Manual Caching: Requiring developers to manually implement <code>cache.get()</code>, <code>compute_value()</code>, <code>cache.set()</code> logic at each necessary point. Drawback: Highly repetitive, error-prone, difficult to maintain consistency, doesn't easily accommodate advanced features.</li> <li>Simple Caching (Get/Set Only): Implementing only basic cache operations. Drawback: Fails to solve cache stampede or abrupt expiration issues.</li> <li>Direct Use of Caching Libraries: Using libraries like <code>cachetools</code> or the <code>redis-py</code> client directly within business logic. Drawback: Would require manual implementation of Jitter, Locking, Refresh Ahead, Fallback logic, and integration with configuration/observability.</li> <li>Web Framework Caching (HTTP Level): Relying solely on framework mechanisms for caching full HTTP responses. Drawback: Less granular, not applicable to internal calculations or service calls, doesn't cache raw data or intermediate results effectively.</li> </ul> </li> </ul>"},{"location":"adr/ADR-005-cache-strategy/#decision","title":"Decision","text":"<ul> <li>Implement the <code>nala.athomic.performance.cache</code> module following the Provider/Registry/Factory pattern (ADR-003) to support multiple backends.</li> <li>Initially support <code>MemoryCacheProvider</code> (based on <code>cachetools.TTLCache</code>) and <code>RedisCacheProvider</code> (using async <code>redis-py</code>), both adhering to the <code>CacheProtocol</code> interface.</li> <li>Implement a <code>FallbackCacheProvider</code> allowing a configurable chain of providers (e.g., try Redis, if it fails, try Memory), enhancing resilience. </li> <li>Create a primary decorator <code>@cache_result</code>  as the main way to apply caching to (async) functions.</li> <li>Embed advanced features within <code>@cache_result</code>, controllable via parameters:<ul> <li><code>ttl</code>: Configurable time-to-live for the cached item.</li> <li><code>use_jitter</code>: Apply random variation (jitter) to the final TTL (<code>apply_jitter</code>) to prevent mass simultaneous expirations.</li> <li><code>use_lock</code>: Implement a lock (mutex) mechanism per cache key to prevent cache stampedes. Only the first request/task hitting an expired/missing key acquires the lock to recompute; others wait for the result or lock release. (Note: Current implementation uses <code>asyncio.Lock</code>, only safe for single-instance).</li> <li><code>refresh_ahead</code>: Allow the value to be recomputed in the background before it expires (based on <code>refresh_threshold</code>). Current requests receive the old (stale) value while the new one is generated, ensuring low latency. The recomputed value replaces the old one for subsequent requests. </li> </ul> </li> <li>Create an <code>@invalidate_cache</code> decorator to easily remove specific cache keys before a function executes (useful after write/update/delete operations).</li> <li>Implement a <code>hash_key</code> utility  to generate deterministic, unique cache keys based on the cached function's name, arguments (<code>*args</code>, <code>**kwargs</code>), and a configurable prefix (global or per-decorator), avoiding collisions.</li> <li>Integrate the caching system with the Observability module by emitting Prometheus metrics for cache hits, misses, and errors (<code>cache_hit_counter</code>, <code>cache_miss_counter</code>, <code>cache_error_counter</code>) via a wrapper function like <code>observed_get</code>. </li> </ul>"},{"location":"adr/ADR-005-cache-strategy/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Significant Performance Improvement: Reduces latency and load on slower backend data sources for frequently accessed data.</li> <li>High Resilience &amp; Availability: Mitigates cache stampedes (with <code>use_lock</code>), smooths out cache expiration impacts (with <code>refresh_ahead</code>, <code>use_jitter</code>), and increases overall availability via <code>FallbackCacheProvider</code>.</li> <li>Cleaner &amp; Declarative Code: Caching complexity is encapsulated within the decorators and the <code>athomic.cache</code> module, keeping business logic focused.</li> <li>Ease of Use: Applying advanced caching strategies becomes as simple as adding and configuring the <code>@cache_result</code> decorator to an async function.</li> <li>Granular Configurability: Developers control TTL, expiration strategies (jitter), stampede protection (lock), proactive updates (refresh ahead), backends, and fallbacks via configuration and decorator parameters.</li> <li>Integrated Observability: Built-in metrics provide visibility into cache effectiveness and health.</li> </ul> </li> <li>Negative:<ul> <li>Intrinsic Complexity: The cache module itself is relatively complex due to the combination of advanced features.</li> <li>Cache Consistency Management: As with any caching system, correct cache invalidation (using <code>@invalidate_cache</code> or other strategies) is crucial to avoid serving stale data after the underlying data has been modified. The <code>refresh_ahead</code> strategy intentionally serves stale data for a short period.</li> <li>Debugging Challenges: The introduction of state (the cache) and asynchronous background logic (refresh ahead) can occasionally make debugging certain flows more complex.</li> <li>Distributed Lock Required for Scale: The <code>use_lock=True</code> feature in the current implementation (<code>asyncio.Lock</code> [source: 766]) only provides stampede protection within a single application instance. For a multi-instance deployment, a distributed lock implementation (e.g., using Redis) would be required for this feature to be fully effective (not currently implemented).</li> </ul> </li> <li>Neutral/Other:<ul> <li>The effectiveness of the cache fundamentally depends on choosing the right operations to cache, setting appropriate TTLs, and having a well-thought-out cache invalidation strategy.</li> </ul> </li> </ul>"},{"location":"adr/ADR-006-rate-limit-with-limits-lib/","title":"ADR-006: Choice of <code>limits</code> Library for Rate Limiting","text":"<ul> <li>Status: Accepted (Implemented)</li> <li>Date: 2025-04-13 (Note: Using current date as placeholder)</li> </ul>"},{"location":"adr/ADR-006-rate-limit-with-limits-lib/#context","title":"Context","text":"<ul> <li>Problem: APIs, whether public or internal, require protection against abuse (intentional or accidental) and overload. A mechanism is needed to limit the number of requests a client (identified by IP, user, token, etc.) can make within a specific time period. Implementing this logic from scratch is complex, requiring management of counters, time windows (fixed, sliding), and state storage (in-memory, Redis, etc.).</li> <li>Goal: Adopt a robust, flexible, well-maintained rate limiting solution for <code>athomic-docs</code> that integrates well with the project's asynchronous Python architecture, supporting different limiting strategies and storage backends. [source: 184]</li> <li>Alternatives Considered:<ul> <li>Manual Implementation: Building all the logic for counting, time windows, and storage internally. Drawback: Very complex to get right (especially sliding window algorithms and distributed consistency), high development and maintenance cost, reinvents the wheel.</li> <li>Framework-Specific Rate Limiting Libraries: Using solutions coupled to specific web frameworks, like <code>slowapi</code> for FastAPI. Drawback: Ties the rate limiting logic to the web framework, making it difficult to reuse outside the FastAPI context (e.g., background tasks, internal calls) or to migrate to another framework later.</li> <li>Other Generic Python Libraries: Evaluating alternatives like <code>pyrate-limiter</code>. Drawback: At the time of evaluation, the <code>limits</code> library appeared more mature, offering more strategies, support for multiple storage backends [source: 1075], and good documentation.</li> <li>Rate Limiting at the Infrastructure Level (Gateway/Infra): Relying solely on rate limiting configured in API Gateways (Kong, Nginx, Cloudflare, etc.) or Load Balancers. Drawback: Offers less granularity (can be hard to apply specific limits per authenticated user or per complex operation within the API), the logic resides outside the application, harder to unit test. Often used as a complementary layer, but doesn't replace the need for application-level rate limiting in many scenarios.</li> </ul> </li> </ul>"},{"location":"adr/ADR-006-rate-limit-with-limits-lib/#decision","title":"Decision","text":"<ul> <li>Adopt the <code>limits</code> Python library [source: 4, 12, 220] as the foundation for rate limiting functionality within <code>athomic-docs</code>.</li> <li>Create a dedicated module <code>nala.athomic.rate_limiter</code> to encapsulate interaction with the <code>limits</code> library.</li> <li>Define an <code>AbstractRateLimiter</code> interface [source: 1065] to standardize interaction with the rate limiting system.</li> <li>Implement a <code>LimitsRateLimiter</code> provider [source: 1099] that utilizes the <code>limits</code> library internally. This provider supports:<ul> <li>Configurable strategies (<code>fixed</code> window, <code>moving</code> window). [source: 1099]</li> <li>Multiple storage backends supported by the library, configured via a <code>storage_uri</code> [source: 853, 1099] (e.g., <code>memory://</code>, <code>redis://</code>, <code>memcached://</code> [source: 1075]).</li> </ul> </li> <li>Expose the functionality through:<ul> <li>An <code>@rate_limited</code> decorator [source: 1092] for easy application to async functions.</li> <li>A <code>core.py</code> module [source: 1076] for more programmatic usage if needed (e.g., in middleware).</li> </ul> </li> <li>Manage configuration (default backend, strategy, storage URI, key prefix) via the <code>RateLimiterSettings</code> Pydantic model [source: 853] within the centralized configuration system (ADR-001).</li> </ul>"},{"location":"adr/ADR-006-rate-limit-with-limits-lib/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Reuses Robust Logic: Leverages a well-tested and maintained library (<code>limits</code>) that correctly implements complex rate limiting algorithms (fixed window, moving/sliding window).</li> <li>Flexibility of Strategy &amp; Backend: Allows choosing the limiting strategy and storage backend via configuration without changing application code. [source: 1075, 1099]</li> <li>Clean Integration: Using the <code>AbstractRateLimiter</code> interface and the <code>@rate_limited</code> decorator keeps business logic decoupled from rate limiting implementation details.</li> <li>Asynchronous Support: The <code>limits</code> library has good support for <code>asyncio</code>.</li> <li>Community &amp; Documentation: <code>limits</code> is reasonably well-established and documented within the Python community.</li> </ul> </li> <li>Negative:<ul> <li>External Dependency: Adds the <code>limits</code> library as a project dependency.</li> <li>Library Limitations: <code>athomic-docs</code> inherits the features and limitations of the <code>limits</code> library. For instance, the difficulty in implementing <code>clear</code> for a specific key [source: 1103] is an inherited limitation.</li> <li>Abstraction Layer: Creating the <code>LimitsRateLimiter</code> wrapper adds a small layer of indirection over the original library.</li> </ul> </li> <li>Neutral/Other:<ul> <li>Correct configuration of the <code>storage_uri</code> and strategy is crucial for expected behavior in different environments (single-instance vs. multi-instance).</li> <li>Requires developers to understand basic rate limiting concepts (limit, window, strategies) to use the feature effectively.</li> </ul> </li> </ul>"},{"location":"adr/ADR-007-abstraction-repository-and-beaine-adoption/","title":"ADR-007: Repository Abstraction and Beanie ODM for MongoDB","text":"<ul> <li>Status: Accepted (Implemented)</li> <li>Date: 2025-04-13 (Placeholder)</li> </ul>"},{"location":"adr/ADR-007-abstraction-repository-and-beaine-adoption/#context","title":"Context","text":"<ul> <li>Problem: Applications need to interact with data persistence layers (databases). Directly using database driver code (e.g., <code>motor</code> for async MongoDB [source: 5]) or even a specific Object-Document Mapper (ODM) / Object-Relational Mapper (ORM) (like <code>Beanie</code> [source: 5], SQLAlchemy) within business logic (services) or API handlers creates tight coupling between the application logic and the specific database technology and its access patterns. This coupling makes it difficult to:<ul> <li>Switch database technologies in the future (e.g., migrating from MongoDB to PostgreSQL) without extensive refactoring.</li> <li>Unit test business logic in isolation without needing a live database connection or complex mocking of the database driver/ODM.</li> <li>Maintain consistency in how data is accessed and manipulated across different parts of the application.</li> <li>Evolve the data model and access logic independently of the business rules.</li> </ul> </li> <li>Goal: Abstract the data access logic behind a stable, consistent interface using the Repository pattern. This aims to decouple the business logic from the specific database implementation, thereby improving testability, maintainability, and the potential for future database migrations. Additionally, leverage an asynchronous ODM for MongoDB to simplify database interactions, provide schema validation close to the database, and integrate well with the FastAPI/asyncio ecosystem.</li> <li>Alternatives Considered:<ul> <li>Direct Driver/ODM Usage: Using <code>motor</code> or <code>Beanie</code> directly within service classes or FastAPI route functions. Drawback: Leads to high coupling, poor testability, and mixes data access concerns with business/API logic.</li> <li>Generic ORM/ODM Only (e.g., SQLAlchemy Core, Pymongo): Using lower-level database interaction tools. Drawback: Requires significantly more boilerplate code for common CRUD (Create, Read, Update, Delete) operations compared to a higher-level ODM like Beanie. Type safety for data models might be less integrated.</li> <li>Data Access Objects (DAOs): A pattern similar to Repositories, but sometimes considered more focused on simple CRUD operations per table/collection. Decision: The Repository pattern felt more aligned with encapsulating operations related to a whole business entity/aggregate, potentially including more complex queries beyond simple CRUD.</li> <li>Other Async ODMs for MongoDB: Evaluating alternatives to Beanie (e.g., <code>MongoEngine</code> - though async support might be less mature, <code>ODMantic</code> - another Pydantic-based option). Decision: Beanie was chosen due to its strong integration with Pydantic and FastAPI, its active development, and its focus on async. [source: 5]</li> </ul> </li> </ul>"},{"location":"adr/ADR-007-abstraction-repository-and-beaine-adoption/#decision","title":"Decision","text":"<ul> <li>Implement the Repository Pattern as the standard way to abstract data persistence logic within <code>athomic-docs</code>.</li> <li>Define a generic <code>IRepository</code> Protocol [source: 1000] using <code>typing.Protocol</code> to specify the common contract for data access, including methods like <code>get_by_id</code>, <code>save</code>, <code>delete</code>, <code>find</code>, <code>find_one</code>.</li> <li>Define specific repository interfaces (e.g., <code>IUserRepository</code> [source: 1006]) that inherit from <code>IRepository</code> for each core domain entity (like User). These specific interfaces can include methods relevant only to that entity (e.g., <code>get_by_email</code>).</li> <li>Utilize the <code>Beanie</code> ODM [source: 5] as the chosen tool for interacting with MongoDB. Beanie leverages <code>motor</code> for asynchronous operations and <code>Pydantic</code> for defining document models (<code>beanie.Document</code>) [source: 1030, 1142], providing data validation and a convenient API.</li> <li>Implement concrete repository classes specifically for the MongoDB backend using Beanie. This includes:<ul> <li>A generic <code>MongoBeanieRepository</code> [source: 1031] class that implements the <code>IRepository</code> protocol using Beanie's methods.</li> <li>Specific repository classes (e.g., <code>MongoUserRepository</code> [source: 1035]) that inherit from the generic Beanie repository and the specific domain interface (e.g., <code>IUserRepository</code>), implementing any additional required methods using Beanie queries.</li> </ul> </li> <li>Utilize the Factory Pattern via the <code>get_repository</code> function [source: 1009]. This function:<ol> <li>Reads the configured database backend from settings (<code>settings.database.backend</code> [source: 846, 1010]).</li> <li>Uses a mapping (<code>_REPOSITORY_IMPLEMENTATIONS</code> [source: 1007]) to look up the appropriate concrete repository class based on the requested interface type and the configured backend.</li> <li>Instantiates and returns the concrete repository class (often cached via <code>@lru_cache</code> for efficiency).</li> </ol> </li> <li>Centralize the MongoDB connection and Beanie initialization logic in <code>nala.athomic.db.mongo.beanie_init.py</code> [source: 1036]. This initialization (<code>init_database_connection</code> [source: 1038]) is designed to be called once during the application startup sequence (e.g., within the FastAPI <code>lifespan</code> manager [source: 1151]).</li> </ul>"},{"location":"adr/ADR-007-abstraction-repository-and-beaine-adoption/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Decoupling: Business logic components (services) depend only on the repository interfaces, making them theoretically agnostic to the underlying database technology (MongoDB/Beanie in this implementation).</li> <li>Improved Testability: Business logic can be easily unit-tested by injecting mock repository objects that conform to the defined interfaces, eliminating the need for a live database in unit tests.</li> <li>Centralized Data Access Logic: All database interaction logic for a given entity is concentrated within its repository implementation, improving code organization, maintainability, and consistency.</li> <li>Developer Productivity (for MongoDB): Beanie significantly simplifies common MongoDB operations (CRUD, indexing, potentially data migrations) and integrates seamlessly with Pydantic for data validation at the ODM level. [source: 1031-1034]</li> <li>Extensibility: Adding support for a different database technology (e.g., PostgreSQL with SQLAlchemy) would primarily involve creating new repository classes implementing the existing interfaces and updating the factory mapping, with minimal changes to the business logic layer. [source: 1008]</li> </ul> </li> <li>Negative:<ul> <li>Layer of Indirection: Introduces an extra layer of abstraction (interfaces, factory) compared to using the ODM or database driver directly.</li> <li>Boilerplate Code: Requires defining interface files and implementation classes for each repository, adding some boilerplate.</li> <li>Potential for Leaky Abstraction: Very complex or database-specific queries might be awkward or inefficient to express through the generic repository interface. This could necessitate adding specialized methods to interfaces or accepting that some database-specific constructs (like Beanie's query syntax) might leak into the criteria passed to methods like <code>find</code> or <code>find_one</code> [source: 1034].</li> <li>Dependency on Beanie (for Mongo): While the business logic is decoupled from MongoDB in principle, the current concrete implementation is tightly coupled to the Beanie ODM. Migrating away from Beanie, even while still using MongoDB, would require rewriting the repository implementations.</li> </ul> </li> <li>Neutral/Other:<ul> <li>The current pattern assumes a single primary database is configured via <code>settings.database</code> [source: 846]. Supporting connections to multiple databases simultaneously (of the same or different types) would require extending the configuration schema and the repository factory logic.</li> <li>The correct functioning relies on the successful initialization of the database connection and Beanie ODM during application startup (<code>init_database_connection</code> [source: 1038]). Failures during startup need to be handled appropriately.</li> </ul> </li> </ul>"},{"location":"adr/ADR-008-observability-stack-choice/","title":"ADR-008: Observability Stack Choice - Prometheus for Metrics, OpenTelemetry for Tracing","text":"<ul> <li>Status: Partially Accepted (Metrics implemented, Tracing in progress)</li> <li>Date: 2025-04-13 (Placeholder)</li> </ul>"},{"location":"adr/ADR-008-observability-stack-choice/#context","title":"Context","text":"<ul> <li>Problem: Operating complex, distributed systems like those built upon <code>athomic-docs</code> requires deep visibility into their runtime behavior. Without adequate observability, diagnosing performance issues, debugging errors across components, and understanding system health under load becomes extremely difficult. We need ways to answer questions like \"Why is this request slow?\", \"What component failed?\", \"Are we hitting resource limits?\", \"What's the error rate for service X?\". This requires collecting different kinds of telemetry data.</li> <li>Needs:<ul> <li>Metrics: Aggregate numerical data over time (e.g., request counts, latencies, error rates, queue sizes, cache hit/miss ratios [source: 722, 967-969]) for monitoring overall health, identifying trends, setting alerts, and capacity planning.</li> <li>Traces: Detailed, request-scoped information showing the path and timing of a request as it flows through different components or services (distributed tracing), essential for pinpointing bottlenecks and understanding complex interactions.</li> <li>Logs: Contextual, event-based information (often textual) for specific points in time, primarily used for detailed error diagnosis and understanding specific events. (Logging itself is covered by ADR-004, but its integration with tracing is relevant here).</li> </ul> </li> <li>Goal: Implement a comprehensive and standardized observability solution for <code>athomic-docs</code>, leveraging industry-standard, preferably open-source, tools and protocols. This ensures interoperability with common monitoring ecosystems (like Prometheus Operator, Grafana, Jaeger) and avoids vendor lock-in where possible. [source: 279]</li> <li>Alternatives Considered:<ul> <li>Proprietary APM Solutions (e.g., Datadog, New Relic, Dynatrace): These offer integrated platforms for metrics, tracing, and logging, often with easy setup via agents. Drawback: Can lead to vendor lock-in, potentially higher costs depending on scale, and less control over the data format and collection mechanisms.</li> <li>Metrics Only (Prometheus): Implementing only metrics collection using Prometheus. Drawback: While essential for monitoring and alerting, it lacks the detailed request-level visibility needed to diagnose complex latency issues or understand distributed workflows, which tracing provides.</li> <li>Tracing Only (e.g., Jaeger Client Libraries Directly): Implementing only distributed tracing. Drawback: Lacks the high-level aggregated view and alerting capabilities provided by metrics. Can be harder to grasp overall system health trends just from traces.</li> <li>Logging-Centric Stack (e.g., ELK - Elasticsearch, Logstash, Kibana): Primarily focused on log aggregation and analysis. Can be extended for metrics (Metricbeat) and tracing (Elastic APM). Drawback: Often more resource-intensive, setup can be complex, and Prometheus/OpenTelemetry are arguably more purpose-built and standard for metrics/tracing respectively in many cloud-native environments.</li> <li>Other Open Standards: Using older standards like OpenCensus (now merged into OpenTelemetry) or focusing solely on OpenMetrics (a Prometheus exposition format standard). Decision: OpenTelemetry emerged as the leading open standard for tracing, integrating aspects of OpenTracing and OpenCensus. Prometheus remains the de facto standard for metrics scraping.</li> </ul> </li> </ul>"},{"location":"adr/ADR-008-observability-stack-choice/#decision","title":"Decision","text":"<ul> <li>Adopt a best-of-breed approach using industry-standard open specifications and tools:<ul> <li>Metrics: Prometheus.<ul> <li>Utilize the official <code>prometheus-client</code> library [source: 5] for defining and exposing custom application metrics (Counters, Gauges, Histograms for requests, cache stats, secret access, auth events, etc.). [source: 967-969]</li> <li>Leverage <code>prometheus-fastapi-instrumentator</code> [source: 5] and/or custom middleware (<code>RequestMetricsMiddleware</code> [source: 1131]) to automatically instrument FastAPI HTTP requests for standard metrics (count, latency, in-progress).</li> <li>Expose all collected metrics via a standard <code>/metrics</code> HTTP endpoint [source: 1150] in the Prometheus exposition format, allowing a Prometheus server to scrape the data.</li> </ul> </li> <li>Tracing: OpenTelemetry (OTel). [source: 13, 187]<ul> <li>Standardize on OpenTelemetry as the vendor-neutral framework for generating, collecting, and exporting trace data. [source: 5]</li> <li>Utilize OTel instrumentation libraries (e.g., <code>opentelemetry-instrumentation-fastapi</code>, <code>opentelemetry-instrumentation-requests</code>, etc. [source: 5]) for automatic trace context propagation and span creation for supported libraries and frameworks. A basic setup exists in <code>src/nala/api/middleware/tracing.py</code>. [source: 1129]</li> <li>Configure an OTLP (OpenTelemetry Protocol) Exporter [source: 1129] (specifically <code>OTLPSpanExporter</code> initially) to send trace data to a compatible backend (e.g., Jaeger, Grafana Tempo, SigNoz, or commercial APM tools). The backend endpoint, protocol (gRPC/HTTP), headers, and sampling rate will be configurable via <code>ObservabilitySettings</code>. [source: 815, 816]</li> <li>Plan for manual instrumentation (creating spans explicitly) within critical sections of the <code>athomic</code> layer or complex business logic where auto-instrumentation doesn't provide sufficient detail (currently a TODO/roadmap item). [source: 187, 541]</li> </ul> </li> <li>Logging Integration: While handled by SafeLogger (ADR-004), ensure that trace context (Trace ID, Span ID) can be automatically injected into log records (a common OTel logging library integration pattern, not yet explicitly shown in the provided code but essential for correlating logs with traces).</li> </ul> </li> <li>Centralize all observability configurations (enable/disable flags, exporter details, sampling rates) within the <code>ObservabilitySettings</code> Pydantic model [source: 813] managed by the central configuration system (ADR-001).</li> </ul>"},{"location":"adr/ADR-008-observability-stack-choice/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Industry Standard &amp; Interoperability: Aligns <code>athomic-docs</code> with widely adopted open standards, ensuring compatibility with a vast ecosystem of monitoring, alerting, and analysis tools (Grafana, Prometheus Operator, Jaeger, various OTel backends).</li> <li>Vendor Neutrality (especially for Tracing): OpenTelemetry allows switching tracing backends without changing the application instrumentation code.</li> <li>Comprehensive Visibility: The combination of metrics (aggregate view, alerting) and traces (deep request-level diagnosis) provides powerful observability.</li> <li>Rich Ecosystem &amp; Community: Benefits from the extensive tooling, documentation, and community support surrounding Prometheus and OpenTelemetry.</li> <li>Automation Potential: OTel auto-instrumentation reduces the effort required for basic tracing coverage.</li> <li>Consistency: Establishes a standardized approach for instrumenting code within the <code>athomic-docs</code>.</li> </ul> </li> <li>Negative:<ul> <li>Operational Complexity: Requires setting up and managing a potentially complex external stack (Prometheus server, OTel Collector (recommended), tracing storage/backend like Jaeger or Tempo, visualization tool like Grafana).</li> <li>Performance Overhead: Collecting detailed metrics and traces inevitably introduces some performance overhead on the application, although tools are designed to minimize this. Trace sampling [source: 816] is often necessary in high-volume systems but means some requests won't be traced.</li> <li>Learning Curve: Developers need to understand the core concepts of metrics (types, labels, cardinality) and distributed tracing (spans, context, propagation) to instrument code effectively and interpret the data.</li> <li>Incomplete Implementation (Current State): While the foundation is laid and metrics are functional, the tracing implementation requires further work to be fully realized (completing OTel setup, adding manual instrumentation where needed, ensuring log correlation). [source: 13, 180, 194]</li> </ul> </li> <li>Neutral/Other:<ul> <li>Effective monitoring requires careful dashboard design (e.g., in Grafana) and well-configured alerts based on the collected metrics.</li> <li>The usefulness of tracing heavily depends on consistent context propagation across asynchronous tasks and service boundaries (if applicable), as well as sufficient instrumentation coverage (both automatic and manual).</li> <li>Managing metric cardinality (the number of unique label combinations) is important to avoid overloading the Prometheus server.</li> </ul> </li> </ul>"},{"location":"adr/ADR-009-resilience-partterns-by-decorators/","title":"ADR-009: Implementation of Resilience Patterns via Decorators","text":"<ul> <li>Status: Accepted (Implemented for Retry and Fallback; Circuit Breaker/Timeout Planned)</li> <li>Date: 2025-04-13 (Placeholder)</li> </ul>"},{"location":"adr/ADR-009-resilience-partterns-by-decorators/#context","title":"Context","text":"<ul> <li>Problem: Network requests, database calls, and interactions with external services are inherently unreliable in distributed systems. Transient failures (network glitches, temporary service unavailability, brief resource contention) and persistent failures can occur. Applications need strategies to handle these failures gracefully to avoid cascading failures, improve user experience, and maintain overall system stability. Implementing logic for retrying operations, falling back to alternative data sources or logic paths, or preventing repeated calls to failing services (circuit breaking) directly within the primary business logic flow makes the code verbose, repetitive, error-prone, and harder to read and test.</li> <li>Goal: Provide reusable, declarative, and minimally invasive mechanisms within <code>athomic-docs</code> to apply common resilience patterns (specifically Retry and Fallback initially, with plans for Circuit Breaker and Timeout) to function calls, particularly those involving I/O or external dependencies.</li> <li>Alternatives Considered:<ul> <li>Manual Implementation: Developers manually write <code>try...except</code> blocks with retry loops, sleep intervals, or conditional fallback logic within each function that needs resilience. Drawback: Highly repetitive, difficult to ensure consistency in retry strategies (e.g., backoff timing), error-prone, significantly clutters business logic.</li> <li>Utility Functions/Classes (Wrappers): Creating helper functions or context managers that accept the function to be executed and apply retry/fallback logic around it. Drawback: Can still be somewhat verbose to apply, requires passing callables around, less declarative than decorators.</li> <li>Aspect-Oriented Programming (AOP) Frameworks: Utilizing more heavyweight AOP frameworks (less common in the Python ecosystem compared to Java/C#). Drawback: Introduces significant complexity, potentially adds \"magic\" that obscures control flow, likely overkill for these specific patterns.</li> <li>Infrastructure-Level Resilience: Relying solely on retry/fallback mechanisms provided by service meshes (like Istio), API gateways, or load balancers. Drawback: Operates at a coarser granularity (HTTP requests), lacks application-specific context (e.g., cannot easily fall back to reading from a cache or using a default value computed by application logic), moves resilience logic outside the application's direct control and testing scope. Often better used as a complementary layer.</li> </ul> </li> </ul>"},{"location":"adr/ADR-009-resilience-partterns-by-decorators/#decision","title":"Decision","text":"<ul> <li>Implement core resilience patterns primarily as Python decorators, residing in <code>nala.athomic.utils.decorators</code>. [source: 942] This approach offers a declarative syntax that separates the resilience concern from the function's core logic.</li> <li>Retry Pattern:<ul> <li>Implement a <code>@retry_handler.with_retry()</code> decorator. [source: 944]</li> <li>Leverage the robust and well-maintained <code>tenacity</code> library [source: 4] internally to handle the retry logic (e.g., <code>stop_after_attempt</code>, <code>wait_exponential</code>, <code>retry_if_exception_type</code>). [source: 946]</li> <li>Make the decorator configurable regarding the number of attempts, wait strategy/timing, and the specific exceptions that should trigger a retry. [source: 943-946]</li> <li>Integrate basic logging to indicate when retries are occurring. [source: 946, 947]</li> </ul> </li> <li>Fallback Pattern:<ul> <li>Implement a <code>@fallback_handler()</code> decorator [source: 950] that accepts a list of one or more fallback functions as arguments. [source: 953]</li> <li>If the decorated function raises an exception, the decorator will iterate through the provided fallback functions, calling them sequentially until one executes successfully (without raising an exception). [source: 951, 955-958]</li> <li>The decorator must correctly handle both synchronous and asynchronous decorated functions and fallback functions, inspecting them to determine whether <code>await</code> is needed. [source: 954-959]</li> <li>If the original function and all provided fallback functions fail, the decorator will raise a custom <code>FallbackError</code> exception [source: 950] containing the list of all encountered errors for diagnostic purposes. [source: 146, 149]</li> </ul> </li> <li>Circuit Breaker &amp; Timeout Patterns: Recognize these as essential resilience patterns but defer their implementation for now. They are noted in the project backlog [source: 186, 219, 591]. The intention is to implement them using a similar decorator-based approach in the future when prioritized.</li> <li>Usage: Encourage the application of these decorators primarily to methods within the <code>athomic</code> layer that interact with external systems (e.g., database repository methods, secret provider <code>get_secret</code> calls [source: 871, 876, 888], cache provider interactions) and potentially in the <code>api</code> service layer for coordinating calls that might fail.</li> </ul>"},{"location":"adr/ADR-009-resilience-partterns-by-decorators/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Improved Code Readability &amp; Focus: Business logic within decorated functions remains clean and focused on its primary responsibility, as the resilience logic is handled declaratively by the decorator.</li> <li>Reduced Boilerplate: Significantly reduces the need for repetitive <code>try...except</code> blocks and manual retry/fallback logic throughout the codebase.</li> <li>Consistency: Promotes a standardized and consistent way of implementing retry and fallback mechanisms across the application.</li> <li>Reusability: Decorators are highly reusable components that can be applied to any compatible function (sync/async depending on the decorator).</li> <li>Configurability: The behavior of retries (attempts, backoff, specific exceptions) and the fallback chain are configurable per usage.</li> <li>Leverages Existing Library: The retry implementation benefits from the robustness and features of the <code>tenacity</code> library.</li> </ul> </li> <li>Negative:<ul> <li>Decorator Implementation Complexity: The decorators themselves, particularly <code>@fallback_handler</code> which needs to correctly manage both sync and async functions [source: 954-959], have non-trivial implementation logic.</li> <li>Potential Obscurity (\"Magic\"): For developers unfamiliar with decorators or the specific implementation, the added behavior might seem implicit or \"magical\". Clear documentation and naming are crucial.</li> <li>Debugging: Debugging issues within the decorator's execution flow can sometimes be less straightforward than debugging inline code.</li> <li>Incomplete Resilience Suite (Currently): The critical patterns of Circuit Breaker and Timeout are currently missing from the implemented set, requiring future work to achieve a more comprehensive resilience strategy. [source: 186]</li> </ul> </li> <li>Neutral/Other:<ul> <li>Requires developers to understand when and where it's appropriate to apply the <code>@retry_handler</code> versus the <code>@fallback_handler</code> (or potentially both, though careful consideration of interaction is needed).</li> <li>The effectiveness of the fallback pattern is entirely dependent on the logic provided within the fallback functions; they must provide a meaningful alternative or default.</li> </ul> </li> </ul>"},{"location":"adr/ADR-010-development-tooling-stack-choice/","title":"ADR-010: Development Tooling Stack Choice","text":"<ul> <li>Status: Accepted (Largely Implemented)</li> <li>Date: 2025-04-13 (Placeholder)</li> </ul>"},{"location":"adr/ADR-010-development-tooling-stack-choice/#context","title":"Context","text":"<ul> <li>Problem: Building and maintaining a high-quality software project like <code>athomic-docs</code> requires a robust ecosystem of development tools beyond just a text editor and the Python interpreter. Without a standardized and automated tooling stack, projects suffer from inconsistencies in code style, undetected bugs, cumbersome dependency management, insecure practices, slow release cycles, and a poor developer experience (DX). Relying on manual checks for quality and security is unreliable and doesn't scale.</li> <li>Goal: Define and implement a modern, integrated, and efficient development tooling stack for the <code>athomic-docs</code> project. This stack should automate quality checks, enforce consistency, manage dependencies reliably, enhance security posture, streamline the release process, and ultimately improve developer productivity and code maintainability.</li> <li>Alternatives Considered:<ul> <li>Minimal/Manual Tooling: Relying only on <code>pip</code> and manual execution of checks. Drawback: Inefficient, inconsistent, error-prone, poor DX.</li> <li>Traditional <code>requirements.txt</code>/<code>setup.py</code>: Standard Python packaging but less integrated than Poetry for dependency locking, environment management, and build lifecycle.</li> <li>Separate Linters/Formatters (Flake8, isort, pycodestyle, Pylint): The traditional stack for code quality. Drawback: Slower overall performance compared to Ruff, requires managing multiple configurations and dependencies. [source: 549]</li> <li>Standard <code>unittest</code> Framework: Python's built-in testing library. Drawback: Generally considered more verbose and less feature-rich (e.g., powerful fixtures, plugin ecosystem) compared to pytest.</li> <li>Alternative CI/CD Systems (Jenkins, GitLab CI, CircleCI): Viable alternatives to GitHub Actions. Decision: GitHub Actions [source: 557] provides tight integration with the GitHub repository, secrets management, and a large marketplace of reusable actions.</li> <li>Manual Versioning/Release Process: Manually updating version numbers, creating tags, and writing changelogs. Drawback: Tedious, error-prone, easy to forget steps.</li> </ul> </li> </ul>"},{"location":"adr/ADR-010-development-tooling-stack-choice/#decision","title":"Decision","text":"<ul> <li>Standardize the <code>athomic-docs</code> project on the following integrated tooling stack, primarily configured via <code>pyproject.toml</code> where possible:<ul> <li>Dependency Management &amp; Packaging: Poetry [source: 4]. Used for declaring, installing, and locking project dependencies (both main and development), managing virtual environments (<code>.venv</code> in project [source: 26]), and building/publishing the package. Configuration in <code>pyproject.toml</code>.</li> <li>Code Formatting: Black [source: 4, 551]. Adopted as the automatic, opinionated code formatter to ensure a consistent style across the entire codebase, eliminating style debates. Configuration in <code>pyproject.toml</code>.</li> <li>Linting: Ruff [source: 4, 548]. Chosen as the extremely fast linter and code quality tool, configured to enforce a wide range of checks (including those traditionally covered by Flake8, isort, pyupgrade, etc.). Configuration in <code>pyproject.toml</code>.</li> <li>Type Checking: mypy [source: 6, 553]. Used for static type analysis to catch type errors before runtime. Configured via <code>mypy.ini</code> [source: 17] (or <code>pyproject.toml</code>), aiming for strict checks where practical.</li> <li>Testing Framework: pytest [source: 5, 555]. Selected for its concise syntax, powerful fixture system, extensive plugin ecosystem, and good support for testing asynchronous code via pytest-asyncio [source: 5]. Configuration in <code>pytest.ini</code> [source: 1].</li> <li>Local Development Hooks: pre-commit [source: 6, 30]. Used to configure and run linters (Ruff), formatters (Black), type checkers (mypy), security scanners (Bandit, detect-secrets), and potentially other checks automatically before each commit, providing fast feedback to developers. Configured via <code>.pre-commit-config.yaml</code> (implied by setup script [source: 30]).</li> <li>Continuous Integration (CI): GitHub Actions [source: 23, 557]. Used to automatically run the full suite of quality checks (linting, formatting check, type checking, unit tests, integration tests, security scans) on every push and pull request to the main branches. Workflow(s) defined in <code>.github/workflows/</code>. [source: 558-561]</li> <li>Automated Versioning &amp; Release: python-semantic-release [source: 4, 199]. Adopted to automate version management according to Semantic Versioning, generate <code>CHANGELOG.md</code> [source: 2] entries, create Git tags, and draft GitHub releases based on Conventional Commit messages. Configured in <code>pyproject.toml</code>.</li> <li>Security Scanning:<ul> <li>Bandit [source: 6, 562]: For static analysis targeting common Python security vulnerabilities within the application code (<code>src/</code>).</li> <li>detect-secrets [source: 6, 611]: To scan the codebase for accidentally committed secrets, managed via an audited <code>.secrets.baseline</code> file [source: 614].</li> <li>(Potentially <code>Safety</code> [source: 562] or Poetry's built-in checks for dependency vulnerabilities).</li> </ul> </li> <li>Documentation Generation: MkDocs [source: 6] with the MkDocs Material theme [source: 6] and the mkdocstrings plugin [source: 6] for generating user-friendly project documentation from Markdown files in the <code>docs/</code> directory and from Python docstrings within the code.</li> <li>Helper Scripting Framework: Click [source: 645]. Used for building internal command-line interface (CLI) tools provided in the <code>helpers/</code> directory (e.g., <code>snapshot.py</code>, <code>mypy-fix</code>, <code>fix-callables</code>).</li> </ul> </li> </ul>"},{"location":"adr/ADR-010-development-tooling-stack-choice/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>High Code Quality &amp; Consistency: Automated enforcement of linting, formatting, and type rules leads to cleaner, more readable, and more maintainable code.</li> <li>Improved Developer Productivity (DX): Automation reduces manual effort (formatting, dependency management, releases). Fast local feedback via <code>pre-commit</code>. Poetry simplifies environment and dependency handling.</li> <li>Increased Reliability &amp; Confidence: Comprehensive automated testing via <code>pytest</code> and CI builds confidence in code changes and helps prevent regressions.</li> <li>Enhanced Security Posture: Automated security scanning (Bandit, detect-secrets) helps identify and prevent common vulnerabilities and accidental secret exposure early in the development cycle.</li> <li>Modern &amp; Standardized: Utilizes tools widely recognized as modern best practices in the Python ecosystem, facilitating onboarding for developers familiar with these tools.</li> <li>Streamlined Releases: Semantic Release automates the often tedious and error-prone process of versioning and releasing software.</li> </ul> </li> <li>Negative:<ul> <li>Initial Setup &amp; Configuration Overhead: Setting up and configuring all these tools requires initial effort (though much is consolidated in <code>pyproject.toml</code>).</li> <li>Learning Curve: Developers need to become familiar with the workflow and conventions imposed by the tools (e.g., Poetry commands, Conventional Commits for semantic-release, interpreting Ruff/mypy errors).</li> <li>Rigid Workflow: Automated checks, especially in <code>pre-commit</code> and CI, enforce standards strictly. This can initially feel restrictive or slow down developers unaccustomed to such checks.</li> <li>Tooling Maintenance: The tool configurations and versions need to be kept up-to-date. CI workflows might require occasional adjustments.</li> </ul> </li> <li>Neutral/Other:<ul> <li>The stack is heavily integrated with the Python ecosystem (Poetry, PyPI) and GitHub (Actions, Releases, Dependabot).</li> <li>Adopting Conventional Commits is necessary to fully leverage <code>python-semantic-release</code>.</li> </ul> </li> </ul>"},{"location":"adr/ADR-011-pybreaker-lib-for-cirtuit-breaker-pattern-implementatio/","title":"ADR-011: Use aiobreaker Library and Refactored Structure for Circuit Breaker","text":"<ul> <li>Status: Accepted (Implemented)</li> <li>Date: 2025-04-14</li> </ul>"},{"location":"adr/ADR-011-pybreaker-lib-for-cirtuit-breaker-pattern-implementatio/#context","title":"Context","text":"<ul> <li>Problem: The application requires protection against cascading failures caused by repeatedly calling dependencies that are experiencing persistent or systemic issues. While Retry (ADR-009) handles transient errors and Fallback (ADR-009) provides alternatives, a mechanism is needed to detect ongoing failures and temporarily block calls to the failing dependency (Circuit Breaker pattern). Initial attempts to use the <code>pybreaker</code> library encountered difficulties with its asynchronous function handling (<code>NameError: name 'gen' is not defined</code>) and inconsistencies in its constructor API across versions or environments (<code>TypeError</code> for arguments like <code>exclude_exception</code> and <code>throw_new_error_on_trip</code>), leading to significant debugging effort. Furthermore, the initial implementation combined the decorator, registry, and listener logic into a single file, reducing maintainability.</li> <li>Goal: Implement a robust, maintainable, and <code>asyncio</code>-native Circuit Breaker pattern within <code>athomic-docs</code>, consistent with existing resilience patterns (decorators) and project structure guidelines. Resolve the issues encountered with <code>pybreaker</code>.</li> <li>Alternatives Considered:<ol> <li>Manual Implementation: Rejected due to high complexity and risk of errors in state management and concurrency.</li> <li>Continue with <code>pybreaker</code> + Workarounds: Attempting workarounds like installing <code>tornado</code> or further debugging the constructor issues. Rejected due to introducing unnecessary dependencies (<code>tornado</code>) or uncertainty about library stability/API.</li> <li>Other Async Libraries (<code>async-circuitbreaker</code>, etc.): Considered, but <code>aiobreaker</code> seemed a well-known async-focused alternative to <code>pybreaker</code>.</li> <li>Infrastructure-Level Circuit Breaking: Viable as a complementary layer, but insufficient for application-level control and context.</li> </ol> </li> </ul>"},{"location":"adr/ADR-011-pybreaker-lib-for-cirtuit-breaker-pattern-implementatio/#decision","title":"Decision","text":"<ol> <li>Adopt <code>aiobreaker</code> Library: Switch from <code>pybreaker</code> to the <code>aiobreaker</code> library as the underlying implementation for the Circuit Breaker logic. <code>aiobreaker</code> is explicitly designed for <code>asyncio</code>, resolving the <code>NameError</code> related to <code>tornado.gen</code>.</li> <li>Refactor Module Structure: Move the circuit breaker implementation from <code>utils/decorators/</code> to a dedicated sub-package <code>nala.athomic.resilience.circuit_breaker/</code>. Divide the logic into separate files for better Separation of Concerns:<ul> <li><code>registry.py</code>: Manages the creation and storage (<code>_breakers</code> dict, <code>_registry_lock</code>) of named <code>aiobreaker.CircuitBreaker</code> instances using the <code>get_or_create_breaker</code> factory function.</li> <li><code>listeners.py</code>: Defines the <code>LoggingCircuitBreakerListener</code> class responsible for logging state changes, failures, and successes using the application's logger. Contains the <code>global_logging_listener</code> instance.</li> <li><code>exceptions.py</code>: Re-exports <code>aiobreaker.CircuitBreakerError</code> for consistent usage.</li> <li><code>decorator.py</code>: Contains the user-facing <code>@circuit_breaker</code> decorator factory and its internal <code>sync_wrapper</code> and <code>async_wrapper</code> functions, which utilize <code>get_or_create_breaker</code> and the correct <code>breaker.call</code>/<code>breaker.call_async</code> methods.</li> <li><code>__init__.py</code>: Exports the public symbols (<code>circuit_breaker</code>, <code>CircuitBreakerError</code>).</li> </ul> </li> <li>API Alignment: Update the implementation (decorator, registry, listeners, tests) to use the correct <code>aiobreaker</code> API, including:<ul> <li>Class name: <code>aiobreaker.CircuitBreaker</code>.</li> <li>Constructor parameters: <code>fail_max</code>, <code>timeout_duration</code> (expects <code>timedelta</code>), <code>exclude</code> (expects iterable), <code>name</code>, <code>listeners</code> (added via <code>add_listener</code>). Parameters like <code>throw_new_error_on_trip</code> are not used.</li> <li>Execution methods: <code>breaker.call()</code> for sync, <code>await breaker.call_async()</code> for async.</li> <li>State comparison: Use <code>type(breaker.state) is aiobreaker.CircuitBreakerState.XXX.value</code>.</li> <li>Exception: <code>aiobreaker.CircuitBreakerError</code>.</li> <li>Listener interface: Use simple classes with methods like <code>state_changed(cb, old_state, new_state)</code>, <code>failure(cb, exc)</code>, <code>success(cb)</code>.</li> </ul> </li> </ol>"},{"location":"adr/ADR-011-pybreaker-lib-for-cirtuit-breaker-pattern-implementatio/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Resolves <code>pybreaker</code> Issues: Eliminates the <code>NameError</code> related to <code>tornado.gen</code> and <code>TypeError</code>s from incorrect constructor parameters encountered previously. Works natively with <code>asyncio</code>.</li> <li>Improved Maintainability: Refactoring into smaller, focused files (<code>registry.py</code>, <code>listeners.py</code>, <code>decorator.py</code>) makes the code easier to understand, test, and modify, adhering to project guidelines.</li> <li>Robust Foundation: Leverages the tested state machine logic of the <code>aiobreaker</code> library.</li> <li>Consistency: Maintains the declarative decorator pattern used for other resilience features (<code>@retry_handler</code>, <code>@fallback_handler</code>).</li> <li>Clearer Structure: Places the Circuit Breaker logic within a more appropriate <code>resilience</code> package.</li> <li>Observability: Retains logging of important state changes and events via the listener.</li> </ul> </li> <li>Negative:<ul> <li>Refactoring Effort: Required significant changes to the decorator, registry, listener implementation, and associated unit tests.</li> <li>New Dependency: Replaced <code>pybreaker</code> dependency with <code>aiobreaker</code>.</li> <li>State Scope Limitation: The primary limitation remains: the default implementation uses in-memory state storage (<code>CircuitMemoryStorage</code>), meaning the circuit state is not shared across multiple horizontally scaled instances of the application. Distributed circuit breaking would require implementing or using a shared <code>aiobreaker.CircuitBreakerStorage</code> (e.g., Redis-based).</li> <li>Library Nuances: Still dependent on understanding the specific API and behavior of <code>aiobreaker</code> (e.g., how <code>exclude</code> works, when exactly listeners are called, how state transitions are triggered).</li> </ul> </li> <li>Neutral:<ul> <li>Still requires careful tuning of <code>fail_max</code> and <code>reset_timeout</code> per use case.</li> <li>Complements Retry and Fallback patterns as part of a larger resilience strategy.</li> </ul> </li> </ul>"},{"location":"adr/ADR-012-refact-loggin-to-solve-circular-dependence/","title":"ADR-012: Refactor Logging Initialization to Resolve Circular Dependencies","text":"<p>Status: Implemented</p> <p>Date: 2025-04-15</p>"},{"location":"adr/ADR-012-refact-loggin-to-solve-circular-dependence/#context","title":"Context","text":"<p>During test execution (<code>make test</code>), the application encountered an <code>ImportError</code> due to a circular dependency between the logging configuration modules and the observability setup/filter modules. Specifically, the main cycle involved:</p> <ol> <li><code>config/schemas/logging_config.py</code>: Needed access to information from the masker registry (<code>observability/log/registry.py</code>) or resolver functions (<code>log_masker_resolver.py</code>) to validate the <code>sensitive_patterns</code> configuration.</li> <li><code>observability/log/setup.py</code>: Needed to import the <code>LoggingSettings</code> model from <code>config/schemas/logging_config.py</code> to apply settings to the logger (Loguru).</li> <li><code>observability/log/filters/sensitive_data_filter.py</code>: Needed the registry (<code>registry.py</code>) and the resolver (<code>log_masker_resolver.py</code>), and was instantiated by <code>setup.py</code>.</li> <li>Additional imports in <code>observability/log/__init__.py</code> and <code>config/schemas/__init__.py</code> completed or exacerbated the cycle, leading to errors where names could not be imported from partially initialized modules.</li> <li>Redundant code for handling log levels (<code>LOG_LEVELS</code>, <code>get_log_level</code>) was also identified, duplicating functionality from the standard <code>logging</code> module.</li> </ol> <p>This issue prevented test execution and potentially the correct application startup in certain import scenarios.</p>"},{"location":"adr/ADR-012-refact-loggin-to-solve-circular-dependence/#decision","title":"Decision","text":"<p>To break the circular dependency and improve the separation of concerns, the following changes were implemented:</p> <ol> <li> <p>Decoupling of <code>setup.py</code> and <code>logging_config.py</code>:</p> <ul> <li>The <code>observability/log/setup.py::configure_logging</code> function was modified to receive the <code>LoggingSettings</code> instance as an argument instead of importing it directly.</li> <li>The responsibility of obtaining the settings (<code>get_settings()</code>) and passing the <code>settings.logging</code> section was moved to the call site of <code>configure_logging</code> (usually in the application startup).</li> </ul> </li> <li> <p>Centralization of Pattern Processing Logic in the Filter:</p> <ul> <li>The responsibility for processing the <code>sensitive_patterns</code> list (compiling regex, resolving masker names via <code>resolve_masker</code> or using callables, and ordering the patterns) was moved entirely into the constructor (<code>__init__</code>) of the <code>observability/log/filters/sensitive_data_filter.py::SensitiveDataFilter</code> class.</li> <li>The helper function <code>_parse_sensitive_patterns</code> was removed from <code>observability/log/setup.py</code>.</li> <li><code>configure_logging</code> now instantiates <code>SensitiveDataFilter</code> by passing the raw configuration list (<code>log_settings.sensitive_patterns</code>).</li> </ul> </li> <li> <p>Simplification of the Pydantic Validator:</p> <ul> <li>The <code>@field_validator(\"sensitive_patterns\")</code> in <code>config/schemas/logging_config.py</code> was simplified to validate only the structure and basic types of the list items (presence of keys, string/callable type, regex validity), without attempting to resolve or validate masker names against the registry. The call to <code>resolve_masker</code> was removed from this validator.</li> </ul> </li> <li> <p>Removal of Explicit Pattern Import:</p> <ul> <li>The explicit import of <code>default_patterns</code> within <code>observability/log/__init__.py</code> (which aimed to pre-populate the registry) was removed to simplify initialization and avoid side effects on import order. Registry population now occurs naturally when modules that use it (like <code>SensitiveDataFilter</code>) are imported.</li> </ul> </li> <li> <p>Usage of Native Logging Functions:</p> <ul> <li>The custom dictionary <code>LOG_LEVELS</code> and function <code>get_log_level</code> in <code>observability/log/utils/log_levels.py</code> were removed.</li> <li>Validators and functions needing to convert log level names to integers now directly use <code>logging._checkLevel()</code>, leveraging Python's native functionality.</li> </ul> </li> </ol>"},{"location":"adr/ADR-012-refact-loggin-to-solve-circular-dependence/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Complete elimination of <code>ImportError</code> related to circular dependencies in the logging system.</li> <li>Improved separation of concerns:<ul> <li>Pydantic Schemas (<code>logging_config.py</code>) focus on validating configuration structure.</li> <li>Logging Setup (<code>setup.py</code>) focuses on orchestrating the logger configuration (Loguru).</li> <li>Filter (<code>sensitive_data_filter.py</code>) encapsulates all the processing and application logic for masking.</li> </ul> </li> <li>Reduced coupling between configuration and observability modules.</li> <li>Cleaner and more maintainable code with the removal of redundant log level logic.</li> <li>Increased robustness of application startup.</li> </ul> </li> <li>Negative:<ul> <li>Validation that a masker name provided in the configuration corresponds to a registered masker now occurs slightly later (during <code>SensitiveDataFilter</code> instantiation within <code>configure_logging</code>), instead of during the initial Pydantic model validation. This is considered an acceptable trade-off for resolving the cycle. Configuration errors related to invalid names will now be detected (and logged) during filter initialization.</li> <li>A failure during <code>SensitiveDataFilter</code> initialization (e.g., an unrecoverable error resolving a masker) could result in logging without masking, although error handling was added in <code>configure_logging</code> to log this failure critically.</li> </ul> </li> </ul>"},{"location":"adr/ADR-012-refact-loggin-to-solve-circular-dependence/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>Keeping the masker name validation in the Pydantic validator and attempting to break the cycle in other ways (e.g., complex conditional imports), but this was considered more fragile and less clear than moving the responsibility to the filter.</li> <li>Moving the masker resolution/validation logic to <code>setup.py</code> instead of the filter. This was rejected because it mixed orchestration and filter logic responsibilities within the same module.</li> </ul>"},{"location":"adr/ADR-013-athomic-modular-architecture/","title":"ADR-013: Modular Architecture for the Nala Athomic Package","text":"<p>Status: Implemented</p> <p>Date: 2025-04-15</p>"},{"location":"adr/ADR-013-athomic-modular-architecture/#context","title":"Context","text":"<p>The <code>nala-athomic</code> package aims to provide a set of core, reusable functionalities (an internal \"framework\" or core library) for Nala applications, covering areas such as configuration, database access, observability, resilience, security, etc. As the complexity and number of features within <code>athomic</code> grow, a well-defined directory and module structure becomes crucial. Without clear modularization, the codebase can become difficult to navigate, maintain, test, and extend. Cross-cutting concerns could become entangled, increasing coupling and hindering reusability and collaboration. It was necessary to define a standard structure to organize the code within <code>src/nala/athomic</code>.</p>"},{"location":"adr/ADR-013-athomic-modular-architecture/#decision","title":"Decision","text":"<p>A modular architecture was adopted for the <code>nala-athomic</code> package, dividing the code into high-level Python packages based on distinct architectural concerns. The primary structure decided upon is:</p> <ol> <li> <p>Top-Level Packages by Concern: The code was organized into the following main directories/packages, each focused on a specific area:</p> <ul> <li><code>config</code>: Configuration management (reading files, environment variables, Pydantic schemas).</li> <li><code>database</code>: Database access (interfaces, NoSQL/SQL implementations, migrations).</li> <li><code>integration</code>: Communication with external services (queues, service discovery like Consul).</li> <li><code>observability</code>: Features related to monitoring and debugging (logging, metrics, tracing, health checks).</li> <li><code>performance</code>: Performance optimizations (caching, load balancing).</li> <li><code>resilience</code>: Resilience patterns (retry, circuit breaker, rate limiter, fallback).</li> <li><code>security</code>: Security features (authentication, authorization, secret management, cryptography).</li> <li><code>storage</code>: Access to file storage systems (if needed, e.g., <code>filestore</code>).</li> <li><code>utils</code>: Generic and cross-cutting utilities (date/string manipulation, custom exceptions, generic decorators).</li> </ul> </li> <li> <p>Internal Subdivision: Each top-level package is further subdivided into more specific modules or subpackages, reflecting the internal structure of that concern. Examples:</p> <ul> <li><code>observability</code> contains <code>log</code>, <code>metrics</code>, <code>tracing</code>, <code>health</code>.</li> <li><code>security</code> contains <code>auth</code>, <code>secrets</code>, <code>crypto</code>.</li> <li><code>config</code> contains <code>providers</code> and <code>schemas</code>.</li> </ul> </li> <li> <p>Organization of Implementations and Interfaces:</p> <ul> <li>Concrete implementations (Providers, Repositories) are generally nested within their respective modules (e.g., <code>config/providers</code>, <code>security/secrets/providers</code>, <code>database/nosql/mongo</code>).</li> <li>Interfaces (using <code>Protocol</code> or abstract base classes <code>ABC</code>) are often defined separately to promote low coupling and dependency inversion (e.g., <code>database/nosql/repository_interfaces.py</code>, <code>performance/cache/interfaces.py</code>).</li> </ul> </li> <li> <p>Utilities: Utility functions are grouped either within the relevant submodule (e.g., <code>observability/log/utils</code>) or in the top-level <code>utils</code> package if they are truly generic and used across multiple packages.</p> </li> </ol>"},{"location":"adr/ADR-013-athomic-modular-architecture/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Improved Maintainability: Easier to locate and modify code related to a specific feature (e.g., all rate limiting logic is under <code>resilience/rate_limiter</code>).</li> <li>Enhanced Readability &amp; Navigability: The clear structure makes the codebase easier to understand for existing and new developers.</li> <li>Better Testability: Modules with well-defined responsibilities are easier to test in isolation (unit testing).</li> <li>Reduced Coupling: Clear boundaries between packages minimize unnecessary dependencies between different functional areas.</li> <li>Increased Reusability: Individual modules (like <code>resilience</code> or specific <code>maskers</code>) could potentially be easier to extract or reuse in other contexts if needed.</li> <li>Facilitates Collaboration: Different developers or teams can work on distinct modules concurrently with fewer conflicts.</li> <li>Clear Separation of Concerns: Each package has a well-defined focus.</li> </ul> </li> <li>Negative:<ul> <li>Potential for Over-Granularity (Minor): In some very simple cases, creating many small modules might seem like overkill. However, for a library/framework like <code>athomic</code>, the organizational benefits usually outweigh this drawback.</li> <li>Requires Discipline: Maintaining the structure consistently requires ongoing discipline during development to avoid placing code in the wrong location.</li> </ul> </li> </ul>"},{"location":"adr/ADR-013-athomic-modular-architecture/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>Monolithic Structure: Placing all code directly under <code>athomic</code> without subpackages. Rejected for significantly hindering maintainability and navigation.</li> <li>Different Grouping: Grouping by component type (e.g., a <code>decorators</code> package, a <code>providers</code> package) instead of by architectural concern. Rejected as it could potentially increase coupling within features and make it harder to locate all code related to an area like \"security\" or \"observability.\" Grouping by architectural concern was deemed superior.</li> </ul>"},{"location":"adr/ADR-014-pytest-standarrize-asyncio-event-loop/","title":"ADR-014: Standardize asyncio Event Loop Management in Pytest","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-04-17</li> </ul>"},{"location":"adr/ADR-014-pytest-standarrize-asyncio-event-loop/#context","title":"Context","text":"<p>The project utilizes <code>pytest</code> and <code>pytest-asyncio</code> for running asynchronous tests, particularly within the messaging integration modules (<code>nala.athomic.integration.messaging</code>). Previously, a custom <code>event_loop</code> fixture with <code>session</code> scope was defined in <code>tests/conftest.py</code> in an attempt to manage the event loop.</p> <p>During test execution, especially when attempting to enable <code>pytest-asyncio</code>'s <code>strict</code> mode for better practices, several issues arose: 1.  <code>DeprecationWarning</code> warnings regarding the redefinition of the <code>event_loop</code> fixture managed by <code>pytest-asyncio</code>. 2.  <code>TypeError: loop must be an instance of AbstractEventLoop or None, not 'async_generator'</code> occurring during fixture setup, indicating a conflict in the loop policy or the type of loop being passed. 3.  <code>RuntimeError: Event loop is closed</code> in multiple asynchronous tests when using <code>asyncio_mode = strict</code>, suggesting problems with the loop's lifecycle and resource/task cleanup. 4.  Difficulties in consistently mocking async iterators (<code>TypeError: 'async for' received an object from __aiter__ ...</code>).</p> <p>Analysis indicated that the custom <code>session</code>-scoped <code>event_loop</code> fixture was incompatible with <code>pytest-asyncio</code>'s function-scoped loop management model (the default and recommended approach), causing the observed conflicts and errors. Attempting to maintain and fix the custom fixture proved complex and fragile.</p>"},{"location":"adr/ADR-014-pytest-standarrize-asyncio-event-loop/#decision","title":"Decision","text":"<ol> <li>Remove the custom <code>event_loop</code> fixture from <code>tests/conftest.py</code>.</li> <li>Explicitly configure <code>pytest-asyncio</code> via <code>pytest.ini</code>:     <code>ini     [pytest]     # ...     asyncio_mode = strict     asyncio_default_fixture_loop_scope = function     # ...</code></li> <li>Fully delegate to <code>pytest-asyncio</code> the responsibility for creating, providing, and closing the event loop for each test (function scope).</li> <li>Adopt as mandatory practice the explicit cleanup of asynchronous resources (e.g., Kafka clients, connections, created tasks) at the end of each test that uses them, preferably using <code>try...finally</code> blocks with <code>await resource.close()</code>.</li> </ol>"},{"location":"adr/ADR-014-pytest-standarrize-asyncio-event-loop/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Simplification: Removes custom and complex loop management code from <code>conftest.py</code>.</li> <li>Standardization: Aligns the project with the recommended practices and default behavior of <code>pytest-asyncio</code>.</li> <li>Error Resolution: Fixes the specific <code>TypeError</code>s and <code>RuntimeError</code>s encountered during the setup and execution of asynchronous tests.</li> <li>Robustness: The <code>strict</code> mode helps identify issues related to unclosed resources or pending asyncio tasks earlier, making tests more reliable.</li> <li>Clarity: The responsibility for the event loop lifecycle is clearly assigned to <code>pytest-asyncio</code>.</li> </ul> </li> <li>Negative:<ul> <li>Test Discipline Required: Requires greater attention and discipline from developers when writing asynchronous tests to ensure that all resources needing <code>await close()</code> (or similar cleanup) are properly closed in <code>finally</code> blocks. (Also considered a good practice).</li> </ul> </li> <li>Neutral/Other:<ul> <li>The explicit configuration of <code>pytest-asyncio</code>'s behavior is now centralized in <code>pytest.ini</code>.</li> <li>The mocking strategy for async iterators (specifically for <code>AIOKafkaConsumer</code>) was adjusted to work correctly with this approach (using <code>mock_instance.__aiter__.return_value = mock_instance</code> and <code>mock_instance.__anext__.side_effect = [...]</code>).</li> </ul> </li> </ul>"},{"location":"adr/ADR-015-locking-distribuido-cache/","title":"ADR-015: Mecanismo de Locking Distribu\u00eddo para Cache Decorator","text":"<p>Status: Aceito</p>"},{"location":"adr/ADR-015-locking-distribuido-cache/#contexto","title":"\ud83c\udfaf Contexto","text":"<p>O decorator <code>@cache_result</code> [source: 2346] possui a op\u00e7\u00e3o <code>use_lock=True</code> com o objetivo de prevenir o problema de cache stampede (m\u00faltiplas inst\u00e2ncias/requisi\u00e7\u00f5es recalculando o mesmo valor de cache simultaneamente quando ele expira). A implementa\u00e7\u00e3o inicial usava um <code>asyncio.Lock</code> [source: 2384], que s\u00f3 garante exclus\u00e3o m\u00fatua dentro de um \u00fanico processo/inst\u00e2ncia da aplica\u00e7\u00e3o.</p> <p>Para ambientes distribu\u00eddos com m\u00faltiplas inst\u00e2ncias da API rodando (ex: Kubernetes, m\u00faltiplos workers Gunicorn/Uvicorn), o <code>asyncio.Lock</code> \u00e9 insuficiente, pois cada inst\u00e2ncia teria seu pr\u00f3prio lock independente, n\u00e3o prevenindo o stampede entre inst\u00e2ncias.</p> <p>\u00c9 necess\u00e1rio um mecanismo de locking distribu\u00eddo, utilizando um recurso compartilhado como o Redis (que j\u00e1 \u00e9 uma depend\u00eancia e op\u00e7\u00e3o de backend para o cache [source: 2356]), e que seja compat\u00edvel com a natureza ass\u00edncrona (<code>asyncio</code>) do projeto.</p>"},{"location":"adr/ADR-015-locking-distribuido-cache/#decisao","title":"\ud83d\ude80 Decis\u00e3o","text":"<p>Adotar o mecanismo de locking distribu\u00eddo nativo fornecido pela biblioteca <code>redis-py</code> (vers\u00e3o &gt;= 4.2), especificamente atrav\u00e9s do m\u00e9todo <code>redis.asyncio.Redis.lock()</code>.</p> <p>A implementa\u00e7\u00e3o ser\u00e1 feita na fun\u00e7\u00e3o helper <code>acquire_lock</code> (localizada em <code>src/nala/athomic/performance/cache/looking/registry.py</code>), que encapsular\u00e1 a l\u00f3gica: 1.  Obter o cliente <code>redis.asyncio.Redis</code> configurado atrav\u00e9s do factory <code>get_redis_client_for_infra</code>. 2.  Se o cliente Redis n\u00e3o estiver dispon\u00edvel, usar como fallback o <code>asyncio.Lock</code> (mantendo a prote\u00e7\u00e3o intra-processo e evitando falhas). 3.  Se o cliente Redis estiver dispon\u00edvel, instanciar o lock nativo: <code>redis_client.lock(name=lock_key, timeout=lock_timeout_sec, blocking=True, blocking_timeout=blocking_timeout)</code>. 4.  Utilizar o objeto lock retornado como um context manager ass\u00edncrono (<code>async with lock_instance:</code>) para adquirir e liberar o lock automaticamente. 5.  Tratar exce\u00e7\u00f5es espec\u00edficas do <code>redis-py</code> (<code>LockError</code>, <code>LockNotOwnedError</code>) dentro do <code>acquire_lock</code> para fornecer um comportamento consistente e evitar propaga\u00e7\u00e3o de erros inesperados na libera\u00e7\u00e3o.</p>"},{"location":"adr/ADR-015-locking-distribuido-cache/#alternativas-consideradas","title":"\ud83e\udd14 Alternativas Consideradas","text":"<ol> <li> <p><code>redis-lock==0.2.0</code>:</p> <ul> <li>Foi a primeira biblioteca considerada e parcialmente implementada.</li> <li>Desvantagem: Verificou-se que esta vers\u00e3o \u00e9 s\u00edncrona. Sua utiliza\u00e7\u00e3o com o cliente <code>redis.asyncio</code> causou <code>TypeError</code>s (tentativa de tratar corrotinas como valores s\u00edncronos) e tornou os testes unit\u00e1rios complexos e a simula\u00e7\u00e3o de concorr\u00eancia problem\u00e1tica.</li> <li>Resultado: Abandonada devido \u00e0 incompatibilidade fundamental com o ambiente ass\u00edncrono do projeto.</li> </ul> </li> <li> <p><code>aioredlock</code>:</p> <ul> <li>Vantagem: Implementa o algoritmo Redlock completo, oferecendo maior robustez te\u00f3rica em cen\u00e1rios de alta disponibilidade com m\u00faltiplos mestres Redis independentes. \u00c9 compat\u00edvel com <code>asyncio</code>.</li> <li>Desvantagem: Adiciona uma depend\u00eancia externa nova ao projeto. Requer configura\u00e7\u00e3o mais complexa (m\u00faltiplos endpoints Redis) para obter os benef\u00edcios do Redlock. Considerado um exagero para os requisitos atuais e cen\u00e1rios de implanta\u00e7\u00e3o mais comuns (Redis \u00fanico, Cluster ou Sentinel gerenciados).</li> <li>Resultado: Descartada por enquanto, mas mantida como uma alternativa futura caso as garantias do Redlock se tornem um requisito expl\u00edcito.</li> </ul> </li> <li> <p><code>asyncio.Lock</code>:</p> <ul> <li>Implementa\u00e7\u00e3o inicial usada no helper <code>acquire_lock</code>.</li> <li>Desvantagem: Ineficaz em ambientes distribu\u00eddos/multi-inst\u00e2ncia.</li> <li>Resultado: Mantido apenas como mecanismo de fallback para quando o Redis n\u00e3o estiver dispon\u00edvel, garantindo que a aplica\u00e7\u00e3o n\u00e3o falhe nesses casos (embora a prote\u00e7\u00e3o distribu\u00edda seja perdida).</li> </ul> </li> </ol>"},{"location":"adr/ADR-015-locking-distribuido-cache/#consequencias","title":"\u2728 Consequ\u00eancias","text":"<p>Positivas:</p> <ul> <li>Resolve efetivamente o problema de cache stampede em implanta\u00e7\u00f5es multi-inst\u00e2ncia que utilizam Redis.</li> <li>Utiliza a biblioteca <code>redis-py</code>, que j\u00e1 \u00e9 uma depend\u00eancia do projeto para o cache Redis, evitando adicionar novas depend\u00eancias externas.</li> <li>Integra-se bem com a infraestrutura <code>asyncio</code> existente e com o factory do cliente Redis (<code>get_redis_client_for_infra</code>).</li> <li>A API do lock nativo (<code>client.lock()</code> usado com <code>async with</code>) \u00e9 relativamente clara e idiom\u00e1tica para Python ass\u00edncrono.</li> </ul> <p>Negativas:</p> <ul> <li>A funcionalidade de locking distribu\u00eddo agora depende de uma inst\u00e2ncia Redis configurada e dispon\u00edvel. O fallback para <code>asyncio.Lock</code> previne erros, mas n\u00e3o oferece a prote\u00e7\u00e3o distribu\u00edda.</li> <li>N\u00e3o implementa o algoritmo Redlock completo. Embora considerado suficiente para a maioria dos casos, n\u00e3o possui as mesmas garantias te\u00f3ricas do <code>aioredlock</code> contra falhas em cen\u00e1rios com m\u00faltiplos mestres Redis independentes.</li> <li>Testar o comportamento concorrente de forma 100% precisa ainda pode ser desafiador apenas com testes unit\u00e1rios e mocks; testes de integra\u00e7\u00e3o com um Redis real s\u00e3o recomendados para validar completamente a preven\u00e7\u00e3o de race conditions.</li> </ul>"},{"location":"adr/ADR-016-repository-multi-tenancy/","title":"ADR-016: Estrat\u00e9gia Multi-Tenancy Configur\u00e1vel para Reposit\u00f3rios","text":"<ul> <li>Status: Aceito</li> <li>Data: 2025-04-18</li> </ul>"},{"location":"adr/ADR-016-repository-multi-tenancy/#contexto","title":"Contexto","text":"<p>O <code>athomic-docs</code> precisa suportar implanta\u00e7\u00f5es no estilo SaaS (Software as a Service), onde uma \u00fanica inst\u00e2ncia da aplica\u00e7\u00e3o atende a m\u00faltiplos clientes (tenants). Isso exige um mecanismo robusto para garantir o isolamento estrito dos dados de cada tenant na camada de persist\u00eancia (<code>athomic/database</code>). A solu\u00e7\u00e3o precisa ser consistente, aplic\u00e1vel a diferentes backends de banco de dados (MongoDB inicialmente, potencialmente SQL no futuro), configur\u00e1vel para permitir flexibilidade (ex: rodar em modo single-tenant ou fazer rollout gradual) e integrada com o sistema de contexto j\u00e1 planejado (<code>tenant_id_ctx_var</code> populado por middleware).</p>"},{"location":"adr/ADR-016-repository-multi-tenancy/#alternativas-consideradas","title":"Alternativas Consideradas","text":"<ol> <li>Filtros Manuais nos Servi\u00e7os: Exigir que a camada de servi\u00e7o (acima do reposit\u00f3rio) sempre adicione filtros de <code>tenant_id</code>.<ul> <li>Rejeitado: Acopla a l\u00f3gica de isolamento aos servi\u00e7os, leva \u00e0 repeti\u00e7\u00e3o, aumenta a chance de erros/esquecimentos.</li> </ul> </li> <li>L\u00f3gica Fixa nos Reposit\u00f3rios: Implementar o filtro de tenant diretamente nos reposit\u00f3rios sem op\u00e7\u00e3o de desativa\u00e7\u00e3o.<ul> <li>Rejeitado: Menos flex\u00edvel para cen\u00e1rios single-tenant, testes ou rollout gradual.</li> </ul> </li> <li>L\u00f3gica Condicional nos Reposit\u00f3rios (com flag): Implementar a l\u00f3gica de filtro/inje\u00e7\u00e3o de <code>tenant_id</code> nos reposit\u00f3rios, mas ativ\u00e1-la/desativ\u00e1-la com uma flag de configura\u00e7\u00e3o.<ul> <li>Aceito: Oferece flexibilidade e encapsulamento.</li> </ul> </li> <li>Defini\u00e7\u00e3o da Coluna <code>tenant_id</code>:<ul> <li>a) Manualmente em cada modelo: Propenso a inconsist\u00eancias no nome, tipo ou indexa\u00e7\u00e3o.</li> <li>b) Via Model Mixin (<code>TenantMixin</code>): Garante consist\u00eancia na defini\u00e7\u00e3o do campo. (Recomendado).</li> </ul> </li> <li>Reutiliza\u00e7\u00e3o da L\u00f3gica de Isolamento:<ul> <li>a) Nenhuma (Repeti\u00e7\u00e3o): Implementar a l\u00f3gica condicional separadamente em cada classe concreta (<code>MongoRepo</code>, <code>SqlRepo</code>). (Rejeitado: Duplica\u00e7\u00e3o).</li> <li>b) Base/Mixin \u00danica para Tudo: Criar uma super classe base/mixin com flags para tenancy, soft delete, auditoria, etc. (Rejeitado: Alta complexidade, viola SRP).</li> <li>c) Base/Mixin Focada (Tenancy) + Mixins Futuras: Criar uma base/mixin para tenancy agora, e outras separadas (SoftDelete, Auditing) no futuro. (Aceito: Equil\u00edbrio entre reuso e complexidade).</li> </ul> </li> <li>Inje\u00e7\u00e3o Din\u00e2mica de Mixins via Config: Usar metaclasses ou <code>type()</code> para construir a classe do reposit\u00f3rio com mixins definidos em <code>settings.toml</code>.<ul> <li>Rejeitado: Aumenta drasticamente a complexidade (\"m\u00e1gica\"), dificulta an\u00e1lise est\u00e1tica, depura\u00e7\u00e3o e tipagem. A flexibilidade adicional n\u00e3o compensa os contras para este caso de uso.</li> </ul> </li> </ol>"},{"location":"adr/ADR-016-repository-multi-tenancy/#decisao","title":"Decis\u00e3o","text":"<p>Ser\u00e1 implementada uma estrat\u00e9gia de multi-tenancy configur\u00e1vel na camada de reposit\u00f3rio (<code>athomic.database</code>), seguindo estes pontos:</p> <ol> <li>Configurabilidade: Uma nova flag booleana <code>multi_tenancy_enabled</code> (default: <code>False</code>) ser\u00e1 adicionada ao schema <code>DatabaseSettings</code> (<code>src/nala/athomic/config/schemas/db_config.py</code>) e lida a partir dos arquivos de configura\u00e7\u00e3o (<code>settings.toml</code>, etc.).</li> <li>L\u00f3gica Condicional: Os m\u00e9todos das classes de reposit\u00f3rio (inicialmente <code>MongoBeanieRepository</code>) ler\u00e3o a flag <code>multi_tenancy_enabled</code>.<ul> <li>Se <code>True</code>: A l\u00f3gica de isolamento de tenant ser\u00e1 aplicada:<ul> <li>Contexto: O <code>tenant_id</code> ser\u00e1 obtido via <code>get_tenant_id()</code> (que l\u00ea o <code>tenant_id_ctx_var</code>). Uma falha em obter o <code>tenant_id</code> (retornar <code>None</code>) resultar\u00e1 em erro ou retorno vazio para opera\u00e7\u00f5es de leitura e erro para opera\u00e7\u00f5es de escrita.</li> <li>Leituras (<code>find</code>, <code>find_one</code>, <code>get_by_id</code>): O filtro <code>{\"tenant_id\": current_tenant_id}</code> (ou equivalente SQL) ser\u00e1 automaticamente adicionado \u00e0s queries.</li> <li>Escritas (<code>save</code>): Para novos documentos, o <code>tenant_id</code> do contexto ser\u00e1 injetado no campo <code>tenant_id</code> da entidade antes de salvar. Para atualiza\u00e7\u00f5es, ser\u00e1 validado se o <code>tenant_id</code> da entidade existente corresponde ao <code>tenant_id</code> do contexto.</li> <li>Dele\u00e7\u00f5es (<code>delete</code>): A entidade ser\u00e1 buscada usando <code>get_by_id</code> (que j\u00e1 inclui o filtro de tenant) antes de prosseguir com a exclus\u00e3o.</li> </ul> </li> <li>Se <code>False</code>: Os m\u00e9todos executar\u00e3o a l\u00f3gica padr\u00e3o do ORM/ODM sem aplicar filtros ou inje\u00e7\u00f5es/valida\u00e7\u00f5es de <code>tenant_id</code>.</li> </ul> </li> <li>Defini\u00e7\u00e3o do Campo <code>tenant_id</code>: Recomenda-se fortemente o uso de uma classe Model Mixin (ex: <code>TenantMixin</code>) para definir o campo <code>tenant_id</code> (com tipo e \u00edndice apropriados) de forma consistente nos modelos de dados (ex: Beanie <code>Document</code>, SQLAlchemy <code>DeclarativeBase</code>) que armazenam dados espec\u00edficos de tenants.</li> <li>Reutiliza\u00e7\u00e3o da L\u00f3gica: A l\u00f3gica condicional de tenancy ser\u00e1 implementada na(s) classe(s) base/concreta(s) atual(is) (ex: <code>MongoBeanieRepository</code>), preferencialmente usando m\u00e9todos auxiliares privados para facilitar a organiza\u00e7\u00e3o. Futuros comportamentos transversais de reposit\u00f3rio (ex: Soft Delete, Auditoria) devem ser implementados como Mixins separados e herdados adicionalmente pelas classes concretas, conforme necess\u00e1rio, em vez de adicionar mais flags e l\u00f3gicas \u00e0 base de tenancy.</li> </ol>"},{"location":"adr/ADR-016-repository-multi-tenancy/#consequencias","title":"Consequ\u00eancias","text":"<p>Positivas:</p> <ul> <li>Garante isolamento de dados entre tenants quando habilitado.</li> <li>Permite que a mesma base de c\u00f3digo opere em modo single-tenant ou multi-tenant via configura\u00e7\u00e3o.</li> <li>Facilita o rollout gradual da funcionalidade multi-tenancy.</li> <li>Desacopla a l\u00f3gica de isolamento da camada de servi\u00e7o.</li> <li>Centraliza a l\u00f3gica de isolamento na camada de reposit\u00f3rio.</li> <li>Promove consist\u00eancia na defini\u00e7\u00e3o do campo <code>tenant_id</code> via Model Mixin.</li> <li>Estabelece um padr\u00e3o arquitetural claro para futuros comportamentos transversais (Mixins).</li> </ul> <p>Negativas:</p> <ul> <li>Adiciona complexidade (l\u00f3gica condicional) aos m\u00e9todos do reposit\u00f3rio.</li> <li>Exige a presen\u00e7a do campo <code>tenant_id</code> nos modelos/tabelas relevantes.</li> <li>RISCO CR\u00cdTICO: A seguran\u00e7a do isolamento depende da configura\u00e7\u00e3o correta da flag <code>multi_tenancy_enabled</code> (deve ser <code>True</code>) em ambientes de produ\u00e7\u00e3o multi-tenant. Um erro de configura\u00e7\u00e3o aqui pode levar a vazamento de dados.</li> <li>N\u00e3o aborda diretamente o acesso a dados \"globais\" (n\u00e3o pertencentes a um tenant), que pode exigir estrat\u00e9gias adicionais (ex: roles especiais, reposit\u00f3rios espec\u00edficos).</li> </ul> <p>Neutras:</p> <ul> <li>Depende da correta implementa\u00e7\u00e3o e popula\u00e7\u00e3o do <code>tenant_id_ctx_var</code> pelo middleware.</li> <li>A performance das queries pode ser ligeiramente impactada pela adi\u00e7\u00e3o do filtro <code>tenant_id</code> (mitigado pela indexa\u00e7\u00e3o correta).</li> </ul>"},{"location":"adr/ADR-017-model-repository-decoupling/","title":"ADR-017: Desacoplamento de Modelos (Documents) dos Reposit\u00f3rios Athomic","text":"<ul> <li>Status: Aceito</li> <li>Data: 2025-04-18</li> </ul>"},{"location":"adr/ADR-017-model-repository-decoupling/#1-contexto","title":"1. Contexto","text":"<p>A camada <code>nala.athomic</code> visa fornecer componentes de infraestrutura reutiliz\u00e1veis e agn\u00f3sticos da aplica\u00e7\u00e3o, incluindo abstra\u00e7\u00f5es para acesso a dados atrav\u00e9s do Padr\u00e3o Reposit\u00f3rio (conforme ADR-007). Isso envolve interfaces gen\u00e9ricas (<code>IRepository</code>) e implementa\u00e7\u00f5es de base gen\u00e9ricas (ex: <code>MongoBeanieRepository</code> para MongoDB/Beanie) dentro do <code>athomic</code>.</p> <p>Por outro lado, a camada da aplica\u00e7\u00e3o (ex: <code>nala.api</code>) define os modelos de dados espec\u00edficos do dom\u00ednio (ex: <code>UserDocument</code>, <code>WorkspaceDocument</code>) que herdam de um ODM/ORM como <code>beanie.Document</code>.</p> <p>Surge a quest\u00e3o: como a implementa\u00e7\u00e3o gen\u00e9rica do reposit\u00f3rio em <code>athomic</code> (que precisa operar sobre esses modelos para executar queries, saves, etc.) pode acessar/conhecer os modelos espec\u00edficos definidos na <code>api</code> sem criar uma depend\u00eancia direta de <code>athomic</code> para <code>api</code>, o que quebraria o princ\u00edpio de <code>athomic</code> ser um core reutiliz\u00e1vel?</p>"},{"location":"adr/ADR-017-model-repository-decoupling/#2-alternativas-consideradas","title":"2. Alternativas Consideradas","text":"<ol> <li> <p>Reposit\u00f3rio Base Importa/Descobre Modelos da Aplica\u00e7\u00e3o: A classe base gen\u00e9rica em <code>athomic</code> (ex: <code>MongoBeanieRepository</code>) poderia tentar importar modelos de um local conhecido na <code>api</code> (ex: <code>nala.api.models</code>) ou usar mecanismos de descoberta.</p> <ul> <li>Contras: Cria uma depend\u00eancia direta <code>athomic</code> -&gt; <code>api</code>, tornando <code>athomic</code> acoplado \u00e0 aplica\u00e7\u00e3o espec\u00edfica e n\u00e3o reutiliz\u00e1vel em outros contextos. Viola a separa\u00e7\u00e3o de camadas.</li> </ul> </li> <li> <p>Definir Modelos (Exemplos/Gen\u00e9ricos) em <code>athomic</code>: <code>athomic</code> poderia definir suas pr\u00f3prias classes <code>Document</code> gen\u00e9ricas ou exemplos.</p> <ul> <li>Contras: Torna <code>athomic</code> espec\u00edfico do dom\u00ednio (precisaria de um <code>UserDocument</code> gen\u00e9rico?), ou as classes gen\u00e9ricas seriam muito limitadas para uso real pela aplica\u00e7\u00e3o. N\u00e3o resolve como operar nos modelos reais da aplica\u00e7\u00e3o.</li> </ul> </li> <li> <p>Inje\u00e7\u00e3o da Classe do Modelo via <code>__init__</code>: A classe base gen\u00e9rica do reposit\u00f3rio em <code>athomic</code> (ex: <code>MongoBeanieRepository</code>) recebe a classe do modelo Beanie (<code>Type[Document]</code>) como um argumento em seu construtor (<code>__init__</code>). A implementa\u00e7\u00e3o concreta do reposit\u00f3rio na camada <code>api</code> (ex: <code>MongoUserRepository</code>) \u00e9 respons\u00e1vel por importar o modelo espec\u00edfico (ex: <code>UserDocument</code>) e pass\u00e1-lo para o <code>super().__init__()</code> da classe base.</p> <ul> <li>Pr\u00f3s: Mant\u00e9m <code>athomic</code> completamente agn\u00f3stico aos modelos da aplica\u00e7\u00e3o. A depend\u00eancia flui corretamente (<code>api</code> -&gt; <code>athomic</code>). Permite que a base gen\u00e9rica opere sobre qualquer modelo compat\u00edvel fornecido. Usa inje\u00e7\u00e3o de depend\u00eancia (de tipo, neste caso) de forma clara.</li> </ul> </li> </ol>"},{"location":"adr/ADR-017-model-repository-decoupling/#3-decisao","title":"3. Decis\u00e3o","text":"<p>Fica decidido que a intera\u00e7\u00e3o entre os reposit\u00f3rios base gen\u00e9ricos em <code>athomic</code> e os modelos de dados espec\u00edficos da aplica\u00e7\u00e3o seguir\u00e1 a Alternativa 3 (Inje\u00e7\u00e3o da Classe do Modelo via <code>__init__</code>):</p> <ol> <li>Localiza\u00e7\u00e3o dos Modelos: Todas as classes de modelo de dados concretas (ex: <code>beanie.Document</code>) que representam entidades do dom\u00ednio da aplica\u00e7\u00e3o devem residir fora do pacote <code>nala.athomic</code>. Tipicamente, ficar\u00e3o na camada da aplica\u00e7\u00e3o (ex: <code>nala.api.models</code>).</li> <li>Construtor do Reposit\u00f3rio Base: As classes base de reposit\u00f3rio gen\u00e9ricas dentro de <code>athomic</code> (ex: <code>MongoBeanieRepository</code>) devem aceitar a classe do modelo de documento espec\u00edfica como um argumento obrigat\u00f3rio em seu m\u00e9todo <code>__init__</code>. Recomenda-se usar <code>Type[ModelDocType]</code> (onde <code>ModelDocType</code> \u00e9 um <code>TypeVar</code> vinculado a <code>Document</code>) para tipagem.     <code>python     # Exemplo em athomic/.../repositories.py     ModelDocType = TypeVar(\"ModelDocType\", bound=Document)     class MongoBeanieRepository(IRepository[ModelDocType]):         def __init__(self, document_model: Type[ModelDocType]):              self.model = document_model              # ...</code></li> <li> <p>Implementa\u00e7\u00e3o Concreta do Reposit\u00f3rio: As classes de reposit\u00f3rio concretas, que residem na camada da aplica\u00e7\u00e3o (ex: <code>nala.api.data.repositories</code>), s\u00e3o respons\u00e1veis por:</p> <ul> <li>Importar o modelo de dom\u00ednio espec\u00edfico (ex: <code>UserDocument</code> de <code>nala.api.models</code>).</li> <li>Importar a classe base gen\u00e9rica (ex: <code>MongoBeanieRepository</code> de <code>nala.athomic...</code>).</li> <li>Importar a interface espec\u00edfica que implementam (ex: <code>IUserRepository</code>).</li> <li>Chamar <code>super().__init__(document_model=UserDocument)</code> dentro do seu pr\u00f3prio <code>__init__</code> para fornecer o modelo \u00e0 classe base. ```python</li> </ul> </li> </ol>"},{"location":"adr/ADR-017-model-repository-decoupling/#exemplo-em-apidatarepositoriesmongo_user_repositorypy","title":"Exemplo em api/data/repositories/mongo_user_repository.py","text":"<p>from nala.athomic.database.nosql.mongo.repositories import MongoBeanieRepository from nala.athomic.database.nosql.repository_interfaces import IUserRepository from nala.api.models.user import UserDocument</p> <p>class MongoUserRepository(MongoBeanieRepository[UserDocument], IUserRepository):     def init(self):         super().init(document_model=UserDocument) # &lt;&lt;&lt; Liga\u00e7\u00e3o AQUI     # ... implementa\u00e7\u00e3o de m\u00e9todos de IUserRepository ... ```</p>"},{"location":"adr/ADR-017-model-repository-decoupling/#4-consequencias","title":"4. Consequ\u00eancias","text":"<p>Positivas:</p> <ul> <li>Desacoplamento Forte: Garante que <code>nala.athomic</code> permane\u00e7a completamente independente e agn\u00f3stico em rela\u00e7\u00e3o aos modelos de dom\u00ednio espec\u00edficos da aplica\u00e7\u00e3o <code>nala.api</code> (ou qualquer outra).</li> <li>Reutiliza\u00e7\u00e3o: Maximiza a capacidade de reutilizar o <code>nala.athomic</code> em diferentes projetos com diferentes modelos de dados.</li> <li>Clareza Arquitetural: Define uma separa\u00e7\u00e3o clara de responsabilidades entre a infraestrutura gen\u00e9rica e a l\u00f3gica/dados espec\u00edficos da aplica\u00e7\u00e3o.</li> <li>Seguran\u00e7a de Tipos: O uso de <code>TypeVar</code> e <code>Generic</code> (<code>IRepository[ModelType]</code>, <code>MongoBeanieRepository[ModelDocType]</code>) permite que a tipagem est\u00e1tica funcione corretamente atrav\u00e9s das camadas.</li> </ul> <p>Negativas:</p> <ul> <li>Boilerplate M\u00ednimo: Exige que cada implementa\u00e7\u00e3o concreta de reposit\u00f3rio na camada de aplica\u00e7\u00e3o tenha um <code>__init__</code> que chame <code>super().__init__(document_model=...)</code>.</li> </ul> <p>Neutras:</p> <ul> <li>A responsabilidade de fornecer o modelo correto para a base gen\u00e9rica recai sobre o desenvolvedor da implementa\u00e7\u00e3o concreta do reposit\u00f3rio.</li> <li>Esta decis\u00e3o complementa a estrat\u00e9gia de registro de reposit\u00f3rios (onde a <code>api</code> registra suas implementa\u00e7\u00f5es concretas na factory de <code>athomic</code>), refor\u00e7ando o fluxo de depend\u00eancia <code>api</code> -&gt; <code>athomic</code>.</li> </ul>"},{"location":"adr/ADR-018-rate-limiter-qos-factory/","title":"ADR-018: Rate Limiter Refactoring for QoS and Factory Pattern (Revised)","text":""},{"location":"adr/ADR-018-rate-limiter-qos-factory/#status","title":"Status","text":"<ul> <li>Status: Aceito</li> <li>Data: 2025-04-21 (Atualizado)</li> </ul>"},{"location":"adr/ADR-018-rate-limiter-qos-factory/#context","title":"Context","text":"<p>The previous rate limiting mechanism (ADR-006), while functional using the <code>limits</code> library via the <code>@rate_limited</code> decorator, lacked flexibility for applying different limits based on client profiles (Quality of Service - QoS) and didn't align with the Factory pattern used in other <code>athomic</code> modules (Cache, Secrets). Debugging also revealed challenges in provider instantiation during testing. This ADR addresses these points by introducing QoS capabilities and standardizing provider creation.</p>"},{"location":"adr/ADR-018-rate-limiter-qos-factory/#decision","title":"Decision","text":"<p>We will refactor the <code>nala.athomic.resilience.rate_limiter</code> module to: 1. Implement configurable Quality of Service (QoS) via policies. 2. Introduce a Factory pattern for provider instantiation. 3. Decouple provider-specific configuration validation from the generic configuration schema. 4. Refactor the <code>@rate_limited</code> decorator to support dynamic policy resolution.</p> <p>Detailed Decisions:</p> <ol> <li> <p>Factory Pattern Implementation:</p> <ul> <li>A new module <code>factory.py</code> contains the factory function <code>get_rate_limiter_provider() -&gt; AbstractRateLimiter</code>.</li> <li>The factory reads <code>settings.rate_limiter</code> from the global configuration (<code>get_settings</code>).</li> <li>It determines the provider implementation name (currently defaults to \"limits\", potentially configurable via settings in the future, e.g., <code>settings.rate_limiter.provider_backend</code>).</li> <li>It looks up the corresponding provider class in <code>registry.REGISTERED_PROVIDERS</code>.</li> <li>It instantiates the provider class, passing the entire <code>settings.rate_limiter</code> object as <code>config: RateLimiterSettings</code>.</li> <li>The factory function is decorated with <code>@lru_cache()</code> to ensure a singleton provider instance.</li> </ul> </li> <li> <p>Provider Constructor and Validation:</p> <ul> <li>All concrete rate limiter provider implementations (e.g., <code>LimitsRateLimiter</code>) must implement an <code>__init__</code> method that accepts a single argument <code>config: RateLimiterSettings</code>.</li> <li>Providers use this <code>config</code> object internally for their settings.</li> <li>Crucially, each provider's <code>__init__</code> is responsible for validating any provider-specific configuration formats. For example, <code>LimitsRateLimiter</code> validates the format of <code>config.default_policy_limit</code> and <code>config.policies</code> values using <code>limits.util.parse</code> during its initialization.</li> </ul> </li> <li> <p>QoS Configuration (<code>RateLimiterSettings</code>):</p> <ul> <li>The Pydantic model <code>RateLimiterSettings</code> (<code>config/schemas/rate_limiter_config.py</code>) is extended with:<ul> <li><code>default_policy_limit: Optional[str]</code>: The default rate limit string (e.g., \"100/hour\", following <code>limits</code> library format) applied when no specific policy matches or the resolved policy name is not found.</li> <li><code>policies: Optional[Dict[str, str]]</code>: A dictionary mapping policy names (e.g., \"free\", \"premium\") to rate limit strings (following <code>limits</code> library format, e.g., \"50/minute\", \"1000/minute\", \"nolimit\").</li> </ul> </li> <li>Provider-specific format validation (like checking if strings are valid for <code>limits.util.parse</code>) is removed from this Pydantic model to keep it provider-agnostic.</li> </ul> </li> <li> <p>Decorator Refactoring (<code>@rate_limited</code>):</p> <ul> <li>The decorator signature is changed to:     <code>python     def rate_limited(         policy_resolver: Callable[..., Awaitable[str]] = default_policy_resolver,         on_block: Optional[Callable[[], Awaitable[Any]]] = None,         provider: Optional[AbstractRateLimiter] = None,     ) -&gt; Callable[..., Awaitable[Any]]:</code><ul> <li><code>policy_resolver</code>: An async function that receives the decorated function's <code>*args</code> and <code>**kwargs</code> and returns a policy name string (e.g., \"premium\"). Defaults to <code>default_policy_resolver</code> (which returns \"default\"). The implementer of the resolver is responsible for accessing necessary context (e.g., request object, user data from <code>kwargs</code>).</li> <li><code>on_block</code>: An optional async handler called when the rate limit is exceeded. If not provided, exceeding the limit might result in a default behavior (e.g., returning <code>None</code> or raising a specific exception if <code>apply_rate_limit</code> were modified to do so).</li> <li><code>provider</code>: Allows injecting a specific provider instance, primarily for testing. If <code>None</code>, the factory <code>get_rate_limiter_provider()</code> is used.</li> </ul> </li> <li>The decorator's internal wrapper logic:<ul> <li>Determines the provider instance (injected or from factory).</li> <li>Calls <code>policy_name = await policy_resolver(*args, **kwargs)</code>. Handles exceptions during resolution by falling back to the policy name <code>\"default\"</code>.</li> <li>Determines a <code>resource_key</code> (e.g., based on the decorated function's name) used for identifying the resource being limited.</li> <li>Calls the core function <code>apply_rate_limit</code>, passing <code>policy_key</code>, <code>policy_name_resolved</code>, <code>provider</code>, the original <code>func</code>, <code>on_block</code>, and the original <code>args</code>/<code>kwargs</code>.</li> </ul> </li> </ul> </li> <li> <p>Core Logic (<code>core.apply_rate_limit</code>):</p> <ul> <li>This central async function (called by the decorator) encapsulates the main rate limiting flow:<ul> <li>Retrieves the <code>RateLimiterSettings</code> via <code>get_settings()</code>.</li> <li>Checks if rate limiting is enabled.</li> <li>Looks up the <code>policy_name_resolved</code> in <code>settings.rate_limiter.policies</code>.</li> <li>If found, uses that limit string; otherwise, uses <code>settings.rate_limiter.default_policy_limit</code>.</li> <li>Constructs the full storage key using <code>utils.build_rate_limit_key</code>.</li> <li>Calls <code>allowed = await provider.allow(full_key, resolved_limit_string)</code>.</li> <li>If <code>allowed</code>, calls and returns the result of the original <code>func(*args, **kwargs)</code>.</li> <li>If not <code>allowed</code>, calls and returns the result of <code>on_block()</code> if provided, otherwise returns <code>None</code>.</li> </ul> </li> </ul> </li> <li> <p>Standard Resolvers/Helpers:</p> <ul> <li>Common/example functions for policy resolution (e.g., based on request state, headers) can be provided in <code>resolvers.py</code> within the rate limiter module for reusability. <code>utils.py</code> remains for key generation helpers.</li> </ul> </li> </ol>"},{"location":"adr/ADR-018-rate-limiter-qos-factory/#consequences","title":"Consequences","text":"<p>Positive:</p> <ul> <li>Decoupling: Configuration schema is decoupled from specific library formats. Decorator is decoupled from provider implementation and core logic flow.</li> <li>Consistency: Aligns with Factory pattern usage in other modules.</li> <li>Configurability &amp; Flexibility: Enables rich, configuration-driven QoS policies. <code>policy_resolver</code> allows dynamic determination based on any request context.</li> <li>Testability: Factory, Provider (including its validation), Core, Decorator, and Resolvers can be tested independently.</li> <li>Clarity: Centralizes provider instantiation (Factory) and core rate limiting logic (<code>apply_rate_limit</code>). Provider-specific validation resides within the provider.</li> </ul> <p>Negative:</p> <ul> <li>Refactoring Effort: Required significant changes across the module (Provider, Factory, Core, Decorator, Config, Tests).</li> <li>Complexity in Use: Developers using <code>@rate_limited</code> for QoS need to implement appropriate <code>policy_resolver</code> functions that can access the necessary context from the decorated function's arguments/kwargs.</li> <li>Later Validation: Configuration format validation (for limit strings) happens during provider instantiation (runtime) rather than purely at Pydantic model parsing time, although this typically occurs early near application startup.</li> </ul>"},{"location":"adr/ADR-018-rate-limiter-qos-factory/#alternatives-considered","title":"Alternatives Considered","text":"<ol> <li>Validation in Pydantic Model: Keep the <code>limits.util.parse</code> validation within <code>RateLimiterSettings</code>.<ul> <li>Rejected because: Tightly couples the configuration schema to a specific library implementation, hindering the addition of providers with different limit formats.</li> </ul> </li> <li>Direct Instantiation in Decorator: Continue instantiating <code>LimitsRateLimiter</code> directly in <code>@rate_limited</code>.<ul> <li>Rejected because: Tight coupling, architecturally inconsistent, doesn't support different providers based on config easily.</li> </ul> </li> <li>Dependency Injection via Request State: Inject <code>RateLimiter</code> instance into <code>request.state</code>.<ul> <li>Rejected because: Adds complexity to startup/request lifecycle. Factory pattern provides sufficient decoupling and singleton management.</li> </ul> </li> <li>Embedding Policy Logic in Decorator: Hardcode logic in the decorator to inspect request context.<ul> <li>Rejected because: Tight coupling to application state structure. Passing <code>policy_resolver</code> is more flexible.</li> </ul> </li> </ol>"},{"location":"adr/ADR-018-rate-limiter-qos-factory/#references","title":"References","text":"<ul> <li>ADR-006: Rate Limiting (Initial decision to use <code>limits</code> library)</li> </ul>"},{"location":"adr/ADR-019-feature-flags/","title":"ADR-019: Creation of the Feature Flags (Toggles) Module","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-04-25</li> </ul>"},{"location":"adr/ADR-019-feature-flags/#context","title":"Context","text":"<ul> <li>Problem: Modern applications like <code>athomic-docs</code> often need to enable or disable features dynamically at runtime without requiring a new deployment. Use cases include gradual rollouts (Canary Releases), A/B testing, enabling features per tenant or user group, and the ability to quickly disable a problematic feature (kill switch). Implementing this logic directly in business code or in an ad-hoc manner makes the system less flexible, harder to manage, and prone to errors.</li> <li>Need: A centralized, configurable, and decoupled mechanism to manage and query the state of feature flags, integrated into the <code>athomic</code> architecture.</li> <li>Alternatives Considered:<ul> <li>Manual Conditional Logic: <code>if settings.get(\"feature_x_enabled\"): ...</code> scattered throughout the code. Rejected: High coupling, difficult centralized management, violates Open/Closed Principle.</li> <li>Include in Existing Modules:<ul> <li><code>athomic/config</code>: Feature flags are a form of configuration, but dynamic and queried at runtime, differing from static configuration loaded at initialization. Rejected: Would dilute the purpose of <code>config</code>.</li> <li><code>athomic/integrations</code>: Some providers (Consul, Redis, external services) involve integration, but the local provider does not, and the core functionality is behavior control, not communication. Rejected: Does not represent the primary responsibility.</li> <li><code>athomic/resilience</code>: Flags can be used for resilience (kill switch), but have other uses (A/B, rollouts) beyond the scope of resilience. Rejected: Limited scope.</li> </ul> </li> <li>Dedicated External Services (No Abstraction): Using SDKs from services like LaunchDarkly directly in the application code. Rejected: Would create direct coupling with the external service in business logic, hindering switching or unit testing.</li> </ul> </li> </ul>"},{"location":"adr/ADR-019-feature-flags/#decision","title":"Decision","text":"<ul> <li>Create a new first-class module <code>nala.athomic.feature_flags</code> within the <code>athomic</code> layer.</li> <li>Adopt the Provider/Registry/Factory pattern, already established in other <code>athomic</code> modules (Cache, Secrets, Rate Limiter, Repository):<ul> <li>Interface: Define a <code>FeatureFlagProvider</code> protocol with essential methods like <code>async def is_enabled(flag_key: str, default: bool = False, context: Optional[dict] = None) -&gt; bool</code> and <code>async def get_variant(flag_key: str, default: Optional[Any] = None, context: Optional[dict] = None) -&gt; Optional[Any]</code>.</li> <li>Providers: Implement concrete classes adhering to the <code>FeatureFlagProvider</code> interface:<ul> <li><code>LocalProvider</code>: Reads flags from a dictionary in the configuration (<code>settings.toml</code>) or a local file. Ideal for development and testing. (Implemented)</li> <li><code>ConsulProvider</code>: (Planned) Reads flags from Consul KV, potentially using caching (<code>aiocache</code>).</li> <li><code>RedisProvider</code>: (Planned) Reads flags from Redis, potentially using caching (<code>aiocache</code>).</li> <li>(Future) Providers for external services (LaunchDarkly, Unleash).</li> </ul> </li> <li>Registry: A simple dictionary mapping backend names (<code>\"local\"</code>, <code>\"consul\"</code>, <code>\"redis\"</code>) to the Provider classes. (Implemented)</li> <li>Factory: A <code>get_feature_flag_provider()</code> function with <code>@lru_cache</code> that reads <code>settings.feature_flags.backend</code>, queries the Registry, and returns the singleton instance of the configured provider. (Implemented)</li> </ul> </li> <li>Configuration: Manage settings (backend, prefixes, cache TTL for external providers, local flags) through a new Pydantic schema <code>FeatureFlagSettings</code> integrated into <code>AppSettings</code>. (Implemented)</li> <li>Usage: Expose the primary functionality through helper functions <code>async def is_feature_enabled(flag_key: str, **context)</code> and <code>async def get_feature_variant(flag_key: str, **context)</code> (which use the factory), and a decorator <code>@feature_enabled(...)</code>. (Implemented)</li> </ul>"},{"location":"adr/ADR-019-feature-flags/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Dynamic Control: Allows enabling/disabling features without deployment, increasing agility and operational safety (quick rollbacks).</li> <li>Decoupling: Business logic checking a flag (<code>is_feature_enabled(\"...\")</code>) doesn't need to know where the flag is stored (Consul, Redis, local config).</li> <li>Architectural Consistency: Reuses the Provider/Registry/Factory pattern, maintaining coherence with other <code>athomic</code> modules.</li> <li>Testability: Facilitates unit testing of business logic by mocking the helper functions or injecting a <code>LocalProvider</code> in tests.</li> <li>Extensibility: Adding support for a new backend (e.g., LaunchDarkly) only requires creating a new provider and registering it, without changing the code that consumes the flags.</li> </ul> </li> <li>Negative:<ul> <li>New Layer of Indirection: Adds the provider/factory abstraction layer.</li> <li>Infrastructure Complexity (if external): Using Consul or Redis as a backend requires those services to be available and configured.</li> <li>Flag Management: Requires a process for managing flags in the chosen backends (who creates, updates, removes flags?).</li> <li>Cache Consistency (if applicable): If external providers use caching (as suggested with <code>aiocache</code>), the invalidation/TTL strategy must be considered so that flag changes are reflected in the application in a timely manner.</li> </ul> </li> <li>Neutral/Other:<ul> <li>Effectiveness depends on the team's discipline in using the feature flags mechanism instead of hardcoded conditionals.</li> <li>The <code>context</code> logic (for tenant/user rollouts) needs to be well-defined and passed correctly to the usage functions/decorator.</li> </ul> </li> </ul>"},{"location":"adr/ADR-020-context-aware-architecture/","title":"ADR-020: Context-Aware Modular Architecture for Core Service Modules","text":"<p>Status: Accepted Date: 2025-04-27</p>"},{"location":"adr/ADR-020-context-aware-architecture/#context","title":"\ud83e\udde0 Context","text":"<p>As the <code>athomic-docs</code> evolves, we are introducing multiple service modules like <code>CacheModule</code>, <code>RateLimiter</code>, <code>FeatureFlags</code>, etc. These modules need to:</p> <ul> <li>Be multi-tenant aware (use <code>tenant_id</code> dynamically)</li> <li>Support lifecycle hooks (<code>init</code>, <code>shutdown</code>)</li> <li>Inject settings and logging automatically</li> <li>Allow key resolution with contextual prefixes (e.g., <code>\"nala:&lt;tenant&gt;:cache:&lt;key&gt;\"</code>)</li> <li>Remain pluggable and backend-agnostic, enabling providers to be swapped via configuration</li> </ul> <p>These requirements are cross-cutting and repeated across modules. Without a standardized base, implementation becomes inconsistent and error-prone.</p>"},{"location":"adr/ADR-020-context-aware-architecture/#decision","title":"\u2705 Decision","text":"<p>We introduce a standardized architecture for Athomic modules, built on the following components:</p> <ol> <li> <p><code>AthomicBaseComponent</code>     Base class for all Athomic modules. Provides:  </p> <ul> <li>Access to the current <code>ExecutionContext</code> via <code>self.context</code> (which reads context vars)  </li> <li>Typed <code>settings</code> injection  </li> <li><code>logger</code> instance aware of the component's name  </li> <li><code>init</code>, <code>close</code>, and <code>shutdown</code> lifecycle hooks</li> </ul> </li> <li> <p><code>ExecutionContext</code> + <code>context_vars</code>     Encapsulates request/session-level information (<code>tenant_id</code>, <code>user_id</code>, <code>request_id</code>, etc.) using thread-safe <code>contextvars</code>. The <code>ExecutionContext</code> class provides a convenient way to access these variables.</p> </li> <li> <p><code>KeyResolver</code> and <code>KeyResolvingKVClient</code> </p> <ul> <li><code>KeyResolver</code>: Ensures keys used in distributed systems are namespaced consistently based on configuration (prefix, namespace, use_tenant, use_user) and dynamically fetches the current <code>ExecutionContext</code> (reading from <code>context_vars</code>) during the <code>resolve()</code> call to incorporate relevant IDs (like tenant_id or user_id) into the final key. It no longer accepts <code>ExecutionContext</code> directly in its constructor.  </li> <li><code>KeyResolvingKVClient</code>: Wraps a <code>KVStoreProtocol</code> client, using <code>KeyResolver</code> internally to automatically prefix keys before delegating operations.</li> </ul> </li> <li> <p><code>KVStoreProtocol</code>     Common async interface for all key-value stores. Enables plug-and-play backends via:  </p> <ul> <li>Registry + Factory (local, redis, upstash...)</li> </ul> </li> <li> <p>Concrete Modules Follow This Pattern:     Each module (e.g., <code>CacheModule</code>) implements its domain logic but delegates:  </p> <ul> <li>Key formatting to <code>KeyResolver</code> (implicitly via <code>KeyResolvingKVClient</code> or used directly)  </li> <li>Persistence to a <code>KVStoreProtocol</code>-based client  </li> <li>Lifecycle and base utilities (context access, settings, logger) potentially by inheriting from <code>AthomicBaseComponent</code></li> </ul> </li> </ol>"},{"location":"adr/ADR-020-context-aware-architecture/#implementation-guidelines","title":"\ud83d\udee0\ufe0f Implementation Guidelines","text":"<pre><code>src/nala/athomic/performance/cache/cache_module.py\n\u2937 class CacheModule(AthomicBaseComponent, CacheProtocol)\n\nsrc/nala/athomic/database/kvstore/\n\u2937 registry.py, factory.py, wrappers/, clients/\n\nsrc/nala/athomic/context/\n\u2937 context_vars.py, execution.py, key_resolver.py\n\nsrc/nala/athomic/base/\n\u2937 base_component.py, core_module.py\n</code></pre>"},{"location":"adr/ADR-020-context-aware-architecture/#consequences","title":"\ud83d\udcc8 Consequences","text":""},{"location":"adr/ADR-020-context-aware-architecture/#positives","title":"\u2705 Positives","text":"<ul> <li>Consistency: All service modules behave similarly and respect multi-tenancy by default.</li> <li>Extensibility: New modules (e.g., <code>RateLimiter</code>) can be added by following the same pattern.</li> <li>Observability: Logs are enriched with tenant/request IDs.</li> <li>Reusability: Key logic (context reading via <code>context_vars</code>/<code>ExecutionContext</code>, key resolution via <code>KeyResolver</code>) is shared.</li> <li>Testability: Easy to mock <code>KVStoreProtocol</code>. Mocking context now involves patching <code>context_vars</code> getters (e.g., <code>get_tenant_id</code>) rather than injecting an <code>ExecutionContext</code> into <code>KeyResolver</code> or its users directly.</li> </ul>"},{"location":"adr/ADR-020-context-aware-architecture/#negatives","title":"\u26a0\ufe0f Negatives","text":"<ul> <li>Slight increase in boilerplate for each module inheriting <code>AthomicBaseComponent</code>.</li> <li>Requires discipline to always use <code>KeyResolver</code> (or wrappers like <code>KeyResolvingKVClient</code>) instead of hardcoded keys.</li> <li>Testing context-dependent logic now requires patching <code>context_vars</code> getters, which might be slightly less direct than injecting a context object in some cases.</li> </ul>"},{"location":"adr/ADR-020-context-aware-architecture/#neutralother","title":"\u2795 Neutral/Other","text":"<ul> <li>Enables future features like:</li> <li><code>SecretsModule</code>, <code>AuthModule</code>, <code>SessionStore</code>, etc.</li> <li>Automatic instrumentation and metric emission using the same context.</li> </ul>"},{"location":"adr/ADR-021-athomic-control/","title":"ADR-021: Introduce <code>athomic.control</code> as Unified Module for Dynamic Behavior","text":"<ul> <li>Status: Accepted  </li> <li>Date: 2025-05-02</li> </ul>"},{"location":"adr/ADR-021-athomic-control/#1-context","title":"1. Context","text":"<p>As the <code>athomic-docs</code> project grows, we are introducing more modules that influence runtime behavior without requiring redeployment. Examples include <code>feature_flags</code>, dynamic <code>config</code>, and service <code>discovery</code>. These modules share key architectural traits:</p> <ul> <li>Provider-based with pluggable backends (e.g., Consul, Redis, static)</li> <li>Context-sensitive (e.g., tenant-aware, environment-aware)</li> <li>Support fallback, dynamic control, and observability</li> <li>Govern service behavior dynamically and externally</li> </ul> <p>To promote cohesion and extensibility, we propose grouping these under a common namespace and architectural category.</p>"},{"location":"adr/ADR-021-athomic-control/#2-decision","title":"2. Decision","text":"<p>We introduce a new parent module:</p> <pre><code>athomic/control/\n</code></pre> <p>This directory will house all modules responsible for runtime behavior control, including but not limited to:</p> Submodule Description <code>feature_flags/</code> Dynamic feature toggles and variants <code>config/</code> Remote/dynamic config providers (e.g., Consul, Vault, etcd) <code>discovery/</code> Service discovery mechanisms (e.g., Consul) <code>kill_switches/</code> Emergency feature/service shutdowns (future) <code>experiments/</code> A/B testing, canary rollout, variant resolution (future) <code>routing/</code> Context-based dynamic routing (future) <code>policies/</code> Dynamic behavior policies and rules (retry, backoff, QoS, etc.) <p>Each module will follow <code>athomic</code> best practices: - Interface-first design (Protocol or ABC) - Registry + Factory pattern - Settings-driven configuration - Optional fallback/caching/tracing support</p>"},{"location":"adr/ADR-021-athomic-control/#3-justification","title":"3. Justification","text":"<ul> <li>Modular Cohesion: All submodules alter runtime behavior dynamically.</li> <li>Scalability: Adds clarity and room for controlled growth.</li> <li>Governance Support: Provides an architectural base for implementing runtime governance and dashboards.</li> <li>Reusability: Encourages shared utilities (context resolver, fallback decorators, metrics).</li> <li>Developer Experience: Clarifies purpose and expectations from modules in this group.</li> </ul>"},{"location":"adr/ADR-021-athomic-control/#4-consequences","title":"4. Consequences","text":""},{"location":"adr/ADR-021-athomic-control/#positive","title":"\u2705 Positive","text":"<ul> <li>Clear grouping of dynamic behavior modules.</li> <li>Easier to extend with new modules (experiments, routing, etc.).</li> <li>Improved documentation and architectural coherence.</li> <li>Aligns with modular, pluggable, observable infrastructure vision.</li> </ul>"},{"location":"adr/ADR-021-athomic-control/#neutralminor-migration","title":"\u26a0\ufe0f Neutral/Minor Migration","text":"<ul> <li>Existing modules (e.g., <code>feature_flags</code>, <code>discovery</code>, <code>config</code>) will be moved into <code>athomic/control/</code>.</li> <li>Internal imports need to be updated accordingly.</li> </ul>"},{"location":"adr/ADR-021-athomic-control/#risk","title":"\u2757 Risk","text":"<ul> <li>Adding a layer of indirection increases complexity slightly.</li> <li>Developers must understand this new layer's purpose and scope.</li> </ul>"},{"location":"adr/ADR-021-athomic-control/#5-action-items","title":"5. Action Items","text":"<ol> <li>Move <code>feature_flags/</code>, <code>discovery/</code>, and <code>config/</code> into <code>athomic/control/</code>.</li> <li>Update all internal imports and tests accordingly.</li> <li>Add new documentation under <code>docs/athomic/control.md</code> to explain this layer\u2019s role.</li> <li>Plan for future submodules like <code>kill_switches</code>, <code>experiments</code>, and <code>policies</code>.</li> </ol>"},{"location":"adr/ADR-022-background-task-abstraction/","title":"ADR-022: Background Task Abstraction Module","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-05-02</li> </ul>"},{"location":"adr/ADR-022-background-task-abstraction/#context","title":"Context","text":"<ul> <li>Problem: Modern APIs like <code>athomic-docs</code> frequently need to offload long-running, resource-intensive, or non-critical operations (e.g., sending emails/notifications, processing uploads, generating reports) to background workers. Implementing this requires integrating with task queue frameworks (like Celery, RQ, Dramatiq) which often depend on external message brokers (Redis, RabbitMQ). Directly using a specific task framework's API within business logic creates tight coupling, hindering framework changes, testing, and consistent application of cross-cutting concerns like context propagation and observability.</li> <li>Goal: Introduce a stable abstraction layer within <code>nala.athomic</code> for enqueuing background tasks. This layer should:<ul> <li>Decouple application code from the specific task queue framework implementation.</li> <li>Allow switching task queue backends via configuration.</li> <li>Integrate seamlessly with existing <code>athomic</code> modules (config, context, observability).</li> <li>Support reliable propagation of execution context (e.g., <code>tenant_id</code>, <code>trace_id</code>) from the web request to the background worker.</li> <li>Follow the established Provider/Registry/Factory pattern within <code>athomic</code>.</li> </ul> </li> </ul>"},{"location":"adr/ADR-022-background-task-abstraction/#decision","title":"Decision","text":"<ol> <li>Module Location: A new module, <code>nala.athomic.integration.tasks</code>, will be created.<ul> <li>Rationale: Since the primary interaction for frameworks like Celery and RQ involves external brokers (Redis, RabbitMQ), placing this abstraction within the <code>integration</code> package aligns with its purpose of managing external system communications, alongside <code>messaging</code> and <code>discovery</code>.</li> </ul> </li> <li>Architecture: Implement the Provider/Registry/Factory pattern:<ul> <li><code>TaskBrokerProtocol</code> (<code>integration/tasks/protocol.py</code>): Define the interface with <code>async enqueue_task(task_name: str, *args, **kwargs) -&gt; Optional[str]</code> (returns optional task ID). Methods like <code>get_task_result</code> or <code>is_available</code> can be added later if needed.</li> <li><code>TaskBrokerSettings</code> (<code>config/schemas/integration/tasks_config.py</code>): Pydantic schema (integrated into <code>AppSettings</code> likely under <code>integration</code>) to configure <code>enabled</code>, <code>backend</code> (e.g., \"local\", \"celery\", \"rq\"), and backend-specific settings (like <code>broker_url</code>, <code>result_backend_url</code>).</li> <li>Providers (<code>integration/tasks/providers/</code>):<ul> <li><code>LocalTaskBroker</code>: Simple in-memory/asyncio executor for development and testing.</li> <li><code>CeleryTaskBroker</code>: (Planned) Adapter for Celery's <code>send_task</code> API.</li> <li><code>RQTaskBroker</code>: (Planned) Adapter for RQ's <code>enqueue</code> API.</li> </ul> </li> <li>Registry (<code>integration/tasks/registry.py</code>): Dictionary mapping backend names to Provider classes. Registered <code>local</code> provider by default.</li> <li>Factory (<code>integration/tasks/factory.py</code>): <code>get_task_broker()</code> function (<code>@lru_cache</code>) reads settings, consults registry, instantiates, and returns the singleton provider.</li> </ul> </li> <li>Context Propagation:<ul> <li>A dedicated utility (<code>integration/tasks/context.py</code>) provides <code>capture_context_for_task()</code> to serialize relevant <code>context_vars</code> (tenant, trace, etc.).</li> <li>This captured context dictionary must be passed along with the task payload to the broker.</li> <li>A corresponding utility/context manager (<code>restore_context_from_task</code>) is provided to be used on the worker side to restore the context before task execution. (Worker setup is outside this ADR's scope).</li> </ul> </li> <li>Observability Integration: Providers should leverage <code>athomic/observability</code> for logging. Integration with tracing and metrics is planned for future phases.</li> </ol>"},{"location":"adr/ADR-022-background-task-abstraction/#alternatives-considered","title":"Alternatives Considered","text":"<ol> <li><code>athomic/tasks</code> (Top-Level Module): Considered but deferred as the initial focus is integration with external brokers, fitting better within <code>athomic/integration</code>. Can be revisited if scope expands significantly to internal scheduling/workflows.</li> <li><code>athomic/workflows/tasks</code>: Considered but deferred as it conflates single task enqueuing with higher-level workflow orchestration.</li> <li>Direct Library Usage: Rejected due to tight coupling and lack of consistency.</li> <li>Abstract via <code>athomic/messaging</code>: Rejected as it couples task semantics with generic messaging.</li> </ol>"},{"location":"adr/ADR-022-background-task-abstraction/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Decouples application services from specific task queue frameworks.</li> <li>Allows switching backends via configuration.</li> <li>Improves testability using <code>LocalTaskBroker</code> or mocks.</li> <li>Consistent architecture with other <code>athomic</code> modules.</li> <li>Provides a clear mechanism for context propagation.</li> </ul> </li> <li>Negative:<ul> <li>Adds a layer of abstraction and some boilerplate code.</li> <li>Context propagation requires careful implementation and testing on both the producer (API) and consumer (worker) sides.</li> <li>Requires separate setup and configuration for background workers.</li> </ul> </li> <li>Neutral/Other:<ul> <li>Relies on clear definition and discovery of task functions by the chosen backend framework.</li> </ul> </li> </ul>"},{"location":"adr/ADR-022-background-task-abstraction/#future-considerations","title":"Future Considerations","text":"<ul> <li>Implement providers for Celery and RQ.</li> <li>Integrate tracing and metrics for task enqueuing and execution.</li> <li>Refine context propagation mechanism if needed.</li> <li>Evaluate adding methods like <code>get_task_result</code>.</li> <li>Consider creating <code>athomic/scheduling</code> or <code>athomic/workflows</code> if requirements evolve significantly beyond broker integration.</li> </ul>"},{"location":"adr/ADR-023-advanced-message-patterns/","title":"ADR-023: Advanced Messaging Patterns within Athomic","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-05-04</li> </ul>"},{"location":"adr/ADR-023-advanced-message-patterns/#1-context","title":"1. Context","text":"<ul> <li>Problem: As we evolve <code>athomic-docs</code> to include advanced asynchronous messaging patterns (Internal Event Bus, Outbox Pattern, Dead Letter Queue (DLQ), Schema Registry Integration), it becomes necessary to define clear and consistent locations for these components within the <code>nala.athomic</code> modular architecture. Improper placement could violate the separation of concerns, increase coupling between modules, or hinder maintainability.</li> <li>Alternatives Considered:<ol> <li>Everything in <code>integration/messaging</code>: Grouping all new components under the existing messaging module. (Rejected: The internal event bus isn't strictly external integration; Outbox storage belongs to the data layer).</li> <li>New Top-Level <code>messaging</code> or <code>events</code> Module: Creating a new top-level module for all messaging/event logic. (Rejected: Communication with external brokers fits well within <code>integration</code>; the persistence part of Outbox belongs in <code>database</code>).</li> <li>Resilience Components in <code>resilience</code>: Placing the Outbox Publisher and DLQ Handler under <code>athomic/resilience</code>. (Rejected: Although they contribute to resilience, they are specific implementations within the messaging domain).</li> <li>Separation by Responsibility: Allocating each component to the <code>athomic</code> module that best represents its primary responsibility (internal communication vs. external integration vs. data persistence). (Accepted).</li> </ol> </li> </ul>"},{"location":"adr/ADR-023-advanced-message-patterns/#2-decision","title":"2. Decision","text":"<p>It is decided that the new advanced messaging features will be located as follows within the <code>nala.athomic</code> structure:</p> <ol> <li> <p>Internal Event Bus:</p> <ul> <li>Location: A new top-level module will be created: <code>athomic/events/</code>.</li> <li>Content: Will contain the <code>EventBusProtocol</code>, implementations (e.g., <code>MemoryEventBus</code>, <code>RedisEventBus</code>), registry, and factory (<code>get_event_bus</code>).</li> <li>Rationale: Clearly separates internal asynchronous communication (between Athomic components or the application) from external communication with brokers (which remains in <code>athomic/integration/messaging</code>).</li> </ul> </li> <li> <p>Outbox Pattern:</p> <ul> <li>Location (Split):<ul> <li>Storage: <code>athomic/database/outbox/</code> (or an appropriate submodule within <code>database</code>). Will contain <code>OutboxStorageProtocol</code>, the <code>OutboxEvent</code> model, and concrete implementations for supported databases (e.g., <code>MongoOutboxStorage</code>).</li> <li>Publisher: <code>athomic/integration/messaging/outbox/</code>. Will contain the <code>OutboxPublisher</code> logic that reads from the storage and publishes using the <code>ProducerProtocol</code> from <code>integration/messaging</code>.</li> </ul> </li> <li>Rationale: Storage is a data layer responsibility, while the act of publishing (even from a table) belongs to messaging integration.</li> </ul> </li> <li> <p>Dead Letter Queue (DLQ) Handling:</p> <ul> <li>Location: Logic will be added within the consumer implementations in <code>athomic/integration/messaging/providers/</code> (e.g., <code>KafkaConsumer</code>).</li> <li>Content: Modifications to consumption methods to include retry counting and redirection to the configured DLQ.</li> <li>Rationale: DLQ handling is intrinsic to how a consumer processes and deals with failures, making it most suitable to enhance the existing provider classes. Related configurations (<code>topic</code>, <code>dlq.max_attempts</code>) will belong to the specific consumer configuration schemas.</li> </ul> </li> <li> <p>Schema Registry Integration:</p> <ul> <li>Location: Serialization/deserialization and schema validation logic will primarily reside within the providers in <code>athomic/integration/messaging/providers/</code> (modifying <code>KafkaProducer</code>, <code>KafkaConsumer</code>, etc.).</li> <li>Content (Optional): If schema handling/serialization logic becomes complex, it could be extracted into a new submodule <code>athomic/integration/messaging/serializers/</code>.</li> <li>Rationale: Directly impacts how producers and consumers format and interpret message data during integration with the broker.</li> </ul> </li> </ol>"},{"location":"adr/ADR-023-advanced-message-patterns/#3-consequences","title":"3. Consequences","text":"<ul> <li>Positive:<ul> <li>Maintains clarity and separation of concerns within the <code>athomic</code> modular architecture.</li> <li>Allocates each component where its primary function resides (persistence in <code>database</code>, external communication in <code>integration</code>, internal communication in <code>events</code>).</li> <li>Facilitates independent evolution and maintenance of each pattern.</li> <li>Avoids overloading the <code>integration/messaging</code> module with logic not strictly related to external broker interaction (like the internal event bus or outbox storage).</li> </ul> </li> <li>Negative:<ul> <li>Introduces a new top-level module (<code>athomic/events</code>), slightly increasing the top-level structure.</li> <li>The Outbox Pattern implementation is split across two modules (<code>database</code> and <code>integration/messaging</code>), requiring clear documentation of their interaction.</li> </ul> </li> <li>Neutral/Other:<ul> <li>Reinforces the need for clear interfaces (<code>EventBusProtocol</code>, <code>OutboxStorageProtocol</code>) to enable collaboration between modules.</li> <li>Requires developers to understand the distinction between internal communication (<code>events</code>) and external communication (<code>integration/messaging</code>).</li> </ul> </li> </ul>"},{"location":"adr/ADR-024-transactional-outbox-parttern/","title":"ADR-024: Transactional Outbox Pattern for Reliable Messaging","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-05-05</li> </ul>"},{"location":"adr/ADR-024-transactional-outbox-parttern/#1-context","title":"1. Context","text":"<ul> <li>Problem: When publishing messages/events to an external broker (e.g., Kafka) after a database operation has been committed, there is an inherent risk of the publication failing (e.g., broker unavailable, network error, serialization error). This leads to an inconsistent state: the database change was persisted, but the corresponding event meant to notify other systems or trigger subsequent processes was not sent. This scenario violates delivery guarantees and can lead to desynchronized data or incomplete workflows.</li> <li>Need: To ensure more reliable delivery (\"at least once delivery\" with higher perceived atomicity) of messages/events that are logically coupled with database transactions. The failure to publish the event should not leave the system in a state where the primary action occurred, but the event was lost.</li> <li>Alternatives Considered:<ul> <li>Direct Synchronous Publishing: Calling <code>producer.publish()</code> directly after the DB transaction commits. Rejected: This is the status quo that exhibits the described consistency problem.</li> <li>Distributed Transactions (Two-Phase Commit - 2PC): Attempting to coordinate the database transaction and the broker publication as a single distributed transaction. Rejected: Extremely complex to implement correctly, poorly supported by most modern brokers and databases, significantly impacts performance and availability (CAP theorem).</li> <li>Direct Async Publishing with Robust Producer Retries: Relying solely on the internal retry mechanisms of the <code>ProducerProtocol</code> (e.g., <code>KafkaProducer</code> with <code>acks='all'</code> and configured retries). Rejected: While improving delivery chance, it still doesn't guarantee the message will be sent if the application crashes after the DB commit but before the publication is acknowledged by the broker. It doesn't solve the fundamental \"dual write\" problem.</li> </ul> </li> </ul>"},{"location":"adr/ADR-024-transactional-outbox-parttern/#2-decision","title":"2. Decision","text":"<p>Adopt the Transactional Outbox Pattern for sending critical messages/events coupled with database operations. The implementation will involve the following steps:</p> <ol> <li>Outbox Storage: Create a dedicated collection/table (<code>outbox_events</code>) within the same database as the primary business operations.</li> <li><code>OutboxEvent</code> Model: Define a model (Beanie <code>Document</code>) to represent events to be published, containing fields like <code>event_id</code>, <code>event_name</code>, <code>payload</code>, <code>status</code> (PENDING, PUBLISHED, FAILED), <code>destination_topic</code>, <code>message_key</code>, <code>attempts</code>, <code>last_error</code>, <code>created_at</code>.</li> <li><code>OutboxStorageProtocol</code> Interface: Define a protocol for interacting with the outbox storage, including methods <code>save_event</code>, <code>get_pending_events</code>, <code>mark_event_published</code>, <code>mark_event_failed</code>.</li> <li><code>MongoOutboxRepository</code> Implementation: Implement the <code>OutboxStorageProtocol</code> using Beanie/Motor for MongoDB. Crucially, the <code>save_event</code> method must accept and use the <code>ClientSession</code> object from the main transaction.</li> <li>Service Layer Modification: Business logic currently calling <code>producer.publish()</code> after a DB operation will be changed to:<ul> <li>Start an explicit database transaction (e.g., using <code>motor_client.start_session()</code> and <code>session.start_transaction()</code>).</li> <li>Perform the main business operation (e.g., <code>user_repo.save(user, session=session)</code>).</li> <li>Call <code>outbox_repo.save_event(..., db_session=session)</code> within the same transaction.</li> <li>Allow the transaction to commit (if both operations succeed) or abort (if either fails).</li> </ul> </li> <li>Dedicated Publisher (<code>OutboxPublisher</code>): Create a separate, continuously running process/worker that:<ul> <li>Periodically queries the <code>OutboxStorage</code> for events with <code>PENDING</code> status (<code>get_pending_events</code>).</li> <li>For each pending event, attempts to publish it to the actual message broker using the configured <code>ProducerProtocol</code> (<code>get_producer()</code>).</li> <li>If publishing succeeds, updates the event status to <code>PUBLISHED</code> in the Outbox (<code>mark_event_published</code>).</li> <li>If publishing fails (after producer retries), increments the attempt counter and records the error in the Outbox (<code>mark_event_failed</code>). If the attempt limit is reached, marks the event as permanently <code>FAILED</code> (potentially moving to a DLQ or just marking).</li> <li>(Optional) Implements logic to delete <code>PUBLISHED</code> or permanently <code>FAILED</code> events after a certain period (<code>delete_processed_event</code>).</li> </ul> </li> </ol>"},{"location":"adr/ADR-024-transactional-outbox-parttern/#3-consequences","title":"3. Consequences","text":"<ul> <li>Positive:<ul> <li>Increased Reliability: Significantly enhances the guarantee that an event associated with a DB transaction will eventually be published, even if temporary failures occur in the broker or the application after the commit. The outbox write is atomic with the main operation.</li> <li>Eventual Consistency: Ensures the external state (reflected by messages) eventually becomes consistent with the internal state (database).</li> <li>Temporal Decoupling: The main business logic no longer needs to wait for confirmation from the message broker, potentially making the primary operation faster.</li> <li>Resilience to Broker Downtime: Temporary broker outages do not prevent the main business operation from completing and being recorded for later publication.</li> </ul> </li> <li>Negative:<ul> <li>Increased Complexity: Introduces new components (outbox table, outbox repository, dedicated publisher) and increases the complexity of the publishing flow.</li> <li>Delivery Latency: Events are not published immediately but only after being picked up and processed by the <code>OutboxPublisher</code>. There's inherent latency introduced by the publisher's polling interval.</li> <li>Transaction Management: Requires explicit and careful management of database transactions in the services utilizing the Outbox pattern.</li> <li>Publisher as Potential Bottleneck: The <code>OutboxPublisher</code> becomes a critical component. Its failure or slowness can delay the delivery of all events. Requires monitoring and potentially high availability.</li> <li>Data Duplication (Temporary): The event payload is temporarily stored in the database.</li> <li>Event Ordering: Guarantees the order of saving to the outbox, but the order of publication depends on the publisher logic and the broker (does not guarantee strict FIFO across different events unless using partitioning keys and single consumers per partition).</li> </ul> </li> <li>Neutral/Other:<ul> <li>Requires a mechanism to run the <code>OutboxPublisher</code> (separate process, scheduled task, worker).</li> <li>Needs a strategy for handling permanently failed events (DLQ, manual alerting, cleanup).</li> </ul> </li> </ul>"},{"location":"adr/ADR-025-mongodb-replica-set-config/","title":"ADR-025: MongoDB ReplicaSet Configuration for Local Host-Based Development and Testing","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-05-12</li> </ul>"},{"location":"adr/ADR-025-mongodb-replica-set-config/#context","title":"Context","text":"<p>For the development and testing of features requiring MongoDB transactions (such as the Outbox Pattern), an environment with a MongoDB ReplicaSet is necessary. The goal is to allow integration tests to be executed directly on the developer's host machine, connecting to a multi-node ReplicaSet running in Docker containers.</p> <p>The primary challenge involves name resolution: 1.  MongoDB containers within the ReplicaSet need to communicate with each other using their Docker service names (e.g., <code>mongo_rs1</code>, <code>mongo_rs2</code>). 2.  The MongoDB driver, running in the test application on the host, after connecting to an initial replica set member (e.g., via <code>localhost:&lt;mapped_port&gt;</code>), receives the ReplicaSet topology which includes the Docker service names of the members. 3.  By default, the developer's host machine cannot resolve these Docker-internal service names, leading to connection failures (e.g., <code>ServerSelectionTimeoutError</code>, \"nodename nor servname provided, or not known\").</p>"},{"location":"adr/ADR-025-mongodb-replica-set-config/#decision","title":"Decision","text":"<p>To enable local integration testing from the host machine against a multi-node MongoDB ReplicaSet running in Docker, the following approach will be adopted:</p> <ol> <li> <p>Docker Compose Configuration:</p> <ul> <li>A <code>docker-compose.yml</code> file (located at <code>infra/database/document/mongo/docker-compose.yml</code>) will define three MongoDB services (<code>mongo1</code>, <code>mongo2</code>, <code>mongo3</code>).</li> <li>Each <code>mongod</code> service within its container will listen on its configured internal port (e.g., <code>mongo1</code> on <code>27017</code>, <code>mongo2</code> on <code>27018</code>, <code>mongo3</code> on <code>27019</code>).</li> <li>These internal container ports will be mapped 1:1 to ports on the host (e.g., host <code>localhost:27017</code> -&gt; <code>mongo1:27017</code>; host <code>localhost:27018</code> -&gt; <code>mongo2:27018</code>, etc.).</li> <li>Authentication and the use of a <code>keyFile</code> for ReplicaSet security will be enabled.</li> </ul> </li> <li> <p>ReplicaSet Initialization:</p> <ul> <li>The <code>rs.initiate()</code> command (executed by <code>mongo1</code>'s healthcheck or an initialization script like <code>devtools/mongo/init-replica-set.sh</code>) will configure the ReplicaSet members using their internal Docker service names and their respective internal container ports (e.g., <code>{_id:0, host:'mongo_rs1:27017'}, {_id:1, host:'mongo_rs2:27018'}, ...}</code>). This is crucial for correct inter-node communication within the ReplicaSet.</li> </ul> </li> <li> <p>Host File Modification on Developer's Machine:</p> <ul> <li>For the test application (running on the host) to resolve the ReplicaSet member names (<code>mongo_rs1</code>, <code>mongo_rs2</code>, <code>mongo_rs3</code>) as advertised by MongoDB, it will be necessary to add entries to the developer's machine <code>hosts</code> file (<code>/etc/hosts</code> on macOS/Linux, <code>C:\\Windows\\System32\\drivers\\etc\\hosts</code> on Windows).</li> <li>These entries will map the Docker service names to the loopback address (<code>127.0.0.1</code>):     <code>127.0.0.1 mongo_rs1     127.0.0.1 mongo_rs2     127.0.0.1 mongo_rs3</code></li> <li>A helper script (<code>devtools/mongo/manage_mongo_hosts.sh</code>) will be provided to assist in adding and removing these entries (requiring <code>sudo</code> privileges).</li> </ul> </li> <li> <p>Connection String in Tests:</p> <ul> <li>The MongoDB connection string in tests (executed on the host) should use the service names and their host-mapped ports:     <code>mongodb://nala_user:nala_password@mongo_rs1:27017,mongo_rs2:27018,mongo_rs3:27019/?replicaSet=rs0</code> <code># pragma: allowlist secret</code></li> </ul> </li> </ol>"},{"location":"adr/ADR-025-mongodb-replica-set-config/#alternatives-considered","title":"Alternatives Considered","text":"<ol> <li> <p>Using <code>host.docker.internal</code>:</p> <ul> <li>Pros: Docker Desktop standard for containers to access the host. Theoretically, if used in <code>rs.initiate</code> members AND in the host's connection string, it could work without manual <code>/etc/hosts</code> editing if the host resolves it.</li> <li>Cons: The user reported that <code>ping host.docker.internal</code> failed on their macOS host, indicating that host-side resolution of this name was not working as expected, making this approach unviable for the specific user environment.</li> <li>Status: Rejected due to resolution failure in the user's host environment.</li> </ul> </li> <li> <p>Single-Node Replica Set for Host-Based Tests:</p> <ul> <li>Pros: Much simpler network setup (connect via <code>localhost:&lt;port&gt;</code>), supports transactions, does not require <code>/etc/hosts</code> changes, and does not depend on <code>host.docker.internal</code> host-side resolution.</li> <li>Cons: Does not fully emulate a multi-node production environment; cannot test failover or multi-node specific behaviors.</li> <li>Status: Considered a strong alternative and fallback, especially if <code>/etc/hosts</code> modification is highly undesirable or problematic for some developers. This was the previous recommendation when <code>host.docker.internal</code> proved problematic.</li> </ul> </li> <li> <p>Running Tests Inside a Docker Container:</p> <ul> <li>Pros: Resolves all host-vs-container name resolution issues, as tests would run on the same Docker network as the MongoDB instances. Connection strings would use Docker service names directly.</li> <li>Cons: Alters the local development workflow; can complicate debugging and IDE integration for some developers.</li> <li>Status: Rejected for now, to maintain the simplicity of running tests directly on the host.</li> </ul> </li> <li> <p><code>network_mode: \"host\"</code> for MongoDB Containers:</p> <ul> <li>Pros: MongoDB containers would use the host's network stack, making them directly accessible via <code>localhost:&lt;port&gt;</code>.</li> <li>Cons: Loss of network isolation, potential for port conflicts, inconsistent behavior across platforms (Linux vs. Docker Desktop for Mac/Windows).</li> <li>Status: Rejected due to isolation and portability drawbacks.</li> </ul> </li> </ol>"},{"location":"adr/ADR-025-mongodb-replica-set-config/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-025-mongodb-replica-set-config/#positive","title":"Positive:","text":"<ul> <li>Enables robust local integration testing against a full multi-node MongoDB ReplicaSet from the developer's host machine.</li> <li>The connection string used in tests can utilize the canonical service names, which is clean and mirrors how services might refer to each other in a fully containerized deployment.</li> <li>The internal ReplicaSet configuration (for inter-node communication) remains correct using Docker service names.</li> <li>The approach is made explicit, and the need for host name resolution is addressed with a helper script.</li> </ul>"},{"location":"adr/ADR-025-mongodb-replica-set-config/#negative","title":"Negative:","text":"<ul> <li>Requires intervention on the developer's system: Modifying the <code>hosts</code> file is an additional setup step that must be performed (either manually or via a script requiring <code>sudo</code>).</li> <li>Potential for setup oversight: Developers might forget to add/remove the <code>hosts</code> file entries, leading to connection failures or, less commonly, conflicts.</li> <li>Dependency on <code>sudo</code>: The helper script to manage the <code>hosts</code> file requires administrator privileges.</li> <li>Possible conflicts: Although unlikely for these specific names, if <code>mongo_rs1</code>, etc., are used for other purposes in a developer's <code>hosts</code> file, conflicts could arise. The use of markers in the <code>manage_mongo_hosts.sh</code> script helps mitigate accidental removal of unrelated entries.</li> </ul>"},{"location":"adr/ADR-025-mongodb-replica-set-config/#neutralmitigation","title":"Neutral/Mitigation:","text":"<ul> <li>The complexity of <code>hosts</code> file management is mitigated by providing the <code>manage_mongo_hosts.sh</code> script.</li> <li>Clear documentation (this ADR and <code>infra/README.md</code>) is essential to guide developers through the setup.</li> <li>The single-node replica set remains a viable, simpler alternative for testing transactional features if full multi-node emulation from the host is not strictly necessary or if <code>hosts</code> file modification is to be avoided.</li> </ul>"},{"location":"adr/ADR-026-agnostic-outbox-architecture/","title":"ADR-026: Agnostic Outbox Architecture","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-17</li> <li>Supersedes: ADR-024</li> <li>Replaces: Outbox limited to messaging systems (Kafka, RabbitMQ)</li> </ul>"},{"location":"adr/ADR-026-agnostic-outbox-architecture/#1-context","title":"1. Context","text":"<p>The original ADR-024 defined a transactional Outbox pattern focused primarily on messaging systems like Kafka and RabbitMQ. However, as our platform evolves to support diverse asynchronous integrations (e.g., webhooks, internal async tasks), we need to generalize the Outbox into an agnostic dispatcher capable of supporting multiple delivery mechanisms beyond messaging.</p> <p>This decision emerged from the limitations and growing complexity when trying to route events to destinations that are not message brokers but still require delivery guarantees.</p>"},{"location":"adr/ADR-026-agnostic-outbox-architecture/#2-decision","title":"2. Decision","text":"<p>We adopted a generalized Outbox Architecture with the following characteristics:</p>"},{"location":"adr/ADR-026-agnostic-outbox-architecture/#generalized-event-model","title":"\u2705 Generalized Event Model","text":"<p>We extended the <code>OutboxEvent</code> model to include:</p> <ul> <li><code>event_type</code>: Defines the type of destination integration (<code>MESSAGING</code>, <code>WEBHOOK</code>, <code>TASK</code>, etc.).</li> <li><code>destination</code>: Target topic, URL, task name, etc.</li> <li>Backward-compatible fields like <code>destination_topic</code> remain supported.</li> </ul>"},{"location":"adr/ADR-026-agnostic-outbox-architecture/#outbox-router-executors","title":"\u2705 Outbox Router &amp; Executors","text":"<p>We introduced a pluggable routing mechanism:</p> <ul> <li><code>OutboxEventRouter</code>: Dynamically resolves the correct <code>OutboxExecutor</code> based on the <code>event_type</code>.</li> <li>Executors implement a common interface: <code>OutboxExecutorProtocol</code>.</li> </ul> <p>The current available executors include:</p> <ul> <li>\u2705 <code>MessagingExecutor</code> \u2192 Integrates with Kafka/RabbitMQ using existing protocols.</li> <li>[ ] <code>WebhookExecutor</code> \u2192 Sends events to HTTP endpoints. (Planned)</li> <li>[ ] <code>TaskExecutor</code> \u2192 Triggers internal async logic or workflows. (Planned)</li> </ul>"},{"location":"adr/ADR-026-agnostic-outbox-architecture/#refactor-of-publisher-logic","title":"\u2705 Refactor of Publisher Logic","text":"<p>The core publisher logic (<code>StandardOutboxPublisher</code>) is now:</p> <ul> <li>Protocol-based (agnostic to event destination).</li> <li>Uses <code>OutboxEventRouter</code> to resolve the correct executor.</li> <li>Maintains retry, failure marking, and delivery tracking.</li> </ul>"},{"location":"adr/ADR-026-agnostic-outbox-architecture/#decorator-support","title":"\u2705 Decorator Support","text":"<p>The <code>@publish_on_success</code> decorator now supports agnostic usage:</p> <pre><code>@publish_on_success(\n    event_name=\"user.created\",\n    event_type=OutboxEventType.WEBHOOK,\n    destination=\"https://hooks.myservice.com/events\"\n)\nasync def create_user(...):\n    ...\n</code></pre>"},{"location":"adr/ADR-026-agnostic-outbox-architecture/#backward-compatibility","title":"\u2705 Backward Compatibility","text":"<p>All legacy usages with Kafka/RabbitMQ remain supported with no change required.</p>"},{"location":"adr/ADR-026-agnostic-outbox-architecture/#3-consequences","title":"3. Consequences","text":""},{"location":"adr/ADR-026-agnostic-outbox-architecture/#positive","title":"\u2705 Positive","text":"<ul> <li>Supports more asynchronous integration patterns beyond messaging.</li> <li>Clean separation of publisher from the transport layer (via executor interface).</li> <li>Reusable executor logic and scalable extension points.</li> <li>Improved observability and testability.</li> </ul>"},{"location":"adr/ADR-026-agnostic-outbox-architecture/#negative","title":"\u26a0\ufe0f Negative","text":"<ul> <li>Increased complexity in routing and executor management.</li> <li>Executors need to be carefully designed to be idempotent and retry-safe.</li> </ul>"},{"location":"adr/ADR-026-agnostic-outbox-architecture/#4-related-tasks","title":"4. Related Tasks","text":"<ul> <li>\u2705 <code>OutboxEventRouter</code> implementation.</li> <li>\u2705 <code>MessagingExecutor</code> refactor.</li> <li>\u2705 <code>StandardOutboxPublisher</code> refactor.</li> <li>\u2705 Decorator enhancements.</li> <li>\u2705 Routing tests, publisher tests, integration coverage.</li> <li>[ ] Implement <code>WebhookExecutor</code>.</li> <li>[ ] Implement <code>TaskExecutor</code>.</li> <li>[ ] Update ADR-024 to reference this ADR as successor.</li> </ul>"},{"location":"adr/ADR-026-agnostic-outbox-architecture/#5-status","title":"5. Status","text":"<p>This ADR supersedes ADR-024, which remains as a historical reference for the messaging-only pattern.</p> <p>The architecture defined here is now the standard for any asynchronous reliable event dispatch in the platform.</p>"},{"location":"adr/ADR-027-messaging-dql-serializer/","title":"ADR-027: Messaging Module Architecture with Kafka, Serializer, and Retry (DLQ)","text":"<p>Status: Implemented Date: 2025-05-20 Author: Valter Hugo Guandaline  </p>"},{"location":"adr/ADR-027-messaging-dql-serializer/#context","title":"Context","text":"<p>The <code>athomic-docs</code> project required an extensible and decoupled messaging architecture with support for: - multiple backends (e.g., Kafka, RabbitMQ, GCP Pub/Sub), - customizable serialization for messages (value, key, and headers), - retry policies with support for DLQ (Dead Letter Queue), - both unit and integration tests validating the full publish-consume lifecycle.</p> <p>We decided to start the implementation with Kafka, leveraging its robust ecosystem and support via the <code>aiokafka</code> library.</p>"},{"location":"adr/ADR-027-messaging-dql-serializer/#decision","title":"Decision","text":"<p>We designed a messaging module based on the following pillars:</p>"},{"location":"adr/ADR-027-messaging-dql-serializer/#architecture","title":"\ud83e\uddf1 Architecture","text":"<ul> <li>Protocols:</li> <li><code>ProducerProtocol</code>, <code>ConsumerProtocol</code>, <code>MessageSerializerProtocol</code>, <code>RetryPolicyProtocol</code>.</li> <li>Factories:</li> <li><code>ProducerFactory</code>, <code>ConsumerFactory</code>, <code>MessageSerializerFactory</code>, <code>DLQHandlerFactory</code>.</li> <li>Registries:</li> <li>Allow dynamic registration of providers depending on the backend.</li> <li>Kafka Providers:</li> <li><code>KafkaProducer</code> and <code>KafkaConsumer</code> integrated with <code>aiokafka</code>.</li> </ul>"},{"location":"adr/ADR-027-messaging-dql-serializer/#serialization","title":"\ud83d\udd01 Serialization","text":"<ul> <li>Introduced <code>BaseMessageSerializer</code> with backend-specific overrides like <code>KafkaMessageSerializer</code>.</li> <li>The serializer is used by both producer and consumer, ensuring consistency across message transport.</li> <li>Full support for <code>value</code>, <code>key</code>, and <code>headers</code>.</li> </ul>"},{"location":"adr/ADR-027-messaging-dql-serializer/#testing","title":"\ud83e\uddea Testing","text":"<ul> <li>Unit tests for <code>DLQHandler</code>, <code>MaxAttemptsPolicy</code>, <code>MessageSerializer</code>.</li> <li>Integration test:</li> <li>Publishes a Kafka message using <code>KafkaProducer</code>,</li> <li>Consumes it via <code>KafkaConsumer</code> instantiated from <code>ConsumerFactory</code>,</li> <li>Asserts the round-trip serialization matches expectations.</li> </ul>"},{"location":"adr/ADR-027-messaging-dql-serializer/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-027-messaging-dql-serializer/#positive","title":"\u2705 Positive","text":"<ul> <li>Decoupled and extensible system for supporting additional backends.</li> <li>Safe and testable serialization logic.</li> <li>Fully supported DLQ and retry mechanism.</li> <li>Reliable end-to-end test coverage.</li> </ul>"},{"location":"adr/ADR-027-messaging-dql-serializer/#challenges","title":"\u26a0\ufe0f Challenges","text":"<ul> <li>The serializer behavior must be carefully tested when using pre-serialized messages (avoid double <code>json.dumps()</code>).</li> <li>It's important to align <code>consumer_settings.topics</code> with the <code>topic</code> used in integration tests.</li> <li>The consumer factory must correctly apply the <code>value_deserializer</code>.</li> </ul>"},{"location":"adr/ADR-027-messaging-dql-serializer/#next-steps","title":"Next Steps","text":"<ul> <li>Add native support for RabbitMQ using <code>aio-pika</code>.</li> <li>Implement message-level observability using OpenTelemetry.</li> <li>Enable fallback to <code>BaseMessageSerializer</code> for non-production environments.</li> <li>Optionally explore schema validation (e.g., Avro contracts).</li> </ul>"},{"location":"adr/ADR-027-messaging-dql-serializer/#attachments","title":"Attachments","text":"<ul> <li>Tests: <code>test_messaging_kafka_consumer.py</code>, <code>test_dlq.py</code></li> <li>Code: <code>KafkaProducer</code>, <code>KafkaConsumer</code>, <code>BaseMessageSerializer</code>, <code>MessagingFactory</code>, <code>DLQHandler</code></li> </ul>"},{"location":"adr/ADR-028-messaging-schema-registry/","title":"ADR-028: Messaging Schema Registry Integration","text":"<p>Date: 2025-06-11 Status: Accepted</p>"},{"location":"adr/ADR-028-messaging-schema-registry/#context","title":"Context","text":"<p>The Nala Athomic messaging module supports various data serialization formats, including binary formats like Protocol Buffers (Protobuf). A primary challenge with binary serialization is \"schema drift,\" where producers and consumers evolve their schemas independently. This can lead to runtime <code>DeserializationError</code> exceptions when a consumer receives a message payload it can no longer parse, compromising data integrity and system reliability.</p> <p>To mitigate this, we need a mechanism to validate and manage schemas centrally, ensuring that producers and consumers always operate with compatible schema versions. A Schema Registry is the standard solution for this problem.</p> <p>This ADR details the architecture for integrating a Schema Registry client into the Nala Athomic serialization layer, making it a configurable, extensible, and type-safe feature.</p>"},{"location":"adr/ADR-028-messaging-schema-registry/#decision","title":"Decision","text":"<p>We will implement a flexible, multi-layered, configuration-driven architecture to integrate schema registry functionality. This design is centered around dependency injection and clear separation of concerns, leveraging the existing Factory/Registry patterns within the Athomic framework.</p> <p>The architecture consists of the following key components:</p>"},{"location":"adr/ADR-028-messaging-schema-registry/#1-centralized-serializer-configuration","title":"1. Centralized Serializer Configuration","text":"<p>All settings for a specific, named serializer instance will be defined in a dedicated Pydantic model, <code>MessagingSerializerSettings</code>. This provides a single source of truth for its behavior.</p> <ul> <li>Path: <code>nala.athomic.config.schemas.serializer.messaging_serializer_config.py</code></li> <li>Key Fields:<ul> <li><code>backend: str</code>: The name of the concrete serializer class to be used (e.g., <code>\"protobuf\"</code>), which must be registered in the <code>MessageSerializerRegistry</code>.</li> <li><code>format: Literal[\"json\", \"protobuf\", \"avro\", ...]</code>: The fundamental data format family. This allows base classes to provide safe, format-specific default logic (e.g., the <code>BaseMessageSerializer</code> provides JSON logic only if <code>format</code> is <code>\"json\"</code>).</li> <li><code>handler: Optional[str]</code>: The name of a specialized schema handler to use (e.g., <code>\"confluent_protobuf\"</code>). If <code>None</code>, no handler is used.</li> <li><code>schema_validation_enabled: bool</code>: A flag to enable or disable schema registry integration for this serializer.</li> <li><code>schema_registry_url: Optional[SecretStr]</code>: The URL of the schema registry service.</li> <li>...and other schema-related settings (<code>schema_auto_register</code>, <code>schema_compatibility_level</code>, etc.).</li> </ul> </li> </ul> <p>These named configurations will be defined in a dictionary within the main <code>MessagingSettings</code> object, allowing for multiple, distinct serializer configurations in the application settings.</p> <p>Example <code>settings.toml</code>:</p> <pre><code>[NALA.INTEGRATION.MESSAGING]\n# ...\n  [NALA.INTEGRATION.MESSAGING.SERIALIZER]\n    # Define a named serializer configuration\n    [NALA.INTEGRATION.MESSAGING.SERIALIZER.PROTOBUF_WITH_REGISTRY]\n      backend = \"protobuf\"\n      format = \"protobuf\"\n      handler = \"confluent_protobuf\"\n      schema_validation_enabled = true\n      schema_registry_url = \"http://localhost:8081\"\n      schema_auto_register = true\n</code></pre>"},{"location":"adr/ADR-028-messaging-schema-registry/#2-abstraction-layer-for-handlers","title":"2. Abstraction Layer for Handlers","text":"<p>To keep the serializers decoupled from any specific schema registry library (e.g., <code>confluent-kafka</code>), we introduce a handler abstraction layer.</p> <ul> <li><code>SchemaHandlerProtocol</code>: An interface defining the public contract (<code>serialize</code>, <code>deserialize</code>) that any schema handler must implement.</li> <li><code>BaseSchemaHandler</code>: An abstract base class that provides common, reusable logic, such as a high-performance caching mechanism for underlying library objects (serializers/deserializers).</li> <li><code>ConfluentProtobufHandler</code>: The concrete implementation that inherits from <code>BaseSchemaHandler</code>, implements the <code>SchemaHandlerProtocol</code>, and contains all logic specific to the <code>confluent-kafka</code> library, including the use of <code>SerializationContext</code>.</li> </ul>"},{"location":"adr/ADR-028-messaging-schema-registry/#3-factories-and-registries","title":"3. Factories and Registries","text":"<p>The system relies on factories to read the configuration and construct objects with their dependencies injected.</p> <ul> <li><code>MessageSerializerFactory</code>: The primary factory for creating serializers. It reads the <code>serializer</code> configuration from <code>MessagingSettings</code>, looks up the appropriate <code>serializer_class</code> from the <code>MessageSerializerRegistry</code> based on the <code>backend</code> field, and injects the complete <code>MessagingSettings</code> object into the serializer's constructor.</li> <li><code>SchemaHandlerFactory</code>: A specialized factory responsible for creating schema handlers. It is called by serializers (like <code>ProtobufMessageSerializer</code>) that need a handler. It reads the <code>handler</code> field from the <code>MessagingSerializerSettings</code> to determine which concrete handler to build, instantiates the necessary client (e.g., <code>confluent_kafka.schema_registry.SchemaRegistryClient</code>), and injects it into the handler's constructor.</li> <li>Registries (<code>MessageSerializerRegistry</code>, <code>SchemaHandlerRegistry</code>): These singleton registries map string names to the corresponding Python classes, allowing the factories to be fully decoupled from concrete implementations.</li> </ul>"},{"location":"adr/ADR-028-messaging-schema-registry/#4-concrete-serializer-implementation","title":"4. Concrete Serializer Implementation","text":"<p>Concrete serializers will inherit from <code>BaseMessageSerializer</code> and override methods as needed.</p> <ul> <li><code>ProtobufMessageSerializer.__init__</code>:<ol> <li>Calls <code>super().__init__(...)</code> to set up the basic configuration.</li> <li>Checks <code>if self.settings.schema_validation_enabled</code>.</li> <li>If true, it calls <code>SchemaHandlerFactory.create(self.settings)</code> to obtain and store a fully configured handler instance in <code>self.handler</code>.</li> </ol> </li> <li><code>ProtobufMessageSerializer.serialize_value</code> / <code>deserialize_value</code>:<ol> <li>These methods now contain simple routing logic:</li> <li><code>if self.handler:</code> delegate the call to <code>self.handler.serialize(...)</code> or <code>self.handler.deserialize(...)</code>.</li> <li><code>else:</code> perform standard, no-registry Protobuf serialization/deserialization.</li> </ol> </li> </ul> <p>This design ensures that the <code>ProtobufMessageSerializer</code> itself remains clean and agnostic to the details of the schema registry interaction.</p>"},{"location":"adr/ADR-028-messaging-schema-registry/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-028-messaging-schema-registry/#positive","title":"Positive","text":"<ul> <li>Highly Decoupled Architecture: The <code>ProtobufMessageSerializer</code> depends on the <code>SchemaHandlerProtocol</code> abstraction, not on any concrete library. This makes the system more modular and easier to maintain.</li> <li>Extensible (Open/Closed Principle): Adding support for a new schema registry provider (e.g., Redpanda, AWS Glue) is straightforward:<ol> <li>Create a new handler class that implements the <code>SchemaHandlerProtocol</code>.</li> <li>Register the new handler with a unique name.</li> <li>Update the <code>SchemaHandlerFactory</code> with the logic to instantiate the new handler's client. No changes are needed in the serializers or other core components.</li> </ol> </li> <li>Configuration-Driven: The entire behavior is controlled via clear, explicit, and type-safe configuration (<code>MessagingSerializerSettings</code>), preventing hardcoded values.</li> <li>Architectural Consistency: This design follows the established <code>Protocol -&gt; Registry -&gt; Factory -&gt; Provider</code> pattern used throughout the Nala Athomic framework, making it predictable for developers.</li> <li>Improved Testability: Components can be tested in isolation by mocking their dependencies at the protocol level (e.g., mocking <code>SchemaHandlerProtocol</code> to test the <code>ProtobufMessageSerializer</code>).</li> </ul>"},{"location":"adr/ADR-028-messaging-schema-registry/#negative","title":"Negative","text":"<ul> <li>Increased Number of Classes: This architecture introduces more classes and files (protocols, base classes, factories, registries, handlers) compared to a monolithic approach. This adds a slight learning curve for new developers, which is a standard trade-off for a more robust and maintainable design.</li> </ul>"},{"location":"adr/ADR-029-agnostic-delayed-messaging-strategy/","title":"ADR-029: Agnostic Strategy for Delayed/Scheduled Messaging","text":"<p>Status: Accepted Date: 2025-06-13</p>"},{"location":"adr/ADR-029-agnostic-delayed-messaging-strategy/#context","title":"Context","text":"<p>The application requires the ability to publish messages that are to be consumed only after a specified delay. Key use cases include: - Sending notifications to users after a specific period (e.g., 24 hours). - Implementing scheduled retries with exponential backoff for failed operations. - Scheduling future jobs via the messaging system.</p> <p>A primary architectural challenge is that different message brokers handle delayed messages differently. Apache Kafka, our primary backend, has no native support for message delays, as it operates as an immutable log. In contrast, brokers like RabbitMQ offer native delayed-delivery mechanisms via plugins.</p> <p>A solution is needed that provides a consistent API for application developers (<code>producer.publish(..., delay_seconds=...)</code>) while remaining agnostic to the underlying broker's implementation, thus preventing implementation details from leaking into the business logic layer.</p>"},{"location":"adr/ADR-029-agnostic-delayed-messaging-strategy/#decision","title":"Decision","text":"<p>We will implement a Strategy Pattern to handle delayed message publishing. This decouples the application's intent (delaying a message) from the broker-specific implementation (how the delay is achieved).</p> <p>The key components of this decision are:</p> <ol> <li> <p><code>DelayedPublishingStrategyProtocol</code>: A new protocol will define the interface for any delay strategy. It will include a <code>publish_delayed(...)</code> method and a class attribute <code>requires_republisher: bool</code> to indicate if the strategy needs a background worker.</p> </li> <li> <p><code>KafkaDelayTopicStrategy</code>: The first implementation of the strategy will be for Kafka. It will:</p> <ul> <li>Reroute messages with a <code>delay_seconds</code> parameter to dedicated, time-bucketed \"delay topics\" (e.g., <code>__nala_delay_5m</code>, <code>__nala_delay_1h</code>).</li> <li>Wrap the original message in an \"envelope\" containing metadata like the final destination topic and the target processing timestamp.</li> <li>Set <code>requires_republisher = True</code>.</li> </ul> </li> <li> <p><code>DelayedMessageRePublisher</code> Service: A new, dedicated background service will be created. Its sole responsibility is to consume messages from the delay topics, wait (via <code>asyncio.sleep</code>) until the target timestamp is reached, and then re-publish the original message to its final destination topic.</p> </li> <li> <p><code>InfrastructureLifecycleManager</code>: The <code>DelayedMessageRePublisher</code> will be managed by a separate lifecycle manager. This separates the concerns of managing application-level business consumers from managing internal infrastructure services.</p> </li> <li> <p>Factories and Registries: A <code>DelayStrategyFactory</code> will be responsible for instantiating the correct strategy based on the application's settings. A <code>Registry</code> pattern will be used to map configuration strings (e.g., <code>\"kafka_topic_delay_strategy\"</code>) to their corresponding strategy classes, making the system extensible.</p> </li> <li> <p>Configuration: The strategy and its parameters will be configurable:</p> <ul> <li><code>settings.integration.messaging.delay_strategy_backend</code> will select the active strategy.</li> <li>Provider-specific settings (like Kafka's <code>delay_topics</code> map and <code>republisher</code> consumer settings) will be nested within their respective configuration schemas (e.g., <code>settings.integration.messaging.provider.</code>).</li> </ul> </li> </ol>"},{"location":"adr/ADR-029-agnostic-delayed-messaging-strategy/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-029-agnostic-delayed-messaging-strategy/#positive","title":"Positive:","text":"<ul> <li>Provider Agnostic: The application's business logic is completely decoupled from the underlying messaging infrastructure. Switching to a broker with native delay support would not require any changes to the application code.</li> <li>Extensible: Adding support for a new broker (e.g., RabbitMQ) is straightforward: create a new strategy class, register it, and no other core components need to be modified.</li> <li>Clean Architecture: Adheres to the Single Responsibility Principle (SRP). The logic for delaying messages is encapsulated within the strategies, and the lifecycle of different types of services (application vs. infrastructure) is managed separately.</li> <li>Highly Testable: The use of protocols, factories, and dependency injection makes all new components easy to unit test in isolation.</li> </ul>"},{"location":"adr/ADR-029-agnostic-delayed-messaging-strategy/#negative","title":"Negative:","text":"<ul> <li>Increased Complexity for Kafka: The workaround for Kafka introduces several new components (Delay Topics, <code>RePublisher</code> Service, <code>InfrastructureLifecycleManager</code>), increasing the overall complexity of the messaging system.</li> <li>Operational Overhead: The new delay topics must be created and managed in the Kafka cluster. The <code>DelayedMessageRePublisher</code> service must be deployed, monitored for health, and scaled if necessary.</li> <li>Delay Inaccuracy (Kafka Strategy): The delay achieved by the Kafka strategy is not millisecond-precise. It depends on consumer poll intervals and <code>asyncio.sleep</code> scheduling, making it suitable for business-level delays but not for real-time, high-precision timing.</li> </ul>"},{"location":"adr/ADR-029-agnostic-delayed-messaging-strategy/#considered-options","title":"Considered Options","text":""},{"location":"adr/ADR-029-agnostic-delayed-messaging-strategy/#1-consumer-side-delay-anti-pattern","title":"1. Consumer-Side Delay (Anti-Pattern)","text":"<ul> <li>Description: The final consumer receives the message, inspects a timestamp header, and calls <code>sleep()</code> if the message is premature.</li> <li>Rejected Because: This is highly inefficient. It blocks the consumer thread/task, destroys message throughput, and makes error handling, retries, and consumer scaling extremely difficult.</li> </ul>"},{"location":"adr/ADR-029-agnostic-delayed-messaging-strategy/#2-hardcoded-kafka-specific-logic","title":"2. Hardcoded Kafka-Specific Logic","text":"<ul> <li>Description: Implement the delay-topic logic directly inside the <code>KafkaProducer</code> without the Strategy pattern.</li> <li>Rejected Because: This would create a \"leaky abstraction,\" tightly coupling the application to Kafka's specific workaround and making it very difficult to support other brokers in the future.</li> </ul>"},{"location":"adr/ADR-029-agnostic-delayed-messaging-strategy/#3-use-a-dedicated-task-queue-eg-celery","title":"3. Use a Dedicated Task Queue (e.g., Celery)","text":"<ul> <li>Description: Introduce a separate system like Celery, which is purpose-built for scheduled and delayed tasks.</li> <li>Rejected Because: While powerful, this would add an entirely new, major piece of infrastructure (Celery workers, a new broker like Redis or RabbitMQ for Celery) to the stack. The decision was made to leverage the existing messaging abstraction to contain operational complexity, as the required delay precision was not high.</li> </ul>"},{"location":"adr/ADR-029-idempotent-consumers/","title":"ADR-027: Implementation of Idempotent Consumers","text":"<p>Status: Accepted</p> <p>Date: 2025-06-11</p>"},{"location":"adr/ADR-029-idempotent-consumers/#context-and-problem","title":"Context and Problem","text":"<p>In messaging systems with \"at-least-once\" delivery guarantees, such as Kafka, the same event may be delivered multiple times to a consumer. This can happen due to network failures, consumer rebalancing, or retries.</p> <p>Duplicate processing of the same message can cause undesirable side effects, such as creating duplicate records in the database, sending multiple notifications for the same event, or general state inconsistencies. The framework needs to provide a mechanism to transparently mitigate this risk.</p>"},{"location":"adr/ADR-029-idempotent-consumers/#decision","title":"Decision","text":"<p>We decided to implement an idempotency mechanism directly in the base consumer layer (<code>BaseConsumer</code>), making it available to all types of consumers (Kafka, RabbitMQ, etc.) in a standardized way.</p> <p>The chosen approach consists of:</p> <ol> <li> <p>Idempotency Key: Use a unique <code>message_id</code> that must be provided by the producer in the body of each message. The responsibility for generating this ID (e.g., a UUID) lies with the system originating the event.</p> </li> <li> <p>State Storage: Leverage the <code>athomic.kvstore</code> module (currently with a Redis backend) to store the <code>message_id</code>s of already processed messages. Redis is ideal for this task due to its high performance and support for atomic operations.</p> </li> <li> <p>Atomic Operation: Use Redis's <code>SETNX</code> (set if not exists) operation. For each new message, the consumer tries to insert the <code>message_id</code> into the KV store with a TTL (Time-To-Live).</p> <ul> <li>If the insertion succeeds (the key did not exist), the message is considered new and is processed.</li> <li>If the insertion fails (the key already existed), the message is considered a duplicate, is discarded, and its receipt is acknowledged (ack) without executing business logic.</li> </ul> </li> <li> <p>Configurability: The functionality will be controlled by settings in <code>MessagingSettings</code>, allowing it to be enabled/disabled globally and the idempotency key TTL to be adjusted:</p> <ul> <li><code>IDEMPOTENCY_CHECK_ENABLED</code> (bool)</li> <li><code>IDEMPOTENCY_KEY_TTL_SECONDS</code> (int)</li> </ul> </li> </ol>"},{"location":"adr/ADR-029-idempotent-consumers/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-029-idempotent-consumers/#positive","title":"Positive","text":"<ul> <li>Increased Resilience: The system becomes more robust and predictable, avoiding side effects from duplicate messages.</li> <li>Transparency for Developers: The idempotency logic is abstracted by the framework. Developers implementing a consumer do not need to worry about this control, as long as the <code>message_id</code> contract is followed.</li> <li>Reusability: The solution is implemented in <code>BaseConsumer</code>, and is automatically inherited by any future consumer implementation.</li> </ul>"},{"location":"adr/ADR-029-idempotent-consumers/#negative-or-to-consider","title":"Negative or To Consider","text":"<ul> <li>Dependency on KV Store: Message processing now has a direct dependency on the availability and performance of the KV store (Redis). A failure in Redis can impact message consumption.</li> <li>Latency Overhead: Each message incurs an extra network call to Redis for idempotency verification, adding a small latency to processing.</li> <li>Contract with Producers: Requires discipline from message producers, who are now required to include a unique and idempotent <code>message_id</code> in their payloads.</li> </ul>"},{"location":"adr/ADR-03-base-service/","title":"ADR-030: Adopt a BaseService Class for Stateful Components","text":"<p>Status: Accepted Date: 2025-06-26</p>"},{"location":"adr/ADR-03-base-service/#context","title":"Context","text":"<p>During the development of <code>nala.athomic</code>, it was observed that multiple components managing an external connection or internal state (e.g., <code>Producer</code>, <code>Consumer</code>, <code>CacheProvider</code>) needed to implement repetitive and complex logic for:</p> <ol> <li>Lifecycle Management: Orchestrating <code>connect()</code> and <code>close()</code> operations.</li> <li>Concurrency Control: Using <code>asyncio.Lock</code> to prevent <code>connect()</code> and <code>close()</code> from being called concurrently.</li> <li>State Management: Maintaining flags like <code>_connected</code>, <code>_is_closed</code> to track the service's current state.</li> <li>Observability: Emitting standardized Prometheus metrics and structured logs for connection attempts, failures, and status.</li> <li>Readiness Signaling: Informing the rest of the application when the service is fully initialized and ready to handle traffic.</li> </ol> <p>The absence of a centralized pattern was leading to code duplication, implementation inconsistencies, and increased complexity in maintenance and testing.</p>"},{"location":"adr/ADR-03-base-service/#decision","title":"Decision","text":"<p>We have decided to introduce a Base Service pattern for all stateful components within <code>nala.athomic</code>. This pattern consists of two main parts:</p> <ol> <li> <p><code>nala.athomic.base.ServiceProtocol</code>: An interface contract that defines the public signature of a manageable service (<code>connect</code>, <code>close</code>, <code>is_ready</code>, <code>is_enabled</code>, <code>health</code>). Code that consumes a service should depend on this protocol, not on a concrete implementation.</p> </li> <li> <p><code>nala.athomic.base.BaseService</code>: An abstract base class that implements the <code>ServiceProtocol</code> and provides all the common logic for orchestrating lifecycle, state, concurrency, and observability.</p> </li> </ol> <p>Subclasses (like <code>KafkaConsumer</code>) now inherit from <code>BaseService</code> and only need to implement the abstract <code>_connect()</code> and <code>_close()</code> methods with technology-specific logic, inheriting all the remaining robust behavior.</p>"},{"location":"adr/ADR-03-base-service/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-03-base-service/#positive","title":"Positive","text":"<ul> <li>Consistency and Standardization: All services will have an identical and predictable lifecycle and state behavior.</li> <li>Reduced Boilerplate: Eliminates the need to rewrite complex logic for locks, <code>try/except/finally</code> blocks, state flags, and metrics in every new component.</li> <li>Observability by Default: Any new service inheriting from <code>BaseService</code> automatically gains connection and readiness metrics, as well as structured logs.</li> <li>Improved Developer Experience (DX): Makes the creation of new, robust components significantly faster and less error-prone.</li> <li>Enhanced Reliability: The lifecycle logic, being centralized, can be rigorously tested in a single place.</li> </ul>"},{"location":"adr/ADR-03-base-service/#negative","title":"Negative","text":"<ul> <li>Coupling to the Base Class: Subclasses now have a direct coupling to <code>BaseService</code>. Although this is mitigated by using the <code>ServiceProtocol</code> in the service's consumers, the implementation itself is tied to inheritance.</li> <li>Learning Curve: New developers must first learn the <code>BaseService</code> pattern before they can contribute new stateful components, rather than creating them ad-hoc.</li> </ul>"},{"location":"adr/ADR-031-kvstore-refactoring/","title":"ADR-031: Refactoring the KVStore Module to Use <code>BaseService</code>","text":"<p>Status: Accepted Date: 2025-06-27</p>"},{"location":"adr/ADR-031-kvstore-refactoring/#context","title":"Context","text":"<p>The Key-Value Store module (<code>athomic/database/kvstore</code>) previously contained multiple provider implementations (e.g., <code>RedisKVClient</code>, <code>LocalMemoryKVClient</code>) that managed their own connection lifecycles and state independently.</p> <p>This resulted in several issues:</p> <ol> <li>Duplicated Logic: Each provider implemented its own connection, reconnection, and concurrency control logic (e.g., <code>_connection_lock</code>).</li> <li>Data Inconsistency: The way data was stored varied between providers. <code>RedisKVClient</code> serialized objects to JSON, while <code>LocalMemoryKVClient</code> stored Python objects directly, leading to different behaviors between production and test environments.</li> <li>Scattered Observability: Metrics and tracing (if present) had to be implemented in each provider, leading to inconsistencies.</li> <li>Implicit Lifecycle: \"Lazy connection\" made connection failures unpredictable, as they could occur during any operation (<code>get</code>, <code>set</code>, etc.) rather than explicitly at application startup.</li> </ol> <p>This approach violated the DRY (Don't Repeat Yourself) principle and made maintenance and the addition of new providers more complex and error-prone.</p>"},{"location":"adr/ADR-031-kvstore-refactoring/#decision","title":"Decision","text":"<p>Based on the pattern established in ADR-030 (Adoption of a <code>BaseService</code> Class), we decided to completely refactor the <code>kvstore</code> module to adopt a clear inheritance architecture with well-defined responsibilities.</p> <p>The new hierarchical structure is: <code>BaseService</code> -&gt; <code>BaseKVStoreProvider</code> -&gt; <code>ConcreteImplementation</code> (e.g., RedisKVClient)</p> <p>The main decisions implemented are:</p> <ol> <li> <p>Creation of <code>BaseKVStoreProvider</code>: A new abstract base class was created in <code>athomic/database/kvstore/providers/base.py</code>. It inherits from <code>BaseService</code> and is responsible for:</p> <ul> <li>Orchestrating the Lifecycle: Provides safe public methods like <code>get</code>, <code>set</code>, <code>delete</code>, etc.</li> <li>Centralizing Observability: Emits Prometheus metrics (<code>kvstore_operations_total</code>, <code>kvstore_operation_duration_seconds</code>) and creates tracing spans for each operation, ensuring uniform monitoring for any provider.</li> <li>Standardizing Serialization: Receives a <code>MessageSerializerProtocol</code> in its constructor, ensuring all data is serialized before being passed to the concrete provider and deserialized when read.</li> <li>Defining an Internal Contract: Defines internal abstract methods (<code>_get</code>, <code>_set</code>, etc.) that subclasses must implement.</li> </ul> </li> <li> <p>Simplification of Concrete Providers: Classes like <code>RedisKVClient</code> and <code>LocalMemoryKVClient</code> were refactored to inherit from <code>BaseKVStoreProvider</code>. Their responsibilities were reduced to:</p> <ul> <li>Implementing backend-specific connection and closing logic in the <code>_connect()</code> and <code>_close()</code> hooks.</li> <li>Implementing internal operation methods (<code>_get</code>, <code>_set</code>, etc.) dealing only with <code>bytes</code>, without worrying about serialization, metrics, or tracing.</li> </ul> </li> <li> <p>Abandonment of Lazy Connection: The KV clients' lifecycle is now explicit. The connection must be established via <code>await kv_client.connect()</code> during application initialization, making the system more predictable.</p> </li> </ol>"},{"location":"adr/ADR-031-kvstore-refactoring/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-031-kvstore-refactoring/#positive","title":"Positive","text":"<ul> <li>Architectural Consistency: The <code>kvstore</code> module now follows the same robust design pattern as other stateful components, such as those in <code>messaging</code>.</li> <li>Elimination of Duplicated Code: All observability, serialization, concurrency control, and state management logic has been moved to a single place.</li> <li>Robustness and Predictability: Explicit initialization and centralized state handling eliminate race conditions and make system behavior in case of connection failure much clearer.</li> <li>Simplified Extensibility: Adding a new provider (e.g., <code>MemcachedKVClient</code>) is now trivial, requiring only the implementation of internal methods with backend logic.</li> <li>Improved Testability: The clear separation of responsibilities makes it easier to create focused unit tests by mocking different layers of the abstraction.</li> </ul>"},{"location":"adr/ADR-031-kvstore-refactoring/#negative","title":"Negative","text":"<ul> <li>Slight Performance Overhead: The serialization layer and the extra call between the public and internal methods introduce a minimal theoretical latency. However, this cost is considered negligible compared to network latency and the immense gains in robustness and maintainability.</li> <li>Adherence to the Pattern: Creating new KV providers is now (correctly) coupled to the <code>BaseKVStoreProvider</code> pattern, requiring developers to follow the defined architecture.</li> </ul>"},{"location":"adr/ADR-031-promote-serializer-to-core/","title":"ADR-031: Promote Serializer to a Core Framework Component","text":"<p>Status: Accepted Date: 2025-06-27</p>"},{"location":"adr/ADR-031-promote-serializer-to-core/#context","title":"Context","text":"<p>The initial implementation of the <code>serializer</code> module was located within <code>nala.athomic.integration.messaging.serializer</code>. This was logical when its only consumer was the messaging system for handling message payloads.</p> <p>However, as the framework evolved, other components, specifically the <code>BaseKVStoreProvider</code> (as per ADR-031), required a standardized way to serialize and deserialize data to ensure consistency across different backends (e.g., Redis and in-memory).</p> <p>This created a problematic semantic dependency, where the <code>database</code> module would need to import from the <code>messaging</code> module (<code>from nala.athomic.integration.messaging.serializer import ...</code>). This coupling is misleading, as the database functionality has no true dependency on the messaging system. It violates the principles of high cohesion and low coupling, making the architecture harder to understand and maintain.</p>"},{"location":"adr/ADR-031-promote-serializer-to-core/#decision","title":"Decision","text":"<p>We have decided to refactor the <code>serializer</code> module, promoting it from a specific tool within the messaging system to a first-class, core component of the <code>athomic</code> framework.</p> <p>The entire module, including its protocol, factory, registry, and providers, will be moved from <code>nala.athomic.integration.messaging.serializer</code> to a new, top-level package: <code>nala.athomic.serializers</code>.</p> <p>The new dependency graph will be as follows: - <code>athomic.integration.messaging</code> -&gt; <code>athomic.serializers</code> - <code>athomic.database.kvstore</code> -&gt; <code>athomic.serializers</code></p> <p>This change makes it clear that serialization is a cross-cutting concern available to any module within the framework that requires it.</p>"},{"location":"adr/ADR-031-promote-serializer-to-core/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-031-promote-serializer-to-core/#positive","title":"Positive","text":"<ul> <li>Improved Decoupling and Cohesion: The <code>messaging</code> module is no longer responsible for a shared, framework-level capability. The <code>serializer</code> module now has a single, clear responsibility.</li> <li>Architectural Clarity: The location <code>athomic/serializers</code> explicitly communicates the module's role as a foundational utility. This makes the overall architecture easier to understand for current and future developers.</li> <li>Breaks Misleading Dependencies: Eliminates the confusing import path where the <code>database</code> module appeared to depend on the <code>messaging</code> module.</li> <li>Enhanced Scalability and Maintainability: Provides a clean, logical, and centralized location to add new serialization formats (e.g., Protobuf, Avro) in the future without affecting unrelated modules.</li> </ul>"},{"location":"adr/ADR-031-promote-serializer-to-core/#negative","title":"Negative","text":"<ul> <li>One-Time Refactoring Effort: Requires a project-wide search-and-replace operation to update all import paths that previously pointed to the old location. This is a minor, one-time cost.</li> </ul>"},{"location":"adr/ADR-032-looking-as-first-class-resilience-module/","title":"ADR-032: Locking as a First-Class Resilience Module","text":"<p>Status: Accepted Date: 2025-06-29</p>"},{"location":"adr/ADR-032-looking-as-first-class-resilience-module/#context","title":"Context","text":"<p>The initial implementation of distributed locking was a utility function (<code>acquire_lock</code>) located within the <code>performance.cache</code> module. This implementation was tightly coupled to a specific Redis client factory and had an implicit fallback to an in-memory <code>asyncio.Lock</code>.</p> <p>This design had several drawbacks: 1.  Tight Coupling: Locking, a core resilience pattern, was hidden inside the caching module. 2.  Lack of Extensibility: It was difficult to introduce new locking backends without modifying the core function. 3.  Implicit Fallback: The automatic fallback to an in-memory lock could mask configuration issues and behave incorrectly in a multi-instance production environment.</p>"},{"location":"adr/ADR-032-looking-as-first-class-resilience-module/#decision","title":"Decision","text":"<p>We have decided to elevate \"Locking\" to a first-class resilience pattern within the <code>athomic</code> framework. A new, dedicated module, <code>nala.athomic.resilience.locking</code>, has been created.</p> <p>This module follows the standard framework pattern: 1.  <code>LockingProtocol</code>: Defines a clear interface for any lock provider. 2.  Providers (<code>RedisLockProvider</code>, <code>InMemoryLockProvider</code>): Concrete implementations for different backends. The <code>RedisLockProvider</code> now depends on a <code>KVStoreProtocol</code> instance, making its dependency explicit. 3.  <code>LockingRegistry</code>: A class-based registry to map backend names to provider classes. 4.  <code>LockingFactory</code>: A factory responsible for creating the appropriate lock provider based on the application's global configuration, making the choice between <code>redis</code> and <code>in_memory</code> explicit.</p> <p>The old <code>acquire_lock</code> function is now deprecated and replaced by calls to <code>LockingFactory.create()</code>.</p>"},{"location":"adr/ADR-032-looking-as-first-class-resilience-module/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-032-looking-as-first-class-resilience-module/#positive","title":"Positive","text":"<ul> <li>Decoupling: Locking is now a standalone, reusable feature, available to any part of the application, not just the cache.</li> <li>Consistency: The module follows the established architectural pattern of the framework, making it predictable and easy to understand.</li> <li>Explicit Configuration: The choice of locking backend is now explicitly determined by the application's configuration, eliminating \"magic\" fallbacks.</li> <li>Improved Testability: The <code>RedisLockProvider</code> and <code>InMemoryLockProvider</code> can be tested in complete isolation.</li> </ul>"},{"location":"adr/ADR-032-looking-as-first-class-resilience-module/#negative","title":"Negative","text":"<ul> <li>A minor increase in the number of files, which is justified by the significant improvement in architectural clarity.</li> </ul>"},{"location":"adr/ADR-033-standardized-contextual-key-generation/","title":"ADR-033: Standardized Contextual Key Generation","text":"<p>Status: Accepted Date: 2025-06-29</p>"},{"location":"adr/ADR-033-standardized-contextual-key-generation/#context","title":"Context","text":"<p>Key generation was fragmented and inconsistent across the codebase. - The caching module used a utility function (<code>hash_key</code>) that attempted to handle multi-tenancy. - The <code>KeyResolvingKVClient</code> wrapper used a separate <code>KeyResolver</code> class for a similar purpose. - This led to code duplication and potential bugs, such as the <code>CacheKeyGenerator</code> initially missing the multi-tenancy logic. There was no single source of truth for how a key should be formatted based on context (tenant, user), namespace, and prefixes.</p>"},{"location":"adr/ADR-033-standardized-contextual-key-generation/#decision","title":"Decision","text":"<p>We have decided to unify all key generation logic into a single, powerful, and versatile class: <code>nala.athomic.context.ContextualKeyGenerator</code>.</p> <p>This class is now the sole authority on creating keys. Its responsibilities include: 1.  Reading default behaviors (e.g., <code>use_tenant</code>, <code>static_prefix</code>) from the global application settings. 2.  Allowing these defaults to be overridden during instantiation for specific use cases. 3.  Dynamically fetching the current <code>ExecutionContext</code> (<code>tenant_id</code>, <code>user_id</code>) when a key is generated. 4.  Providing two main methods:     - <code>generate(*parts)</code>: For creating simple, namespaced keys (used by <code>KeyResolvingKVClient</code>).     - <code>generate_for_function(func, args, kwargs)</code>: For creating deterministic, hashed keys for function caching (used by the <code>@cache_result</code> decorator).</p> <p>The old <code>KeyResolver</code> class and <code>hash_key</code> utility function have been deprecated and removed in favor of this new, centralized service.</p>"},{"location":"adr/ADR-033-standardized-contextual-key-generation/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-033-standardized-contextual-key-generation/#positive","title":"Positive","text":"<ul> <li>Single Source of Truth: All key generation logic is now in one place, eliminating inconsistencies and making the system easier to reason about.</li> <li>Consistency (DRY): Both the KVStore wrappers and the caching decorators now use the exact same underlying mechanism, ensuring keys are always formatted identically.</li> <li>Bug Prevention: Critical logic, such as including the <code>tenant_id</code> in keys when multi-tenancy is enabled, is now handled centrally and reliably.</li> <li>Improved Cohesion: The <code>context</code> module is now properly responsible for all context-aware logic, including key generation.</li> </ul>"},{"location":"adr/ADR-033-standardized-contextual-key-generation/#negative","title":"Negative","text":"<ul> <li>None. This change represents a pure architectural improvement, simplifying the code and increasing robustness.</li> </ul>"},{"location":"adr/ADR-034-background-worker-base/","title":"ADR-034: Dedicated Abstraction for Background Services (Workers)","text":"<p>Date: 2025-06-30</p> <p>Status: Accepted</p>"},{"location":"adr/ADR-034-background-worker-base/#context","title":"Context","text":"<p>During the refactoring of <code>OutboxPublisherBase</code>, we identified a design issue in the <code>athomic</code> architecture. The <code>BaseService</code> class was designed to manage the lifecycle of \"connectable\" resources (like database or cache clients), with a public API of <code>connect()</code> and <code>close()</code>.</p> <p>We were attempting to force services that run a continuous background process (such as <code>OutboxPublisher</code> or <code>DelayedMessageRePublisher</code>) to inherit from <code>BaseService</code>. This resulted in a semantically confusing API, where <code>connect()</code> meant \"start a loop\" and <code>close()</code> meant \"stop a loop\". Furthermore, the \"connection\" metrics (<code>service_connection_status</code>, etc.) did not accurately describe the state of a background service, which should be \"running\" or \"stopped\".</p> <p>The complexity and lack of clarity indicated the need for a more suitable abstraction for this type of component.</p>"},{"location":"adr/ADR-034-background-worker-base/#decision","title":"Decision","text":"<p>We have decided to introduce a new, first-class abstraction dedicated to services that execute background tasks, which we will call Workers.</p> <ol> <li> <p>New Protocol (<code>RunnableServiceProtocol</code>):</p> <ul> <li>A new <code>RunnableServiceProtocol</code> interface will be created.</li> <li>This interface will define a clear and semantically correct public API for workers, with the following methods:<ul> <li><code>start()</code>: Starts the background process.</li> <li><code>stop()</code>: Gracefully stops the background process.</li> <li><code>is_running() -&gt; bool</code>: Checks if the worker's task is active.</li> <li><code>is_ready() -&gt; bool</code>: Checks if the worker is ready to process work (it might be running but still initializing).</li> <li><code>health() -&gt; dict</code>: Returns a dictionary with the detailed health status of the worker.</li> </ul> </li> </ul> </li> <li> <p>New Base Class (<code>RunnableServiceBase</code>):</p> <ul> <li>A new base class <code>RunnableServiceBase</code> will be created that implements the <code>RunnableServiceProtocol</code>.</li> <li>This class will not inherit from <code>BaseService</code> to maintain a clear separation of concepts.</li> <li>It will contain its own robust logic to manage the <code>asyncio.Task</code> lifecycle of the worker, including locks to ensure <code>start()</code> and <code>stop()</code> are idempotent, and an <code>asyncio.Event</code> to manage the <code>readiness</code> state.</li> </ul> </li> <li> <p>Dedicated Metrics:</p> <ul> <li>A new set of Prometheus metrics will be created, prefixed with <code>runnable_service_*</code> and using the label <code>worker_name</code>.</li> <li>The metrics will include:<ul> <li><code>runnable_service_running_status</code>: A <code>Gauge</code> with 3 states: <code>-1</code> (not started), <code>0</code> (stopped), <code>1</code> (running).</li> <li><code>runnable_service_readiness_status</code>: A <code>Gauge</code> for the readiness state (0 or 1).</li> <li><code>runnable_service_starts_total</code>: A <code>Counter</code> for <code>start</code> attempts.</li> <li><code>runnable_service_start_failures_total</code>: A <code>Counter</code> for <code>start</code> failures.</li> </ul> </li> <li>This avoids polluting the <code>service_*</code> metrics (which remain for connectable components) and makes observability more precise.</li> </ul> </li> <li> <p>Refactoring of <code>OutboxPublisherBase</code>:</p> <ul> <li>The <code>OutboxPublisherBase</code> will be refactored to inherit from the new <code>RunnableServiceBase</code>.</li> <li>Its manual management logic for <code>_run_task</code> and <code>_stop_event</code> will be completely removed.</li> <li>The class will now only need to implement the <code>_run_loop()</code> method, containing the specific business logic of the publisher.</li> </ul> </li> </ol>"},{"location":"adr/ADR-034-background-worker-base/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-034-background-worker-base/#positive","title":"Positive","text":"<ul> <li>Semantic Clarity: The architecture now clearly distinguishes between \"connectable\" services (<code>BaseService</code>) and \"runnable\" services (<code>RunnableServiceBase</code>), with public APIs (<code>connect/close</code> vs. <code>start/stop</code>) that match their function.</li> <li>Developer Simplicity: Developing new background workers (like <code>DelayedMessageRePublisher</code>) becomes much simpler. One only needs to inherit from <code>RunnableServiceBase</code> and implement the <code>_run_loop</code> logic.</li> <li>Precise Observability: The new dedicated metrics provide a much clearer and more actionable view of the state of background workers, without ambiguity with connection metrics.</li> <li>Maintainability: Centralizing the background task management logic into a robust base class reduces code duplication and the likelihood of bugs.</li> </ul>"},{"location":"adr/ADR-034-background-worker-base/#negative","title":"Negative","text":"<ul> <li>Increased Framework Complexity: The introduction of a new base class (<code>RunnableServiceBase</code>) and a new protocol slightly increases the conceptual surface area of the <code>athomic</code> framework. However, the gain in clarity and maintainability justifies this addition.</li> </ul>"},{"location":"adr/ADR-035-security-error-handle-policy/","title":"ADR-035: Standardize Error Handling in Secret Providers to Always Raise Exceptions","text":"<p>Status: Accepted</p> <p>Date: 2025-07-03</p>"},{"location":"adr/ADR-035-security-error-handle-policy/#context","title":"Context","text":"<p>The <code>nala.athomic.security.secrets</code> module uses a provider pattern (<code>SecretsBase</code>) to abstract how secrets are obtained (Vault, environment variables, files).</p> <p>It was identified that the error handling contract was inconsistent. The <code>get_secret</code> method could: 1.  Return <code>None</code> in case of connection failures or server errors (HTTP 5xx). 2.  Raise <code>SecretNotFoundError</code> if a resource was not found (HTTP 404).</p> <p>This dual error signaling (returning <code>None</code> and raising exceptions) makes consumer code more complex, requiring both a return value check and wrapping the call in a <code>try...except</code> block. This increases the chance of bugs, especially if the <code>None</code> case is not handled correctly.</p>"},{"location":"adr/ADR-035-security-error-handle-policy/#decision","title":"Decision","text":"<p>To create a more robust, explicit, and consistent API contract, we decided that all data-fetching methods in providers (<code>get_secret</code>, etc.) must adhere to the following error handling rule:</p> <ul> <li>On success, the method must return the requested value (e.g., <code>str</code>). The return value must never be <code>None</code> to indicate a failure.</li> <li>On any failure, the method MUST raise a specific exception that inherits from a common base class (e.g., <code>SecretProviderError</code>). This includes, but is not limited to:<ul> <li>Resource not found (<code>SecretNotFoundError</code>).</li> <li>Connection or network failure (<code>SecretConnectionError</code>).</li> <li>Permission or authentication error (<code>SecretAuthenticationError</code>).</li> <li>Unexpected provider errors.</li> </ul> </li> </ul> <p>Returning <code>None</code> as a way to signal an operational error is prohibited.</p>"},{"location":"adr/ADR-035-security-error-handle-policy/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-035-security-error-handle-policy/#positive","title":"Positive","text":"<ul> <li>Consistent API: All secret providers will have the same behavior in case of failure, simplifying their usage.</li> <li>Explicit Failures (Fail-Fast): Errors no longer silently pass as a <code>None</code> value. An unhandled exception interrupts the flow, making problems immediately visible during development and testing.</li> <li>Simplified Consumer Code: Code using the provider only needs a <code>try...except</code> block, without additional checks like <code>if secret is None:</code>.</li> <li>Improved Debugging: Exceptions carry more context about the nature of the failure (e.g., the original exception such as <code>httpx.ConnectError</code> as <code>__cause__</code>) than a simple <code>None</code>.</li> </ul>"},{"location":"adr/ADR-035-security-error-handle-policy/#negative","title":"Negative","text":"<ul> <li>More Verbose Consumer Code: Code consuming the providers is now required to use <code>try...except</code> blocks for any call, which may be seen as more verbose. However, for a critical component like secret management, this verbosity is justified by the security and robustness it provides.</li> </ul>"},{"location":"adr/ADR-036-standardized-integration-pattern/","title":"ADR-036: Standardized Integration Pattern for External Stateful Services via a Managed Client","text":"<p>Status: Accepted</p> <p>Date: 2025-07-03</p>"},{"location":"adr/ADR-036-standardized-integration-pattern/#context","title":"Context","text":"<p>The application needs to integrate with various external stateful services, starting with HashiCorp Consul. These services require managing a connection lifecycle (connect, health check, disconnect) and sharing a single, efficient connection pool across the application to avoid resource exhaustion.</p> <p>Without a standardized pattern, different modules might create their own client instances, leading to: -   Multiple unmanaged connection pools. -   Inconsistent configuration handling. -   Difficulty in managing the application's startup and shutdown sequences gracefully. -   Poor testability, as mocking direct client instantiations is complex and error-prone.</p>"},{"location":"adr/ADR-036-standardized-integration-pattern/#decision","title":"Decision","text":"<p>We have decided to establish a standardized, three-part pattern for integrating external stateful services, using Consul as the first implementation:</p> <ol> <li> <p>Managed Client Wrapper (<code>ConsulClient</code>): A dedicated class that encapsulates the third-party client (e.g., <code>consul.aio.Consul</code>). This class must inherit from our <code>BaseService</code> to integrate its lifecycle (<code>_connect</code>, <code>_close</code>) with the application's startup and shutdown events. It is responsible for all direct interaction logic with the external service.</p> </li> <li> <p>Singleton Factory (<code>ConsulClientFactory</code>): A singleton factory will be the sole entry point for accessing the managed client instance. Its <code>create()</code> method ensures that only one instance of the <code>ConsulClient</code> is created throughout the application's lifetime, enforcing a single connection pool. This factory reads its configuration directly from the global <code>get_settings()</code> function.</p> </li> <li> <p>Consumer Interaction: Any part of the application that needs to interact with the service must obtain the client instance via the factory (<code>ConsulClientFactory.create()</code>). It should then use <code>await instance.wait_ready()</code> before making calls to ensure the connection has been successfully established by the <code>BaseService</code> lifecycle manager.</p> </li> </ol>"},{"location":"adr/ADR-036-standardized-integration-pattern/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-036-standardized-integration-pattern/#pros","title":"Pros","text":"<ul> <li>Architectural Consistency: This pattern can be replicated for any future external service integration (e.g., StatsD, gRPC clients), making the codebase predictable and easier to navigate.</li> <li>Guaranteed Singleton: The factory pattern prevents resource leaks and performance issues by ensuring only one connection pool is used per service.</li> <li>Graceful Lifecycle Management: By inheriting from <code>BaseService</code>, all external connections are automatically established on application startup and gracefully closed on shutdown, improving reliability.</li> <li>High Testability: This pattern is extremely test-friendly.<ul> <li>Unit Tests: Can easily mock the factory (<code>@patch('...ConsulClientFactory.create')</code>) to inject a mock client into consumer services.</li> <li>Integration Tests: Can instantiate the client directly with explicit test configurations, bypassing the global settings and ensuring test isolation.</li> </ul> </li> <li>Centralized Configuration: All configuration logic is centralized within the factory, which reads from the global settings module. Consumers of the client do not need to be aware of configuration details.</li> </ul>"},{"location":"adr/ADR-036-standardized-integration-pattern/#cons","title":"Cons","text":"<ul> <li>Increased Boilerplate: This pattern is more verbose than direct instantiation. It requires creating two new classes (Wrapper and Factory) for each new service integration.</li> <li>Layer of Indirection: Developers must know to use the factory instead of directly instantiating the client library. This requires proper documentation and adherence to the established pattern.</li> </ul> <p>This trade-off is considered highly favorable, as the initial boilerplate provides significant long-term benefits in robustness, consistency, and maintainability.</p>"},{"location":"adr/ADR-037-standardized-architecture-for-the-feature-flag-module/","title":"ADR-037: Standardized Architecture for the Feature Flag Module","text":"<p>Status: Accepted</p> <p>Date: 2025-07-04</p>"},{"location":"adr/ADR-037-standardized-architecture-for-the-feature-flag-module/#context","title":"Context","text":"<p>The application requires a feature flag system to enable or disable features dynamically without deploying new code. This system needs to be flexible, supporting different backends (local configuration for development, Redis or Consul for production), while remaining performant, observable, and easy to use across the codebase.</p> <p>An initial implementation could lead to scattered logic, inconsistent error handling, and poor testability. A clear architectural pattern is needed to ensure the feature flag module is robust, extensible, and consistent with the rest of the <code>athomic-docs</code> architecture.</p>"},{"location":"adr/ADR-037-standardized-architecture-for-the-feature-flag-module/#decision","title":"Decision","text":"<p>We have decided to implement a comprehensive, multi-layered architecture for the feature flag module, based on the following key principles:</p> <ol> <li> <p>Abstract Protocol (<code>FeatureFlagProtocol</code>): A clear, abstract interface defines the core capabilities (<code>check</code>, <code>get_variant_details</code>). This ensures that any consumer of the feature flag system interacts with a consistent API, regardless of the underlying backend.</p> </li> <li> <p>Abstract Base Class (<code>FeatureFlagBase</code>): A shared base class was created to handle all common, cross-cutting concerns for the providers. This class is responsible for:</p> <ul> <li>Lifecycle Management: It inherits from <code>BaseService</code> to hook into the application's startup and shutdown lifecycle.</li> <li>Metrics Collection: It automatically records Prometheus metrics (<code>feature_flag_evaluations_total</code>, <code>feature_flag_evaluation_duration</code>) for every flag evaluation.</li> <li>Robust Error Handling: It wraps all data-fetching calls in a <code>try...except</code> block, ensuring that any exception from a provider (e.g., a network error) is caught, logged, and results in a safe default value being returned to the consumer.</li> </ul> </li> <li> <p>Concrete Providers (<code>FFLocalProvider</code>, <code>FFConsulProvider</code>, <code>FFRedisProvider</code>): Each backend is implemented as a specific class that inherits from <code>FeatureFlagBase</code>. The sole responsibility of these classes is to implement the <code>_fetch_*</code> methods, containing only the logic required to retrieve data from their specific source (a dictionary, Consul KV, or Redis).</p> </li> <li> <p>Singleton Factory (<code>FeatureFlagFactory</code>): A singleton factory is the single entry point for the entire application to get a feature flag provider.</p> <ul> <li>It reads the application's configuration (<code>get_settings()</code>).</li> <li>It uses a <code>_provider_map</code> (acting as a manual registry) to select and instantiate the correct provider based on the configured <code>backend</code>.</li> <li>It guarantees that only one instance of the provider is created, ensuring resource efficiency.</li> </ul> </li> <li> <p>Reusability of Existing Clients: The <code>FFConsulProvider</code> and <code>FFRedisProvider</code> do not create their own connections. They reuse the existing singleton clients provided by the <code>ConsulClientFactory</code> and <code>KVStoreFactory</code>, respectively. This enforces a single connection pool and centralizes connection management.</p> </li> </ol>"},{"location":"adr/ADR-037-standardized-architecture-for-the-feature-flag-module/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-037-standardized-architecture-for-the-feature-flag-module/#pros","title":"Pros","text":"<ul> <li>High Cohesion, Low Coupling: Each component has a single, well-defined responsibility. Providers only know how to fetch data; the base class knows how to handle metrics and errors; the factory knows how to build the provider.</li> <li>Extensibility (Open/Closed Principle): Adding a new backend (e.g., LaunchDarkly) is straightforward. It only requires creating a new provider class that implements the abstract methods and adding it to the <code>_provider_map</code> in the factory. No other part of the system needs to change.</li> <li>Guaranteed Observability: Because metrics are handled in the base class, it is impossible to add a new provider that forgets to be instrumented. All flag evaluations are automatically monitored.</li> <li>Increased Robustness: The centralized error handling in the base class ensures that a failure in a backend service (like Consul or Redis) will not crash the application. It will gracefully degrade by returning a default value.</li> <li>Consistency: The pattern aligns perfectly with other modules like <code>Secrets</code> and <code>ConsulClient</code>, making the entire codebase easier to understand and maintain.</li> </ul>"},{"location":"adr/ADR-037-standardized-architecture-for-the-feature-flag-module/#cons","title":"Cons","text":"<ul> <li>Increased Boilerplate: This pattern is more verbose than direct instantiation. It requires creating two new classes (Wrapper and Factory) for each new service integration.</li> <li>Indirection: Developers need to understand the pattern and know to use the <code>FeatureFlagFactory</code> instead of trying to instantiate the client library. This requires proper documentation and adherence to the established pattern.</li> </ul> <p>This trade-off is deemed highly favorable, as the initial setup effort provides a foundation that is significantly more robust, maintainable, and scalable for the long-term health of the project.</p>"},{"location":"adr/ADR-038-base-registry-refact/","title":"ADR-038: Refactoring BaseRegistry with <code>__init_subclass__</code> for State Isolation","text":"<p>Status: Accepted</p> <p>Date: 2025-07-05</p>"},{"location":"adr/ADR-038-base-registry-refact/#context-and-problem","title":"Context and Problem","text":"<p>The original implementation of <code>BaseRegistry</code> required each subclass (e.g., <code>EventBusRegistry</code>) to initialize its own registry dictionary (<code>_registry</code>). While functional, this pattern posed risks and inconsistencies:</p> <ol> <li>Risk of Shared State: If a subclass forgot to redefine <code>_registry</code>, it could accidentally share the parent class's dictionary, leading to state leakage between different types of registries.</li> <li>Lack of a Formal Contract: There was no mechanism enforcing that subclasses consistently register their default providers. Registering defaults was a convention, not an architectural contract.</li> <li>Code Duplication: Each new registry subclass needed similar initialization logic.</li> </ol> <p>This violated the <code>athomic</code> design principles, which aim for robustness, clarity, and low coupling.</p>"},{"location":"adr/ADR-038-base-registry-refact/#decision","title":"Decision","text":"<p>We decided to refactor <code>BaseRegistry</code> to use Python's <code>__init_subclass__</code> magic method and the Template Method Pattern. The new implementation adopts the following strategies:</p> <ol> <li> <p>Automatic Initialization via <code>__init_subclass__</code>: <code>BaseRegistry</code> now uses the <code>__init_subclass__</code> hook to create an empty, isolated <code>_registry</code> dictionary for each new subclass that inherits from it. This completely eliminates the risk of shared state.</p> </li> <li> <p>Initialization Contract with Abstract Method: A new abstract class method, <code>@abstractmethod def register_defaults(cls)</code>, was created. This forces any subclass of <code>BaseRegistry</code> to implement this method, explicitly defining its default providers.</p> </li> <li> <p>Guaranteed Execution: The base class's <code>__init_subclass__</code> calls <code>cls.register_defaults()</code> automatically, ensuring that defaults are registered at class definition time, consistently and predictably.</p> </li> <li> <p>Type Safety with <code>Generic</code>: <code>BaseRegistry</code> is now generic (<code>BaseRegistry[T]</code>), and <code>__init_subclass__</code> requires the expected protocol to be passed (e.g., <code>class EventBusRegistry(BaseRegistry, protocol=EventBusProtocol)</code>). This allows the <code>register</code> method to validate at runtime whether registered items conform to the protocol, increasing framework safety.</p> </li> </ol>"},{"location":"adr/ADR-038-base-registry-refact/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-038-base-registry-refact/#positive","title":"Positive","text":"<ul> <li>Guaranteed State Isolation: The state of each registry (<code>EventBusRegistry</code>, <code>MessagingRegistry</code>, etc.) is automatically isolated by the base class, making the creation of new registries safer and less error-prone.</li> <li>Declarative Design: Subclasses become much simpler and more declarative. Their only responsibility is to inherit from <code>BaseRegistry</code> and implement the <code>register_defaults</code> method, filling in the \"template\".</li> <li>Architectural Consistency: Establishes a single, robust, and reusable pattern for all modules that require a provider registration mechanism.</li> <li>Fail-Fast: Type checking against the protocol and the requirement of the abstract method ensure that implementation errors are caught during development, not in production.</li> </ul>"},{"location":"adr/ADR-038-base-registry-refact/#negative-or-impact","title":"Negative or Impact","text":"<ul> <li>Breaking Change: All existing registry implementations had to be refactored to fit the new pattern. This is an acceptable cost in exchange for the benefits of robustness and consistency.</li> <li>Learning Curve: Developers contributing to the project will need to be aware of this new pattern to create registries. This ADR serves as the main documentation to mitigate this.</li> </ul>"},{"location":"adr/ADR-039-documents-module-ddd/","title":"ADR-039: Domain Persistence Module Architecture","text":"<p>Status: Accepted</p> <p>Date: 2025-07-09</p>"},{"location":"adr/ADR-039-documents-module-ddd/#context","title":"Context","text":"<p>The application requires a robust, flexible, and testable architectural pattern for persisting business entities, starting with the \"Document\" entity. The initial implementation was tightly coupled to both business logic and the database technology (MongoDB), making unit testing, code evolution, and potential backend replacement difficult.</p> <p>The goal is to establish a pattern that promotes decoupling, following Domain-Driven Design (DDD) and Clean Architecture principles, and can be reused for all domain entities in the system (<code>Users</code>, <code>Orders</code>, etc.).</p>"},{"location":"adr/ADR-039-documents-module-ddd/#decision","title":"Decision","text":"<p>We will adopt a layered architecture for all domain modules that require persistence. This architecture clearly separates responsibilities and inverts dependencies, ensuring that business layers do not depend on infrastructure details.</p> <p>The structure will be divided into:</p> <ol> <li> <p>Domain Layer (<code>domain</code>):</p> <ul> <li>Entities: Pure Pydantic models encapsulating data and intrinsic business rules (e.g., <code>Document.archive()</code>).</li> <li>Repository Protocols: Abstract interfaces (e.g., <code>DocumentRepositoryProtocol</code>) defining the \"contract\" for persistence operations (<code>save</code>, <code>find_by_id</code>). These protocols will be generic (<code>Generic[T]</code>) to remain agnostic to the entity they handle.</li> </ul> </li> <li> <p>Application Layer (<code>application</code>):</p> <ul> <li>Application Services: Orchestrate system use cases (e.g., <code>DocumentService</code>). They contain workflow logic, but not core business logic.</li> <li>Factories: Application services will not instantiate infrastructure dependencies directly. They will use a <code>Factory</code> (e.g., <code>DocumentRepositoryFactory</code>) to obtain a concrete repository implementation, remaining decoupled.</li> </ul> </li> <li> <p>Infrastructure Layer (<code>infrastructure</code>):</p> <ul> <li>Concrete Repositories: Implement the domain layer protocols for a specific technology (e.g., <code>MongoDocumentRepository</code>, <code>LocalMemoryDocumentRepository</code>).</li> <li>Inheritance from <code>BaseService</code>: Implementations managing external connections (MongoDB, PostgreSQL) will inherit from <code>BaseService</code> to standardize lifecycle (<code>connect</code>, <code>close</code>), state (<code>is_ready</code>), and observability (metrics and tracing).</li> <li>Persistence Models: Database-specific models (e.g., Beanie models with <code>class Settings</code>) will reside in this layer, separate from domain entities.</li> <li>Mapping and Registry: A <code>Registry</code> (e.g., <code>DocumentRepositoryRegistry</code>) will register available \"drivers\" (e.g., \"mongodb\", \"local\"). A <code>Mapper</code> (<code>DomainToDbModelMapper</code>) will associate the domain entity with its corresponding persistence model.</li> </ul> </li> </ol>"},{"location":"adr/ADR-039-documents-module-ddd/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-039-documents-module-ddd/#positive","title":"Positive","text":"<ul> <li>High Flexibility: Switching a database backend becomes a matter of configuration and creating a new adapter, with no impact on business logic.</li> <li>Improved Testability:<ul> <li>Domain logic can be tested in complete isolation.</li> <li>Application services can be tested with in-memory repositories, resulting in fast and reliable unit tests.</li> <li>Integration tests can focus exclusively on validating the repository implementation against the real infrastructure.</li> </ul> </li> <li>Consistency and Reusability: The pattern can be consistently replicated for all new business entities, accelerating development and maintaining architectural quality.</li> <li>Standardized Observability: Inheriting from <code>BaseService</code> ensures all repository I/O operations have uniform metrics and tracing by default.</li> </ul>"},{"location":"adr/ADR-039-documents-module-ddd/#negative","title":"Negative","text":"<ul> <li>More Boilerplate: The structure requires more files (Protocol, Factory, Registry, etc.) compared to a simpler Active Record approach, which may seem excessive for very simple domains.</li> <li>Learning Curve: The team will need to understand Dependency Inversion flow and the role of each layer, instead of instantiating classes directly.</li> </ul> <p>Justification: The long-term benefits in maintainability, scalability, and testability significantly outweigh the initial cost of a more formal structure. This architecture is an investment in the longevity and quality of the software.</p>"},{"location":"adr/ADR-040-event-bus-resilience/","title":"ADR-017: Enhance Resilience of Event Bus Background Processing","text":"<p>Date: 2025-07-09</p> <p>Status: Accepted</p>"},{"location":"adr/ADR-040-event-bus-resilience/#context","title":"Context","text":"<p>The event bus system, built upon the <code>EventBusBase</code> and the <code>RunnableServiceBase</code> (as defined in ADR-008), is designed for reliable asynchronous communication. <code>RunnableServiceBase</code> provides a managed background task that executes a <code>_run_loop</code> method, which is intended to run continuously for the service's lifetime.</p> <p>The <code>RedisEventBus</code> implementation relies on this loop to poll for messages from Redis Pub/Sub channels. During integration and unit testing, we identified critical resilience issues that could cause the background processing task to terminate silently, leading to a non-functional event bus without clear error signals.</p>"},{"location":"adr/ADR-040-event-bus-resilience/#problem-statement","title":"Problem Statement","text":"<p>The initial implementation of the event processing loop was not robust enough to handle exceptions occurring during a single message processing cycle. Two primary issues were found:</p> <ol> <li> <p>Unhandled Exceptions in the Processing Loop: Any unexpected exception within the <code>_run_loop</code> of <code>EventBusBase</code> (e.g., a <code>TypeError</code> from attempting to decode a <code>string</code> instead of <code>bytes</code>, or a <code>JSONDecodeError</code> from a malformed message) would break out of the <code>while self.is_running():</code> block, causing the background task to exit permanently. This made the service fail silently.</p> </li> <li> <p>Race Condition on Subscription: The <code>RedisEventBus._process_events</code> method would immediately attempt to call <code>_pubsub_client.get_message()</code>. However, in tests and potentially in production, the background task could start before any <code>subscribe()</code> call was made. Calling <code>get_message()</code> on a <code>PubSub</code> client that is not subscribed to any channel raises a <code>redis.exceptions.RuntimeError</code>, which was another unhandled exception that killed the loop.</p> </li> </ol> <p>These issues resulted in flaky tests (<code>TimeoutError</code> while waiting for events that would never arrive) and a critical lack of resilience in the production code.</p>"},{"location":"adr/ADR-040-event-bus-resilience/#decision","title":"Decision","text":"<p>To address these problems, we decided to implement two key changes to bolster the resilience of the event bus system:</p> <ol> <li> <p>Isolate Failures within the <code>EventBusBase._run_loop</code>:     We will refactor the <code>_run_loop</code> method in <code>EventBusBase</code> to wrap the call to <code>_process_events()</code> and the subsequent <code>asyncio.sleep()</code> within a <code>try...except Exception:</code> block. This block is placed inside the main <code>while self.is_running():</code> loop. This change ensures that if an exception occurs during one processing cycle, it is logged, and the loop continues to the next iteration instead of terminating the entire service.</p> </li> <li> <p>Implement a Subscriber Guard in <code>RedisEventBus</code>:     We will add a guard condition at the beginning of the <code>RedisEventBus._process_events</code> method. The method will now check <code>if not self._subscribers:</code>. If there are no active subscribers, it will return early (with a small <code>asyncio.sleep</code> to prevent tight-looping). This completely prevents the race condition by ensuring <code>get_message()</code> is only called when the client is guaranteed to be subscribed to at least one channel.</p> </li> </ol>"},{"location":"adr/ADR-040-event-bus-resilience/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-040-event-bus-resilience/#positive","title":"Positive","text":"<ul> <li>Increased System Resilience: The event bus service is no longer susceptible to silent failures caused by transient errors or malformed individual messages. The background task will remain active and continue processing subsequent valid messages.</li> <li>Improved Test Reliability: Integration tests are now more stable as the event bus service remains operational, eliminating the <code>TimeoutError</code> issues caused by the background task dying prematurely.</li> <li>Enhanced Efficiency: The subscriber guard in <code>RedisEventBus</code> prevents unnecessary polling of the Redis connection when no events are being listened for, reducing needless network I/O and CPU usage.</li> <li>Clearer Error Logging: Failures within a processing cycle are now explicitly logged as non-fatal errors, providing better visibility for monitoring without halting the entire service.</li> </ul>"},{"location":"adr/ADR-040-event-bus-resilience/#negative","title":"Negative","text":"<ul> <li>Masking of Persistent Errors: While isolating failures is beneficial, it could potentially mask a persistent underlying issue (e.g., a recurring malformed message type) that now only appears as repeated log entries instead of a service crash. This requires diligent monitoring of error logs to detect such patterns.</li> <li>Slightly Increased Complexity: The logic in the base class loop is marginally more complex due to the nested exception handling.</li> </ul>"},{"location":"adr/ADR-041-route-level-dependency-security-policy/","title":"ADR 041 route level dependency security policy","text":""},{"location":"adr/ADR-041-route-level-dependency-security-policy/#adr-041-router-level-dependency-based-security-policy","title":"ADR-041: Router-Level, Dependency-Based Security Policy","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-11</li> </ul>"},{"location":"adr/ADR-041-route-level-dependency-security-policy/#context","title":"Context","text":"<p>The <code>athomic-docs</code> project requires a robust, scalable, and highly auditable security system to manage different access levels for its endpoints. As the number of endpoints grows, a clear and consistent method for applying security policies is needed to prevent accidental exposure of sensitive routes. A key requirement is to enforce a \"private by default\" philosophy, making it difficult for developers to accidentally create an unprotected endpoint.</p> <p>The primary requirements for this system are: 1.  Default to Secure: All new endpoints must be secure by default, requiring a valid authentication token. 2.  Multiple Security Levels: The system must support at least three distinct access policies:     * <code>public</code>: Open access, no authentication required (e.g., for <code>/health</code> checks).     * <code>internal</code>: Access restricted by the request's origin (e.g., IP Allowlist), without a user token. This is for machine-to-machine communication.     * <code>private</code>: Requires a valid user authentication token (e.g., JWT). 3.  Architectural Decoupling: The core security logic must reside in the <code>athomic</code> layer and remain completely agnostic of the web framework (<code>FastAPI</code>). The <code>api</code> layer should act as an adapter. 4.  Clarity and Auditability: The security policy for any given endpoint must be explicit and easy to determine.</p>"},{"location":"adr/ADR-041-route-level-dependency-security-policy/#decision","title":"Decision","text":"<p>We will implement a security architecture based on FastAPI's native Dependency Injection system, applied directly at the <code>APIRouter</code> level. This pattern enforces a default security policy for an entire group of routes based on which router they are defined in.</p> <p>The architecture is composed of two main layers:</p> <p>1. <code>athomic</code> Layer (Framework-Agnostic Core):</p> <ul> <li><code>AuthorizationService</code>: A central service that acts as a pure orchestrator. Its main method, <code>authorize(context, policy_name)</code>, dispatches the security check to the appropriate strategy.</li> <li>AuthExecutors (Strategy Pattern): A set of classes implementing the <code>AuthExecutor</code> protocol. Each class encapsulates the logic for a single security policy (e.g., <code>PrivateTokenExecutor</code>, <code>InternalIPExecutor</code>).</li> <li>ExecutorRegistry : A standardized registry that maps policy names (e.g., <code>\"private\"</code>) to instances of the corresponding <code>AuthExecutor</code> classes.</li> <li>Generic Models &amp; Exceptions: The service layer operates exclusively on framework-agnostic objects like <code>SecurityRequestContext</code>  and raises generic exceptions like <code>AthomicAuthError</code> and <code>AthomicForbiddenError</code>.</li> </ul> <p>2. <code>api</code> Layer (FastAPI-Specific Adapter):</p> <ul> <li>Security Dependencies (<code>api/dependencies/security.py</code>): A set of reusable FastAPI dependency functions (<code>require_token</code>, <code>require_internal_ip</code>).<ul> <li>These functions are the \"glue\" or \"adapter\" layer.</li> <li>They are responsible for:<ol> <li>Creating the generic <code>SecurityRequestContext</code> from the <code>fastapi.Request</code> object.</li> <li>Calling the <code>AuthorizationService</code> with the correct policy name (e.g., <code>\"private\"</code>).</li> <li>Catching <code>AthomicSecurityError</code> exceptions and translating them into standard <code>fastapi.HTTPException</code> responses with the appropriate status codes (401, 403).</li> </ol> </li> </ul> </li> <li> <p>Router-Level Policy Application:</p> <ul> <li>We will organize routes into distinct <code>APIRouter</code> instances based on their required security level (e.g., <code>private_router.py</code>, <code>public_router.py</code>, <code>internal_router.py</code>).</li> <li>The security policy is applied to all routes within a router by adding the corresponding dependency to the router's constructor. This makes the security policy for that group of routes explicit and non-negotiable.</li> </ul> <p>```python</p> </li> </ul> <p>This approach replaces any need for a complex, stateful authentication middleware, favoring FastAPI's explicit and powerful dependency injection system.</p>"},{"location":"adr/ADR-041-route-level-dependency-security-policy/#example-for-a-private-router-that-enforces-token-authentication-on-all-its-routes","title":"Example for a private router that enforces token authentication on all its routes","text":"<p>from nala.api.dependencies.security import require_token</p> <p>private_router = APIRouter(     prefix=\"/private\",     tags=[\"Private\"],     dependencies=[Depends(require_token)] )</p> <p>@private_router.get(\"/users/me\") async def get_user_data(request: Request):     # This endpoint is automatically protected by require_token.     # No need to add Depends() here.     ... ```</p>"},{"location":"adr/ADR-041-route-level-dependency-security-policy/#consequences","title":"Consequences","text":"<p>Positive: * Extremely Clear and Auditable: The security policy for any endpoint is immediately obvious from its file location and the <code>APIRouter</code> definition. There is no \"magic\" or hidden logic. * Secure by Default: Developers working on private features inside a <code>private_router</code> cannot accidentally create an unprotected endpoint. The security is enforced by the router they choose to use. This directly fulfills the \"private by default\" requirement. * DRY (Don't Repeat Yourself): Avoids the need to add <code>Depends(require_token)</code> to every single private route, reducing code duplication and the chance of error. * Leverages Native Framework Features: Uses FastAPI's Dependency Injection as intended, making the solution robust, performant, and likely to be compatible with future versions of the framework. * Maintains Decoupling: The <code>athomic</code> layer remains completely independent and reusable, fulfilling a key architectural requirement.</p> <p>Negative: * Requires Code Organization Discipline: This pattern relies on developers placing routes in the correct router module. This is also a strength, as it enforces a clean and predictable project structure. * Explicitness over Automation: Developers must explicitly choose which router to add a new endpoint to. This is a trade-off against a middleware that might automatically protect routes based on URL patterns. For security, explicitness is generally preferred.</p>"},{"location":"adr/ADR-042-decouplin-core-primitives/","title":"ADR 042 decouplin core primitives","text":""},{"location":"adr/ADR-042-decouplin-core-primitives/#adr-042-decoupling-core-primitives-for-acyclic-dependencies","title":"ADR-042: Decoupling Core Primitives for Acyclic Dependencies","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-11</li> </ul>"},{"location":"adr/ADR-042-decouplin-core-primitives/#context","title":"Context","text":"<p>As the <code>athomic</code> framework matured with the introduction of a sophisticated <code>LifecycleManager</code>, a critical architectural risk was identified: the potential for circular dependencies. The initial design placed core components like <code>BaseRegistry</code> and service skeletons (<code>BaseService</code>, <code>RunnableServiceBase</code>) within the same low-level package, <code>athomic.base</code>.</p> <p>The <code>LifecycleManager</code> and its <code>LifecycleRegistry</code> need to depend on the service protocols (from a <code>services</code> layer) and the registry primitives (from a <code>base</code> layer). However, the service base classes themselves might indirectly need components from the lifecycle manager in the future, creating a fragile, cyclical dependency graph (<code>lifecycle</code> -&gt; <code>services</code> -&gt; <code>base</code> -&gt; <code>lifecycle</code>). This would make the system difficult to maintain, test, and understand.</p> <p>A decision was needed to restructure these core primitives to enforce a strict, unidirectional dependency flow.</p>"},{"location":"adr/ADR-042-decouplin-core-primitives/#decision","title":"Decision","text":"<p>We will refactor the core <code>athomic</code> packages to establish a clear, layered hierarchy and ensure acyclic dependencies. The responsibilities will be segregated as follows:</p> <ol> <li> <p><code>athomic.base</code> (The Foundation):</p> <ul> <li>Responsibility: To provide the most fundamental, dependency-free building blocks of the framework.</li> <li>Contents: <code>BaseRegistry</code>, <code>BaseInstanceRegistry</code>, and other primitive components that have zero internal <code>athomic</code> dependencies.</li> </ul> </li> <li> <p><code>athomic.services</code> (The Service Layer):</p> <ul> <li>Responsibility: To define the \"what\" and \"how\" of a service within the framework. It defines the contracts and base implementations for services.</li> <li>Contents: <code>RunnableServiceProtocol</code> (the lifecycle contract), <code>BaseService</code> (for connection-based services), and <code>RunnableServiceBase</code> (for task-based services).</li> <li>Dependency Rule: This layer can depend on <code>athomic.base</code>, but not vice-versa.</li> </ul> </li> <li> <p><code>athomic.lifecycle</code> (The Lifecycle Management Layer):</p> <ul> <li>Responsibility: To orchestrate the startup and shutdown of all manageable services.</li> <li>Contents: <code>LifecycleRegistry</code> (which registers <code>RunnableServiceProtocol</code> instances) and the <code>LifespanManager</code>.</li> <li>Dependency Rule: This layer can depend on both <code>athomic.services</code> (for the protocol) and <code>athomic.base</code> (for the registry implementation), but neither of them can depend on <code>lifecycle</code>.</li> </ul> </li> </ol> <p>This structure guarantees a clear, one-way flow of dependencies: <code>lifecycle</code> -&gt; <code>services</code> -&gt; <code>base</code>.</p>"},{"location":"adr/ADR-042-decouplin-core-primitives/#consequences","title":"Consequences","text":"<p>Positive: * Circular Dependencies Eliminated: This is the primary benefit. The architecture now fundamentally prevents circular import errors, making the codebase more stable and predictable. * Clear Separation of Concerns: Each package has a distinct and well-defined responsibility, making the framework easier to navigate and understand for new developers. * Improved Modularity and Testability: Each layer (<code>base</code>, <code>services</code>, <code>lifecycle</code>) can be tested in greater isolation. * Enhanced Maintainability: Changes within one layer are less likely to have unintended side effects on others. For example, a change in the <code>LifespanManager</code> cannot break the <code>BaseRegistry</code>.</p> <p>Negative: * Increased Number of Packages: The refactoring introduces more directories and modules. This is a deliberate and accepted trade-off for architectural clarity and robustness. * Stricter Design Discipline: Developers must be mindful of the dependency flow and place new components in the correct layer. This is also a strength, as it enforces a clean architecture.</p>"},{"location":"adr/ADR-043-application-layered-architecture/","title":"ADR 043 application layered architecture","text":""},{"location":"adr/ADR-043-application-layered-architecture/#adr-043-application-layered-architecture-athomic-domain-api","title":"ADR-043: Application Layered Architecture (athomic, domain, api)","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-11</li> </ul>"},{"location":"adr/ADR-043-application-layered-architecture/#context","title":"Context","text":"<p>As the <code>athomic-docs</code> project evolved, a critical architectural challenge emerged: the initial design started to blend specific business logic (e.g., the <code>documents</code> module) with the generic, reusable core framework (<code>athomic</code>). This coupling presented several long-term risks:</p> <ol> <li>Limited Reusability: The <code>athomic</code> layer could not be easily extracted for use in other projects or be open-sourced, as it contained business-specific knowledge.</li> <li>Violation of Separation of Concerns: Changes in business requirements could force changes in the core framework, and vice-versa, leading to a brittle architecture.</li> <li>Decreased Testability: It was becoming difficult to test core framework features in isolation from the business logic, and to test the business logic without the overhead of the entire framework.</li> </ol> <p>A decision was needed to establish a clear, scalable, and maintainable architectural pattern that strictly separates the core framework, the business domain, and the application/delivery mechanism.</p>"},{"location":"adr/ADR-043-application-layered-architecture/#decision","title":"Decision","text":"<p>We will adopt a layered architecture based on the principles of Clean Architecture / Hexagonal Architecture. The project will be structured into three primary, high-level Python packages, each with a distinct responsibility and a strict dependency rule.</p> <p>The layers are defined as follows, from the inside out:</p> <p>1. <code>nala.domain</code> (The Business Core) * Responsibility: To contain the \"soul\" of the application. This layer encapsulates all the business-specific logic, entities, and rules. It is the most valuable and stable part of the project. * Contents: Domain entities (e.g., <code>Document</code>, <code>Order</code>) with rich behavior, repository protocols (interfaces), and domain services. * Core Principle: This layer has zero dependencies on any other layer within the <code>nala</code> project. It is pure business logic.</p> <p>2. <code>nala.athomic</code> (The Core Framework / Toolkit) * Responsibility: To provide generic, reusable, and business-agnostic technical infrastructure and tools. This is the foundation upon which applications are built. * Contents: Generic services (<code>BaseService</code>), shared infrastructure clients (<code>database.document.MongoProvider</code>), security logic (<code>AuthService</code>, <code>Executors</code>), and other cross-cutting concerns. * Core Principle: It knows nothing about the specific business rules of <code>nala.domain</code>. It provides tools, not business solutions.</p> <p>3. <code>nala.api</code> (The Application &amp; Infrastructure Layer) * Responsibility: To act as the adapter and orchestrator that connects the outside world (e.g., HTTP) to the business core. It also contains the concrete implementations of the infrastructure defined by the domain. * Contents:     * FastAPI routers, request/response models.     * Dependency Injection setup (<code>dependencies/security.py</code>).     * Concrete repository implementations (<code>infrastructure/repositories/mongo_document_repository.py</code>) that implement protocols from <code>nala.domain</code> using tools from <code>nala.athomic</code>.     * The main application entrypoint (<code>main.py</code>) and the lifecycle management (<code>lifespan.py</code>, <code>services.py</code>) that wires everything together.</p> <p>The Dependency Rule: Dependencies must only point inwards. * <code>nala.api</code> depends on <code>nala.domain</code> and <code>nala.athomic</code>. * <code>nala.athomic</code> has no dependencies on other <code>nala</code> layers. * <code>nala.domain</code> has no dependencies on other <code>nala</code> layers.</p>"},{"location":"adr/ADR-043-application-layered-architecture/#consequences","title":"Consequences","text":"<p>Positive: * True Separation of Concerns: The business logic (<code>domain</code>) is completely isolated from technological choices (<code>athomic</code>, <code>api</code>). We can change the database or web framework without touching the business rules. * High Cohesion, Low Coupling: Logic related to a specific business concept is highly cohesive within its domain module. The layers are loosely coupled through well-defined interfaces (protocols). * Enhanced Testability: Each layer can be tested in complete isolation. The <code>domain</code> can be tested with simple unit tests, while the <code>api</code> layer can be tested by mocking the domain interfaces. * Future-Proofing <code>athomic</code>: The <code>athomic</code> layer remains generic and can be extracted into its own library, potentially for open-sourcing or for use in other Nala projects, fulfilling a key strategic goal. * Clarity for Developers: The structure provides a clear mental model for where to place new code and how components interact.</p> <p>Negative: * Increased Initial Complexity: The architecture introduces more layers and files, which can have a steeper learning curve for new developers. * More Boilerplate: Requires a more disciplined approach and can lead to more \"ceremony\" (e.g., defining protocols, dependency injection wiring) for simple features. This is a deliberate trade-off in favor of long-term maintainability.</p>"},{"location":"adr/ADR-044-local-monitoring-stack-with-docker-composer/","title":"ADR 044 local monitoring stack with docker composer","text":""},{"location":"adr/ADR-044-local-monitoring-stack-with-docker-composer/#adr-044-local-monitoring-stack-with-docker-compose","title":"ADR-044: Local Monitoring Stack with Docker Compose","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-11</li> </ul>"},{"location":"adr/ADR-044-local-monitoring-stack-with-docker-composer/#context","title":"Context","text":"<p>To ensure the reliability and performance of <code>athomic-docs</code>, a robust monitoring solution is required. We need the ability to collect, store, and visualize key application metrics, such as request rates, error rates, and response times (RED metrics), as well as custom business and performance metrics (e.g., cache hit ratio).</p> <p>The solution must be easy to set up for local development, consistent across different developer machines, and architecturally decoupled from the main application's infrastructure. It should consume the metrics already being exposed by the API via the <code>/internal/metrics</code> endpoint, which is protected by an IP allowlist.</p>"},{"location":"adr/ADR-044-local-monitoring-stack-with-docker-composer/#decision","title":"Decision","text":"<p>We will implement a local monitoring stack using Prometheus for metrics collection and Grafana for visualization, both orchestrated by Docker Compose.</p> <ol> <li> <p>Dedicated Infrastructure: The entire monitoring stack will reside in a new <code>monitoring/</code> directory at the root of the project. This directory is separate from the application's <code>infra/</code> directory, maintaining a clear separation of concerns between the application's dependencies and its monitoring tools.</p> </li> <li> <p>Orchestration: A <code>docker-compose.yml</code> file within the <code>monitoring/</code> directory will define and manage the lifecycle of the Prometheus and Grafana containers. This ensures a consistent, one-command setup (<code>docker-compose up</code>) for any developer. <code>Makefile</code> shortcuts (<code>mup</code>, <code>mdown</code>) will be provided for convenience.</p> </li> <li> <p>Prometheus Configuration (<code>prometheus.yml</code>):</p> <ul> <li>Prometheus will be configured to \"scrape\" (collect) metrics from the <code>athomic-docs</code> application.</li> <li>The target will be <code>host.docker.internal:8000/internal/metrics</code> (or the appropriate host/port), allowing the Prometheus container to access the API running on the host machine.</li> </ul> </li> <li> <p>Grafana Provisioning:</p> <ul> <li>Grafana will be configured to automatically provision its resources on startup.</li> <li>Datasource: A <code>datasource.yml</code> will automatically configure Prometheus as the default data source for Grafana, pointing to the Prometheus container (<code>http://prometheus:9090</code>).</li> <li>Dashboard: A default dashboard (<code>api-overview.json</code>) will be automatically loaded, providing an immediate, out-of-the-box visualization of key RED metrics.</li> </ul> </li> <li> <p>Networking:</p> <ul> <li>A custom Docker bridge network (<code>monitoring-net</code>) will be created to allow stable communication between the containers and to assign a static, predictable IP address to the Prometheus container.</li> <li>This static IP is crucial for it to be added to the <code>IPAllowlist</code> in the <code>athomic-docs</code>'s configuration, allowing Prometheus to bypass the security checks on the <code>/internal/metrics</code> endpoint.</li> </ul> </li> </ol>"},{"location":"adr/ADR-044-local-monitoring-stack-with-docker-composer/#consequences","title":"Consequences","text":"<p>Positive: * Reproducibility: Any developer can spin up the entire monitoring stack with a single command, ensuring a consistent development environment. * Decoupling: The monitoring stack is completely decoupled from the application's core logic and infrastructure. It can be started, stopped, or modified without affecting the API. * Immediate Visibility: The provisioned dashboard provides instant insights into the application's health as soon as it's running, lowering the barrier to observability. * Production-Ready Pattern: This setup mirrors common production architectures, providing valuable experience and a clear path for deploying a similar stack in a production environment (e.g., in Kubernetes).</p> <p>Negative: * Increased Local Resource Usage: Running two additional containers (Prometheus, Grafana) will consume more local machine resources (CPU, RAM). * Configuration Management: The monitoring configuration (<code>prometheus.yml</code>, Grafana dashboards) now becomes part of the project's codebase and needs to be maintained.</p>"},{"location":"adr/ADR-045-integration-testingstrategy-for-database-dependent-components/","title":"ADR 045 integration testingstrategy for database dependent components","text":""},{"location":"adr/ADR-045-integration-testingstrategy-for-database-dependent-components/#adr-045-integration-testing-strategy-for-database-dependent-components","title":"ADR-045: Integration Testing Strategy for Database-Dependent Components","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-13</li> </ul>"},{"location":"adr/ADR-045-integration-testingstrategy-for-database-dependent-components/#context","title":"Context","text":"<p>As the application's architecture was refactored to follow Domain-Driven Design (DDD) and Clean Architecture principles, a clear need arose for a testing strategy that could validate the infrastructure layer, specifically the database repositories (e.g., <code>MongoDocumentRepository</code>).</p> <p>Unit tests using mocks are insufficient for this purpose as they cannot verify: 1.  The correctness of the interaction with the actual database. 2.  The behavior of the Object-Document Mapper (ODM), in our case, Beanie. 3.  The proper application of database-specific logic like multi-tenancy filters.</p> <p>We required a testing pattern that was reliable, isolated, and accurately reflected the application's dependency injection and lifecycle management architecture.</p>"},{"location":"adr/ADR-045-integration-testingstrategy-for-database-dependent-components/#decision","title":"Decision","text":"<p>We will adopt an integration testing pattern using <code>pytest</code> fixtures to manage the lifecycle of a real database connection for each test session. A central, reusable <code>pytest</code> fixture named <code>db_provider</code> will be the cornerstone of this strategy.</p> <p>The responsibilities of the <code>db_provider</code> fixture are as follows:</p> <ol> <li> <p>Setup Phase (Before each test):</p> <ul> <li>It reads the base database configuration from the application's settings.</li> <li>To ensure test isolation, it creates a new, temporary settings object, overriding the database name with a unique identifier (e.g., <code>test_db_&lt;uuid&gt;</code>).</li> <li>It instantiates the concrete database infrastructure service (e.g., <code>MongoProvider</code>) using these temporary settings.</li> <li>It calls the provider's <code>.start()</code> method to establish a real connection to the database service.</li> <li>It immediately calls <code>init_beanie</code>, passing the live database connection and the specific Beanie models required for the test.</li> <li>Finally, it <code>yields</code> the fully initialized and ready-to-use database provider instance.</li> </ul> </li> <li> <p>Teardown Phase (After each test):</p> <ul> <li>After the test function completes, the fixture resumes and performs a clean teardown.</li> <li>It drops the entire temporary test database to ensure no data leaks between tests.</li> <li>It calls the provider's <code>.stop()</code> method to gracefully close the database connection.</li> </ul> </li> </ol> <p>A second, simpler fixture (e.g., <code>repository</code>) will depend on <code>db_provider</code> and will be responsible for instantiating the <code>MongoDocumentRepository</code>, injecting the live provider into it.</p> <p>Example Fixture Implementation:</p> <pre><code>@pytest_asyncio.fixture\nasync def db_provider() -&gt; AsyncGenerator[MongoProvider, None]:\n    \"\"\"\n    Creates and manages the lifecycle of a real database connection for tests.\n    \"\"\"\n    base_mongo_settings = get_settings().database.document.provider\n    test_mongo_settings = base_mongo_settings.model_copy(\n        update={\"database_name\": f\"test_db_{uuid4().hex}\"}\n    )\n\n    provider = MongoProvider(test_mongo_settings)\n\n    await provider.start()\n    db = await provider.get_database()\n    await init_beanie(database=db, document_models=[SampleBeanieModel])\n\n    yield provider\n\n    # Teardown logic\n    if provider.client:\n        await provider.client.drop_database(test_mongo_settings.database_name)\n    await provider.stop()\n\n@pytest.fixture\ndef repository(db_provider: MongoProvider) -&gt; MongoDocumentRepository:\n    \"\"\"Injects the live db_provider into the repository.\"\"\"\n    return MongoDocumentRepository(\n        db_provider=db_provider,\n        # ... other required args\n    )\n</code></pre>"},{"location":"adr/ADR-045-integration-testingstrategy-for-database-dependent-components/#consequences","title":"Consequences","text":"<p>Positive:</p> <ul> <li>High Fidelity: Tests are run against a real database instance, providing high confidence that the repository logic, queries, and ODM mappings are correct.</li> <li>Complete Isolation: Each test (or test module, depending on fixture scope) operates on its own ephemeral database, which is created and destroyed for that test alone. This completely prevents state leakage between tests.</li> <li>DRY (Don't Repeat Yourself): The complex setup and teardown logic is centralized in the <code>db_provider</code> fixture, making the actual test functions clean and focused solely on testing business logic.</li> <li>Architectural Alignment: The testing pattern perfectly mirrors the application's real architecture (dependency injection of the provider into the repository), which increases the value and reliability of the tests.</li> </ul> <p>Negative:</p> <ul> <li>Slower Execution: As these are integration tests involving network I/O to a database service, they are inherently slower than unit tests. They should be run as part of a separate test suite from fast-running unit tests.</li> <li>Infrastructure Dependency: Running these tests requires a live MongoDB service to be available to the test runner (typically provided via Docker).</li> </ul>"},{"location":"adr/ADR-046-retry-refact/","title":"ADR-014: Standardization and Architecture of the Retry Module","text":"<ul> <li>Status: Accepted  </li> <li>Date: 2025-07-19</li> </ul>"},{"location":"adr/ADR-046-retry-refact/#context","title":"Context","text":"<p>Reliable retry mechanisms are essential in distributed systems, resilient APIs, and integrations with external resources that may experience transient failures. The athomic-docs requires a robust, extensible, and observable approach for retries, enabling:</p> <ul> <li>Configuration-driven and code-level customization (e.g., via RetrySettings, RetryPolicy, RetryFactory, and decorators).</li> <li>Support for callbacks on success, failure, circuit breaker hooks, and custom exception handling.</li> <li>Native integration with metrics, logging, and tracing (observability).</li> <li>Coverage for both synchronous and asynchronous functions.</li> </ul> <p>Alternatives considered included: - Direct use of libraries like Tenacity without an abstraction layer. - Hardcoded retry logic at usage points, without a global/configurable pattern. - Ad-hoc implementations detached from the rest of the framework.</p> <p>The decision was to create a standardized and decoupled module, centralizing logic, exposing a configurable decorator, and adapters for dynamic settings and runtime overrides.</p>"},{"location":"adr/ADR-046-retry-refact/#decision","title":"Decision","text":"<ul> <li> <p>The Retry module will consist of these main components:</p> <ul> <li><code>RetrySettings</code>: Configuration class (settings/config).</li> <li><code>RetryPolicy</code>: Class representing the business rules and retry policy.</li> <li><code>RetryHandler</code>: Orchestrator for execution, observability integration, callbacks, etc.</li> <li><code>retry</code>: Decorator for practical use, supporting both sync and async.</li> <li><code>RetryFactory</code>: Responsible for creating policies/decorators from application settings.</li> <li><code>create_policy_from_settings</code> (adapter): Decoupled function to convert settings to policy.</li> </ul> </li> <li> <p>Retry configuration should preferably be made via settings, but can be overridden in code via policy or decorator/factory parameters.</p> </li> <li>The adapter accepts exceptions as either strings (\"ValueError\") or types (ValueError), resolving them as needed for the project.</li> <li>The decorator supports both synchronous and asynchronous functions, forwarding all logic to the handler.</li> <li>The module does not require mandatory coupling between settings and policy; it is the caller\u2019s responsibility to ensure valid configuration.</li> <li>Callbacks such as <code>on_retry</code>, <code>on_fail</code>, and <code>circuit_breaker_hook</code> are passed directly to the handler/decorator.</li> </ul>"},{"location":"adr/ADR-046-retry-refact/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Centralization and standardization of retry throughout the system.</li> <li>Extensibility: easy to create custom policies and decorators for different contexts.</li> <li>Straightforward integration with observability (metrics, tracing, logging).</li> <li>Easier unit testing and behavior mocking.</li> <li>Transparent support for both sync and async.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Slight abstraction overhead (factory, adapter, policy vs. using Tenacity directly).</li> <li>Potential for configuration duplication if usage is not standardized.</li> <li>Need to avoid non-serializable functions in settings (e.g., lambda in retry_on_result).</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>Modular design allows for future evolution (native circuit breaker, fallback strategies, etc.).</li> <li>May require maintenance if the global configuration schema changes.</li> </ul> </li> </ul>"},{"location":"adr/ADR-047-fallback-fully-implemented/","title":"ADR-001: Implementation of a Configurable and Observable Fallback System","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-22</li> </ul>"},{"location":"adr/ADR-047-fallback-fully-implemented/#context","title":"Context","text":"<p>Our application requires a high degree of resilience against failures in its external dependencies, particularly caching services (like Redis). A simple inline <code>try/except</code> block for fetching data is insufficient as it leads to code duplication, lacks observability, and is hard to maintain or configure.</p> <p>Several alternatives were considered to address this:</p> <ol> <li> <p>Monolithic Provider with Flags: A single <code>FallbackKVProvider</code> class that uses boolean flags (e.g., <code>fallback_on_miss=True</code>) to control its internal logic with <code>if/else</code> statements. This was considered simpler for the initial implementation but was rejected because it violates the Open/Closed Principle, making it difficult to add new strategies without modifying the core class. It would also lead to high cyclomatic complexity.</p> </li> <li> <p>Simple Decorator: A basic decorator could encapsulate <code>try/except</code> logic. This was rejected as it offers limited configurability and is not well-suited for complex, stateful operations like multi-layered caching with self-healing.</p> </li> <li> <p>A Decoupled, Strategy-Based System: An approach where the core logic of how to perform fallback and write operations is encapsulated in separate \"Strategy\" objects. This was deemed the most robust and scalable solution for a core framework component.</p> </li> </ol> <p>The primary constraint was to build a solution that is not only resilient but also highly configurable and observable, fitting into our existing metrics and logging infrastructure. </p>"},{"location":"adr/ADR-047-fallback-fully-implemented/#decision","title":"Decision","text":"<p>We have decided to implement a comprehensive fallback system composed of two main parts: a specific <code>FallbackKVProvider</code> for data stores and a generic <code>@fallback_handler</code> for any function. The architecture is based on several key design patterns to ensure flexibility and maintainability.</p> <ol> <li> <p>Strategy Pattern: The core behaviors of the <code>FallbackKVProvider</code> are controlled by injectable strategies.</p> <ul> <li>Read Strategies (<code>ReadStrategyProtocol</code>): Dictate whether the fallback is triggered only on provider errors (<code>FallbackOnErrorStrategy</code>) or also on cache misses (<code>FallbackOnMissOrErrorStrategy</code>). </li> <li>Write Strategies (<code>WriteStrategyProtocol</code>): Control the write behavior, allowing for \"write-around\" (primary only)  or \"write-through\" (primary and then propagated to fallbacks)  operations.</li> </ul> </li> <li> <p>Factory Pattern: To decouple the <code>FallbackKVProvider</code> from the concrete creation of strategy objects, we use factories (<code>ReadStrategyFactory</code>, <code>WriteStrategyFactory</code>).  The provider requests a strategy from the factory based on an enum type (<code>FallbackReadStrategyType</code>, <code>FallbackWriteStrategyType</code>), and the factory is responsible for instantiating the correct class. </p> </li> <li> <p>Dependency Injection: To resolve the circular dependency between the provider and its strategies, the provider injects the necessary dependencies (<code>primary</code>, <code>fallbacks</code>, <code>heal_callable</code>) into the strategy methods at runtime.  The strategies are no longer aware of the <code>FallbackKVProvider</code> class, only of the protocols they need to operate on.</p> </li> <li> <p>Rich Observability: The system is instrumented with detailed Prometheus metrics that are specific to the fallback mechanism itself, separate from the metrics of the underlying data stores. This includes:</p> <ul> <li><code>fallback_operations_total</code>: A counter for the final success or failure of a fallback chain. </li> <li><code>fallback_operation_duration_seconds</code>: A histogram to measure the latency of fallback chain execution. </li> </ul> </li> <li> <p>Self-Healing: An optional self-healing mechanism is included. When a value is successfully retrieved from a lower-level fallback, it is written back to the primary provider and any other providers that failed along the way, improving cache consistency and future performance. </p> </li> <li> <p>Handler/Decorator Separation: The generic fallback logic was refactored into a <code>FallbackLogicHandler</code> class.  A simple <code>fallback_handler</code> decorator now acts as a convenient facade for this class, allowing the core logic to be used programmatically and tested more easily. </p> </li> </ol>"},{"location":"adr/ADR-047-fallback-fully-implemented/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>High Configurability: The system's behavior can be fundamentally changed via configuration (enums) without any code modification. </li> <li>Excellent Extensibility: Adding new read or write strategies is straightforward: create a new class and register it in the corresponding factory. The core components remain untouched.</li> <li>Decoupled and Highly Testable: Each component (Provider, Strategy, Factory) has a single responsibility and can be unit-tested in isolation.</li> <li>Deep Observability: Provides critical insights into how often the system relies on contingency plans and how well those plans perform. </li> <li>Increased System Resilience: The self-healing mechanism actively repairs cache state, reducing the impact of transient failures over time. </li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Initial Complexity: The number of classes and files is higher than a monolithic approach. This introduces a slight learning curve for developers new to the module.</li> <li>Indirection: The use of multiple patterns means a developer might need to navigate a few files to understand the full execution flow, as logic is delegated from the provider to the strategy.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This architecture strongly favors long-term maintainability and scalability over the speed of initial implementation.</li> <li>The performance of the fallback chain and the success rate of the <code>heal</code> operation become important metrics for operational monitoring.</li> </ul> </li> </ul>"},{"location":"adr/ADR-048-integration-configuration/","title":"ADR-048: Separation of Bootstrap and Dynamic Configuration","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-23</li> </ul>"},{"location":"adr/ADR-048-integration-configuration/#context","title":"Context","text":"<p>An application requires two fundamentally different types of configuration:</p> <ol> <li>Bootstrap Configuration: Static values needed for the application to start. This includes database connection strings, API keys for external services, and log levels. These are typically read once at startup from local files (<code>.toml</code>, <code>.env</code>) and are considered immutable for the application's lifecycle.</li> <li>Dynamic Configuration: Values that can change while the application is running and should be fetched from a remote source. This includes feature flags, rate-limiting thresholds, or A/B testing parameters. Fetching these values allows the application's behavior to be modified in real-time without a redeployment.</li> </ol> <p>The initial approach risked conflating these two concepts, potentially leading to a single, complex configuration mechanism that was ill-suited for both use cases. Using a file-based loader like Dynaconf for dynamic, runtime values is inefficient, and using a remote provider like Consul for static bootstrap values introduces an unnecessary dependency at startup.</p>"},{"location":"adr/ADR-048-integration-configuration/#decision","title":"Decision","text":"<p>To ensure architectural clarity and adhere to the Single Responsibility Principle, it was decided to formally separate these two concerns within the <code>athomic</code> framework.</p> <ol> <li>The existing <code>athomic/config</code> module will be solely responsible for Bootstrap Configuration. It will continue to use Dynaconf and Pydantic to load, validate, and provide immutable settings from local files at application startup.</li> <li>A new module, <code>athomic/configuration</code>, will be created to handle Dynamic Configuration. This module will provide a <code>ConfigurationProviderProtocol</code> and a factory to fetch values from external, remote sources (like Consul) during runtime. It will leverage <code>athomic</code>'s own caching layer to ensure high performance.</li> </ol>"},{"location":"adr/ADR-048-integration-configuration/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Architectural Clarity: The framework now has a clear, explicit distinction between static startup settings and dynamic runtime settings, making the system easier to reason about.</li> <li>Improved Testability: It's easier to mock bootstrap settings (via environment variables or file mocks) separately from mocking a remote configuration provider (like Consul) in tests.</li> <li>Enhanced Flexibility: The <code>athomic/configuration</code> module can be extended with new providers (e.g., for LaunchDarkly, Vault, etc.) without affecting the core bootstrap mechanism.</li> <li>Decoupling: The application's core services are not coupled to a specific remote source like Consul; they only depend on the <code>ConfigurationProviderProtocol</code>.</li> </ul> </li> <li>Negative:<ul> <li>Increased Surface Area: Introduces a new module and a new pattern (<code>ConfigurationProviderFactory</code>) that developers must learn, slightly increasing the framework's initial conceptual overhead.</li> </ul> </li> <li>Neutral/Other:<ul> <li>Developers must now consciously decide which configuration mechanism to use. Values needed at startup must be accessed via <code>get_settings()</code>, while values that can change at runtime must be accessed via the <code>ConfigurationProviderFactory</code>. This enforces a good design practice.</li> </ul> </li> </ul>"},{"location":"adr/ADR-049-consul-connectivity/","title":"ADR-049: Unification of Consul Connectivity","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-23</li> </ul>"},{"location":"adr/ADR-049-consul-connectivity/#context","title":"Context","text":"<p>It was identified that multiple modules within the <code>athomic</code> framework required a connection to Consul, including <code>feature_flags</code>, <code>configuration</code> (for remote configs), and <code>discovery</code> (for service discovery). The initial approach led to each module having its own logic for creating and managing a Consul client.</p> <p>This resulted in several problems: * Code Duplication: Redundant client implementation and connection logic across different modules. * Scattered Configuration: Connection settings for Consul were dispersed, making it difficult to manage and update. * Resource Inefficiency: Potentially multiple connection pools to the same Consul cluster, consuming unnecessary resources. * Inconsistency: Risk of different modules using different connection settings or client versions, leading to unpredictable behavior.</p>"},{"location":"adr/ADR-049-consul-connectivity/#decision","title":"Decision","text":"<p>To resolve these issues and create a more robust architecture, it was decided to centralize all Consul-related interactions.</p> <ol> <li>The <code>athomic/integration/consul</code> module is designated as the single source of truth for Consul connectivity.</li> <li>It provides a singleton <code>ConsulClientFactory</code> that manages the lifecycle (connection, disconnection) of a shared <code>ConsulClient</code> instance.</li> <li>All other modules (<code>feature_flags</code>, <code>configuration</code>, <code>discovery</code>, etc.) are refactored to be consumers of this shared client. They will no longer manage their own connections but will instead request the singleton instance from the <code>ConsulClientFactory</code>.</li> </ol>"},{"location":"adr/ADR-049-consul-connectivity/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>DRY Principle: Eliminates all redundant connection logic, making the codebase cleaner and easier to maintain.</li> <li>Centralized Configuration: All connection settings for Consul now reside in a single, well-defined section of the application's configuration, improving manageability.</li> <li>Resource Efficiency: A single, shared connection pool to Consul is used across the entire application, which is more efficient.</li> <li>Improved Reliability: Ensures that all parts of the application use the exact same, lifecycle-managed client, leading to more consistent and reliable behavior.</li> </ul> </li> <li>Negative:<ul> <li>Tighter Coupling to the Factory: Modules that consume the Consul client now have a direct dependency on the <code>ConsulClientFactory</code>. This is a deliberate architectural trade-off made to gain the significant benefits of centralization.</li> </ul> </li> <li>Neutral/Other:<ul> <li>This decision reinforces the \"shared infrastructure provider\" pattern within the <code>athomic</code> framework, where a central module provides a fundamental capability (like a database or broker connection) that other modules consume.</li> </ul> </li> </ul>"},{"location":"adr/ADR-050-performance-optimization/","title":"ADR-050: Multi-layered Performance Optimization Strategy","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-23</li> </ul>"},{"location":"adr/ADR-050-performance-optimization/#context","title":"Context","text":"<p>Application performance is not determined by a single factor but is rather the result of optimizations across different stages of a request's lifecycle. Relying solely on one technique (like caching) can leave significant bottlenecks unaddressed in other areas, such as network transfer, CPU-bound operations, or I/O event loop management.</p> <p>A holistic strategy was needed to ensure the <code>athomic</code> framework could provide comprehensive performance enhancements, making applications built with it fast by default.</p>"},{"location":"adr/ADR-050-performance-optimization/#decision","title":"Decision","text":"<p>It was decided that the <code>athomic/performance</code> module would be expanded to address performance optimization across three distinct layers. Instead of just being a caching module, it will now provide a suite of configurable performance tools.</p> <ol> <li> <p>Caching Layer: The existing advanced, multi-backend caching abstraction remains the core of data-level performance, with features like refresh-ahead and cache stampede prevention.</p> </li> <li> <p>Computation Layer: To optimize CPU-bound and I/O-bound operations, <code>athomic</code> will provide managed bootstraps for high-performance, third-party components. This includes:</p> <ul> <li>Event Loop: A bootstrap utility (<code>install_uvloop_if_available</code>) to replace Python's default asyncio event loop with <code>uvloop</code>.</li> <li>Serialization: A pluggable serializer architecture that supports high-speed libraries like <code>orjson</code>.</li> </ul> </li> <li> <p>Network Layer: To optimize data transfer between the server and the client, <code>athomic</code> will provide an abstraction for response compression. A <code>CompressionMiddlewareFactory</code> was created to support different algorithms (<code>gzip</code>, <code>brotli</code>), making the choice a configuration detail rather than an API-level implementation.</p> </li> </ol>"},{"location":"adr/ADR-050-performance-optimization/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Holistic Performance: The framework now offers a comprehensive performance toolkit, addressing bottlenecks from the event loop up to network delivery.</li> <li>Architectural Cohesion: All performance-related concerns are now logically grouped within the <code>athomic/performance</code> module, improving maintainability and discoverability.</li> <li>Flexibility &amp; Configurability: Developers can easily enable or switch between different performance optimizations (e.g., swapping <code>gzip</code> for <code>brotli</code>, enabling <code>uvloop</code>) through configuration, without changing application code.</li> <li>Improved Developer Experience: By providing managed bootstraps and factories, <code>athomic</code> simplifies the integration of these powerful but sometimes complex libraries.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Dependencies: Supporting these features adds new third-party dependencies (<code>uvloop</code>, <code>brotli-asgi</code>) to the framework, slightly increasing its installation footprint. This is mitigated by making the imports optional where possible.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This decision solidifies <code>athomic/performance</code> as a key pillar of the framework, on par with <code>resilience</code> and <code>observability</code>. It establishes a clear pattern for adding future performance-related enhancements.</li> </ul> </li> </ul>"},{"location":"adr/ADR-051-base-service-refact/","title":"ADR-051: Unify Service Lifecycle Management with a Single BaseService Class","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-24</li> </ul>"},{"location":"adr/ADR-051-base-service-refact/#context","title":"Context","text":"<p>The <code>athomic</code> framework initially had two separate base classes for managing service lifecycles: 1.  <code>BaseService</code>: Designed for services that manage a connection (e.g., database clients), providing <code>connect()</code> and <code>close()</code> methods. 2.  <code>RunnableServiceBase</code>: Designed for services that run a continuous background task (e.g., a message consumer or publisher), providing <code>start()</code>, <code>stop()</code>, and <code>_run_loop()</code> logic.</p> <p>This separation created a design conflict for services that are both connectable and runnable, such as the <code>VaultSecretsProvider</code>. The provider needs to connect to Vault and run a background task to renew its token.</p> <p>Attempting to solve this with multiple inheritance (<code>class VaultProvider(RunnableServiceBase, BaseService)</code>) led to technical issues, specifically a Method Resolution Order (MRO) conflict. The <code>set_ready()</code> method was defined with different signatures in both base classes (synchronous in <code>RunnableServiceBase</code>, asynchronous in <code>BaseService</code>), causing <code>TypeError</code> exceptions during testing.</p> <p>This indicated a flaw in the service model, creating complexity and forcing developers to choose between two conflicting abstractions or deal with fragile multiple inheritance.</p>"},{"location":"adr/ADR-051-base-service-refact/#decision","title":"Decision","text":"<p>To resolve the conflict and simplify the framework's service model, we decided to merge the functionality of <code>RunnableServiceBase</code> into <code>BaseService</code>.</p> <ol> <li>The <code>RunnableServiceBase</code> class is now deprecated and has been removed.</li> <li>The <code>BaseService</code> class is now the single, unified base class for all manageable services in the framework.</li> <li>The new <code>BaseService</code> intelligently manages both connection and background task lifecycles:<ul> <li>Subclasses can optionally implement <code>async def _connect()</code> and <code>async def _close()</code> for connection management.</li> <li>Subclasses can optionally implement <code>async def _run_loop()</code> to define a background task.</li> </ul> </li> <li>The public <code>start()</code> and <code>stop()</code> methods of <code>BaseService</code> now orchestrate the entire lifecycle. The <code>start()</code> method will call <code>_connect()</code> and, if <code>_run_loop()</code> is implemented, it will also start it in a background <code>asyncio.Task</code>.</li> <li>All existing services that previously inherited from <code>RunnableServiceBase</code> (e.g., <code>OutboxPublisherBase</code> ) and <code>BaseService</code> (e.g., <code>DocumentDatabaseRegistry</code> ) now inherit solely from the new <code>BaseService</code>.</li> <li>The corresponding protocols (<code>ConnectionServiceProtocol</code>, <code>RunnableServiceProtocol</code>) have also been merged into a single, unified <code>BaseServiceProtocol</code> to maintain consistency between implementation and interface.</li> </ol>"},{"location":"adr/ADR-051-base-service-refact/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-051-base-service-refact/#positive","title":"Positive:","text":"<ul> <li>Unified Service Model: Developers now only need to learn and inherit from a single <code>BaseService</code>, reducing cognitive load and simplifying the framework's API.</li> <li>Increased Flexibility &amp; Extensibility: Any service can easily be made \"runnable\" by simply implementing the <code>_run_loop</code> method, without needing to change its inheritance. This makes services like <code>VaultSecretsProvider</code> trivial to implement correctly.</li> <li>Conflict Resolution: The MRO conflict and the <code>TypeError</code> related to different method signatures are completely eliminated.</li> <li>Consistent Lifecycle Management: The application's <code>LifespanManager</code> can treat all services uniformly, calling <code>start()</code> and <code>stop()</code> on every registered component without needing to know its internal implementation details.</li> </ul>"},{"location":"adr/ADR-051-base-service-refact/#negative","title":"Negative:","text":"<ul> <li>Minor Violation of SRP/ISP: A service that is purely \"connectable\" will now inherit the logic and attributes for managing a background task (e.g., an empty <code>_run_loop</code> method and a <code>_run_task</code> attribute) that it does not use. This is a minor, pragmatic violation of the Single Responsibility and Interface Segregation principles.</li> <li>Slightly Larger Base Class: The <code>BaseService</code> class now has more responsibilities than before, though the added logic is minimal and intelligently applied only when needed.</li> </ul>"},{"location":"adr/ADR-051-base-service-refact/#neutralother","title":"Neutral/Other:","text":"<ul> <li>Refactoring Effort: A one-time effort was required to update all classes that inherited from the old <code>RunnableServiceBase</code> and <code>BaseServiceProtocol</code> to use the new unified versions.</li> </ul>"},{"location":"adr/ADR-053-vault-secrets-management/","title":"ADR-002: Adoption of HashiCorp Vault for Secrets Management","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-24</li> </ul>"},{"location":"adr/ADR-053-vault-secrets-management/#context","title":"Context","text":"<p>The project requires a secure, centralized, and auditable method for managing application secrets, such as database credentials, API keys, and certificates. The existing practice of using environment variables (<code>.env</code> files) is suitable for local development but poses significant security risks in production environments. It lacks audit trails, makes secret rotation difficult, and increases the risk of accidental exposure.</p> <p>We considered the following alternatives:</p> <ol> <li>Environment Variables: Simple to implement but insecure, hard to manage at scale, and not auditable. Considered the baseline to improve upon.</li> <li>Cloud-Native Secret Managers (e.g., AWS Secrets Manager, GCP Secret Manager): These are powerful, managed services that integrate well with their respective cloud ecosystems. However, they can lead to vendor lock-in for a critical security component.</li> <li>Encrypted Files in Version Control (e.g., git-crypt): An improvement over plain text, but still presents challenges with key management, access control, and dynamic rotation.</li> </ol> <p>The chosen solution needed to be abstractable within the <code>athomic</code> framework (via <code>SecretsProtocol</code>), support both local development and production environments, and offer strong security guarantees.</p>"},{"location":"adr/ADR-053-vault-secrets-management/#decision","title":"Decision","text":"<p>We have decided to adopt HashiCorp Vault as the standard secrets management solution for the project.</p> <p>The integration is implemented through a <code>VaultSecretsProvider</code> class, which conforms to the framework's generic <code>SecretsProtocol</code>. This decision encapsulates all Vault-specific logic into a single, reusable component.</p> <p>Key implementation details include: * Authentication: The provider supports multiple authentication methods, primarily direct <code>token</code> auth (for development and specific use cases) and AppRole, which is the recommended method for secure, automated machine-to-machine authentication in production. * Resilience: The provider features a built-in, resilient background task for automatic token renewal. This allows the use of short-lived tokens, which is a security best practice, without causing service interruptions. * Configuration: The connection to Vault (address, auth method, etc.) is managed through the application's central configuration (<code>settings.toml</code>), allowing for easy environment-specific setup. * Abstraction: Application code does not interact with Vault directly. It requests secrets through the <code>SecretsBase</code> or <code>SecretsProtocol</code> interface, making the system agnostic to the underlying secrets backend.</p>"},{"location":"adr/ADR-053-vault-secrets-management/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-053-vault-secrets-management/#positive","title":"Positive:","text":"<ul> <li>Centralized Security &amp; Auditability: All secrets are stored, encrypted, and managed in a single location. Vault provides detailed audit logs, showing who accessed which secret and when.</li> <li>Strong Access Control: Vault's policy-based access control allows for granular permissions, adhering to the principle of least privilege.</li> <li>Improved Developer Experience: Developers can run a local Vault instance via Docker for a consistent development environment that mirrors production.</li> <li>Cloud Agnostic: Vault is an open-source tool that can be deployed in any cloud or on-premises, avoiding vendor lock-in for this critical piece of infrastructure.</li> <li>Enables Dynamic Secrets: This decision opens the door to using Vault's more advanced features in the future, such as dynamic database credentials, which further reduces the attack surface.</li> </ul>"},{"location":"adr/ADR-053-vault-secrets-management/#negative","title":"Negative:","text":"<ul> <li>Increased Operational Overhead: We are now responsible for deploying, maintaining, monitoring, and backing up a new, stateful, and critical infrastructure component (the Vault server).</li> <li>Learning Curve: Developers and DevOps personnel will need to become familiar with Vault's concepts (e.g., policies, auth methods, leases, unsealing).</li> <li>Potential Single Point of Failure: Applications depend on Vault to retrieve secrets at startup. A Vault outage can prevent applications from starting. This necessitates a highly available Vault setup in production environments.</li> </ul>"},{"location":"adr/ADR-054-athomic-secrets-starting/","title":"ADR-003: Architectural Refactoring of Service Initialization and Dependency Management","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-24</li> </ul>"},{"location":"adr/ADR-054-athomic-secrets-starting/#context","title":"Context","text":"<p>The initial architecture of the <code>athomic</code> layer presented several design challenges that became apparent during the implementation of new features and tests:</p> <ol> <li> <p>Circular Imports: The attempt to create a cohesive system led to circular dependencies between modules. For example, the <code>services</code> module needed <code>security</code> (to resolve secrets), but the <code>security</code> module needed <code>services</code> (to inherit from <code>BaseService</code>), resulting in <code>ImportError</code> and blocking development.</p> </li> <li> <p>Tight Coupling and \"Magic\": Initialization logic was scattered. Services attempted to create their own dependencies (e.g., <code>RedisRateLimiter</code> calling <code>KVStoreFactory</code>) or relied on a global state (<code>get_settings()</code>) within their constructors. This created strong coupling, made the initialization flow implicit (\"magic\"), and made isolated unit testing extremely difficult.</p> </li> <li> <p>Code Repetition: Common logic, such as resolving credentials from different sources (<code>str</code>, <code>SecretStr</code>, <code>Callable</code>), was beginning to be repeated across multiple services (e.g., <code>MongoProvider</code>, <code>RedisKVClient</code>).</p> </li> <li> <p>Leaking Abstraction: The <code>api</code> layer (<code>lifespan.py</code>) was becoming responsible for orchestrating <code>athomic</code>'s startup sequence (e.g., calling <code>prepare</code>, then <code>start</code>), which violates the principle of separation of concerns. The <code>api</code> should use <code>athomic</code>, not manage it.</p> </li> </ol>"},{"location":"adr/ADR-054-athomic-secrets-starting/#decision","title":"Decision","text":"<p>To address these issues, we decided to refactor the <code>athomic</code> architecture to adopt a set of more robust and explicit software design patterns.</p> <ol> <li> <p>Inversion of Control (IoC) with Dependency Injection (DI): This is the core principle of the new architecture. Components no longer create their own dependencies. Instead, they receive their dependencies ready-made via their constructors (<code>__init__</code>). This breaks circular imports and makes dependencies explicit.</p> </li> <li> <p>Centralized Composition Root: The responsibility of creating and \"wiring\" all infrastructure services has been centralized into a single function: <code>register_athomic_infra_services</code>. This function acts as the \"composition root,\" where the entire object graph of <code>athomic</code> is constructed in the correct order (e.g., the <code>SecretsProvider</code> is created first and then injected into others).</p> </li> <li> <p>Facade Pattern (<code>AthomicFacade</code>): To hide the complexity of initialization and provide a single, simple entry point for the <code>api</code> layer, the <code>AthomicFacade</code> class was created. The <code>api</code> now interacts only with this facade, calling simple methods like <code>await athomic.startup()</code>. The facade encapsulates the <code>SecretsManager</code> and <code>LifespanManager</code>.</p> </li> <li> <p>Lazy Secret Resolution with Proxies: To support secret rotation and keep services decoupled from the security module, the <code>SecretsManager</code> was implemented. At startup, it traverses the configuration and replaces secret references (<code>SecretValue</code>) with <code>CredentialProxy</code> objects, which act as lazy, asynchronous getters.</p> </li> <li> <p>Logic Reusability with Specific Inheritance: To avoid repeating the credential \"unwrapping\" code, the <code>CredentialResolve</code> base class was created. Services that need this functionality (like <code>MongoProvider</code> and <code>RedisKVClient</code>) inherit from it, gaining the <code>_resolve_credential</code> method without polluting the main <code>BaseService</code>.</p> </li> </ol>"},{"location":"adr/ADR-054-athomic-secrets-starting/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Circular Imports Eliminated: The primary technical blocker has been resolved, unblocking tests and development.</li> <li>Exceptional Testability: Services can now be tested in complete isolation by injecting mocks of their dependencies into their constructors.</li> <li>Clarity and Explicit Code: The \"magic\" is gone. The initialization flow is now explicit, orchestrated by the <code>AthomicFacade</code>, and each service's dependencies are clearly visible in its <code>__init__</code> signature.</li> <li>Robust Separation of Concerns: The <code>api</code> layer is now a clean consumer of the <code>athomic</code> layer. All complex infrastructure logic is fully encapsulated within <code>athomic</code>.</li> <li>DRY Principle Adhered To: Common logic for credential resolution and service lifecycle is centralized in reusable base classes and orchestrators.</li> <li>Support for Secret Rotation: The lazy-loading proxy pattern ensures that services always fetch the latest version of a secret, making the system compatible with dynamic secrets.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Indirection: The new architecture introduces more classes and patterns (Facade, DI, Factories, Proxies). This adds a layer of indirection that may require a slightly steeper learning curve for new developers on the project.</li> <li>Centralized Complexity: The \"Composition Root\" (<code>register_athomic_infra_services</code>) and the <code>AthomicFacade</code> become critical, central points of the architecture. Changes in these files can have a wide-ranging impact.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>The framework is now more \"opinionated\" about how services are constructed and managed. This enforces consistency but requires developers to follow the established patterns.</li> </ul> </li> </ul>"},{"location":"adr/ADR-056-api-key-client-and-shared-auth/","title":"ADR-056: Policy-Based, Decoupled Authentication and Authorization Service","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-28</li> </ul>"},{"location":"adr/ADR-056-api-key-client-and-shared-auth/#context","title":"Context","text":"<p>The application requires a robust, flexible, and testable system for securing API endpoints. The primary needs include: 1.  Multiple Security Levels: Endpoints must support different access levels: fully public, internal (accessible only by trusted IPs), and private (requiring a valid user token). 2.  Decoupling: The core security logic should be independent of the web framework (FastAPI) to promote portability, reusability, and easier unit testing. It must integrate with the existing Athomic infrastructure layer. 3.  Extensibility: The architecture must be extensible to support future authentication methods (e.g., API Keys, OAuth2) and more complex authorization rules (e.g., role-based or scope-based access) without requiring significant refactoring.</p> <p>Several alternatives were considered: * Logic within FastAPI Dependencies: Placing all validation logic directly inside FastAPI dependency functions. This was rejected as it leads to code duplication and tightly couples the security logic to the web framework. * Third-Party Auth Libraries: Using a comprehensive library like <code>fastapi-users</code>. This was rejected in favor of a custom solution that integrates more deeply with the Athomic layer's patterns, such as configuration management, secret resolution, and observability. * A Monolithic Auth Service: Creating a single service to handle both IP validation and token validation. This was rejected because it violates the Single Responsibility Principle and makes the system less flexible.</p>"},{"location":"adr/ADR-056-api-key-client-and-shared-auth/#decision","title":"Decision","text":"<p>We have decided to implement a decoupled, policy-based authentication and authorization service within the Athomic layer. This architecture is composed of several distinct components with clear responsibilities:</p> <ol> <li> <p>Separation of Authentication and Authorization:</p> <ul> <li>Authentication (<code>AuthProviderProtocol</code>): Responsible for validating credentials and proving identity. The primary implementation is <code>JWTAuth</code>, which handles the creation and verification of JSON Web Tokens. It is an abstraction that proves who a user is.</li> <li>Authorization (<code>AuthService</code> and <code>AuthExecutorProtocol</code>): Responsible for determining if a request is permitted to access a resource based on a given policy. This service decides what a user (or system) is allowed to do.</li> </ul> </li> <li> <p>Policy-Based Execution:</p> <ul> <li>The <code>AuthService</code> acts as an orchestrator. It does not contain any validation logic itself. Instead, it accepts a <code>policy_name</code> (e.g., \"private\", \"internal\") and uses a registry to find the appropriate <code>AuthExecutor</code>.</li> <li>Executors (<code>AuthExecutorProtocol</code>): Each executor implements the logic for a specific security policy. We have implemented three default executors:<ul> <li><code>PublicExecutor</code>: A no-op for public endpoints.</li> <li><code>InternalIPExecutor</code>: Enforces IP allow-listing by delegating to the <code>IPAllowlist</code> service.</li> <li><code>PrivateTokenExecutor</code>: Enforces that a valid token is present. It uses the <code>AuthProviderProtocol</code> to validate the token and returns an <code>AuthenticatedUser</code> model on success.</li> </ul> </li> </ul> </li> <li> <p>Framework Decoupling:</p> <ul> <li>The core security logic resides entirely within the <code>athomic</code> layer and has no dependency on FastAPI.</li> <li>A generic <code>SecurityRequestContext</code> model is used to pass request data (headers, client IP) to the <code>AuthService</code>, making the core logic portable.</li> <li>Integration with FastAPI is done exclusively at the application's edge, within the <code>api/dependencies/security.py</code> module. Dependencies like <code>require_token</code> and <code>require_internal_ip</code> adapt the FastAPI <code>Request</code> object into a <code>SecurityRequestContext</code> before calling the <code>AuthService</code>.</li> </ul> </li> </ol>"},{"location":"adr/ADR-056-api-key-client-and-shared-auth/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>High Decoupling &amp; Testability: Each component (AuthProvider, AuthService, Executors) can be unit-tested in complete isolation by mocking its dependencies. The core logic can be tested without a web server.</li> <li>Excellent Extensibility:<ul> <li>Adding a new authentication method (e.g., API Key) only requires creating a new class that implements <code>AuthProviderProtocol</code> and updating the factory.</li> <li>Adding a new authorization policy (e.g., role-based checks) is as simple as creating a new <code>AuthExecutor</code> and registering it.</li> </ul> </li> <li>Declarative and Clean Routes: The security rules at the API route level are highly declarative and readable (e.g., <code>Depends(require_token)</code>), hiding the underlying complexity.</li> <li>Single Responsibility Principle: Each class has a single, well-defined purpose, making the system easier to understand and maintain.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Boilerplate: This decoupled approach introduces more classes and files (Protocols, Factories, Registries, Services, Executors) compared to a monolithic solution. This can present a slightly steeper learning curve for new developers.</li> <li>Potential for Over-Engineering: If the application's security needs remain extremely simple, this layered architecture could be considered more complex than necessary. However, it provides a solid foundation for future growth.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This architecture strongly enforces the distinction between Authentication (\"who you are\") and Authorization (\"what you can do\"), which is a core security principle.</li> </ul> </li> </ul>"},{"location":"adr/ADR-057-bulkhead-pattern/","title":"ADR-057: Bulkhead Pattern for Concurrency Control","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-28</li> </ul>"},{"location":"adr/ADR-057-bulkhead-pattern/#context","title":"Context","text":"<p>The Athomic layer provides several resilience patterns like Retry and Circuit Breaker. However, the application remains vulnerable to a specific type of cascading failure: resource exhaustion due to a slow or unresponsive dependency.</p> <p>If a downstream service becomes slow, all concurrent requests to it can hang, consuming finite resources in our application (e.g., database connections, memory, or simply overwhelming the asyncio event loop with pending tasks). This can starve other, unrelated parts of the application of resources, causing them to fail as well. The Circuit Breaker pattern is reactive and only trips after a certain number of failures, by which time resources may already be exhausted.</p> <p>We need a proactive mechanism to limit the number of concurrent executions for a given operation, thereby isolating the fault to one part of the system.</p> <p>Alternatives Considered:</p> <ul> <li>Relying on Timeouts and Circuit Breakers alone: This was rejected because it's a reactive approach. A surge of requests can still exhaust system resources before timeouts are hit or the circuit breaker trips.</li> <li>Using the existing Rate Limiter: This was rejected because Rate Limiting and Bulkheading solve different problems. A rate limiter restricts the rate of calls over time (e.g., 100 requests/minute), whereas a bulkhead restricts the number of concurrent calls at any single moment. They are complementary patterns.</li> </ul>"},{"location":"adr/ADR-057-bulkhead-pattern/#decision","title":"Decision","text":"<p>We will implement the Bulkhead pattern to isolate concurrent operations and prevent resource exhaustion. The implementation will be native to the Athomic resilience module and follow our established architectural patterns.</p> <ol> <li> <p>Core Implementation: The bulkhead will be implemented using <code>asyncio.Semaphore</code> for each defined policy. This is a lightweight and efficient mechanism for controlling concurrency in an <code>asyncio</code>-based application.</p> </li> <li> <p>Configuration: A new <code>BulkheadSettings</code> schema will be added to <code>resilience.resilience_config</code>. This will allow developers to:</p> <ul> <li>Enable or disable the pattern globally.</li> <li>Set a <code>default_limit</code> for concurrency.</li> <li>Define a dictionary of named <code>policies</code> with specific integer limits (e.g., <code>policies = {\"payment_api\": 5, \"user_service\": 15}</code>).</li> </ul> </li> <li> <p>Service and Factory:</p> <ul> <li>A singleton <code>BulkheadService</code> will be responsible for creating and managing the <code>asyncio.Semaphore</code> instances based on the loaded configuration.</li> <li>A <code>BulkheadFactory</code> will provide access to the singleton <code>BulkheadService</code> instance.</li> </ul> </li> <li> <p>Developer API (<code>@bulkhead</code> decorator):</p> <ul> <li>A simple decorator, <code>@bulkhead(policy=\"...\")</code>, will be provided. Developers can apply this to any <code>async def</code> function.</li> <li>The decorator will use the <code>BulkheadService</code> to acquire a semaphore slot before executing the function.</li> <li>If a slot cannot be acquired immediately (i.e., the bulkhead is full), a <code>BulkheadRejectedError</code> exception will be raised, which can be caught at the API layer and translated into an appropriate HTTP response (e.g., <code>503 Service Unavailable</code> or <code>429 Too Many Requests</code>).</li> </ul> </li> <li> <p>Observability:</p> <ul> <li>The <code>BulkheadService</code> will be instrumented with Prometheus metrics to track:<ul> <li><code>nala_bulkhead_concurrent_requests</code> (Gauge): The number of currently active requests for each policy.</li> <li><code>nala_bulkhead_calls_total</code> (Counter): The total number of calls that were <code>accepted</code> or <code>rejected</code> by each policy.</li> </ul> </li> </ul> </li> </ol>"},{"location":"adr/ADR-057-bulkhead-pattern/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Fault Isolation: Prevents a slow or failing dependency from causing a cascading failure that affects the entire application. The impact is contained within the bulkhead.</li> <li>Resource Protection: Proactively protects the application from resource exhaustion (memory, connections, event loop saturation) under high load or during partial system degradation.</li> <li>Declarative API: The <code>@bulkhead</code> decorator provides a clean, simple, and declarative way for developers to apply concurrency limits without needing to manage semaphores manually.</li> <li>Enhanced Observability: New metrics will provide direct insight into which parts of the application are under high concurrency pressure, helping to identify bottlenecks.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Configuration Overhead: Requires developers to analyze their services and define sensible concurrency limits. A limit that is set too low can unnecessarily throttle the application and reduce performance.</li> <li>Introduces a new failure mode: Requests can now be explicitly rejected with a <code>BulkheadRejectedError</code>, which must be handled gracefully by the calling code (e.g., at the API layer).</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This pattern is highly complementary to the existing Circuit Breaker and Retry patterns. A typical flow might be: <code>Retry</code> -&gt; <code>Circuit Breaker</code> -&gt; <code>Bulkhead</code> -&gt; <code>Function Execution</code>. The bulkhead ensures that even retries do not overwhelm a service.</li> </ul> </li> </ul>"},{"location":"adr/ADR-058-first-class-distributed-lock-decorator/","title":"ADR-058: First-Class Distributed Lock Decorator","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-29</li> </ul>"},{"location":"adr/ADR-058-first-class-distributed-lock-decorator/#context","title":"Context","text":"<p>The application needs to perform business-critical operations that are not idempotent (e.g., processing a payment, updating inventory). In a distributed environment with multiple running instances, there is a significant risk of race conditions, where two processes attempt to modify the same resource simultaneously, leading to data corruption or inconsistent states.</p> <p>While the <code>athomic</code> layer contained an underlying locking mechanism (<code>LockingFactory</code>, <code>RedisLockProvider</code>) used internally by the caching decorator , it was not exposed as a first-class, easy-to-use feature for developers to protect their business logic. The goal was to provide a simple, declarative way to enforce mutual exclusion for critical code sections.</p>"},{"location":"adr/ADR-058-first-class-distributed-lock-decorator/#decision","title":"Decision","text":"<p>We have decided to implement a high-level <code>@distributed_lock</code> decorator to expose the existing locking functionality as a primary resilience feature.</p> <ol> <li> <p>Decorator Implementation: A decorator named <code>@distributed_lock</code> was created in <code>athomic/resilience/locking/decorator.py</code>. It accepts two main arguments:</p> <ul> <li><code>key</code>: A string template that is formatted at runtime using the arguments of the decorated function. This allows for dynamic, resource-specific keys (e.g., <code>key=\"payment:{payment_id}\"</code>).</li> <li><code>timeout</code>: An integer specifying the maximum time in seconds to wait to acquire the lock.</li> </ul> </li> <li> <p>Core Logic: The decorator wraps the target <code>async</code> function. Before execution, it resolves the lock key, obtains a lock provider from the existing <code>LockingFactory</code> , and attempts to acquire the lock using <code>async with provider.acquire(...)</code>.</p> </li> <li> <p>Exception Handling: If the lock cannot be acquired within the specified timeout (the provider raises <code>asyncio.TimeoutError</code>), the decorator catches it and raises a new, specific <code>LockAcquisitionError</code> exception. This allows the application layer (e.g., an API route) to catch this specific error and return a meaningful response, such as <code>HTTP 409 Conflict</code>.</p> </li> <li> <p>Exporting: The decorator and its exception were made easily accessible by exporting them from the top-level <code>athomic/resilience/__init__.py</code> module.</p> </li> </ol>"},{"location":"adr/ADR-058-first-class-distributed-lock-decorator/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Developer Experience: Protecting a critical section from race conditions is now reduced to a single, declarative decorator line, significantly simplifying development.</li> <li>Safety and Correctness: Prevents data corruption and inconsistent states in distributed environments by ensuring mutual exclusion for critical operations.</li> <li>Reusability: Leverages the existing and tested <code>LockingFactory</code> and providers (<code>RedisLockProvider</code>, <code>LocalLockProvider</code>) .</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>New Failure Mode: Introduces a new, expected failure mode (<code>LockAcquisitionError</code>) that developers must handle. API endpoints using this decorator need <code>try...except</code> blocks to translate this error into a user-friendly response (e.g., HTTP 409).</li> <li>Performance Overhead: Each decorated call introduces a small overhead due to the network round-trip to acquire and release the lock from Redis.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>The reliability of this feature is now directly tied to the availability and performance of the underlying KVStore (Redis).</li> </ul> </li> </ul>"},{"location":"adr/ADR-059-high-Level-ackground-task-abstraction-decorator/","title":"ADR-059: High-Level Background Task Abstraction (@task)","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-29</li> </ul>"},{"location":"adr/ADR-059-high-Level-ackground-task-abstraction-decorator/#context","title":"Context","text":"<p>Many API operations need to trigger long-running or non-essential side effects (e.g., sending a welcome email, processing an image, generating a report). Executing these tasks synchronously within the request-response cycle increases API latency and couples the API's success to the success of the side effect.</p> <p>The <code>athomic</code> layer already had the low-level infrastructure to solve this, including a <code>TaskBrokerProtocol</code> , a <code>get_task_broker</code> factory , and robust context propagation utilities (<code>capture_context_for_task</code>, <code>restore_context_from_task</code>) . However, using these components directly was manual and required significant boilerplate from the developer. The goal was to create a high-level, developer-friendly API similar to established libraries like Celery.</p>"},{"location":"adr/ADR-059-high-Level-ackground-task-abstraction-decorator/#decision","title":"Decision","text":"<p>We have decided to implement a two-part abstraction layer to simplify the definition and execution of background tasks.</p> <ol> <li> <p>Client-Side Decorator (<code>@task</code>):</p> <ul> <li>A decorator named <code>@task</code> was created in <code>athomic/integration/tasks/decorator.py</code>.</li> <li>This decorator wraps an <code>async</code> function and returns a <code>TaskWrapper</code> object. This object retains the original callable behavior but adds a new <code>.delay()</code> method.</li> <li>The <code>.delay(*args, **kwargs)</code> method is the primary interface for enqueuing a job. It automatically performs the following steps:<ol> <li>Calls <code>get_task_broker()</code> to get the configured provider.</li> <li>Calls <code>capture_context_for_task()</code> to serialize the current execution context (trace ID, tenant ID, etc.).</li> <li>Bundles the captured context into the task's arguments under a reserved key (<code>_nala_context</code>).</li> <li>Calls the provider's <code>enqueue_task</code> method with the function's fully qualified name, arguments, and the propagated context.</li> </ol> </li> </ul> </li> <li> <p>Worker-Side Helper (<code>run_task_with_context</code>):</p> <ul> <li>A helper function/decorator named <code>run_task_with_context</code> was created in <code>athomic/integration/tasks/worker.py</code>.</li> <li>This wrapper is intended to be used in the background worker process.</li> <li>It inspects the incoming task's arguments, extracts the <code>_nala_context</code> dictionary, and uses the <code>restore_context_from_task</code> context manager to re-establish the execution context before calling the original task logic.</li> </ul> </li> </ol>"},{"location":"adr/ADR-059-high-Level-ackground-task-abstraction-decorator/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Drastically Simplified DX: Defining and dispatching background tasks is now a simple, declarative process (<code>@task</code> and <code>.delay()</code>), abstracting away all the complexity of brokers and context management.</li> <li>Improved API Performance: Allows API endpoints to offload work and respond to users more quickly.</li> <li>Guaranteed Observability: The automatic context propagation ensures that logs and distributed traces generated within a background task are correctly correlated with the original request that triggered them.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Infrastructure Complexity: The system now formally relies on an external message broker (like Redis/Kafka for Celery/RQ) and a separate, running worker process, which adds operational overhead.</li> <li>Layer of Indirection: The decorator adds a layer of \"magic\" that might require developers to understand the underlying mechanism if debugging is needed.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This decision formalizes the \"Background Task\" as a core pattern within the application's architecture.</li> </ul> </li> </ul>"},{"location":"adr/ADR-060-dynamic-configuration-hot-reload-system/","title":"ADR-060: Dynamic Configuration (Hot Reload) System","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-07-29</li> </ul>"},{"location":"adr/ADR-060-dynamic-configuration-hot-reload-system/#context","title":"Context","text":"<p>The application's configuration is loaded from static files (<code>.toml</code>) at startup using Dynaconf . This model is robust but rigid. Modifying any runtime behavior controlled by configuration (e.g., rate limits, circuit breaker thresholds, log levels, feature flags) requires a service restart or a new deployment. This introduces downtime and reduces operational agility, especially when responding to incidents.</p> <p>The goal was to create a system that could update its configuration at runtime by observing changes in a central, external source.</p>"},{"location":"adr/ADR-060-dynamic-configuration-hot-reload-system/#decision","title":"Decision","text":"<p>We have decided to implement a multi-component system for dynamic configuration that leverages existing <code>athomic</code> modules.</p> <ol> <li> <p>Provider Enhancement: The low-level <code>ConfigurationProviderProtocol</code> was enhanced with a <code>watch(key, index)</code> method to support long-polling. This was implemented in the <code>ConsulConfigProvider</code> using the native long-polling capabilities of the Consul client . Caching was explicitly removed from this provider to ensure it always fetches the latest state.</p> </li> <li> <p><code>ConfigWatcherService</code> (The Watcher):</p> <ul> <li>A new background service, inheriting from <code>BaseService</code>, was created in <code>athomic/control/live_config/service.py</code>.</li> <li>Its <code>_run_loop</code> starts a concurrent task for each key specified in the new <code>control.live_config.keys_to_watch</code> configuration section.</li> <li>Each task calls the provider's <code>watch</code> method in an infinite loop. When a change is detected, it publishes a <code>config.updated</code> event to the internal <code>EventBus</code>.</li> </ul> </li> <li> <p><code>DynamicConfigService</code> (The Manager):</p> <ul> <li>A singleton service was created, also in <code>service.py</code>.</li> <li>Upon initialization, it flattens the entire static <code>AppSettings</code> object into an in-memory key-value dictionary to serve as the initial state.</li> <li>It subscribes to the <code>config.updated</code> event on the <code>EventBus</code> . The event handler updates the corresponding key in its in-memory state.</li> <li>It exposes a simple <code>get(key, default)</code> method, which acts as the new, single source of truth for runtime configuration.</li> </ul> </li> <li> <p>Module Adaptation:</p> <ul> <li>To prove the concept and enable dynamic control, the <code>RateLimiterService</code> was refactored. Instead of reading its policy limits from its static settings object, its <code>check</code> method now calls <code>live_config_service.get(...)</code> on every execution to retrieve the most up-to-date limit string .</li> </ul> </li> </ol>"},{"location":"adr/ADR-060-dynamic-configuration-hot-reload-system/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>High Operability: Allows operators to change application behavior (e.g., tighten rate limits, adjust timeouts) in real-time without downtime, which is critical for incident response and performance tuning.</li> <li>Reduced Deployments: Simple configuration changes no longer require a full deployment cycle.</li> <li>Architecturally Sound: The solution is highly decoupled, cleanly separating the concerns of watching, managing state, and consuming the configuration. It leverages existing components like the <code>EventBus</code> and <code>ConsulClient</code>.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Complexity: The architecture now has more moving parts (a watcher service, event bus communication) which can be more complex to debug than a simple static configuration.</li> <li>Dependency on External System: The application's runtime stability becomes more tightly coupled to the availability and correctness of the external configuration store (Consul).</li> <li>Potential for Transient Inconsistency: In a distributed system, there can be a small delay between a config change and all service instances receiving the update.</li> </ul> </li> </ul>"},{"location":"adr/ADR-061-transparent-dynamic-configuration-system-with-live-config-model/","title":"ADR-061: Transparent Dynamic Configuration System with <code>LiveConfigModel</code>","text":"<p>Status: Accepted Date: 2025-07-29</p> <p>Context: ADR-060 introduced a dynamic configuration system that allowed real-time parameter updates. The initial implementation, while functional, required consuming services to explicitly know about <code>LiveConfigService</code> and use its <code>.get()</code> method to fetch values, resulting in more verbose and tightly coupled code. The goal evolved to find an architectural solution that would make dynamic configuration access completely transparent to consumer code, improving maintainability and developer experience.</p> <p>Decision: We decided to implement a transparent dynamic configuration pattern, where the logic for fetching updated values is encapsulated within the Pydantic configuration schemas themselves. The architecture consists of four pillars:</p> <ol> <li> <p><code>LiveConfigModel</code> (The Central Piece):</p> <ul> <li>A new base class, <code>LiveConfigModel</code>, was created, inheriting from <code>pydantic.BaseModel</code>.</li> <li>This class overrides the <code>__getattribute__</code> method to intercept access to all its fields.</li> <li>The internal logic checks if the accessed field is mapped as \"dynamic\". If so, it fetches the latest value from <code>LiveConfigService</code>; otherwise, it returns the static value loaded from the configuration file at startup.</li> <li>This makes dynamic behavior completely transparent to code using the settings object.</li> </ul> </li> <li> <p>Declarative Configuration via TOML:</p> <ul> <li>The \"wiring\" of which fields are dynamic was moved from Python code to the <code>settings.toml</code> file.</li> <li>Each configuration section can now contain an optional map called <code>_live_keys_config</code>, which associates a Pydantic schema attribute with its corresponding key in <code>LiveConfigService</code>.</li> <li>The <code>__init__</code> of <code>LiveConfigModel</code> reads this map during instantiation of the settings object.</li> </ul> <p>Example in <code>settings.toml</code>: ```toml [resilience.circuit_breaker]</p> </li> <li> <p><code>LiveConfigService</code> (The State Guardian):</p> <ul> <li>The former <code>DynamicConfigService</code> was renamed to <code>LiveConfigService</code> for clarity.</li> <li>It remains the singleton responsible for maintaining the state dictionary (<code>_config_state</code>) with the latest configuration values, updated by events from the <code>EventBus</code>.</li> </ul> </li> <li> <p><code>ConfigWatcher</code> (The Observer):</p> <ul> <li>The former <code>ConfigWatcherService</code> was renamed to <code>ConfigWatcher</code>.</li> <li>It continues as the background service that monitors an external source (e.g., Consul), reading the list of keys from <code>control.live_config.keys_to_watch</code> and publishing <code>config.updated</code> events when changes are detected.</li> </ul> </li> </ol> <p>Data Flow: 1.  An operator changes a value in Consul. 2.  <code>ConfigWatcher</code> detects the change and publishes a <code>config.updated</code> event on the <code>EventBus</code>. 3.  <code>LiveConfigService</code> (subscribed to the event) receives the payload and updates its internal state dictionary. 4.  Anywhere in the application, code accesses an attribute of a settings object (e.g., <code>settings.resilience.circuit_breaker.default_fail_max</code>). 5.  The <code>__getattribute__</code> of <code>LiveConfigModel</code> is triggered, queries <code>LiveConfigService</code>, and returns the latest value.</p> <p>Consequences:</p> <p>Positives: * Total Transparency: Code consuming configurations is extremely clean and simple (e.g., <code>settings.some_value</code>). It does not need to know whether the value is static or dynamic. * Maximum Decoupling: Services (<code>RateLimiterService</code>, <code>CircuitBreakerService</code>, etc.) no longer have any direct dependency on <code>LiveConfigService</code>. * Centralized Configuration: The decision to make a field dynamic is made entirely in the <code>.toml</code> file, which is the appropriate place for such \"wiring\". * Robustness: The implementation of <code>LiveConfigModel</code> was tested to avoid <code>RecursionError</code> with internal attributes, making it safe for general use. * High Maintainability: Adding new dynamic fields only requires a change in the <code>.toml</code> and ensuring the schema inherits from <code>LiveConfigModel</code>, with no need to alter service business logic.</p> <p>Negatives: * Internal Complexity: The implementation of <code>LiveConfigModel</code> uses metaprogramming (<code>__getattribute__</code>), which is an advanced part of Python and may be harder to understand for less experienced developers. * Small Overhead: There is a small but measurable performance cost for each access to a dynamic attribute due to the extra logic in <code>__getattribute__</code>. However, for configuration objects, this cost is considered negligible. * \"Magic\" Behavior: Because it is so transparent, it may not be immediately obvious to a new developer that the value of an attribute can change in real time. Good internal documentation is essential.</p>"},{"location":"adr/ADR-061-transparent-dynamic-configuration-system-with-live-config-model/#map-that-enables-dynamicity-for-fields-in-this-schema","title":"Map that enables \"dynamicity\" for fields in this schema","text":"<p>_live_keys_config = { default_fail_max = \"resilience.circuit_breaker.default_fail_max\" }</p>"},{"location":"adr/ADR-061-transparent-dynamic-configuration-system-with-live-config-model/#static-value-used-at-initialization-and-as-fallback","title":"Static value used at initialization and as fallback","text":"<p>DEFAULT_FAIL_MAX = 5 ```</p>"},{"location":"adr/ADR-062-resilient-and-configurable-http-client/","title":"ADR-062: Resilient and Configurable HTTP Client","text":"<p>Status: Accepted</p> <p>Date: 2025-07-31</p>"},{"location":"adr/ADR-062-resilient-and-configurable-http-client/#context","title":"Context","text":"<p>Application services frequently need to communicate with external or internal APIs via HTTP. Without a standardized approach, developers would implement their own clients, leading to inconsistent handling of critical cross-cutting concerns such as:</p> <ul> <li>Resilience: Ad-hoc implementations of retries and timeouts, with no circuit breaker pattern.</li> <li>Observability: Lack of standardized tracing, metrics, and structured logging for outbound requests.</li> <li>Authentication: Inconsistent and insecure handling of credentials like API keys and bearer tokens.</li> <li>Caching: No unified strategy for caching responses from external services.</li> <li>Configuration: Settings like base URLs and timeouts would be scattered and managed inconsistently.</li> </ul> <p>This would result in a codebase that is brittle, difficult to debug, and lacks visibility into the performance and reliability of its integrations. The <code>athomic</code> layer, which already standardizes other infrastructure concerns, was the logical place to provide a centralized solution.</p>"},{"location":"adr/ADR-062-resilient-and-configurable-http-client/#decision","title":"Decision","text":"<p>We decided to create a new, comprehensive <code>http</code> module within the <code>athomic</code> layer. This module provides a <code>HttpClientFactory</code> as a single entry point for creating named, fully-featured, and production-ready HTTP client instances.</p> <p>The architecture follows established patterns within the <code>athomic</code> framework to ensure consistency and extensibility:</p> <ol> <li> <p>Provider-Agnostic Design:</p> <ul> <li>The core logic is decoupled from the underlying HTTP library. A generic <code>HttpResponse</code> Pydantic model is used as the data transfer object between the provider and the base layer.</li> <li>A <code>HttpxProvider</code> is created as the concrete implementation, using the modern <code>httpx</code> library. The architecture allows for other providers (e.g., <code>AiohttpProvider</code>) to be added in the future.</li> </ul> </li> <li> <p>Centralized Orchestration in a Base Class:</p> <ul> <li>An abstract <code>HTTPClientBase</code> class encapsulates all shared logic, ensuring every client instance automatically benefits from:<ul> <li>Observability: Automatic creation of OpenTelemetry traces and Prometheus metrics for every request.</li> <li>Context Propagation: Automatic forwarding of context headers (<code>x-request-id</code>, <code>x-tenant-id</code>, etc.) on all outgoing requests.</li> <li>Lifecycle Management: Integration with <code>BaseService</code> for managed startup and shutdown.</li> </ul> </li> </ul> </li> <li> <p>Configurable, Injected Resilience:</p> <ul> <li>Instead of hardcoding resilience logic, we created a generic <code>ResilienceOrchestrator</code>.</li> <li>The <code>HttpClientFactory</code> reads the client\\'s configuration and creates a <code>ResilienceOrchestrator</code> instance with the appropriate <code>RetryHandler</code> and <code>CircuitBreakerService</code> based on named policies defined in <code>.toml</code> files.</li> <li>This orchestrator is then injected into the <code>HTTPClientBase</code>, completely decoupling the client from the resilience implementation.</li> </ul> </li> <li> <p>Strategy Pattern for Authentication:</p> <ul> <li>An <code>AuthStrategyProtocol</code> defines the contract for authentication methods.</li> <li>The <code>HttpClientFactory</code> instantiates and injects the correct strategy (e.g., <code>BearerTokenAuth</code>, <code>APIKeyAuth</code>) based on the client\\'s configuration.</li> <li>This pattern reuses the existing <code>CredentialResolve</code> logic for secure handling of secrets.</li> </ul> </li> <li> <p>Dynamic, Configurable Caching:</p> <ul> <li>The <code>HttpClientFactory</code> can dynamically apply the existing <code>@cache</code> decorator to a client instance\\'s <code>get</code> method if caching is configured for it in the settings via a <code>CacheSettings</code> object.</li> </ul> </li> </ol>"},{"location":"adr/ADR-062-resilient-and-configurable-http-client/#consequences","title":"Consequences","text":""},{"location":"adr/ADR-062-resilient-and-configurable-http-client/#positive","title":"Positive","text":"<ul> <li>Developer Experience: Developers can now obtain a fully resilient, observable, and authenticated HTTP client with a single line of code (<code>HttpClientFactory.create(\\'my_api\\')</code>), drastically reducing boilerplate and letting them focus on business logic.</li> <li>Standardization &amp; Consistency: All outbound HTTP calls now behave uniformly, making the system more predictable and easier to maintain.</li> <li>Increased Robustness: The built-in retry and circuit breaker patterns make the application significantly more resilient to transient failures in downstream services.</li> <li>Deep Observability: Every HTTP call is now a \"white box\", with detailed metrics and traces available out-of-the-box, simplifying debugging and performance monitoring.</li> <li>High Flexibility: Every aspect of a client (timeout, auth, retry policy, cache TTL, provider) is configurable via <code>.toml</code> files on a per-client basis.</li> <li>Extensibility: The architecture makes it easy to add new providers, authentication methods, or resilience strategies in the future with minimal changes to existing code.</li> </ul>"},{"location":"adr/ADR-062-resilient-and-configurable-http-client/#negative","title":"Negative","text":"<ul> <li>Increased Abstraction: The new module introduces several layers of abstraction (<code>Factory</code>, <code>Base</code>, <code>Provider</code>, <code>Orchestrator</code>, <code>Strategy</code>). This adds a learning curve for new developers to understand the complete flow.</li> <li>New Test Dependency: The <code>pytest-httpx</code> library is now a mandatory development dependency for writing tests against any service that uses the new HTTP client.</li> </ul>"},{"location":"adr/ADR-063-dynamic-plugin-system/","title":"ADR-063: Dynamic Plugin System","text":"<p>Status: Proposed</p> <p>Context: The <code>athomic-docs</code> is evolving into a central platform that will serve multiple products and teams. There is a need to extend the core functionality in an isolated and scalable way, without having to modify the main source code for each new product-specific feature.</p> <p>Decision: We will implement a plugin system based on Python's <code>entry_points</code>. The core application (<code>athomic-docs</code>) will define an <code>entry_point</code> group (e.g., <code>\"nala.plugins\"</code>). Plugins will be independent, installable Python packages that register themselves in this <code>entry_point</code>.</p> <p>During initialization, the application will discover and load these plugins dynamically, allowing them to extend system functionality, such as registering new API routes, adding providers, etc.</p> <p>Consequences: - Positive:     - Decoupling: The core remains agnostic to product-specific features.     - Development Scalability: Teams can develop, test, and distribute plugins independently.     - Flexibility: Different instances of <code>athomic-docs</code> can have different sets of features enabled simply by installing the required plugins. - Negative:     - Initial Complexity: Introduces a layer of indirection and requires plugins to follow a clear interface contract.     - Dependency Management: Dependencies of plugins must be managed and compatibility with the core version ensured.</p>"},{"location":"adr/ADR-064-replace-celery-with-taskiq-for-asynchronous-task-processing/","title":"ADR-001: Replace Celery with Taskiq for Asynchronous Task Processing","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-06</li> </ul>"},{"location":"adr/ADR-064-replace-celery-with-taskiq-for-asynchronous-task-processing/#context","title":"Context","text":"<p>Our application\\'s architecture is heavily based on <code>asyncio</code> for high-performance I/O-bound operations. The existing task queue system, Celery, while powerful, was not originally designed for an <code>asyncio</code>-native world. This has led to several challenges: * Asynchronous Code Complexity: Integrating <code>asyncio</code> code with Celery requires careful management of event loops, which can add complexity and potential performance overhead. * Legacy Tooling: Celery\\'s dependency and configuration management feel disconnected from our modern stack, which relies on Pydantic for validation and FastAPI-style dependency injection for clear, testable code. * Verbosity: Setting up and configuring Celery tasks often involves more boilerplate code compared to more modern alternatives.</p> <p>We considered the following alternatives: 1.  Keep Celery: Continue with the current implementation. This was rejected due to the ongoing friction with our <code>asyncio</code>-first architecture and the desire for a more streamlined developer experience. 2.  RQ (Redis Queue): A simpler alternative to Celery. While robust for basic queuing, it lacks the sophisticated <code>asyncio</code> integration and built-in dependency injection system that Taskiq provides. 3.  Taskiq: A modern, <code>asyncio</code>-native task queue library. It is built from the ground up with <code>asyncio</code> support and integrates seamlessly with modern Python tooling like Pydantic and dependency injection.</p> <p>The primary driver for this change is to adopt a task queue system that aligns perfectly with our core architectural principles of being asynchronous, strongly typed, and leveraging dependency injection.</p>"},{"location":"adr/ADR-064-replace-celery-with-taskiq-for-asynchronous-task-processing/#decision","title":"Decision","text":"<p>We will officially adopt Taskiq as the standard library for all asynchronous background task processing.</p> <p>This transition will be implemented by: 1.  Integrating a new <code>TaskiqTaskBroker</code> provider into our \"Athomic\" infrastructure layer. This provider is configurable, allowing developers to select the underlying Redis transport mechanism (e.g., <code>RedisStreamBroker</code> or <code>ListQueueBroker</code>) via application settings. 2.  Writing all new asynchronous tasks using the Taskiq framework. 3.  Gradually migrating existing Celery tasks to Taskiq as modules are updated or refactored. Both systems may coexist during a transitional period.</p>"},{"location":"adr/ADR-064-replace-celery-with-taskiq-for-asynchronous-task-processing/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Native <code>asyncio</code> Integration: Code for tasks will be cleaner, more performant for I/O-bound operations, and will eliminate the complexity of managing separate event loops.</li> <li>Modern Developer Experience: Taskiq\\'s use of Pydantic for parameter validation  and its FastAPI-style dependency injection system (<code>Depends</code>)  will make tasks easier to write, read, and test.</li> <li>Architectural Consistency: The new provider fits perfectly within our existing <code>TaskBroker</code> abstraction, requiring no changes to the application-level code that dispatches tasks.</li> <li>Increased Flexibility: The implementation allows for easy configuration of different backend brokers (e.g., Streams, Lists) without code changes, adapting to different reliability requirements.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Migration Effort: There is a non-trivial effort required to rewrite all existing Celery tasks into the Taskiq format. This will need to be planned and executed over time.</li> <li>Smaller Ecosystem: Celery has a very mature and large ecosystem of plugins and a larger community knowledge base. Taskiq, being newer, has a smaller community and fewer third-party integrations. This might require us to build custom solutions for niche problems that Celery plugins might have already solved.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>Team Learning: The development team will need to familiarize themselves with the Taskiq API and best practices. However, given its similarity to FastAPI, the learning curve is expected to be low.</li> <li>Monitoring and Observability: Existing monitoring tools for Celery (like Flower) will need to be replaced. We will leverage Taskiq\\'s built-in Prometheus middleware  to integrate with our existing observability stack.</li> </ul> </li> </ul>"},{"location":"adr/ADR-065-context-drive-timeout-management/","title":"ADR-065: Context-Driven Timeout Management for Services","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-09</li> </ul>"},{"location":"adr/ADR-065-context-drive-timeout-management/#context","title":"Context","text":"<p>The framework\\'s core services, inheriting from <code>BaseService</code>, initially managed their own timeouts through a <code>timeout</code> parameter in lifecycle methods like <code>start()</code>, <code>stop()</code>, and <code>wait_ready()</code>. The implementation within <code>BaseService</code> would use <code>asyncio.wait_for</code> to enforce this limit.</p> <p>This pattern presented several architectural challenges: 1.  Lack of Overall Request Budget: Each service call had its own isolated timeout. A sequence of calls (e.g., connect to DB, call external API, publish to Kafka) could cumulatively exceed the total time a user or system was willing to wait, leading to wasted server resources on requests that would ultimately time out at a higher level (e.g., at the load balancer). 2.  Coupled Concerns: The service methods were responsible for both their primary function (e.g., starting) and for managing their own timeout logic. This violates the Single Responsibility Principle. The responsibility for how long to wait should belong to the caller (orchestrator), not the callee. 3.  Inconsistent Application: While core services had this parameter, business logic functions and other I/O operations required a separate <code>@timeout</code> decorator, leading to two different ways of handling timeouts.</p>"},{"location":"adr/ADR-065-context-drive-timeout-management/#decision","title":"Decision","text":"<p>We decided to refactor the timeout mechanism to be caller-driven and context-aware, removing the <code>timeout</code> parameter from all service method signatures.</p> <ol> <li> <p>Caller-Driven Timeouts: The responsibility of enforcing a timeout is moved to the component that calls the service method. For the application\\'s main lifecycle, the <code>LifecycleManager</code> now wraps each <code>service.start()</code> and <code>service.stop()</code> call within an <code>async with asyncio.timeout()</code> block.</p> </li> <li> <p>Context-Aware Timeouts for Operations: For granular control within a request\\'s lifecycle, we will leverage <code>context_vars</code>.</p> <ul> <li>A <code>timeout_deadline</code> is set in the context at the beginning of a request, typically in a middleware. This deadline represents the absolute time when the entire operation must be completed.</li> <li>A new decorator, <code>@cancellable_operation</code>, was created. It reads the <code>deadline</code> from the context, calculates the remaining time, and applies it as a dynamic timeout to the decorated function using <code>asyncio.wait_for</code>.</li> <li>If the deadline has already passed when the function is called, it raises a <code>CancelledException</code> immediately (fail-fast).</li> </ul> </li> </ol> <p>This unified approach is applied to all potentially long-running, asynchronous I/O operations, such as database queries, external API calls, and interactions with infrastructure like Vault.</p>"},{"location":"adr/ADR-065-context-drive-timeout-management/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Centralized Timeout Control: The overall timeout policy is now managed by orchestrators (like the <code>LifecycleManager</code> or API middleware) instead of being scattered across dozens of service implementations.</li> <li>Resource Savings (Fail-Fast): The system now respects the entire request\\'s \"timeout budget\". Operations late in a call chain will fail immediately if the budget is exceeded, preventing wasted CPU and I/O on doomed requests.</li> <li>Cleaner Code and API: Service method signatures are simpler (<code>start()</code> instead of <code>start(timeout=...)</code>). The internal logic of <code>BaseService</code> is also simplified, as it no longer needs to catch <code>asyncio.TimeoutError</code> itself.</li> <li>Consistency: We now have a single, consistent pattern for handling timeouts on any <code>async</code> operation, whether it\\'s a service lifecycle method or a business logic function.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Caller Responsibility: Developers must remember to wrap service calls in a timeout context where appropriate. This is mitigated by centralizing most lifecycle calls within the already-refactored <code>LifecycleManager</code>.</li> <li>Test Refactoring: All unit and integration tests that previously passed a <code>timeout</code> parameter to service methods had to be updated to either remove the parameter or wrap the call in <code>asyncio.wait_for</code> to test the timeout behavior correctly.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>The framework now relies more heavily on standard <code>asyncio</code> patterns (<code>asyncio.timeout</code>). This increases consistency but requires developers working on the core framework to be familiar with them.</li> </ul> </li> </ul>"},{"location":"adr/ADR-066-multi-backend-task-scheduling-system/","title":"ADR-066: Multi-Backend Task Scheduling System","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-13</li> </ul>"},{"location":"adr/ADR-066-multi-backend-task-scheduling-system/#context","title":"Context","text":"<p>The application required a reliable and flexible system to execute tasks at a future time or after a specific delay. This is essential for features like sending follow-up notifications, generating periodic reports, and handling other asynchronous time-based operations. The initial challenge was to design a system that was not tightly coupled to a single technology (like Redis or a specific message broker), allowing for future extensibility without refactoring the application code that schedules the tasks. Alternatives considered were implementing scheduling logic directly within specific services, which would lead to code duplication, or relying on a single third-party library, which would limit future architectural choices.</p>"},{"location":"adr/ADR-066-multi-backend-task-scheduling-system/#decision","title":"Decision","text":"<p>We decided to create a new, abstracted <code>scheduler</code> module within the <code>athomic</code> framework. The architecture is built on these key principles:</p> <ol> <li>SchedulerProtocol: A clear interface defining the core capabilities (<code>schedule</code>, <code>schedule_at</code>, <code>cancel</code>).</li> <li>Factory and Creator Patterns: A <code>SchedulerFactory</code> is used to create the scheduler instance. It delegates the actual instantiation logic to dedicated <code>Creator</code> classes (<code>KVSchedulerCreator</code>, <code>DramatiqSchedulerCreator</code>) found in a <code>scheduler_creator_registry</code>. This adheres to the Open/Closed Principle, as adding new providers only requires adding a new Creator, not modifying the factory.</li> <li>Multi-Backend Support: Two initial backend providers were implemented:<ul> <li><code>KVSchedulerProvider</code>: A custom solution using a KVStore (Redis) with Sorted Sets to persist tasks. This provides fine-grained control and visibility.</li> <li><code>DramatiqProvider</code>: An adapter for the <code>dramatiq</code> library, leveraging its robust message broker capabilities.</li> </ul> </li> <li>Dedicated Worker: A <code>SchedulerWorkerService</code> was created to poll the KVStore for due tasks scheduled by the <code>KVSchedulerProvider</code> and dispatch them to the application's main task broker.</li> </ol>"},{"location":"adr/ADR-066-multi-backend-task-scheduling-system/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>The system is highly extensible and decoupled. New backend providers (e.g., Celery, RQ) can be added with minimal friction by implementing the protocol and creating a corresponding creator class.</li> <li>The application's business logic is completely isolated from the scheduling implementation. A service simply calls <code>scheduler.schedule(...)</code> without needing to know which backend is active.</li> <li>The <code>custom_kv</code> provider offers excellent control and observability over the scheduled task queue.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>The initial architectural setup is more complex than a single-library solution due to the number of components involved (Protocol, Base, Providers, Worker, Factory, Creators, Registry).</li> <li>The custom cancellation logic for <code>dramatiq</code> (using a denylist in Redis) introduces a dependency on a KV store even when using a broker-based scheduler.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This decision establishes a clear pattern for creating other multi-backend systems within the <code>athomic</code> framework in the future.</li> </ul> </li> </ul>"},{"location":"adr/ADR-067-generic-resilience-module-for-exponential-backoff/","title":"ADR-067: Generic Resilience Module for Exponential Backoff","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-13</li> </ul>"},{"location":"adr/ADR-067-generic-resilience-module-for-exponential-backoff/#context","title":"Context","text":"<p>Background services that operate on a polling mechanism, such as the <code>SchedulerWorkerService</code>, can create unnecessary load on their dependencies (e.g., databases, KV stores) when they are idle. A fixed-interval polling strategy is inefficient, leading to wasted CPU cycles and network traffic. The logic for a smarter, adaptive polling interval (exponential backoff) was needed, but implementing it directly within the worker would lead to code duplication if other polling services were created in the future.</p>"},{"location":"adr/ADR-067-generic-resilience-module-for-exponential-backoff/#decision","title":"Decision","text":"<p>Instead of embedding the backoff logic within a single service, we decided to create a generic, reusable component. A new <code>backoff</code> submodule was created within the existing <code>athomic/resilience</code> package, elevating it to a first-class resilience pattern alongside <code>Retry</code> and <code>Circuit Breaker</code>.</p> <p>The implementation consists of: 1.  <code>BackoffSettings</code>: A Pydantic schema allowing for the configuration of multiple, named backoff policies directly in <code>settings.toml</code> files. 2.  <code>BackoffPolicy</code>: A simple data class holding the parameters for a single backoff strategy (min/max delay, factor). 3.  <code>BackoffHandler</code>: A stateful class that manages the backoff logic. It provides <code>wait()</code> and <code>reset()</code> methods, encapsulating the complexity of calculating the sleep interval. It also includes an <code>on_error</code> callback hook for enhanced observability. 4.  <code>BackoffFactory</code>: Creates configured instances of the <code>BackoffHandler</code> based on the application's settings.</p>"},{"location":"adr/ADR-067-generic-resilience-module-for-exponential-backoff/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>The backoff logic is now a standardized and reusable component available to any part of the application, promoting DRY (Don't Repeat Yourself) principles.</li> <li>The code for services using this component, like <code>SchedulerWorkerService</code>, is significantly cleaner and more focused on its primary responsibility.</li> <li>Polling behavior is now highly configurable through settings files without requiring code changes, allowing for fine-tuning in different environments.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Introduces a minor layer of abstraction that developers need to be aware of when building new polling services.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This solidifies the design pattern for resilience modules within the <code>athomic</code> framework, making future additions (like Hedging) more predictable to implement.</li> </ul> </li> </ul>"},{"location":"adr/ADR-069-refactoring-base-consumer-with-the-strategy-pattern-for-message-processing/","title":"ADR-069: Refactoring BaseConsumer with the Strategy Pattern for Message Processing","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-13</li> </ul>"},{"location":"adr/ADR-069-refactoring-base-consumer-with-the-strategy-pattern-for-message-processing/#context","title":"Context","text":"<p>The <code>BaseConsumer</code> class was responsible for a wide range of tasks: managing the connection to the message broker, running the consumption loop, deserializing messages, checking for idempotency, executing the business logic callback, and handling failures (retry/DLQ). This concentration of responsibilities made the class complex, difficult to unit test in isolation, and violated the Single Responsibility Principle (SRP). Furthermore, the logic to handle different types of messages (e.g., messages needing a delay versus immediate processing) was poised to become a series of conditional <code>if/else</code> statements within the consumer, making it hard to extend in the future.</p>"},{"location":"adr/ADR-069-refactoring-base-consumer-with-the-strategy-pattern-for-message-processing/#decision","title":"Decision","text":"<p>We decided to refactor the consumer architecture by introducing two new patterns:</p> <ol> <li> <p><code>MessageProcessor</code> Class: A new class, <code>MessageProcessor</code>, will be created to encapsulate the entire processing pipeline for a single message. The <code>BaseConsumer</code>'s responsibility will be reduced to only managing the broker connection and fetching raw messages, delegating all processing to an injected <code>MessageProcessor</code> instance.</p> </li> <li> <p>Strategy Pattern: Inside the <code>MessageProcessor</code>, the decision of how to execute the business logic will be delegated to a <code>ConsumerStrategyProtocol</code>. We will provide two initial concrete implementations:</p> <ul> <li><code>ImmediateExecutionStrategy</code>: Calls the business logic callback directly.</li> <li><code>DelayedExecutionStrategy</code>: Does not call the callback, but instead uses the <code>Scheduler</code> service to schedule the message for later processing.</li> </ul> </li> </ol> <p>The <code>MessageProcessor</code> will contain a resolver method that selects the appropriate strategy based on message attributes (e.g., the presence of a <code>x-delay-seconds</code> header).</p>"},{"location":"adr/ADR-069-refactoring-base-consumer-with-the-strategy-pattern-for-message-processing/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Improved Single Responsibility Principle (SRP): The <code>BaseConsumer</code> is now solely responsible for consumption, and the <code>MessageProcessor</code> is solely responsible for processing. The different execution strategies are also isolated in their own classes.</li> <li>Enhanced Testability: The entire message processing pipeline can be unit tested via the <code>MessageProcessor</code> without needing a live message broker connection. Each strategy can also be tested independently.</li> <li>Extensibility (Open/Closed Principle): The system is now open for extension. New ways of handling messages (e.g., batching, priority queuing) can be added by creating new strategy classes without modifying the <code>MessageProcessor</code> or <code>BaseConsumer</code>.</li> <li>Increased Clarity: The flow of a message through the system is now more explicit and easier to follow.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Complexity: The refactoring introduces more classes and files (<code>MessageProcessor</code>, <code>ConsumerStrategyProtocol</code>, two strategy providers) compared to a single monolithic <code>BaseConsumer</code>. This adds a level of indirection that new developers will need to learn.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This decision solidifies the use of Dependency Injection within the messaging module, as the <code>MessageProcessor</code> and its dependencies are now constructed by the <code>ConsumerFactory</code>.</li> </ul> </li> </ul>"},{"location":"adr/ADR-070-persistent-message-delay-strategy-using-the-scheduler/","title":"ADR-070: Persistent Message Delay Strategy using the Scheduler","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-13</li> </ul>"},{"location":"adr/ADR-070-persistent-message-delay-strategy-using-the-scheduler/#context","title":"Context","text":"<p>The existing message delay mechanism (<code>KafkaDelayTopicStrategy</code>) relied on a background service (<code>DelayedMessageRePublisher</code>) that consumed from dedicated delay topics and used an in-memory <code>asyncio.sleep</code>. This approach had a significant drawback: if the service was restarted while \"sleeping,\" the message's delay state would be lost. The message would only be reprocessed after the Kafka consumer offset expired, leading to unpredictable and potentially long delays, which is not acceptable for a resilient framework. We needed a delay mechanism that was persistent and could survive application restarts.</p>"},{"location":"adr/ADR-070-persistent-message-delay-strategy-using-the-scheduler/#decision","title":"Decision","text":"<p>We decided to implement a new, persistent delay strategy that leverages existing <code>athomic</code> components: the <code>Scheduler</code> (<code>control.scheduler</code>) and the <code>Task Broker</code> (<code>integration.tasks</code>).</p> <ol> <li> <p><code>SchedulerDelayStrategy</code>: A new provider for the <code>DelayStrategyProtocol</code> was created. When a producer calls <code>publish</code> with <code>delay_seconds</code>, this strategy does not send the message to a different Kafka topic. Instead, it uses the <code>SchedulerFactory</code> to schedule a task for future execution.</p> </li> <li> <p>Generic Republish Task: A new, generic, framework-level background task (<code>generic_republish_message_task</code>) was created within the messaging module. The <code>Scheduler</code> schedules this task.</p> </li> <li> <p>Flow:</p> <ul> <li>A producer requests a delay.</li> <li>The <code>SchedulerDelayStrategy</code> creates a <code>GenericTask</code> containing all the original message details (payload, topic, headers) and tells the <code>Scheduler</code> to execute it in <code>N</code> seconds.</li> <li>The <code>Scheduler</code> persists this request (e.g., in a Redis Sorted Set).</li> <li>When the time comes, the <code>SchedulerWorkerService</code> picks up the request and enqueues the <code>generic_republish_message_task</code> into the <code>Task Broker</code> (Taskiq).</li> <li>A <code>Taskiq</code> worker executes the task, which uses the <code>ProducerFactory</code> to publish the original message to its final topic.</li> </ul> </li> </ol> <p>This entire mechanism is made available to applications by simply changing a configuration key: <code>NALA.INTEGRATION.MESSAGING.REPUBLISHER.delay_strategy_backend = \"scheduler\"</code>.</p>"},{"location":"adr/ADR-070-persistent-message-delay-strategy-using-the-scheduler/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Resilience and Persistence: The delay mechanism is now fully persistent. If the application restarts at any point, the scheduled task remains safely in Redis and will be processed when the system is back online.</li> <li>Reusability and Abstraction: The solution is highly reusable and well-abstracted. It leverages existing robust components (<code>Scheduler</code>, <code>Task Broker</code>) and is exposed to the user via the same <code>DelayStrategyProtocol</code>, requiring only a configuration change to activate.</li> <li>Zero Boilerplate: Applications that use <code>athomic</code> can get persistent delays without writing any new logic; they just need to configure and use it.</li> <li>Backend Agnostic: The solution is agnostic to the messaging backend, as the final step uses the abstract <code>ProducerFactory</code>.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Dependencies: This strategy introduces a dependency on a running <code>SchedulerWorkerService</code> (within the main app) and a separate <code>Taskiq</code> worker process, in addition to the application itself. This increases operational complexity compared to the simpler (but less reliable) in-memory approach.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This solidifies the <code>Scheduler</code> and <code>Task Broker</code> as core, interconnected components of the <code>athomic</code> framework.</li> </ul> </li> </ul>"},{"location":"adr/ADR-071-dedicated-service-for-dead-letter-queue-processing/","title":"ADR-071: Dedicated Service for Dead Letter Queue Processing","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-16</li> </ul>"},{"location":"adr/ADR-071-dedicated-service-for-dead-letter-queue-processing/#context","title":"Context","text":"<p>The existing Dead Letter Queue (DLQ) mechanism was resilient in routing failed messages, but the logic for processing those messages was not well-defined. Initial attempts to integrate a DLQ consumer into the standard <code>MessagingLifecycleManager</code> created several architectural issues. This approach tightly coupled application consumer logic with infrastructure-level failure processing, violating the Single Responsibility Principle. It made the DLQ consumer difficult to configure, disable, or test in isolation without instantiating the entire application's consumer stack. Furthermore, the logic for handling a terminally failed message (e.g., logging, alerting, archiving) was not flexible or easily extensible. We needed a robust, decoupled, and configurable architecture for handling messages that have permanently failed.</p>"},{"location":"adr/ADR-071-dedicated-service-for-dead-letter-queue-processing/#decision","title":"Decision","text":"<p>We decided to refactor the DLQ processing into a dedicated, standalone background service named <code>DeadLetterProcessorService</code>, which inherits from our <code>BaseService</code> class.</p> <p>This service is not auto-discovered by the <code>MessagingLifecycleManager</code>. Instead, it is explicitly instantiated and started at the application's entry point, running in parallel with other core infrastructure services.</p> <p>To ensure flexibility, the internal logic of the service implements the Strategy Pattern: 1.  A <code>DeadLetterStrategyProtocol</code> defines the interface for handling a failed message. 2.  Concrete implementations like <code>LoggingOnlyStrategy</code> and <code>RepublishToParkingLotStrategy</code> provide specific behaviors. 3.  A <code>DeadLetterStrategyFactory</code> reads the application's configuration from the settings files and injects the appropriate strategy into the <code>DeadLetterProcessorService</code> upon its creation.</p> <p>This design completely decouples the act of consuming from the DLQ (the service's responsibility) from the logic of what to do with the failed message (the strategy's responsibility).</p>"},{"location":"adr/ADR-071-dedicated-service-for-dead-letter-queue-processing/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>High Decoupling: The <code>MessagingLifecycleManager</code> is now solely responsible for application consumers. The DLQ processing logic is fully encapsulated within its own dedicated service and can be maintained independently.</li> <li>Improved Testability: The <code>DeadLetterProcessorService</code> can be tested as an integrated component, while individual strategies can be unit-tested in isolation.</li> <li>Extensible by Design: Adding new failure-handling behaviors (e.g., creating a Jira ticket, archiving to S3) is now as simple as creating a new strategy class and registering it, requiring no changes to the core service.</li> <li>Configuration-Driven Behavior: The entire DLQ handling process can be altered via settings files (e.g., switching from logging to a parking-lot topic) without a code deployment.</li> <li>Architectural Clarity: The application's startup sequence now explicitly shows all running infrastructure services, making the system's topology easier to understand.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Component Count: This solution introduces several new classes (a service, a factory, a registry, multiple strategies). This is a deliberate trade-off for a much cleaner and more maintainable design.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>The application's top-level startup logic is now explicitly responsible for initializing and managing the lifecycle of one additional service.</li> </ul> </li> </ul>"},{"location":"adr/ADR-072-unify-delayed-message-handling-with-standard-consumer-architecture/","title":"ADR-072: Unify Delayed Message Handling with Standard Consumer Architecture","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-16</li> </ul>"},{"location":"adr/ADR-072-unify-delayed-message-handling-with-standard-consumer-architecture/#context","title":"Context","text":"<p>The current delayed messaging mechanism is implemented via the <code>KafkaDelayTopicStrategy</code>, which publishes messages to dedicated delay topics (e.g., <code>__nala_delay_5m</code>). A custom background service, <code>DelayedMessageRepublisher</code>, consumes from these topics.</p> <p>This service implements its own consumption loop, manually handles deserialization, calculates any remaining delay with <code>asyncio.sleep</code>, and then re-serializes and re-publishes the message to its final destination.</p> <p>This implementation has several architectural drawbacks: 1.  Lack of Resilience: If the final republish action fails, the service logs an exception, but the message is effectively lost. It does not benefit from the robust retry and DLQ mechanisms available to application consumers. 2.  Inconsistent Observability: The service bypasses the standard <code>MessageProcessor</code>, meaning it lacks the automatic distributed tracing, metrics, and structured logging that are standard for all other consumers. 3.  Code Duplication: It contains bespoke logic for creating a producer, and for serializing/deserializing messages, which is already managed by our core messaging factories and processors.</p>"},{"location":"adr/ADR-072-unify-delayed-message-handling-with-standard-consumer-architecture/#decision","title":"Decision","text":"<p>We have decided to deprecate the custom <code>DelayedMessageRepublisher</code> service and replace it with a standard messaging consumer that leverages the existing <code>MessageProcessor</code>.</p> <p>The new implementation is as follows: 1.  A new, callable class, named <code>DelayedMessageHandler</code>, was created. Its sole responsibility is to extract the original message payload and metadata from the delay envelope and use a standard, injected <code>MessagingProducer</code> to republish it to its final destination topic. Using a class improves testability through dependency injection. 2.  A new <code>setup</code> function (<code>register_delay_republisher_from_settings</code>) was created. At application startup, this function reads the configured delay topics from the settings and programmatically registers a single, shared infrastructure consumer using the <code>consumer_registry</code>. 3.  This consumer is configured to listen to all delay topics and uses an instance of the <code>DelayedMessageHandler</code> as its callback.</p> <p>This change reframes the republisher from a special-purpose worker into a standard, albeit infrastructure-level, consumer.</p>"},{"location":"adr/ADR-072-unify-delayed-message-handling-with-standard-consumer-architecture/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Massive Resilience Improvement: Delayed message handling will now fully benefit from the <code>DLQHandler</code>. If a message fails to be republished, the action will be retried, and if it continues to fail, the message will be safely routed to the DLQ instead of being lost.</li> <li>Standardized Observability: The entire process is automatically instrumented with the same high-quality tracing, metrics, and logs as any other application consumer.</li> <li>Code Simplification &amp; Consistency: The complex, custom <code>_run_loop</code> of the <code>DelayedMessageRepublisher</code> is eliminated and replaced by a small, clean, and testable handler class. This reduces code duplication and aligns all message consumption to a single architectural pattern.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Slight Overhead: Introducing the <code>MessageProcessor</code> adds a few layers of abstraction. This overhead is negligible and a worthwhile trade-off for the gains in resilience and observability.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This decision solidifies the pattern of using explicitly registered, infrastructure-level consumers for background messaging tasks, creating a clear distinction from auto-discovered application consumers.</li> </ul> </li> </ul>"},{"location":"adr/ADR-073-decouple-consumption-logic-via-strategy-pattern/","title":"ADR-074: Introduce Batch Message Processing for High-Throughput Consumers","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-16</li> </ul>"},{"location":"adr/ADR-073-decouple-consumption-logic-via-strategy-pattern/#context","title":"Context","text":"<p>The current consumer architecture processes one message at a time. The <code>KafkaConsumer</code> iterates over messages using <code>async for msg in self._consumer</code>, passing each individual message to the <code>MessageProcessor</code>. While robust for many use cases, this one-by-one approach can become a performance bottleneck for high-volume topics where throughput is critical. The overhead of the processing pipeline (deserialization, idempotency checks, context creation) for each single message can limit the overall consumption rate. We need a mechanism to process messages in batches to maximize efficiency in these scenarios, enabling use cases like bulk database inserts.</p>"},{"location":"adr/ADR-073-decouple-consumption-logic-via-strategy-pattern/#decision","title":"Decision","text":"<p>We will introduce an optional batch processing mode for consumers. This will be an opt-in feature, configured directly at the consumer level, ensuring that existing consumers continue to function without any changes.</p> <p>The implementation will involve the following key changes:</p> <ol> <li> <p>Decorator Enhancement: The <code>@subscribe_to</code> decorator will be enhanced with a new optional parameter: <code>batch_size: int</code>. If this parameter is provided, the consumer will be configured to run in batch mode.</p> </li> <li> <p>Configuration Update: The <code>ConsumerHandlerConfig</code> will be updated to store the <code>batch_size</code> and a new <code>batch_mode</code> flag.</p> </li> <li> <p>Consumer Logic Modification (Strategy Pattern):</p> <ul> <li>The consumption logic will be extracted from the <code>KafkaConsumer</code> into dedicated strategy classes.</li> <li>A <code>SingleMessageConsumptionStrategy</code> will encapsulate the current <code>async for</code> loop.</li> <li>A new <code>BatchMessageConsumptionStrategy</code> will use <code>await self._consumer.getmany(max_records=batch_size)</code> to fetch batches of messages.</li> <li>The <code>ConsumerFactory</code> will be responsible for choosing and injecting the correct strategy into the consumer instance based on the <code>handler_config</code>.</li> </ul> </li> <li> <p>New Processing Path: A new method, <code>process_batch</code>, will be added to the <code>MessageProcessor</code>. This method will receive the list of raw messages, deserialize them, and then pass the entire list of deserialized messages to the user's callback.</p> </li> <li> <p>Resilience for Batches: Error handling for batches will be \"all-or-nothing\". If the user's callback fails for the entire batch (by returning <code>ProcessingOutcome.RETRY</code> or raising an exception), the entire batch of original messages will be sent to the <code>DLQHandler</code> for the standard retry/DLQ process.</p> </li> <li> <p>Callback Contract Evolution: The contract for batch-enabled handlers will change. They will be expected to receive a <code>List</code> of messages, e.g., <code>async def my_batch_handler(messages: List[MyModel], ...)</code>.</p> </li> </ol>"},{"location":"adr/ADR-073-decouple-consumption-logic-via-strategy-pattern/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Significant Performance Gains: For high-throughput scenarios, batch processing can dramatically increase the number of messages processed per second by reducing per-message overhead.</li> <li>Opt-In and Backwards Compatible: The change is entirely opt-in via a new decorator argument. Existing consumers are unaffected.</li> <li>Enables New Use Cases: Allows for efficient implementation of patterns like batch database inserts or bulk API calls triggered by messages.</li> <li>Clean Architecture: Using the Strategy Pattern for consumption logic keeps the consumer providers (like <code>KafkaConsumer</code>) clean and adheres to the Open/Closed Principle.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Atomic Batch Error Handling: The initial implementation will treat the batch as an atomic unit. A failure in processing a single message will cause the entire batch to be retried. More granular error handling is a possible future enhancement but adds significant complexity.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This introduces a second processing path within the <code>MessageProcessor</code> and a new abstraction layer (Consumption Strategies), which will need to be maintained. However, the logic is self-contained and activated by a clear configuration flag.</li> </ul> </li> </ul>"},{"location":"adr/ADR-074-refine-messaging-consumer-responsibilities-between-strategy-and-rocessor/","title":"ADR-074: Refine Messaging Consumer Responsibilities between Strategy and Processor","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-20</li> </ul>"},{"location":"adr/ADR-074-refine-messaging-consumer-responsibilities-between-strategy-and-rocessor/#context","title":"Context","text":"<p>During a recent refactoring, the <code>MessageProcessor</code> was simplified to handle only single messages. The goal was to delegate batch iteration logic to the newly created <code>BatchMessageConsumptionStrategy</code>. However, this led to two critical issues: 1.  Incorrect Callback Invocation: The <code>MessageProcessor</code>'s <code>process</code> method was designed to call single-message handlers (expecting <code>message</code>, <code>message_key</code>, etc.). When the <code>BatchMessageConsumptionStrategy</code> iterated and called <code>process</code>, it incorrectly attempted to pass single-message arguments to a batch handler (which expects a single <code>messages: List</code> argument), causing a <code>TypeError</code>. 2.  Logic Duplication &amp; Unclear Responsibility: The <code>BatchMessageConsumptionStrategy</code> had to contain logic for iterating over a batch and calling the processor repeatedly. This blurred its primary responsibility, which should be exclusively about how messages are fetched and grouped from the broker, not how they are processed.</p> <p>The core problem was that the <code>MessageProcessor</code> had lost its capability to act as a cohesive \"Unit of Work\" processor for batches, violating the Single Responsibility Principle (SRP) at a higher level.</p>"},{"location":"adr/ADR-074-refine-messaging-consumer-responsibilities-between-strategy-and-rocessor/#decision","title":"Decision","text":"<p>We have decided to re-introduce a dedicated batch processing method to the <code>MessageProcessor</code> and clarify the responsibilities between the components:</p> <ol> <li> <p><code>MessageProcessor</code> Responsibilities:</p> <ul> <li>It will now have two distinct public methods: <code>process()</code> for single messages and <code>process_batch()</code> for lists of messages.</li> <li><code>process_batch()</code> is responsible for the entire batch lifecycle: iterating the raw messages, deserializing each one, handling any deserialization failures for the entire batch as a single transaction, and, if all successful, invoking the user's batch callback once with the complete list of deserialized objects. It also centralizes error handling (DLQ) for the entire batch.</li> </ul> </li> <li> <p><code>ConsumptionStrategy</code> Responsibilities:</p> <ul> <li>Its sole responsibility is to fetch messages from the broker and group them.</li> <li>The <code>SingleMessageConsumptionStrategy</code> fetches one message and delegates it to <code>processor.process()</code>.</li> <li>The <code>BatchMessageConsumptionStrategy</code> accumulates a batch and delegates the entire raw batch to <code>processor.process_batch()</code>. It does not iterate or process individual messages.</li> </ul> </li> </ol> <p>This decision restores the <code>MessageProcessor</code>'s role as the central \"guardian\" that prepares data securely and resiliently before handing it off to the business logic, whether as a single item or a complete batch.</p>"},{"location":"adr/ADR-074-refine-messaging-consumer-responsibilities-between-strategy-and-rocessor/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Corrects Batch Functionality: Fixes the <code>TypeError</code> and <code>TimeoutError</code> in batch processing tests by ensuring the correct handler signature is used.</li> <li>Strengthens SRP: The <code>ConsumptionStrategy</code> is now purely concerned with I/O and message grouping (the \"how\"), while the <code>MessageProcessor</code> is concerned with the processing pipeline of a unit of work (the \"what\").</li> <li>Centralized Batch Error Handling: Logic for handling a failure of a single message within a batch (which should fail the whole batch) is now correctly centralized in the <code>MessageProcessor</code>, rather than being scattered.</li> <li>Cleaner Business Logic: The user's decorated handler remains simple, receiving either a single Pydantic object or a clean <code>List[PydanticObject]</code>, without any knowledge of the underlying consumption strategy.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased <code>MessageProcessor</code> Complexity: The <code>MessageProcessor</code> is slightly larger as it now contains logic for two distinct processing paths (single vs. batch). However, this is considered a necessary trade-off for creating a clearer and more robust abstraction.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This reinforces the architectural pattern of separating I/O-bound strategies from CPU-bound (or logic-bound) processing pipelines.</li> </ul> </li> </ul>"},{"location":"adr/ADR-075-introduce-messaging-orchestrator-for-separation-of-concerns/","title":"ADR-075: Introduce Messaging Orchestrator for Clearer Separation of Concerns","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-21</li> </ul>"},{"location":"adr/ADR-075-introduce-messaging-orchestrator-for-separation-of-concerns/#context","title":"Context","text":"<p>The <code>MessagingLifecycleManager</code> was previously responsible for both building the entire messaging stack and managing its runtime lifecycle (<code>start</code>/<code>stop</code>). This included discovering decorated consumers, calling various factories (<code>ConsumerFactory</code>, <code>ProducerFactory</code>, <code>DLQHandlerFactory</code>), creating shared dependencies, and assembling the final consumer objects.</p> <p>This approach violated the Single Responsibility Principle (SRP), making the <code>LifecycleManager</code> overly complex and tightly coupled to the implementation details of its managed components. Its responsibility was diluted from being a pure lifecycle operator to also being a master builder.</p>"},{"location":"adr/ADR-075-introduce-messaging-orchestrator-for-separation-of-concerns/#decision","title":"Decision","text":"<p>We decided to introduce a new class, the <code>MessagingOrchestrator</code>, to cleanly separate these responsibilities.</p> <ol> <li> <p><code>MessagingOrchestrator</code>: This class is now solely responsible for the construction of the entire messaging stack. It creates shared dependencies like the <code>Producer</code> for the DLQ, discovers all consumer handlers, and uses the appropriate factories (<code>ConsumerFactory</code>, <code>DLQHandlerFactory</code>, etc.) to build a complete list of ready-to-use consumer services.</p> </li> <li> <p><code>MessagingLifecycleManager</code>: This class has been simplified to be a pure lifecycle operator. It now receives a pre-configured <code>MessagingOrchestrator</code> instance during its initialization. Its only responsibilities are to get the list of fully-built services from the orchestrator and then call <code>start()</code> on them during application startup and <code>stop()</code> during shutdown.</p> </li> </ol>"},{"location":"adr/ADR-075-introduce-messaging-orchestrator-for-separation-of-concerns/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Strong SRP Adherence: The responsibilities are now atomic and clear: the <code>Orchestrator</code> builds, and the <code>LifecycleManager</code> runs.</li> <li>Improved Decoupling: The <code>LifecycleManager</code> no longer has direct dependencies on <code>ConsumerFactory</code>, <code>ProducerFactory</code>, or other builder components. It only depends on the <code>Orchestrator</code> abstraction.</li> <li>Enhanced Testability: Both components can be unit-tested in isolation more effectively. The <code>LifecycleManager</code> can be tested with a mock orchestrator that returns a simple list of mock services.</li> <li>Centralized Construction Logic: All logic for assembling the messaging components is now located in a single, dedicated place, making it easier to understand and maintain.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Indirection: This change adds one more layer of abstraction (<code>Orchestrator</code>) to the messaging startup flow. This can slightly increase the cognitive load for developers new to this part of the codebase.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>The application's main startup sequence (e.g., in <code>register_services.py</code>) is now responsible for the two-step process of first creating the <code>Orchestrator</code> and then injecting it into the <code>MessagingLifecycleManager</code>.</li> </ul> </li> </ul>"},{"location":"adr/ADR-076-use-adapter-pattern-for-consistent-failure-handling/","title":"ADR-076: Use Adapter Pattern (FailureContext) for Consistent Failure Handling","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-21</li> </ul>"},{"location":"adr/ADR-076-use-adapter-pattern-for-consistent-failure-handling/#context","title":"Context","text":"<p>The <code>DLQHandler.handle_failure</code> method previously accepted a long list of individual parameters (<code>original_message</code>, <code>original_key</code>, <code>original_headers</code>, etc.). A significant problem was that the structure of these parameters, particularly <code>original_headers</code>, was different for single-message failures (<code>List[Tuple]</code>) versus batch failures (<code>List[List[Tuple]]</code>).</p> <p>This forced the <code>DLQHandler</code> to contain conditional logic (<code>if isinstance...</code>) to inspect and determine the shape of the data it received. This made the handler's logic complex, brittle, and violated the principle of a clean, predictable interface.</p>"},{"location":"adr/ADR-076-use-adapter-pattern-for-consistent-failure-handling/#decision","title":"Decision","text":"<p>To resolve this, we implemented the Adapter Pattern by creating a dedicated data class, <code>FailureContext</code>.</p> <ol> <li> <p><code>FailureContext</code> Class: This <code>dataclass</code> encapsulates all information about a failure event into a single, consistent structure. It includes fields for messages, keys, headers, the exception, the topic, and an explicit <code>consumption_type</code> enum (<code>SINGLE</code> or <code>BATCH</code>).</p> </li> <li> <p><code>MessageProcessor</code> as the Adapter Creator: The responsibility of creating the <code>FailureContext</code> object was given to the <code>MessageProcessor</code>. It uses named factory methods (<code>FailureContext.for_single_message</code> and <code>FailureContext.for_batch</code>) to adapt the raw failure data into this consistent object. For single messages, it wraps the message, key, and headers in lists to match the structure of a batch.</p> </li> <li> <p>Simplified <code>DLQHandler</code>: The <code>DLQHandler.handle_failure</code> signature was simplified to accept only one argument: <code>context: FailureContext</code>. It can now operate on this object with full confidence in its data structure, without needing any conditional type or shape checking.</p> </li> </ol>"},{"location":"adr/ADR-076-use-adapter-pattern-for-consistent-failure-handling/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Simplified <code>DLQHandler</code>: All conditional logic for parsing different data shapes was removed from the handler, making it dramatically simpler and more robust. The code is now declarative instead of interrogative.</li> <li>Clear and Explicit Contract: The <code>FailureContext</code> provides a strong, predictable contract between the <code>MessageProcessor</code> and <code>DLQHandler</code>. The intent of the failure is explicit via <code>consumption_type</code>.</li> <li>Improved Encapsulation and SRP: The responsibility of adapting data is now correctly placed in the <code>MessageProcessor</code>, while the <code>DLQHandler</code> is only responsible for acting on the pre-adapted, consistent data.</li> <li>Enhanced Extensibility: If more data is needed for failure handling in the future (e.g., consumer metadata), only the <code>FailureContext</code> class needs to be changed, not every method signature in the call chain.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Introduces a New Class: Adds a new data class (<code>FailureContext</code>) and an enum (<code>ConsumptionType</code>) to the codebase. This is a very minor trade-off for the significant gain in clarity and robustness.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This pattern establishes a clear and reusable way to handle complex error information throughout the messaging module.</li> </ul> </li> </ul>"},{"location":"adr/ADR-077-refine-messaging-consumer-responsibilities-between-strategy-and-processor/","title":"ADR-077: Refine Messaging Consumer Responsibilities Between Strategy and Processor","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-21</li> </ul>"},{"location":"adr/ADR-077-refine-messaging-consumer-responsibilities-between-strategy-and-processor/#context","title":"Context","text":"<p>The <code>nala/athomic</code> messaging system requires a robust and extensible way to consume messages from different backends (like Kafka) and using different patterns (single message, batch). The initial consumer logic needed a clearer separation of concerns to enhance testability, adhere to the Single Responsibility Principle (SRP), and allow for future expansion without significant refactoring.</p> <p>Alternatives considered included: 1.  A monolithic consumer class: A single class would handle fetching, batching, deserializing, and processing. This was rejected as it violates SRP, mixes infrastructure concerns with processing logic, and makes isolated unit testing nearly impossible. 2.  Processor handles batching: Placing batch accumulation logic within the <code>MessageProcessor</code>. This was rejected because it would make the processor stateful and responsible for both how messages are acquired and what to do with them, again violating SRP.</p> <p>The primary architectural driver is to create a clean separation between the infrastructure-aware logic of message acquisition and the broker-agnostic logic of message processing.</p>"},{"location":"adr/ADR-077-refine-messaging-consumer-responsibilities-between-strategy-and-processor/#decision","title":"Decision","text":"<p>We will enforce a strict separation of responsibilities between two main components, connected by a protocol:</p> <ol> <li> <p><code>ConsumptionStrategyProtocol</code>: Implementations of this protocol are solely responsible for interacting with the message broker client. Their single responsibility is to fetch messages and group them according to the chosen strategy (e.g., one by one, or accumulating a batch based on size, time, or byte count). The strategy is the \"how\" and \"when\" of message acquisition.</p> </li> <li> <p><code>MessageProcessor</code>: This class is responsible for the entire processing pipeline of a message or a batch of messages delivered to it by a strategy. Its responsibilities include:</p> <ul> <li>Orchestrating telemetry (tracing and metrics).</li> <li>Deserializing message components.</li> <li>Performing idempotency checks.</li> <li>Invoking the specific business logic handler (<code>callback</code>).</li> <li>Orchestrating error handling (forwarding to the <code>DLQHandler</code>).</li> </ul> </li> </ol> <p>The <code>MessageProcessor</code> is completely agnostic of the message broker. The <code>ConsumptionStrategy</code>'s <code>consume</code> method receives the appropriate <code>MessageProcessor</code> at runtime to execute the processing logic.</p>"},{"location":"adr/ADR-077-refine-messaging-consumer-responsibilities-between-strategy-and-processor/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Strong SRP Adherence: The Strategy's concern is message acquisition; the Processor's concern is the processing pipeline. This creates highly cohesive, loosely coupled components.</li> <li>Enhanced Testability: The <code>MessageProcessor</code> can be unit-tested in complete isolation by simply passing it mock message data. The strategies can be tested by mocking only the broker client.</li> <li>Improved Extensibility (Open/Closed Principle): We can introduce new consumption strategies (e.g., for a different broker like RabbitMQ, or a transactional batch strategy) without any changes to the <code>MessageProcessor</code>.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Atomic Batch Error Handling: The initial implementation will treat the batch as an atomic unit. A failure in processing a single message will cause the entire batch to be retried. More granular error handling is a possible future enhancement but adds significant complexity.</li> <li>Increased Abstraction: This pattern introduces an additional layer of abstraction, which can slightly increase the initial cognitive load for developers new to this module. This is a deliberate trade-off for long-term robustness and flexibility.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This design solidifies the pattern where broker-agnostic components (like the Processor) are driven by broker-aware components (like the Strategy).</li> </ul> </li> </ul>"},{"location":"adr/ADR-078-saga-pattern-for-distributed-transactions/","title":"ADR-078: Saga Pattern for Distributed Transactions","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-21</li> </ul>"},{"location":"adr/ADR-078-saga-pattern-for-distributed-transactions/#context","title":"Context","text":"<p>Managing data consistency across multiple microservices is a primary challenge in distributed architectures. Traditional distributed transactions using two-phase commit (2PC) do not scale well and introduce tight coupling and blocking, making them unsuitable for high-performance systems. Business processes like order fulfillment or user registration often span several services (e.g., Orders, Payments, Stock, Notifications) and require a mechanism to ensure they either complete fully or are safely rolled back to a consistent state.</p>"},{"location":"adr/ADR-078-saga-pattern-for-distributed-transactions/#decision","title":"Decision","text":"<p>We will implement first-class support for the Saga pattern within <code>nala/athomic</code>. This will be located in a new module, <code>nala/athomic/resilience/sagas</code>.</p> <p>The implementation will separate the Saga Definition from the Saga Execution Strategy: 1.  Definition Layer: A <code>SagaBuilder</code> will provide a fluent, declarative API for developers to define the sequence of saga steps. Each step will consist of an <code>action</code> (the business logic to execute) and a corresponding <code>compensation</code> action (the logic to undo the action). 2.  Execution Layer: A <code>SagaExecutorProtocol</code> will define the contract for running a saga. We will provide two initial implementations, selectable via configuration:     * Choreography Executor: Uses the <code>ProducerProtocol</code> from the messaging module to publish events upon step completion, allowing services to react and trigger the next step in a decoupled manner.     * Orchestration Executor: Acts as a central coordinator, making direct requests (e.g., via an <code>HttpClientProtocol</code>) to participant services to execute steps. 3.  State Management: A <code>SagaStateRepositoryProtocol</code>, implemented using the existing <code>KVStoreProtocol</code>, will be used to persist the state of long-running saga instances.</p>"},{"location":"adr/ADR-078-saga-pattern-for-distributed-transactions/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Enables Robust Distributed Transactions: Provides a standardized, resilient way to manage complex, multi-service business processes.</li> <li>Improves Data Consistency: Prevents the system from being left in a partially complete, inconsistent state after a failure.</li> <li>Flexible Coordination: Developers can choose between decentralized choreography (for decoupling) and centralized orchestration (for visibility) via a simple configuration change, without altering the business logic.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Complexity: Writing business logic in terms of actions and compensations introduces a higher cognitive load compared to traditional monolithic transactions.</li> <li>Debugging Challenges: Tracing a distributed saga across multiple services can be complex and will rely heavily on the existing observability (distributed tracing) module.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This establishes a formal framework for designing and implementing long-running business workflows within the platform.</li> </ul> </li> </ul>"},{"location":"adr/ADR-079-claim-check-pattern-for-large-message-payloads/","title":"ADR-079: Claim Check Pattern for Large Message Payloads","text":"<ul> <li>Status: Proposed</li> <li>Date: 2025-08-21</li> </ul>"},{"location":"adr/ADR-079-claim-check-pattern-for-large-message-payloads/#context","title":"Context","text":"<p>Message brokers, including Kafka, are optimized for high-throughput, low-latency transport of relatively small messages. Sending large payloads (e.g., images, documents, detailed reports exceeding 1MB) directly through the broker can lead to several problems: * Degraded broker performance. * Increased network bandwidth and storage costs. * Hitting hard limits on message size configured on the broker (<code>max.request.size</code>).</p>"},{"location":"adr/ADR-079-claim-check-pattern-for-large-message-payloads/#decision","title":"Decision","text":"<p>We will implement the Claim Check pattern as an opt-in feature within the messaging module. The implementation will be integrated into the serialization/publishing layer.</p> <ol> <li>A <code>LargeMessageHandlingStrategy</code> will be introduced, configurable at the <code>Producer</code> level.</li> <li>When publishing a message, this strategy will check the serialized message size against a configurable threshold (e.g., <code>large_message_threshold_bytes</code>).</li> <li>If the message exceeds the threshold, the strategy will:<ul> <li>Use the existing <code>StorageProtocol</code> to upload the large payload to an external blob store (e.g., S3, GCS).</li> <li>Replace the original message payload with a \"claim check\" \u2013 a small JSON object containing the reference or URI to the stored payload.</li> <li>Add a specific header (e.g., <code>x-claim-check: true</code>) to the message.</li> </ul> </li> <li>On the consumer side, the <code>MessageProcessor</code> will be enhanced to detect this header. If present, it will use the <code>StorageProtocol</code> to retrieve the full payload from the blob store before passing it to the business logic handler.</li> </ol>"},{"location":"adr/ADR-079-claim-check-pattern-for-large-message-payloads/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Efficient Broker Usage: Keeps the message bus lean and fast, using it only for small notification/command messages.</li> <li>Scalable Large Data Handling: Allows the system to handle arbitrarily large payloads by leveraging scalable blob storage.</li> <li>Cost Optimization: Can reduce network and broker storage costs associated with large messages.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Latency: The end-to-end processing time for large messages will increase due to the two extra network calls to the blob store (one write, one read).</li> <li>Additional Point of Failure: The system now depends on the availability and reliability of the external blob store for the flow to complete.</li> <li>Orphaned Data Risk: A robust lifecycle/garbage-collection policy for the stored objects will be required to prevent orphaned data if a consumer fails to process a claim check.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>Decouples the message transport lifecycle from the large payload storage lifecycle.</li> </ul> </li> </ul>"},{"location":"adr/ADR-080-message-lineage-for-data-governance-and-traceability/","title":"ADR-080: Data Lineage and Message Tracing using OpenLineage","text":"<ul> <li>Status: Proposed</li> <li>Date: 2025-09-03</li> </ul>"},{"location":"adr/ADR-080-message-lineage-for-data-governance-and-traceability/#context","title":"Context","text":"<p>As the system scales, understanding the flow of data across different services becomes increasingly complex. It is difficult to answer critical questions such as: \"Which services consume events of type X?\" or \"What was the originating service for the data that caused this error?\". This lack of visibility complicates debugging, impact analysis for changes, and data governance efforts. We need a standardized way to trace message consumption across the entire platform.</p>"},{"location":"adr/ADR-080-message-lineage-for-data-governance-and-traceability/#decision","title":"Decision","text":"<p>We will implement a data lineage system within the <code>athomic</code> layer, based on the OpenLineage open standard. This system will automatically capture an event whenever a message consumer successfully processes a message.</p> <p>The architecture will consist of the following core components:</p> <ol> <li><code>LineageProcessor</code>: A central service responsible for constructing a canonical <code>LineageEvent</code> model and translating it into the OpenLineage JSON format.</li> <li><code>LineageStorageProtocol</code>: An interface that decouples the <code>LineageProcessor</code> from the storage backend, allowing lineage events to be sent to different destinations (e.g., logs, a message topic, or a dedicated OpenLineage collector).</li> <li>Header Injection: The base message publishing strategy (<code>BasePublishingStrategy</code>) will be modified to inject standardized headers (<code>x-nala-source-service</code>, <code>x-nala-event-type</code>, <code>message_id</code>) into all outgoing messages.</li> <li>Consumption Hook: The core message consumer logic (<code>MessageProcessor</code>) will call the <code>LineageProcessor</code> after a message has been successfully processed by a handler.</li> <li>Initial Storage Backends: We will provide an initial <code>LoggingLineageStore</code> for easy debugging and development. The architecture also supports a <code>MessagingLineageStore</code> to publish lineage events to a dedicated Kafka topic.</li> </ol>"},{"location":"adr/ADR-080-message-lineage-for-data-governance-and-traceability/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Enhanced Observability: Provides a clear, event-driven audit trail of data flow between services.</li> <li>Standardization: Adopting the OpenLineage standard makes our platform compatible with a growing ecosystem of data governance and observability tools (e.g., Marquez).</li> <li>Decoupling &amp; Extensibility: The storage protocol allows us to easily switch or add new storage backends in the future (e.g., a direct HTTP collector) without changing the core lineage logic (Open/Closed Principle).</li> <li>Simplified Debugging: Makes it easier to trace the path of a specific message through multiple consumer services.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Minor Performance Overhead: Adds a small overhead to every message publication (header injection) and consumption (lineage event creation and storage). This is considered an acceptable trade-off for the increased visibility.</li> <li>Dependency on Collector: To realize the full benefits of OpenLineage, a collector service (like Marquez) must eventually be deployed and maintained. However, the <code>LoggingLineageStore</code> provides immediate value without this dependency.</li> </ul> </li> </ul>"},{"location":"adr/ADR-081-field-level-encryption-for-message-payloads/","title":"ADR-081: Field-Level Encryption for Message Payloads","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-21</li> </ul>"},{"location":"adr/ADR-081-field-level-encryption-for-message-payloads/#context","title":"Context","text":"<p>While TLS encrypts data in transit between services and the message broker, the data itself often sits at rest on the broker's disk in plain text. For systems handling highly sensitive or regulated information (PII, financial data, health records), this represents a significant security risk. If the broker is compromised or its storage is accessed improperly, the sensitive data is exposed.</p>"},{"location":"adr/ADR-081-field-level-encryption-for-message-payloads/#decision","title":"Decision","text":"<p>We will implement end-to-end, field-level encryption as part of the serialization layer to provide \"zero-trust\" security for data at rest.</p> <ol> <li>A new serializer, <code>EncryptedJsonSerializer</code>, will be created that wraps an existing serializer (like <code>JsonPydanticSerializer</code>).</li> <li>Developers will mark sensitive fields in their Pydantic message models using the <code>SecretStr</code> type from Pydantic.</li> <li>On Serialization (Producer side): The <code>EncryptedJsonSerializer</code> will inspect the Pydantic model. For any field of type <code>SecretStr</code>, it will encrypt the field's value before the full message is serialized to JSON. The encrypted value could be stored in a structured way, e.g., <code>{\"_encrypted\": \"ciphertext\", \"key_id\": \"v1\"}</code>.</li> <li>On Deserialization (Consumer side): The serializer will detect the <code>_encrypted</code> structure, use the <code>key_id</code> to retrieve the appropriate decryption key, decrypt the value, and place it back into the Pydantic model as a <code>SecretStr</code>.</li> <li>Key management will be delegated to the existing <code>SecretsProtocol</code>, allowing keys to be fetched securely from providers like HashiCorp Vault.</li> </ol>"},{"location":"adr/ADR-081-field-level-encryption-for-message-payloads/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Zero-Trust Security: Sensitive data is protected end-to-end and remains encrypted at rest on the broker, drastically reducing the attack surface.</li> <li>Seamless Developer Experience: Encryption is handled transparently by the framework. Developers only need to mark fields as <code>SecretStr</code> in their Pydantic models, requiring no cryptographic knowledge.</li> <li>Granular Control: Only specified fields are encrypted, allowing non-sensitive data to remain in plain text for potential filtering or routing by broker-side tools.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Computational Overhead: Encryption and decryption add CPU overhead to both the producer and consumer, which will impact end-to-end latency and maximum throughput.</li> <li>Key Management Complexity: Key rotation and management become critical operational security tasks. An outage or misconfiguration of the secret management system (e.g., Vault) would halt message processing.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>The message payload is no longer a simple, human-readable JSON for sensitive fields, which may require specialized tooling for debugging.</li> </ul> </li> </ul>"},{"location":"adr/ADR-082-fifo-ordering-guarantee-for-the-transactional-outbox-pattern/","title":"ADR-082: FIFO Ordering Guarantee for the Transactional Outbox Pattern","text":"<ul> <li>Status: Proposed</li> <li>Date: 2025-08-22</li> </ul>"},{"location":"adr/ADR-082-fifo-ordering-guarantee-for-the-transactional-outbox-pattern/#context","title":"Context","text":"<p>The existing <code>Outbox</code> implementation successfully guarantees at-least-once delivery of events atomically with business data transactions. It achieves this by persisting events in a local <code>outbox</code> collection within the same database transaction and having a background service (<code>OutboxPublishingService</code>) relay these messages to the message broker.</p> <p>However, the current relay service processes pending messages in parallel without respect to their original order. For business processes that span multiple events against the same entity (e.g., <code>OrderCreated</code>, <code>ItemAddedToOrder</code>), this can lead to consumers processing events out of sequence, causing state inconsistencies and processing errors. The challenge is to introduce FIFO (First-In, First-Out) ordering per business aggregate without sacrificing the scalability and resilience of the pattern.</p>"},{"location":"adr/ADR-082-fifo-ordering-guarantee-for-the-transactional-outbox-pattern/#decision","title":"Decision","text":"<p>We will evolve the <code>Outbox</code> pattern to enforce strict FIFO ordering for messages belonging to the same business aggregate. This will be achieved through the following architectural changes:</p> <ol> <li> <p>Aggregate Key in Data Model: The <code>OutboxMessage</code> model will be extended with an <code>aggregate_key: str</code> field. This key will be mandatory and will contain the identifier of the business aggregate (e.g., <code>order_id</code>, <code>customer_id</code>).</p> </li> <li> <p>Lock-Based Sequential Processing: The <code>OutboxPublishingService</code> will be refactored to a more sophisticated, lock-based workflow:     a.  Discovery: The service will first query the outbox collection to find distinct <code>aggregate_key</code>s that have pending messages.     b.  Serialized Processing per Aggregate: The service will iterate through these keys. For each <code>aggregate_key</code>, it will attempt to acquire a distributed lock (e.g., <code>lock:outbox:order-123</code>) using the existing <code>LockingProtocol</code>.     c.  Ordered Publication: If the lock is acquired, the service will fetch all pending messages for that specific <code>aggregate_key</code>, sort them chronologically by their creation timestamp, and publish them to the message broker in that strict sequence.     d.  State Update: Only after a message is successfully published will it be marked as <code>processed</code>. If a publication fails, the relay will stop processing for that aggregate, leaving the lock to be released, and will retry the entire sequence for that aggregate in the next cycle.</p> </li> <li> <p>Leverage Existing <code>athomic</code> Components: This solution will be built on top of our robust, existing <code>athomic</code> modules. The <code>LockingProtocol</code> is critical for preventing multiple publisher instances from processing the same aggregate concurrently, which would violate the ordering guarantee.</p> </li> </ol>"},{"location":"adr/ADR-082-fifo-ordering-guarantee-for-the-transactional-outbox-pattern/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Guaranteed Ordering: Provides a strong guarantee that events for a specific business entity are processed in the correct order, which is critical for system consistency.</li> <li>Increased Robustness: Makes the entire event-driven architecture more predictable and resilient to common distributed systems problems.</li> <li>High Cohesion: The solution is self-contained within the <code>outbox</code> module and reuses existing framework components (<code>LockingProtocol</code>, <code>ProducerProtocol</code>), strengthening the <code>athomic</code> ecosystem.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Reduced Parallelism (Per Aggregate): Throughput for a single, high-traffic aggregate is now limited to a single publisher at a time. This is a deliberate trade-off in favor of correctness, and the overall system throughput remains high as different aggregates are processed in parallel.</li> <li>\"Poison Pill\" Scenario: If a single message for an aggregate is \"stuck\" (e.g., consistently fails to publish due to a non-transient error), it will block all subsequent messages for that same aggregate. This will require robust monitoring and DLQ (Dead Letter Queue) strategies, which are already part of our messaging module.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This change makes the contract with the developer more explicit: when saving an <code>OutboxMessage</code>, they are now required to provide a meaningful <code>aggregate_key</code>.</li> </ul> </li> </ul>"},{"location":"adr/ADR-083-optimistic-event-ordering-with-consumer-side-resolution/","title":"ADR-083: Optimistic Event Ordering with Consumer-Side Resolution","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-23 (Updated 2025-09-23)</li> </ul>"},{"location":"adr/ADR-083-optimistic-event-ordering-with-consumer-side-resolution/#context","title":"Context","text":"<p>The lock-based FIFO ordering for the Transactional Outbox pattern (<code>ADR-078</code>) provides strong consistency guarantees. However, it introduces a performance trade-off: the throughput for a single, high-traffic business aggregate is serialized, as only one publisher worker can process its messages at a time. This can become a bottleneck for \"hot\" aggregates (e.g., a popular product's inventory).</p> <p>The challenge is to explore an alternative approach that can provide the same strict ordering guarantee while allowing for higher parallelism and throughput on the publisher side, thereby improving the computational cost in the common \"happy path\" scenario where events arrive in order.</p>"},{"location":"adr/ADR-083-optimistic-event-ordering-with-consumer-side-resolution/#decision","title":"Decision","text":"<p>We will implement a novel, \"optimistic ordering\" mechanism. This approach decouples the publisher from the ordering responsibility and delegates the re-sequencing logic to an intelligent consumer-side framework component within <code>athomic</code>.</p> <p>The implementation will be based on the following principles:</p> <ol> <li> <p>Sequenced-at-Source (Publisher):</p> <ul> <li>When an event is written to the <code>Outbox</code> collection, it will be atomically assigned a <code>sequence_id</code> that is strictly and gap-lessly incrementing per <code>aggregate_key</code>.</li> <li>The <code>OutboxPublishingService</code> is then free to publish these messages in parallel, with each message carrying its <code>aggregate_key</code> and <code>sequence_id</code> as metadata.</li> </ul> </li> <li> <p>Consumer-Side Resolution (<code>@ordered_consumer</code>):</p> <ul> <li>A new decorator, <code>@ordered_consumer</code>, will wrap the business logic of a message handler.</li> <li>State Tracking: It will use the <code>KVStoreProtocol</code> to maintain the <code>last_processed_sequence</code> for each <code>aggregate_key</code>.</li> <li>The \"Waiting Room\": When a message arrives out of order, it is placed into a \"waiting room\" buffer (a Redis Sorted Set).</li> <li>Backlog Processing: After successfully processing an in-order message, the handler immediately checks the waiting room for the next message in the sequence.</li> </ul> </li> </ol>"},{"location":"adr/ADR-083-optimistic-event-ordering-with-consumer-side-resolution/#refined-implementation-details-the-three-layer-strategy-architecture","title":"Refined Implementation Details: The Three-Layer Strategy Architecture","text":"<p>To ensure the implementation is modular, extensible, and adheres to the Single Responsibility Principle (SRP), the logic within the consumer pipeline is structured using three distinct layers of strategies:</p> <ol> <li> <p>Consumption Strategy: Its sole responsibility is how messages are fetched from the broker. It is unaware of business logic. Implementations include <code>SingleMessageConsumptionStrategy</code> and <code>BatchMessageConsumptionStrategy</code>.</p> </li> <li> <p>Orchestration Strategy: This is the core workflow layer. Its responsibility is to manage what happens before the user's business logic is called. The <code>OrderedOrchestrationStrategy</code> implements the state tracking and \"waiting room\" logic described above. A simpler <code>DirectOrchestrationStrategy</code> exists for standard consumers.</p> </li> <li> <p>Execution Strategy: Its sole responsibility is how the user's callback is executed. For now, an <code>ImmediateExecutionStrategy</code> is used, but this layer allows for future extensions like delayed or scheduled execution.</p> </li> </ol> <p>This entire pipeline is assembled by the <code>ConsumerFactory</code>. It reads the strategy name (e.g., <code>\"ordered\"</code>) from the handler configuration, uses a <code>Registry</code> and <code>Creator</code> pattern to build the appropriate <code>OrchestrationStrategy</code>, and injects it into a central <code>MessageProcessor</code>, which coordinates the entire flow for a given message.</p>"},{"location":"adr/ADR-083-optimistic-event-ordering-with-consumer-side-resolution/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Higher Publisher Throughput: Removes the distributed lock bottleneck from the <code>OutboxPublishingService</code>.</li> <li>Increased Resilience: The system can gracefully handle and re-order out-of-sequence messages.</li> <li>State-of-the-Art Feature: Provides a highly advanced feature for the <code>athomic</code> framework.</li> <li>Highly Modular and Testable Consumer: The final implementation using the Creator and Strategy patterns makes the consumer pipeline extremely modular, consistent, and easy to test and extend.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Consumer Complexity: The internal logic is significantly more complex, though this is encapsulated by the framework.</li> <li>Increased Latency for Out-of-Order Messages: Buffered messages will have a higher end-to-end processing time.</li> <li>Higher Redis Usage: The consumer will perform more read/write operations against the <code>KVStore</code>.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This represents a philosophical shift from pessimistic locking to optimistic processing.</li> </ul> </li> </ul>"},{"location":"adr/ADR-084-payload-processing-pipeline/","title":"ADR-084: Payload Processing Pipeline","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-31</li> </ul>"},{"location":"adr/ADR-084-payload-processing-pipeline/#context","title":"Context","text":"<p>The existing messaging system handles message payloads as opaque blocks of data. As the system evolves, there is a growing need to apply systematic, ordered transformations to payloads before they are published and after they are consumed. Such transformations include compression to reduce message size and cost, and encryption to secure sensitive data in transit. Implementing these as ad-hoc steps within the business logic of each publisher or consumer would lead to code duplication, inconsistent application of transformations, and a tightly coupled architecture.</p>"},{"location":"adr/ADR-084-payload-processing-pipeline/#decision","title":"Decision","text":"<p>We will introduce a formal payload processing pipeline, managed by a <code>PayloadProcessor</code>. This pipeline will consist of a sequence of well-defined processing steps, each conforming to a <code>ProcessingStepProtocol</code>.</p> <ol> <li><code>ProcessingStepProtocol</code>: A protocol will be defined for a single processing step. It will have two methods: <code>apply</code> for outbound messages (e.g., compress, encrypt) and <code>revert</code> for inbound messages (e.g., decrypt, decompress).</li> <li><code>PayloadProcessor</code>: This class will be responsible for managing and executing a list of processing steps in the correct order. For an outbound message, it will iterate through the steps and call <code>apply</code> on each. For an inbound message, it will iterate in reverse order and call <code>revert</code>.</li> <li>Integration: The <code>MessageFactory</code> and <code>MessageProcessor</code> will be updated to use the <code>PayloadProcessor</code>. This ensures that all messages passing through the system are consistently processed according to the configured pipeline.</li> </ol>"},{"location":"adr/ADR-084-payload-processing-pipeline/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Extensibility: New processing steps (e.g., signing, validation, different compression algorithms) can be added easily without changing the core messaging logic.</li> <li>Decoupling: The logic for each transformation (compression, encryption) is isolated into its own component, respecting the Single Responsibility Principle.</li> <li>Consistency: Guarantees that all message payloads undergo the same sequence of transformations, improving reliability and security.</li> </ul> </li> <li>Negative:<ul> <li>Increased Complexity: Introduces new layers of abstraction (<code>PayloadProcessor</code>, <code>ProcessingStepProtocol</code>) that must be understood by developers.</li> <li>Performance Overhead: Each step in the pipeline adds a small amount of processing overhead. For very high-throughput, low-latency scenarios, this could be a factor.</li> </ul> </li> <li>Neutral/Other:<ul> <li>This decision establishes a foundational pattern for handling message transformations that can be built upon in the future.</li> </ul> </li> </ul>"},{"location":"adr/ADR-085-payload-compression/","title":"ADR-085: Payload Compression","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-31</li> </ul>"},{"location":"adr/ADR-085-payload-compression/#context","title":"Context","text":"<p>Message payloads, especially those containing verbose data formats like JSON, can be large. Transmitting large messages over the message bus can lead to increased network bandwidth usage, higher storage costs for brokers that persist messages, and potentially slower performance. A mechanism to reduce the size of the message payloads before transmission is needed to mitigate these issues. Several compression algorithms exist, with different trade-offs between compression ratio and CPU usage.</p>"},{"location":"adr/ADR-085-payload-compression/#decision","title":"Decision","text":"<p>We will add payload compression as a step in the new Payload Processing Pipeline (ADR-084).</p> <ol> <li><code>CompressionProtocol</code>: A protocol will be created to define the interface for any compression provider, with <code>compress</code> and <code>decompress</code> methods.</li> <li>Initial Provider (Gzip): The first implementation of this protocol will be <code>GzipProvider</code>. Gzip is chosen for its widespread availability, good compression ratio for text-based data, and reasonable performance, making it a balanced default choice.</li> <li>Integration via Adapter: A <code>CompressionStepAdapter</code> will be created to adapt the <code>CompressionProtocol</code> to the <code>ProcessingStepProtocol</code>, allowing it to be seamlessly inserted into the <code>PayloadProcessor</code>'s pipeline.</li> </ol>"},{"location":"adr/ADR-085-payload-compression/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Reduced Costs: Significantly lowers network and message broker storage costs, especially for high-volume, text-heavy payloads.</li> <li>Improved Throughput: Smaller message sizes can lead to higher message throughput.</li> <li>Flexibility: The protocol-based approach allows for easily swapping or adding other compression algorithms (like Zstd, Brotli) in the future.</li> </ul> </li> <li>Negative:<ul> <li>CPU Overhead: Compression and decompression consume CPU cycles on both the publisher and consumer sides, which could impact latency.</li> <li>Debugging Difficulty: Compressed message payloads are not human-readable, making debugging directly from the message broker more difficult without proper tooling.</li> </ul> </li> <li>Neutral/Other:<ul> <li>This feature makes the system more efficient for a broad range of use cases where payload size is a concern.</li> </ul> </li> </ul>"},{"location":"adr/ADR-086-payload-encryption/","title":"ADR-086: Payload Encryption","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-31</li> </ul>"},{"location":"adr/ADR-086-payload-encryption/#context","title":"Context","text":"<p>Some messages may carry sensitive user data or system information. While transport-level security (TLS) encrypts data in transit between the service and the message broker, the data resides unencrypted (\"at rest\") within the broker's queues. This poses a security risk, as anyone with access to the broker could potentially inspect message contents. A mechanism for end-to-end encryption of the payload itself is required to ensure data confidentiality.</p>"},{"location":"adr/ADR-086-payload-encryption/#decision","title":"Decision","text":"<p>We will introduce payload encryption as a step in the Payload Processing Pipeline (ADR-084).</p> <ol> <li><code>EncryptionProtocol</code>: A protocol will define the standard interface for encryption providers, featuring <code>encrypt</code> and <code>decrypt</code> methods.</li> <li>Initial Provider (Fernet): The initial implementation will be <code>FernetProvider</code>, which uses the Fernet specification (AES-128 in CBC mode with PKCS7 padding, signed with HMAC using SHA256). Fernet is part of the Python <code>cryptography</code> library and provides symmetric authenticated encryption, which is secure and straightforward to use.</li> <li>Key Management: The encryption key will be managed by the <code>SecretsManager</code>, ensuring it is stored securely and retrieved at runtime.</li> <li>Integration via Adapter: An <code>EncryptionStepAdapter</code> will adapt the <code>EncryptionProtocol</code> to the <code>ProcessingStepProtocol</code> for use in the pipeline.</li> </ol>"},{"location":"adr/ADR-086-payload-encryption/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>Enhanced Security: Provides end-to-end encryption for message payloads, ensuring data confidentiality even if the message broker is compromised.</li> <li>Compliance: Helps meet regulatory and compliance requirements (like GDPR, HIPAA) for handling sensitive data.</li> <li>Centralized Logic: Encryption logic is centralized and applied consistently, avoiding scattered and potentially insecure implementations.</li> </ul> </li> <li>Negative:<ul> <li>Key Management Complexity: Introduces the operational burden of managing encryption keys. Key rotation and access control policies must be carefully handled.</li> <li>Performance Overhead: Encryption and decryption are computationally intensive operations that will add latency to message processing.</li> <li>Data Recovery: If the encryption key is lost, all data encrypted with that key becomes irrecoverable.</li> </ul> </li> <li>Neutral/Other:<ul> <li>This positions security as a first-class citizen in the messaging architecture.</li> </ul> </li> </ul>"},{"location":"adr/ADR-087-distributed-leasing-mechanism-for-concurrency-control/","title":"ADR-087: Distributed Leasing Mechanism for Concurrency Control","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-09-06</li> </ul>"},{"location":"adr/ADR-087-distributed-leasing-mechanism-for-concurrency-control/#context","title":"Context","text":"<p>The implementation of FIFO ordering in the Outbox Pattern (ADR-078) requires an exclusive lock per <code>aggregate_key</code> to prevent multiple publisher instances from processing messages for the same aggregate concurrently. A simple distributed lock (e.g., <code>SETNX</code> with a TTL) is vulnerable to \"stuck lock\" scenarios where a worker acquires a lock and crashes, forcing other workers to wait until the TTL expires. We need a more resilient, fault-tolerant concurrency mechanism that ensures a lock is automatically released if the holding process dies.</p>"},{"location":"adr/ADR-087-distributed-leasing-mechanism-for-concurrency-control/#decision","title":"Decision","text":"<p>We will implement a distributed leasing mechanism within the <code>athomic.resilience</code> module. This system treats a lock as a \"lease\" that must be actively maintained by the holder.</p> <ol> <li> <p>Core Components:</p> <ul> <li><code>LeaseProtocol</code>: An interface defining the contract for lease providers (<code>acquire</code>, <code>renew</code>, <code>release</code>).</li> <li><code>RedisLeaseProvider</code>: A concrete implementation using Redis. It will use atomic operations:<ul> <li><code>acquire</code>: <code>SET key value NX EX seconds</code> for atomic acquisition.</li> <li><code>renew</code> / <code>release</code>: Atomic Lua scripts that first verify the lease <code>owner_id</code> before modifying or deleting the key, preventing race conditions.</li> </ul> </li> <li><code>LeaseManager</code> &amp; <code>LeaseHolder</code>: Client-side components providing a clean <code>async with</code> interface.</li> </ul> </li> <li> <p>Heartbeat Mechanism:</p> <ul> <li>When a lease is successfully acquired, the <code>LeaseManager</code> returns a <code>LeaseHolder</code> context manager.</li> <li>Upon entering the <code>with</code> block, the <code>LeaseHolder</code> starts a background <code>asyncio.Task</code> that periodically calls the provider's <code>renew</code> method. This acts as a \"heartbeat,\" continuously extending the lease's TTL.</li> <li>If the worker process crashes, the heartbeat stops. The lease in Redis will naturally expire after its TTL, allowing another worker to safely acquire it.</li> <li>Upon exiting the <code>with</code> block (normally or via exception), the <code>LeaseHolder</code> cancels the heartbeat task and explicitly releases the lease.</li> </ul> </li> </ol>"},{"location":"adr/ADR-087-distributed-leasing-mechanism-for-concurrency-control/#consequences","title":"Consequences","text":"<ul> <li>Positive:<ul> <li>High Fault Tolerance: Eliminates \"stuck lock\" scenarios. If a worker crashes, the system automatically recovers after the lease TTL expires.</li> <li>Developer Experience: The <code>async with manager.acquire(...)</code> pattern provides a clean, safe, and easy-to-use interface for developers, abstracting away the complexity of heartbeats and renewals.</li> <li>Centralized &amp; Reusable: Creates a robust, reusable concurrency primitive within the <code>Athomic</code> framework that can be used by any component, not just the Outbox.</li> </ul> </li> <li>Negative:<ul> <li>Increased Complexity: The implementation (especially the client-side heartbeat task and Lua scripts) is more complex than a simple distributed lock.</li> <li>Dependency: Relies on the target backend (Redis) supporting Lua scripting for atomic operations.</li> </ul> </li> <li>Neutral/Other:<ul> <li>The system's reliability is tied to the lease duration and renewal interval configurations. These must be balanced to provide quick failure recovery without generating excessive network traffic from heartbeats.</li> </ul> </li> </ul>"},{"location":"adr/ADR-088-generic-sharding-service/","title":"ADR-088: Generic Sharding Service for Distributed Workloads","text":"<ul> <li>Status: Proposed</li> <li>Date: 2025-09-06</li> </ul>"},{"location":"adr/ADR-088-generic-sharding-service/#context","title":"Context","text":"<p>The current implementation of the <code>OutboxPublisherService</code> identifies a potential scalability bottleneck. While multiple instances of the service can run concurrently, they all fetch the complete list of pending aggregates and compete for leases on them. This leads to high contention, wasted work on failed lease acquisitions, and does not provide true horizontal scaling, as adding more workers primarily increases contention rather than throughput.</p> <p>This pattern of distributing a list of work items (e.g., aggregate keys, cache keys to invalidate, etc.) across a pool of workers is a recurring architectural challenge.</p>"},{"location":"adr/ADR-088-generic-sharding-service/#alternatives-considered","title":"Alternatives Considered","text":"<ol> <li>Leasing-Only (Current State): Rely solely on the existing leasing mechanism for coordination. This was rejected due to the high contention and inefficient resource usage described above.</li> <li>Broker-Side Partitioning: Utilize the native partitioning capabilities of the message broker (e.g., Kafka partitions). While powerful, this approach would tightly couple the sharding logic to a specific broker technology and would not be a generic solution for workloads that do not originate from a message queue.</li> <li>Outbox-Specific Sharding Logic: Implement the sharding logic directly within the <code>OutboxPublisherBase</code>. This was rejected as it violates the Single Responsibility Principle (SRP) and misses the opportunity to create a reusable component for other potential use cases within the Athomic framework.</li> </ol>"},{"location":"adr/ADR-088-generic-sharding-service/#decision","title":"Decision","text":"<p>We will create a new, generic, and reusable Sharding Service as a first-class component within the Nala Athomic framework, located at <code>nala/athomic/resilience/sharding/</code>.</p> <p>This service will be responsible for distributing a list of string-based items across a dynamic group of workers. Its implementation will be based on the following principles:</p> <ol> <li>Service Discovery Integration: The <code>ShardingService</code> will be initialized with a <code>group_name</code> (e.g., \"outbox-publishers\"). It will use the existing <code>ServiceDiscoveryProtocol</code> within Athomic to fetch a list of all healthy, active service instances belonging to that group.</li> <li>Consistent Hashing: It will use a consistent hashing algorithm (via a library like <code>uhashring</code>) to build an in-memory hash ring from the list of active worker IDs. This ensures minimal item re-shuffling when workers are added or removed.</li> <li>Core Functionality: The service will expose a primary method, <code>filter_owned_items(items: List[str]) -&gt; List[str]</code>, which accepts a complete list of items and returns only the subset that is mapped to the current worker's unique instance ID.</li> <li>Worker Lifecycle: Service instances intended to participate in a sharded group (like the <code>OutboxPublisherService</code>) will be responsible for registering themselves with the service discovery on startup and deregistering on shutdown. The new <code>ShardingService</code> will provide helper methods (<code>register_self</code>, <code>deregister_self</code>) to facilitate this.</li> </ol> <p>The <code>OutboxPublisherService</code> will be the first client of this new service. It will be refactored to: * Instantiate the <code>ShardingService</code> with a specific group name. * Register/Deregister itself upon startup/shutdown. * In its processing loop, fetch all pending aggregate keys and then use <code>sharding_service.filter_owned_items()</code> to determine its specific workload for that cycle.</p>"},{"location":"adr/ADR-088-generic-sharding-service/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Horizontal Scalability: The workload for processing outbox aggregates can now be distributed across multiple service instances, directly improving throughput.</li> <li>Reduced Contention: Eliminates race conditions and failed lease acquisitions, as each worker will only attempt to lease aggregates it already owns according to the hash ring.</li> <li>Reusability: The <code>ShardingService</code> is generic and can be immediately reused for other distributed workloads within the application (e.g., batch processing, cache invalidation, etc.).</li> <li>Decoupling &amp; SRP: The <code>OutboxPublisherService</code> is simplified, focusing on its core logic of processing events, while the complex logic of workload distribution is encapsulated in the <code>ShardingService</code>.</li> <li>Resilience: The system automatically rebalances the workload as workers are added or removed from the service discovery pool.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>New Dependency: The sharding mechanism introduces a hard dependency on a functioning Service Discovery system (e.g., Consul). If the discovery service is down, workers cannot determine the sharding topology. A fallback strategy (e.g., a single worker processing all items) must be considered.</li> <li>Potential for Uneven Load: Consistent hashing algorithms provide good, but not perfectly uniform, distribution. Some workers may receive a slightly higher or lower number of items, especially with a small number of workers. This is an acceptable trade-off for the scalability gains.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>The state of the hash ring is ephemeral and recalculated on each cycle based on the \"live\" state from service discovery. This is simple and resilient but introduces a small overhead for discovery queries and ring reconstruction on each processing cycle.</li> <li>Observability will become more critical. We will need to monitor the number of active workers per sharding group and the number of items processed by each to detect imbalances.</li> </ul> </li> </ul>"},{"location":"adr/ADR-089-centralized-connection-management/","title":"ADR-089: Centralized Connection Management","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-09-18</li> </ul>"},{"location":"adr/ADR-089-centralized-connection-management/#context","title":"Context","text":"<p>The Athomic framework relies on various external data stores (document databases, KV stores) for its modules, including Caching, Leasing, Sagas, Feature Flags, and Outbox. Historically, the configuration was monolithic, allowing for only a single document database and a single KV store. Components would create their clients through specific factories (<code>KVStoreFactory</code>, <code>DocumentsDatabaseFactory</code>), leading to scattered connection logic and making it difficult to support scenarios requiring multiple connections to different databases of the same type (e.g., a primary DB and an audit DB). This approach violated the Single Responsibility Principle, as multiple components were concerned with connection lifecycle, and limited the framework's flexibility.</p> <p>Alternatives considered: 1.  Keep decentralized factories: Rejected due to increasing complexity and poor scalability. 2.  Introduce a full DI container library: Rejected as it would add a significant external dependency for a problem that could be solved natively within the framework's existing service lifecycle pattern.</p>"},{"location":"adr/ADR-089-centralized-connection-management/#decision","title":"Decision","text":"<p>We will introduce a <code>ConnectionManager</code>, a central <code>BaseService</code> responsible for the entire lifecycle of all database connections within the Athomic application.</p> <ol> <li>Centralized Management: The <code>ConnectionManager</code> will be the single source of truth for creating, starting, stopping, and providing access to database clients.</li> <li>Named Connections: The configuration in <code>settings.toml</code> will be refactored to support a dictionary of named connections under <code>database.documents</code> and <code>database.kvstore</code>. This allows defining multiple, distinct connections (e.g., <code>default_mongo</code>, <code>audit_mongo</code>, <code>default_redis</code>, <code>rate_limiter_redis</code>).</li> <li>Singleton Factory: A <code>ConnectionManagerFactory</code> will be created to provide singleton access to the <code>ConnectionManager</code> instance throughout the application.</li> <li>Refactoring Consumers: All modules and factories that previously created their own database clients (e.g., <code>CacheFallbackFactory</code>, <code>SagaStateRepositoryFactory</code>, <code>APIKeyDBProvider</code>) will be refactored to request a named connection instance from the <code>ConnectionManagerFactory</code>.</li> </ol>"},{"location":"adr/ADR-089-centralized-connection-management/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Increased Flexibility: The framework now natively supports connecting to multiple databases of the same type, a critical feature for complex microservice architectures.</li> <li>Improved SRP: The responsibility of connection lifecycle is now encapsulated entirely within the <code>ConnectionManager</code>, making other components simpler as they only need to request a connection by name.</li> <li>Enhanced Consistency: All parts of the framework now acquire database connections through a single, consistent mechanism.</li> <li>Clearer Configuration: The settings are more explicit, as components now reference connections by a logical name (e.g., <code>kv_store_connection_name = \"default_redis\"</code>).</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increased Indirection: Components have an additional layer of indirection to get a connection (Component -&gt; <code>ConnectionManagerFactory</code> -&gt; <code>ConnectionManager</code> -&gt; Connection Client).</li> <li>Initial Refactoring Effort: This decision required a significant refactoring effort across multiple modules to adapt to the new connection acquisition pattern.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>The <code>ConnectionManager</code> becomes a critical core service, essential for the startup of most other data-dependent services.</li> </ul> </li> </ul>"},{"location":"adr/ADR-090-agnostic-database-migration-system/","title":"ADR-090: Agnostic Database Migration System","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-09-18</li> </ul>"},{"location":"adr/ADR-090-agnostic-database-migration-system/#context","title":"Context","text":"<p>As the application schema evolves, there is no automated, version-controlled, or reproducible process for applying changes to the database (e.g., creating collections, adding indexes, backfilling data). This relies on manual, error-prone scripts and processes, which is unsustainable and risky, especially across different environments (development, staging, production). The framework needs a robust migration tool that is integrated with its core principles.</p> <p>Alternatives considered: 1.  Rely on manual scripts: Rejected as it is not safe, reproducible, or scalable. 2.  Use a third-party library like <code>yoyo-migrations</code>: This was heavily considered, and the final design is inspired by its script-based approach. However, a custom solution was chosen to achieve deeper integration with Athomic's async-first nature, Pydantic-based configuration, observability stack (tracing/metrics), and the new <code>ConnectionManager</code>.</p>"},{"location":"adr/ADR-090-agnostic-database-migration-system/#decision","title":"Decision","text":"<p>We will implement a new, database-agnostic migration module within Athomic (<code>athomic.database.migrations</code>).</p> <ol> <li>Protocol-Driven Design: The system will be built around a <code>MigrationBackendProtocol</code>, defining a contract for database-specific operations (connect, apply, record, etc.). This ensures extensibility.</li> <li>Initial MongoDB Backend: The first implementation will be <code>MongoDbBackend</code>, which uses the <code>ConnectionManager</code> to get its database connection and <code>beanie</code> to manage a <code>MigrationStatus</code> collection for tracking applied migrations.</li> <li>Python-Based Scripts: Migrations will be plain Python files containing <code>upgrade</code> and <code>downgrade</code> async functions. This allows migrations to be powerful, leveraging any other part of the Athomic framework if needed.</li> <li>Orchestration via <code>MigrationRunner</code>: A <code>MigrationRunner</code> class will orchestrate the entire process. It discovers scripts from configured paths, compares them against the <code>MigrationStatus</code> collection, and executes pending migrations via the appropriate backend.</li> <li>CLI Integration: The migration runner will be primarily invoked through a new CLI command, <code>athomic db upgrade</code>.</li> </ol>"},{"location":"adr/ADR-090-agnostic-database-migration-system/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Traceability &amp; Reproducibility: Database schema changes are now versioned, auditable, and can be applied automatically and consistently across all environments.</li> <li>Deep Framework Integration: Being a native Athomic component, the migration system seamlessly integrates with the framework's lifecycle, configuration, and observability, providing tracing and metrics for migration operations out-of-the-box.</li> <li>Extensibility: The protocol-based design makes it straightforward to add support for other databases (e.g., SQL via an adapter) in the future.</li> <li>Improved Developer Experience: Provides a clear and standardized workflow for managing database evolution.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Internal Maintenance: As a custom solution, its maintenance and future enhancements are our responsibility, unlike a third-party open-source tool.</li> <li>MongoDB Transactions: The framework does not provide automatic transaction management for NoSQL migrations. Developers writing migration scripts for MongoDB must manually handle atomicity using client sessions if a migration involves multiple write operations.</li> </ul> </li> </ul>"},{"location":"adr/ADR-091-unified-command-line-interface/","title":"ADR-091: Unified Command-Line Interface (CLI)","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-09-18</li> </ul>"},{"location":"adr/ADR-091-unified-command-line-interface/#context","title":"Context","text":"<p>The Athomic framework lacked a standardized, user-friendly entry point for developer and operational tasks, such as running database migrations, generating security keys, or managing services. Such tasks were either not possible or would require custom, disconnected Python scripts, leading to an inconsistent developer experience and making CI/CD automation more complex.</p> <p>Alternatives considered: 1.  Use standard <code>argparse</code>: Rejected as it is more verbose and less modern compared to newer alternatives. 2.  Use <code>Click</code>: A strong candidate. However, <code>Typer</code> was chosen because it is built on top of Click and its type-hint-based approach aligns perfectly with the modern Python practices used throughout the rest of the Athomic codebase.</p>"},{"location":"adr/ADR-091-unified-command-line-interface/#decision","title":"Decision","text":"<p>We will implement a new, unified Command-Line Interface (CLI) for the Athomic framework, accessible via a single <code>nala</code> (or <code>athomic</code>) entry point.</p> <ol> <li>Technology: The CLI will be built using the <code>Typer</code> library.</li> <li>Extensible Structure: The main CLI application (<code>nala.cli.main</code>) will dynamically discover and register subcommands from the <code>nala.cli.commands</code> package. This allows new commands to be added as simple modules (e.g., <code>database.py</code>, <code>keys.py</code>) without modifying the main entry point, adhering to the Open/Closed Principle.</li> <li>Initial Commands: The initial implementation will include <code>nala db upgrade</code> to run database migrations and <code>nala keys generate</code> as a security utility.</li> <li>Project Integration: The CLI will be registered as a script entry point in <code>pyproject.toml</code>, making it available directly after installation.</li> </ol>"},{"location":"adr/ADR-091-unified-command-line-interface/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Improved Developer Experience: Provides a single, powerful, and self-documenting interface for interacting with the framework from the command line.</li> <li>Automation-Friendly: The CLI offers a stable contract for integration into CI/CD pipelines for tasks like automated migrations.</li> <li>High Extensibility: The modular, discovery-based design makes it trivial to add new commands and functionalities as the framework grows.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>New Dependency: Adds <code>typer</code> and its dependencies to the project.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>Establishes a formal pattern for creating and managing operational tasks within the project.</li> </ul> </li> </ul>"},{"location":"adr/ADR-092-implementation-of-a-backpressure-mechanism-for-resilient-services/","title":"ADR-092: Implementation of a Backpressure Mechanism for Resilient Services","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-09-18</li> </ul>"},{"location":"adr/ADR-092-implementation-of-a-backpressure-mechanism-for-resilient-services/#context","title":"Context","text":"<p>Distributed components, such as the <code>OutboxPublisher</code> running across multiple workers, are susceptible to transient downstream failures (e.g., an unavailable message broker) or high contention on specific resources (i.e., \"hot aggregates\"). In these scenarios, retrying operations immediately and aggressively exacerbates the problem, leading to wasted resources (CPU, network) and increasing the risk of cascading failures across the system. A mechanism is required to allow services to detect these conditions and gracefully \"back off\" to allow the struggling components to recover.</p>"},{"location":"adr/ADR-092-implementation-of-a-backpressure-mechanism-for-resilient-services/#decision","title":"Decision","text":"<p>We will implement a distributed, time-based backpressure mechanism. The implementation consists of:</p> <ol> <li>A generic <code>BackpressureManager</code> that leverages a shared <code>KVStoreProtocol</code> (e.g., Redis) to manage the state of embargoed resources.</li> <li>The embargo is created by setting a key in the KV store with a specific Time-To-Live (TTL). This ensures the backpressure is temporary and the system automatically attempts to resume processing after the cooldown period.</li> <li>The <code>OutboxPublisherBase</code> will be the first component to utilize this pattern. It will apply backpressure on a specific <code>aggregate_key</code> when it detects a <code>PublishError</code> or a <code>LeaseAcquisitionError</code>.</li> <li>Before processing a batch of aggregates, the publisher will query the <code>BackpressureManager</code> to filter out and skip any aggregates that are currently under an embargo.</li> </ol>"},{"location":"adr/ADR-092-implementation-of-a-backpressure-mechanism-for-resilient-services/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Increased System Stability: Prevents downstream failures or contention from overwhelming the publishers and causing cascading failures.</li> <li>Reduced Resource Waste: Avoids pointless, rapid-fire retries against a component that is known to be temporarily unavailable, saving CPU and network resources.</li> <li>Automatic Recovery: The TTL-based approach ensures that processing for an affected resource resumes automatically without manual intervention.</li> <li>Reusable Pattern: The <code>BackpressureManager</code> is a generic, decoupled component that can be reused in other parts of the system (e.g., API calls, task processing) to manage load.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Introduces Intentional Latency: By design, the processing for a specific aggregate will be delayed when backpressure is applied. This is the trade-off for overall system health.</li> <li>Adds a Dependency: The mechanism relies on a fast, distributed Key-Value store to share state across multiple workers. The performance of this store is critical to the effectiveness of the backpressure system.</li> </ul> </li> </ul>"},{"location":"adr/ADR-093-provider-specific-consumption-strategies/","title":"ADR-093: Abstract Consumer Interface for Provider Consistency","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-09-24</li> </ul>"},{"location":"adr/ADR-093-provider-specific-consumption-strategies/#context","title":"Context","text":"<p>During the consumer architecture refactoring (<code>ADR-083</code>), a key design decision was the interface between the <code>ConsumptionStrategy</code> and the concrete <code>Consumer</code> providers (like <code>KafkaConsumer</code>). Two primary approaches were debated:</p> <ol> <li> <p>Provider-Specific Strategies: Make the <code>BaseConsumer</code> minimal, only managing the client lifecycle. The <code>ConsumptionStrategy</code> would be specialized for each provider (e.g., <code>KafkaBatchConsumptionStrategy</code>), interacting directly with the native client. This approach simplifies initial provider creation.</p> </li> <li> <p>Abstract Interface on <code>BaseConsumer</code>: Define a generic, abstract interface for fetching messages (e.g., <code>get_message_iterator()</code>, <code>get_batch()</code>) on the <code>BaseConsumer</code> class. All providers would be required to implement this interface. This allows <code>ConsumptionStrategy</code> implementations to be generic and reusable.</p> </li> </ol> <p>While Approach 1 offers ease of provider implementation, it was determined that it could lead to inconsistencies between providers and a less clear contract for the strategies. A formal, abstract interface was deemed necessary to ensure long-term maintainability, predictability, and consistency across the framework.</p>"},{"location":"adr/ADR-093-provider-specific-consumption-strategies/#decision","title":"Decision","text":"<p>We will adopt Approach 2: an Abstract Interface on <code>BaseConsumer</code>.</p> <p>The <code>BaseConsumer</code> abstract class will define two abstract methods: * <code>get_message_iterator() -&gt; AsyncGenerator[RawMessage, None]</code> * <code>get_batch(max_records: int, timeout_ms: int) -&gt; List[RawMessage]</code></p> <p>All concrete provider implementations (<code>KafkaConsumer</code>, <code>LocalConsumer</code>, etc.) must implement this interface. This moves the responsibility of adapting the specific client library's output into a standardized <code>List[RawMessage]</code> into the provider itself (using the Adapter Pattern where necessary, like <code>AdaptedKafkaMessage</code>).</p> <p>As a result, the <code>ConsumptionStrategy</code> implementations (<code>SingleMessageConsumptionStrategy</code>, <code>BatchMessageConsumptionStrategy</code>) become generic, provider-agnostic, and reusable across the framework.</p>"},{"location":"adr/ADR-093-provider-specific-consumption-strategies/#consequences","title":"Consequences","text":"<ul> <li> <p>Positive:</p> <ul> <li>Enforces Consistency: Guarantees that all consumer providers behave in a predictable and consistent manner.</li> <li>Improves Encapsulation: Strategies interact with a clean, public interface on the <code>BaseConsumer</code>, not its internal implementation details.</li> <li>Enables Generic Strategies: Allows for the creation of simple, reusable <code>ConsumptionStrategy</code> classes that are not tied to any specific broker technology.</li> <li>Clear Contract: Provides an explicit, formal contract for developers creating new consumer providers, improving long-term maintainability.</li> </ul> </li> <li> <p>Negative:</p> <ul> <li>Increases Provider Implementation Workload: The initial effort to create a new provider is higher, as the developer must implement both abstract methods, potentially having to simulate one pattern (e.g., batching) if it's not natively supported by the underlying client library.</li> </ul> </li> <li> <p>Neutral/Other:</p> <ul> <li>This decision prioritizes long-term architectural integrity and consistency over the short-term speed of implementing new providers.</li> </ul> </li> </ul>"},{"location":"architecture/","title":"Architecture Overview","text":"<p>The <code>athomic-docs</code> follows a clean, layered architecture designed for separation of concerns, testability, and scalability. It is heavily inspired by principles from Domain-Driven Design (DDD) and Clean Architecture.</p> <p>The main layers are:</p> <ul> <li>Domain Layer: Contains the core business logic, entities, and rules. It is the heart of your application and has no dependencies on other layers.</li> <li>Application Layer: Orchestrates the domain logic. It contains use cases and application services that handle requests from the outside world.</li> <li>Infrastructure Layer: Implements external concerns like databases, message brokers, and third-party API clients.</li> <li>Athomic Layer: A cross-cutting \"chassis\" that provides foundational capabilities like caching, resilience, and observability to all other layers.</li> </ul>"},{"location":"architecture/dependency-injection/","title":"Dependency Injection &amp; Service Location","text":""},{"location":"architecture/dependency-injection/#overview","title":"Overview","text":"<p>The <code>athomic-docs</code> project manages dependencies and object lifecycles without relying on an external dependency injection framework. Instead, it employs a robust and clear approach using a combination of two classic design patterns:</p> <ol> <li>The Facade Pattern: The <code>Athomic</code> class acts as a central service locator, providing a single, unified entry point to all core framework services.</li> <li>The Factory Pattern: Dedicated <code>Factory</code> classes are responsible for creating and managing singleton instances of specific components, handling their unique dependencies internally.</li> </ol> <p>This approach promotes loose coupling, simplifies testing through easy mocking, and makes the flow of dependency resolution explicit and easy to follow.</p>"},{"location":"architecture/dependency-injection/#the-athomic-facade","title":"The <code>Athomic</code> Fa\u00e7ade","text":"<p>The main entry point to the entire framework is the <code>nala.athomic.facade.Athomic</code> class. During application startup, this class is instantiated once. Its constructor (<code>__init__</code>) is responsible for wiring together the primary, high-level services.</p> <pre><code># From: nala.athomic.facade.py\nclass Athomic:\n    def __init__(self, ...):\n        # ...\n        self.settings = settings or get_settings()\n        self.secrets_manager = SecretsManager(self.settings)\n        self.lifecycle_manager = LifecycleManager(...)\n        self.tracer = get_tracer(__name__)\n        self.plugin_manager: PluginManager = PluginManager()\n        # ...\n</code></pre> <p>The <code>nala.athomic.provider</code> module then makes this single <code>Athomic</code> instance globally accessible via the <code>get_athomic_instance()</code> function, allowing any part of the application to access core services like the <code>LifecycleManager</code> or <code>PluginManager</code> when needed.</p>"},{"location":"architecture/dependency-injection/#the-factory-pattern","title":"The Factory Pattern","text":"<p>For most components within the Athomic Layer, a dedicated factory is responsible for its creation. This pattern provides several key advantages:</p> <ul> <li>Centralized Creation Logic: The logic for creating a complex object (e.g., a <code>ConnectionManager</code> that needs <code>DatabaseSettings</code>) is encapsulated in one place.</li> <li>Singleton Management: Factories typically manage a singleton instance of the object they create, ensuring resources like connection pools are shared efficiently.</li> <li>Simplified Dependency Injection: Components that need a dependency can simply call the factory's <code>create()</code> method without needing to know how to construct it.</li> <li>Testability: Factories usually include a <code>.clear()</code> method, which is crucial for resetting state between tests and ensuring test isolation.</li> </ul>"},{"location":"architecture/dependency-injection/#example-flow","title":"Example Flow","text":"<p>A great example is the <code>MongoOutboxRepository</code>. It needs a database connection to work. Here is how its dependencies are resolved:</p> <ol> <li>The application needs an <code>OutboxStorageProtocol</code> instance.</li> <li>It calls <code>OutboxStorageFactory.create()</code>.</li> <li>The <code>OutboxStorageFactory</code> knows it needs a database connection. It calls <code>ConnectionManagerFactory.create()</code>.</li> <li>The <code>ConnectionManagerFactory</code> creates the singleton <code>ConnectionManager</code>.</li> <li>The <code>OutboxStorageFactory</code> then asks the <code>ConnectionManager</code> for the specific database connection (<code>get_document_db(...)</code>).</li> <li>Finally, it injects the database connection into a new <code>MongoOutboxRepository</code> instance and returns it.</li> </ol> <p>This chain of factory calls ensures that dependencies are resolved correctly and only when needed.</p>"},{"location":"architecture/dependency-injection/#api-reference","title":"API Reference","text":""},{"location":"architecture/dependency-injection/#core-facade-and-provider","title":"Core Fa\u00e7ade and Provider","text":""},{"location":"architecture/dependency-injection/#nala.athomic.facade.Athomic","title":"<code>nala.athomic.facade.Athomic</code>","text":"<p>A Fa\u00e7ade that provides a single, simple entry point to all of Athomic's services.</p> <p>This class acts as the main interaction point for an application utilizing the Athomic framework. It manages the initialization, dependency injection, and lifecycle of all infrastructure components, such as secrets management, plugins, and background services.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <code>AppSettings</code> <p>The validated application settings.</p> <code>secrets_manager</code> <code>SecretsManager</code> <p>The manager responsible for resolving secrets.</p> <code>lifecycle_manager</code> <code>LifecycleManager</code> <p>The orchestrator for the service lifecycle.</p> <code>plugin_manager</code> <code>PluginManager</code> <p>The manager for the plugin system.</p> <code>tracer</code> <code>Tracer</code> <p>The OpenTelemetry Tracer instance for this component.</p> <code>observability</code> <p>A namespace for observability components like loggers.</p>"},{"location":"architecture/dependency-injection/#nala.athomic.facade.Athomic.__init__","title":"<code>__init__(domain_initializers_registrar=None, settings=None)</code>","text":"<p>Initializes the Athomic layer.</p> <p>Parameters:</p> Name Type Description Default <code>domain_initializers_registrar</code> <code>Optional[Callable[[], None]]</code> <p>A function from the application layer (e.g., API) that registers all business domain-specific initializers.</p> <code>None</code> <code>settings</code> <code>Optional[AppSettings]</code> <p>An instance of application settings. If not provided, it will be loaded globally. Ideal for dependency injection in tests.</p> <code>None</code>"},{"location":"architecture/dependency-injection/#nala.athomic.facade.Athomic.shutdown","title":"<code>shutdown()</code>  <code>async</code>","text":"<p>Runs the graceful shutdown sequence for all services.</p>"},{"location":"architecture/dependency-injection/#nala.athomic.facade.Athomic.startup","title":"<code>startup()</code>  <code>async</code>","text":"<p>Runs the complete, ordered startup sequence for the application's infrastructure.</p> <p>This method orchestrates the startup of services in the correct dependency order: 1. Resolves all secret references within the configuration. 2. Discovers and loads all available plugins. 3. Calls the 'on_athomic_startup' hook, allowing plugins to initialize. 4. Starts all registered services (e.g., database, messaging).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If any critical step in the startup sequence fails.</p>"},{"location":"architecture/dependency-injection/#nala.athomic.provider","title":"<code>nala.athomic.provider</code>","text":""},{"location":"architecture/dependency-injection/#nala.athomic.provider.clear_athomic_instance","title":"<code>clear_athomic_instance()</code>","text":"<p>For use in tests, to ensure isolation.</p>"},{"location":"architecture/dependency-injection/#nala.athomic.provider.get_athomic_instance","title":"<code>get_athomic_instance()</code>","text":"<p>Returns the singleton instance of the Athomic facade that has already been initialized.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the instance has not yet been created and set.</p>"},{"location":"architecture/dependency-injection/#nala.athomic.provider.set_athomic_instance","title":"<code>set_athomic_instance(instance)</code>","text":"<p>Sets the global singleton instance of the Athomic facade. This function should be called ONLY ONCE during application startup.</p>"},{"location":"architecture/dependency-injection/#example-factories","title":"Example Factories","text":""},{"location":"architecture/dependency-injection/#nala.athomic.database.factory.ConnectionManagerFactory","title":"<code>nala.athomic.database.factory.ConnectionManagerFactory</code>","text":""},{"location":"architecture/dependency-injection/#nala.athomic.database.factory.ConnectionManagerFactory.clear","title":"<code>clear()</code>  <code>async</code> <code>classmethod</code>","text":"<p>Clears the singleton instance of the ConnectionManager.</p>"},{"location":"architecture/dependency-injection/#nala.athomic.database.factory.ConnectionManagerFactory.create","title":"<code>create(settings=None)</code>  <code>classmethod</code>","text":"<p>Creates and returns the singleton instance of the ConnectionManager.</p>"},{"location":"architecture/dependency-injection/#nala.athomic.serializer.factory.SerializerFactory","title":"<code>nala.athomic.serializer.factory.SerializerFactory</code>","text":"<p>Factory for instantiating message serializers based on messaging backend. Falls back to BaseSerializer if no specific implementation is registered. Caches instances by backend name for singleton-like behavior per backend.</p>"},{"location":"architecture/dependency-injection/#nala.athomic.serializer.factory.SerializerFactory.clear","title":"<code>clear()</code>  <code>classmethod</code>","text":"<p>Clears the singleton cache of serializer instances.</p>"},{"location":"architecture/dependency-injection/#nala.athomic.serializer.factory.SerializerFactory.create","title":"<code>create(settings=None)</code>  <code>classmethod</code>","text":"<p>Creates and returns a singleton instance of the configured SerializerProtocol by delegating to a registered creator.</p>"},{"location":"architecture/domain-layer/","title":"The Domain Layer","text":"<p>The Domain Layer is the core of the application. It contains the business logic, rules, and state that are unique to your specific problem domain. This layer is designed to be completely independent of any infrastructure concerns\u2014it knows nothing about databases, web frameworks, or message brokers.</p>"},{"location":"architecture/domain-layer/#key-concepts","title":"Key Concepts","text":"<ul> <li>Entities &amp; Aggregates: These are the primary objects that model your domain. An Aggregate is a cluster of associated objects that we treat as a single unit for data changes.</li> <li>Value Objects: Objects that represent a descriptive aspect of the domain with no conceptual identity.</li> <li>Domain Events: Objects that represent something that happened in the domain. They are often used to communicate changes between different parts of the application without direct coupling.</li> <li>Repositories: Interfaces defined in the Domain Layer that specify the contract for data persistence (e.g., <code>save(user)</code>, <code>find_by_id(user_id)</code>). The implementation of these interfaces lives in the Infrastructure Layer.</li> </ul> <p>By keeping the Domain Layer pure, you ensure that your most critical business logic is easy to test, maintain, and evolve independently of technology choices.</p>"},{"location":"architecture/athomic-layer/","title":"The Athomic Layer","text":"<p>The Athomic Layer is the heart of the <code>athomic-docs</code> project. It is an application chassis that provides a rich set of production-ready, cross-cutting concerns as a service to the rest of the application.</p> <p>Its primary goal is to allow developers to focus on writing business logic (in the Domain and Application layers) without having to repeatedly solve complex infrastructure problems.</p> <p>Athomic provides abstractions and concrete implementations for: -   Data &amp; Persistence (Databases, Caches, Storage) -   Observability (Logging, Tracing, Metrics) -   Security (Secrets, Auth, Crypto) -   Resilience Patterns (Retry, Circuit Breaker, etc.) -   Integrations (Messaging, Tasks, Service Discovery)</p> <p>By using a consistent, protocol-based approach, components within the Athomic Layer are designed to be loosely coupled and easily replaceable or extensible.</p>"},{"location":"backlog/_ice-box/future-backlog/","title":"Future backlog","text":""},{"location":"backlog/_ice-box/future-backlog/#epic-1-documentation-developer-experience-dx","title":"Epic 1: Documentation &amp; Developer Experience (DX)","text":"<p>Goal: To make the framework easy to understand, use, and contribute to, transforming it into a true \"product\" with an excellent developer experience.</p>"},{"location":"backlog/_ice-box/future-backlog/#task-integrate-mkdocs-for-automated-documentation-site","title":"Task: Integrate MkDocs for Automated Documentation Site","text":"<ul> <li>Description: As a developer, I want an automated documentation website generated from our code and markdown files so that I can easily browse and search for information about the <code>Athomic</code> framework and its modules.</li> <li>Acceptance Criteria:</li> <li><code>mkdocs</code>, <code>mkdocs-material</code>, and <code>mkdocstrings</code> are added to the project's development dependencies.</li> <li>A <code>mkdocs.yml</code> configuration file is created at the project root.</li> <li>The <code>mkdocstrings</code> plugin is configured to automatically parse docstrings from the <code>nala.athomic</code> package.</li> <li>A basic landing page (<code>docs/index.md</code>) is created.</li> <li>The command <code>mkdocs serve</code> successfully builds and serves a local version of the site.</li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#task-refine-docstrings-for-core-athomic-components","title":"Task: Refine Docstrings for Core Athomic Components","text":"<ul> <li>Description: As a developer, I want high-quality docstrings for the main <code>Athomic</code> components so that their purpose, parameters, and usage are clear directly from the code and in the generated documentation.</li> <li>Acceptance Criteria:</li> <li>The <code>Athomic</code> facade class (<code>nala/athomic/facade.py</code>) has a complete Google-style docstring explaining its role as the central entry point.</li> <li>The <code>LifecycleManager</code> and <code>SecretsManager</code> classes have complete docstrings detailing their responsibilities.</li> <li>The generated MkDocs site correctly renders these new docstrings on their respective pages.</li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#10-simulacao-de-carga-com-dados-realistas","title":"10. Simula\u00e7\u00e3o de Carga com Dados Realistas","text":"<ul> <li>Descri\u00e7\u00e3o: Testes de carga com perfis reais de uso.</li> <li>Objetivo: Garantir performance em cen\u00e1rios reais.</li> <li>A\u00e7\u00e3o Sugerida: Definir datasets e automa\u00e7\u00f5es com Locust ou Artillery.</li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#3-auditoria-e-logs-de-atividades","title":"3. Auditoria e Logs de Atividades","text":"<ul> <li>Descri\u00e7\u00e3o: Registro de a\u00e7\u00f5es sens\u00edveis como login, falhas e altera\u00e7\u00f5es.</li> <li>Objetivo: Conformidade e rastreamento de comportamento suspeito.</li> <li>A\u00e7\u00e3o Sugerida: Criar camada <code>audit_log</code> com masking e persist\u00eancia.</li> </ul> <ol> <li>[ ] Estrat\u00e9gia de Versionamento da API<ul> <li>Descri\u00e7\u00e3o: Definir e documentar a estrat\u00e9gia oficial para versionar endpoints da API (ex: via path <code>/v1/</code>, header <code>Accept</code>, etc.) para gerenciar breaking changes.</li> <li>Justificativa: Essencial para a evolu\u00e7\u00e3o sustent\u00e1vel de APIs p\u00fablicas ou usadas por m\u00faltiplos clientes.</li> <li>Tags: <code>[gap-api]</code>, <code>[process]</code></li> <li>Prioridade: <code>[m\u00e9dia]</code></li> </ul> </li> </ol> <ol> <li>[ ] Mecanismo de Idempot\u00eancia para Requisi\u00e7\u00f5es<ul> <li>Descri\u00e7\u00e3o: Implementar um mecanismo (ex: middleware que respeita o header <code>Idempotency-Key</code>) para garantir que opera\u00e7\u00f5es de escrita possam ser seguramente retentadas pelos clientes.</li> <li>Justificativa: Aumentar a resili\u00eancia e a confiabilidade de opera\u00e7\u00f5es cr\u00edticas que podem sofrer falhas de rede.</li> <li>Tags: <code>[gap-api]</code>, <code>[resilience]</code></li> <li>Prioridade: <code>[m\u00e9dia]</code></li> </ul> </li> </ol>"},{"location":"backlog/_ice-box/future-backlog/#7-geracao-e-validacao-de-openapi","title":"7. Gera\u00e7\u00e3o e Valida\u00e7\u00e3o de OpenAPI","text":"<ul> <li>Descri\u00e7\u00e3o: Gera\u00e7\u00e3o autom\u00e1tica e valida\u00e7\u00e3o do schema Swagger/OpenAPI.</li> <li>Objetivo: Garantir contrato da API consistente e version\u00e1vel.</li> <li>Sugest\u00e3o: Adicionar task no CI/CD para gerar e validar schema OpenAPI.</li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#8-suporte-opcional-a-graphql","title":"8. Suporte opcional a GraphQL","text":"<ul> <li>Descri\u00e7\u00e3o: Interface alternativa ao REST, com queries flex\u00edveis.</li> <li>Objetivo: Melhor experi\u00eancia para frontends ricos ou clientes exigentes.</li> <li>A\u00e7\u00e3o Sugerida: Gateway opcional com Strawberry ou Graphene.</li> </ul> <ol> <li>[ ] Suporte a Comunica\u00e7\u00e3o Real-time (WebSockets/SSE)<ul> <li>Descri\u00e7\u00e3o: Avaliar e potencialmente adicionar suporte ou abstra\u00e7\u00f5es para WebSockets ou Server-Sent Events na camada <code>api</code>.</li> <li>Justificativa: Habilitar funcionalidades interativas em tempo real, se necess\u00e1rio para aplica\u00e7\u00f5es que usam o core.</li> <li>Tags: <code>[gap-api]</code>, <code>[feature]</code></li> <li>Prioridade: <code>[baixa]</code> (Depende da necessidade)</li> </ul> </li> </ol>"},{"location":"backlog/_ice-box/future-backlog/#infraestrutura-e-comportamento-dinamico","title":"\u2699\ufe0f Infraestrutura e Comportamento Din\u00e2mico","text":""},{"location":"backlog/_ice-box/future-backlog/#4-politica-de-retencao-e-expurgo-de-dados","title":"4. Pol\u00edtica de Reten\u00e7\u00e3o e Expurgo de Dados","text":"<ul> <li>Descri\u00e7\u00e3o: Defini\u00e7\u00e3o de TTLs e estrat\u00e9gias de limpeza.</li> <li>Objetivo: Evitar ac\u00famulo desnecess\u00e1rio e reduzir custos.</li> <li>A\u00e7\u00e3o Sugerida: Adicionar geradores de expurgo e TTL configur\u00e1veis por dom\u00ednio.</li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#5-testes-de-chaos-engineering","title":"5. Testes de Chaos Engineering","text":"<ul> <li>Descri\u00e7\u00e3o: Simular falhas de rede, indisponibilidade ou lentid\u00e3o.</li> <li>Objetivo: Validar resili\u00eancia em ambiente realista.</li> <li>A\u00e7\u00e3o Sugerida: Integrar <code>chaostoolkit</code> ou criar mocks injet\u00e1veis no CI.</li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#processos-metodologias","title":"Processos &amp; Metodologias","text":"<ol> <li>[ ] Padr\u00e3o para Valida\u00e7\u00e3o de Regras de Neg\u00f3cio Complexas<ul> <li>Descri\u00e7\u00e3o: Documentar ou definir um padr\u00e3o recomendado para implementar valida\u00e7\u00f5es de regras de neg\u00f3cio que v\u00e3o al\u00e9m da valida\u00e7\u00e3o de schema do Pydantic (ex: na camada de servi\u00e7o).</li> <li>Justificativa: Garantir consist\u00eancia e manutenibilidade na l\u00f3gica de valida\u00e7\u00e3o mais complexa.</li> <li>Tags: <code>[gap-process]</code>, <code>[docs]</code></li> <li>Prioridade: <code>[baixa]</code></li> </ul> </li> </ol>"},{"location":"backlog/_ice-box/future-backlog/#idea","title":"[idea]","text":"<ul> <li>[ ] Endpoint de debug mostrando quais secrets est\u00e3o sendo usados e de onde v\u00eam (com m\u00e1scara). [m\u00e9dia]</li> <li>[ ] Plug de audit trail para acessos a secrets. [baixa]</li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#feature","title":"[feature]","text":"<ul> <li>[ ] Suporte a criptografia com GCP KMS e AWS KMS. [baixa]</li> <li>[ ] Interface para rota\u00e7\u00e3o ativa (ex: tokens do Vault ou certificados PKI). [m\u00e9dia]</li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#a-fazer","title":"\ud83e\uddea A fazer","text":"<ul> <li>[ ] Auto-reconex\u00e3o com Consul se ele estiver offline e voltar</li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#iac-infraestrutura-como-codigo","title":"IaC (Infraestrutura como C\u00f3digo)","text":"<ul> <li>[ ] permitir criar toda a estrutura localmente ou em outro host usando Pulumi</li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#estrategia-e-arquitetura","title":"\ud83e\udde0 Estrat\u00e9gia e Arquitetura","text":""},{"location":"backlog/_ice-box/future-backlog/#9-orquestracao-de-workflows","title":"9. Orquestra\u00e7\u00e3o de Workflows","text":"<ul> <li>Descri\u00e7\u00e3o: Executar fluxos de neg\u00f3cio em etapas, com controle.</li> <li>Objetivo: Facilitar automa\u00e7\u00f5es e integra\u00e7\u00e3o de sistemas externos.</li> <li>A\u00e7\u00e3o Sugerida: Criar mini engine ou integrar com Prefect / Temporal.</li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#6-data-validation-e-transformation-pipelines","title":"6. Data Validation e Transformation Pipelines","text":"<p>O que \u00e9: Pipeline padronizado para validar, transformar e limpar dados.</p> <p>Por que adicionar ao athomic: - Centraliza\u00e7\u00e3o de regras de valida\u00e7\u00e3o. - Reutiliza\u00e7\u00e3o de l\u00f3gica. - Garantia de conformidade (ex: GDPR).</p> <p>Tecnologias/Abordagem: - Interfaces: <code>Validator</code>, <code>Transformer</code>. - Pipeline modular. - Integra\u00e7\u00e3o com FastAPI ou servi\u00e7os.</p>"},{"location":"backlog/_ice-box/future-backlog/#para-o-front","title":"Para o FRONT","text":"<ul> <li>[ ] Permitir light/dark mode por defaul e configur\u00e1vel </li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#seguranca","title":"Seguran\u00e7a","text":"<ul> <li>integra\u00e7\u00e3o com iam google e outros providers</li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#ai","title":"ai","text":"<ul> <li>[ ] Arquitetura do projeto</li> <li>[ ] Gerar documentos \"how to\" para agents executarem tarefas no c\u00f3digo</li> <li>[ ] Evoluir o agente de IA com base no nala-core-agent-instructions.md: Permitir que ele leia configura\u00e7\u00f5es e sugira melhorias automaticamente por m\u00f3dulo.</li> </ul> <ul> <li>Data Sourcers - store, drive, collectors, etc</li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#otimizar-imports","title":"Otimizar imports","text":"<ul> <li>[ ] Permitir importar nala-athomic-google vai usar a stack google e n\u00e3o vai importar outras libs com azure e aws         ex: imports de instrumentantors do opentelemetry</li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#seguranca-athomicsecurity","title":"\ud83d\udd10 Seguran\u00e7a (<code>athomic/security</code>)","text":""},{"location":"backlog/_ice-box/future-backlog/#autenticacaoautorizacao","title":"\ud83d\udd39 Autentica\u00e7\u00e3o/Autoriza\u00e7\u00e3o","text":"<ul> <li>[TODO] Implementar outros providers (OAuth2, IdPs, API Keys). (Alta)</li> <li>[TODO] Centralizar controle de acesso com <code>AccessManager</code>.</li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#inteligencia-artificial-agentes","title":"\ud83e\udd16 Intelig\u00eancia Artificial (Agentes)","text":""},{"location":"backlog/_ice-box/future-backlog/#agent-dev","title":"\ud83d\udd39 Agent Dev","text":"<ul> <li>[TODO] Integra\u00e7\u00e3o com GitHub API (PRs).</li> <li>[TODO] Suporte a gera\u00e7\u00e3o de ADRs.</li> <li>[TODO] Intera\u00e7\u00e3o com Poetry (libs).</li> <li>[TODO] An\u00e1lise de logs e sugest\u00f5es.</li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#aiaas","title":"\ud83d\udd39 AIaaS","text":"<ul> <li>[TODO] Implementar <code>insights_engine</code>, <code>anomaly_detector</code>, <code>suggestion_engine</code>.</li> <li>[TODO] Definir contrato <code>IAgent</code>.</li> <li>[TODO] Implementar <code>llm_gateway</code>.</li> <li>[TODO] Criar <code>usage_logger</code> e <code>audit_trail</code>.</li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#visao-de-futuro-o-proximo-grande-passo","title":"Vis\u00e3o de Futuro: O Pr\u00f3ximo Grande Passo","text":"<p>Se voc\u00ea quiser pensar na pr\u00f3xima grande feature para o Athomic, a sugest\u00e3o mais natural seria:</p>"},{"location":"backlog/_ice-box/future-backlog/#enriquecer-a-documentacao-para-contribuintes","title":"Enriquecer a Documenta\u00e7\u00e3o para Contribuintes:","text":"<p>Problema: A documenta\u00e7\u00e3o est\u00e1 excelente em termos de decis\u00f5es de arquitetura (ADRs), mas pode ser enriquecida com guias pr\u00e1ticos.</p> <p>Sugest\u00e3o: Crie um \"Cookbook\" na pasta docs/ com receitas para tarefas comuns, como:</p> <p>\"Como adicionar um novo provedor de secrets\"</p> <p>\"Como proteger um novo endpoint com rate limiting\"</p> <p>\"Como emitir uma nova m\u00e9trica customizada\" Isso reduzir\u00e1 a curva de aprendizado e acelerar\u00e1 o desenvolvimento.</p>"},{"location":"backlog/_ice-box/future-backlog/#4-interface-de-linha-de-comando-cli","title":"4. Interface de Linha de Comando (CLI)","text":"<p>Um framework maduro oferece ferramentas para o desenvolvedor. Uma CLI para gerenciar a aplica\u00e7\u00e3o \u00e9 uma adi\u00e7\u00e3o de alt\u00edssimo valor.</p> <p>O Problema: Tarefas operacionais como iniciar um worker espec\u00edfico, rodar migra\u00e7\u00f5es de banco de dados, limpar um cache ou popular dados de teste s\u00e3o feitas com comandos manuais e dispersos.</p> <p>A Solu\u00e7\u00e3o athomic: Criar um ponto de entrada de linha de comando (manage.py ou nala) usando bibliotecas como Typer ou Click.</p> <p>Como Implementar:</p> <p>Escolha da Ferramenta: Typer \u00e9 uma excelente escolha, pois \u00e9 do mesmo criador do FastAPI e tem uma integra\u00e7\u00e3o muito natural.</p> <p>Estrutura de Comandos: Crie comandos para tarefas comuns:</p> <p>nala run-api: Inicia o servidor Uvicorn.</p> <p>nala run-worker messaging: Inicia o MessagingLifecycleManager.</p> <p>nala run-worker outbox: Inicia o OutboxPublisher.</p> <p>nala db-migrate: (Se usar SQL no futuro) Roda migra\u00e7\u00f5es.</p> <p>nala cache-clear: Limpa o cache do Redis.</p> <p>Benef\u00edcios:</p> <p>Developer Experience (DX): Melhora drasticamente a experi\u00eancia do desenvolvedor, centralizando todas as opera\u00e7\u00f5es em um \u00fanico lugar.</p> <p>Consist\u00eancia Operacional: Garante que as tarefas sejam executadas da mesma forma em desenvolvimento, CI/CD e produ\u00e7\u00e3o.</p>"},{"location":"backlog/_ice-box/future-backlog/#consistencia-no-tratamento-de-excecoes-na-api","title":"Consist\u00eancia no Tratamento de Exce\u00e7\u00f5es na API:","text":"<p>Observa\u00e7\u00e3o: O projeto define exce\u00e7\u00f5es customizadas (StorageOperationError , SecretNotFoundError ), mas o middleware principal (main.py ) registra um exception_handler apenas para HTTPException.</p> <p>Recomenda\u00e7\u00e3o: Implementar handlers de exce\u00e7\u00e3o na camada da API (usando app.exception_handler) que capturem as exce\u00e7\u00f5es customizadas da camada athomic (ex: ObjectNotFoundError) e as convertam em respostas HTTP apropriadas (ex: status 404). Isso centraliza o tratamento de erros e mant\u00e9m a camada athomic agn\u00f3stica ao HTTP.</p> <ol> <li>Criar uma API de Gest\u00e3o da DLQ A Melhoria: Construir uma pequena API (ou endpoints de administra\u00e7\u00e3o) para visualizar e gerir as mensagens que est\u00e3o na DLQ.</li> </ol> <p>O Problema que Resolve: Quando uma mensagem vai para a DLQ, significa que uma interven\u00e7\u00e3o manual \u00e9 necess\u00e1ria. Atualmente, a \u00fanica forma de saber o que est\u00e1 na DLQ \u00e9 consumir as mensagens com uma ferramenta de linha de comando do Kafka e olhar para os logs. Este processo \u00e9 manual e propenso a erros.</p> <p>Como Implementar:</p> <p>Criar Endpoints de Admin: Adicione endpoints seguros \u00e0 sua API, por exemplo:</p> <p>GET /admin/dlq/messages: Lista as mensagens na DLQ (com pagina\u00e7\u00e3o).</p> <p>GET /admin/dlq/messages/{message_id}: Mostra o conte\u00fado detalhado de uma mensagem espec\u00edfica.</p> <p>POST /admin/dlq/messages/{message_id}/republish: Reenvia uma mensagem espec\u00edfica para o seu t\u00f3pico original para ser reprocessada (\u00fatil ap\u00f3s a corre\u00e7\u00e3o de um bug).</p> <p>DELETE /admin/dlq/messages/{message_id}: Remove uma mensagem da DLQ.</p> <p>L\u00f3gica do Servi\u00e7o: O servi\u00e7o por tr\u00e1s destes endpoints usaria um consumidor Kafka configurado para ler do t\u00f3pico da DLQ \"sob demanda\", e um produtor para a a\u00e7\u00e3o de republicar.</p> <p>Resultado: A sua equipa de opera\u00e7\u00f5es (ou de desenvolvimento) ganha uma ferramenta poderosa para gerir falhas de produ\u00e7\u00e3o de forma r\u00e1pida e segura, sem precisar de acesso direto ao broker Kafka.</p>"},{"location":"backlog/_ice-box/future-backlog/#nala-framework-evolution-backlog","title":"Nala Framework Evolution - Backlog","text":"<p>This backlog outlines the next architectural and quality improvements for the Nala framework. Each epic represents a major area of focus, broken down into actionable tasks.</p>"},{"location":"backlog/_ice-box/future-backlog/#epic-2-code-quality-automation","title":"Epic 2: Code Quality &amp; Automation","text":"<p>Goal: To implement objective metrics and automated checks that guarantee the code's quality, robustness, and maintainability, catching issues before they reach production.</p>"},{"location":"backlog/_ice-box/future-backlog/#task-integrate-pytest-cov-for-code-coverage-analysis","title":"Task: Integrate Pytest-Cov for Code Coverage Analysis","text":"<ul> <li>Description: As an architect, I want to measure test coverage to objectively identify untested code paths, ensure the framework's robustness, and guide future test writing.</li> <li>Acceptance Criteria:</li> <li><code>pytest-cov</code> is added to the project's development dependencies.</li> <li>The <code>pyproject.toml</code> (or <code>pytest.ini</code>) is configured to run coverage analysis during tests.</li> <li>Configuration includes branch coverage (<code>--cov-branch</code>).</li> <li>A coverage failure threshold is set to <code>90%</code>.</li> <li>The CI/CD pipeline is updated to run tests with the <code>--cov</code> flag and to fail the build if the coverage drops below the defined threshold.</li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#task-configure-and-enforce-strict-static-analysis","title":"Task: Configure and Enforce Strict Static Analysis","text":"<ul> <li>Description: As a developer, I want automated static analysis tools to catch potential bugs, enforce a consistent code style, and guarantee type safety before my code is merged.</li> <li>Acceptance Criteria:</li> <li><code>ruff</code> and <code>mypy</code> are added to the project's development dependencies.</li> <li>A <code>ruff.toml</code> file is created with a comprehensive ruleset, including rules from <code>pydantic</code>, <code>flake8</code>, <code>isort</code>, and <code>pyupgrade</code>.</li> <li><code>mypy</code> is configured in <code>pyproject.toml</code> or <code>mypy.ini</code> to run in <code>--strict</code> mode.</li> <li>The CI/CD pipeline is updated to include two new steps: <code>ruff check .</code> and <code>mypy .</code>.</li> <li>The build will fail if either <code>ruff</code> or <code>mypy</code> reports any errors.</li> </ul>"},{"location":"backlog/_ice-box/future-backlog/#epic-3-architectural-refactoring","title":"Epic 3: Architectural Refactoring","text":"<p>Goal: To refine the internal architecture of the framework, eliminating code smells and improving adherence to SOLID principles, thereby increasing modularity and long-term maintainability.</p>"},{"location":"backlog/_ice-box/future-backlog/#task-refactor-serializerfactory-to-remove-circular-dependency","title":"Task: Refactor SerializerFactory to Remove Circular Dependency","text":"<ul> <li>Description: As an architect, I want to refactor the <code>SerializerFactory</code> and its registry to eliminate the circular import workaround. This will make the dependency graph cleaner and align the code more strictly with the Dependency Inversion Principle.</li> <li>Acceptance Criteria:</li> <li>The <code>from nala.athomic.serializer.registry import serializer_registry</code> import is removed from inside the <code>create</code> method of <code>SerializerFactory</code>.</li> <li>The application starts and functions correctly without any <code>CircularImport</code> errors.</li> <li>All unit and integration tests related to serialization and messaging continue to pass without modification.</li> <li>Tech Notes:</li> <li>The proposed solution is to use Dependency Injection.</li> <li>The <code>LifecycleManager</code> or a new <code>SerializerInitializer</code> service should be responsible for instantiating both the <code>SerializerFactory</code> and the <code>SerializerRegistry</code>.</li> <li>After instantiation, the manager will inject the registry instance into the factory instance via a new method, e.g., <code>factory.set_registry(registry)</code>. This breaks the compile-time dependency cycle.</li> <li> <p>For example, in <code>SerializerFactory</code>:   ```python   class SerializerFactory:       def init(self):           self._registry = None</p> <p>def set_registry(self, registry: SerializerRegistry):       self._registry = registry</p> <p>def create(self, ...):       # Now uses self._registry without a local import       provider_class = self._registry.get(...)       # ...</p> </li> </ul>"},{"location":"backlog/_ice-box/future-world-first-class-messaging/","title":"Backlog de Evolu\u00e7\u00e3o: M\u00f3dulo de Mensageria nala/athomic","text":"<p>Este documento descreve o backlog de funcionalidades avan\u00e7adas propostas para o m\u00f3dulo de mensageria agn\u00f3stico. O objetivo \u00e9 guiar o desenvolvimento de novas capacidades que solidificar\u00e3o o <code>athomic</code> como uma plataforma de microsservi\u00e7os de elite.</p>"},{"location":"backlog/_ice-box/future-world-first-class-messaging/#prioridades","title":"Prioridades","text":"<ul> <li>P1 - Alt\u00edssima: Features com alt\u00edssimo valor de neg\u00f3cio ou que resolvem um problema de seguran\u00e7a cr\u00edtico.</li> <li>P2 - Alta: Features com alto valor de neg\u00f3cio, de robustez ou de experi\u00eancia do desenvolvedor (DX).</li> </ul>"},{"location":"backlog/_ice-box/future-world-first-class-messaging/#backlog-de-epicos","title":"Backlog de \u00c9picos","text":""},{"location":"backlog/_ice-box/future-world-first-class-messaging/#1-inteligencia-e-autonomia-do-sistema","title":"1. Intelig\u00eancia e Autonomia do Sistema","text":"<p>Esta categoria de funcionalidades visa tornar o sistema de mensageria mais aut\u00f4nomo e auto-regul\u00e1vel, movendo-o de um transportador de mensagens passivo para um gerenciador de fluxo ativo.</p>"},{"location":"backlog/_ice-box/future-world-first-class-messaging/#12-filas-de-prioridade","title":"1.2. Filas de Prioridade","text":"<ul> <li>O qu\u00ea? A capacidade de processar mensagens cr\u00edticas (ex: \"reset de senha\") antes de mensagens de baixa prioridade (ex: \"enviar newsletter\"), independentemente da ordem de chegada.</li> <li>Por qu\u00ea? Para garantir que opera\u00e7\u00f5es de neg\u00f3cio essenciais e sens\u00edveis ao tempo n\u00e3o sejam atrasadas por um grande volume de tarefas de background menos importantes.</li> <li>Proposta para o <code>athomic</code>:<ol> <li>Estender o <code>ProducerProtocol</code> para achave, back pra aceitar um par\u00e2metro opcional <code>priority: int</code>.</li> <li>O <code>KafkaProducer</code> implementaria isso publicando mensagens em t\u00f3picos fisicamente separados com base na prioridade (ex: <code>meu-topico.high</code>, <code>meu-topico.medium</code>, <code>meu-topico.low</code>).</li> <li>O <code>MessagingLifecycleManager</code> seria aprimorado para alocar mais inst\u00e2ncias de consumidores ou recursos para os t\u00f3picos de maior prioridade.</li> </ol> </li> </ul>"},{"location":"backlog/_ice-box/future-world-first-class-messaging/#2-padroes-avancados-de-fluxo-de-dados","title":"2. Padr\u00f5es Avan\u00e7ados de Fluxo de Dados","text":"<p>Estas funcionalidades movem o foco do processamento de mensagens individuais para o gerenciamento de fluxos de eventos e transa\u00e7\u00f5es de neg\u00f3cio complexas.</p>"},{"location":"backlog/_ice-box/future-world-first-class-messaging/#21-processamento-de-streams-com-estado-stateful-stream-processing","title":"2.1. Processamento de Streams com Estado (Stateful Stream Processing)","text":"<ul> <li>O qu\u00ea? A capacidade de realizar opera\u00e7\u00f5es de agrega\u00e7\u00e3o (contagens, m\u00e9dias, etc.) sobre janelas de tempo de eventos, mantendo o estado entre as mensagens.</li> <li>Por qu\u00ea? Habilita casos de uso avan\u00e7ados como detec\u00e7\u00e3o de fraude em tempo real (\"3 tentativas de login falhas do mesmo IP em 10 segundos\"), dashboards de analytics ao vivo e sistemas de recomenda\u00e7\u00e3o din\u00e2micos.</li> <li>Proposta para o <code>athomic</code>:<ol> <li>Criar um novo decorador <code>@StreamProcessor</code> que permita definir uma janela (<code>Window.tumbling(seconds=10)</code>) e uma chave de agrupamento (<code>group_by=\"user_id\"</code>).</li> <li>O framework gerenciaria o estado da janela, utilizando o <code>KVStoreProtocol</code> (com um backend como RocksDB ou Redis) para persistir as agrega\u00e7\u00f5es.</li> <li>O handler do desenvolvedor receberia a chave da janela e a lista de eventos, focando apenas na l\u00f3gica de neg\u00f3cio.</li> </ol> </li> </ul>"},{"location":"backlog/_ice-box/future-world-first-class-messaging/#22-padrao-event-sourcing","title":"2.2. Padr\u00e3o Event Sourcing","text":"<ul> <li>O qu\u00ea? Um padr\u00e3o arquitetural onde o estado de uma entidade n\u00e3o \u00e9 armazenado como um registro final, mas como uma sequ\u00eancia imut\u00e1vel de eventos que o afetaram.</li> <li>Por qu\u00ea? Fornece uma trilha de auditoria completa e \u00e0 prova de adultera\u00e7\u00e3o, permite reconstruir o estado em qualquer ponto no tempo e desacopla a escrita da leitura, permitindo m\u00faltiplas proje\u00e7\u00f5es de dados (views).</li> <li>Proposta para o <code>athomic</code>:<ol> <li>Criar uma classe base <code>EventSourcedRepository</code>.</li> <li>Ao salvar, em vez de um <code>UPDATE</code>, o reposit\u00f3rio faria um <code>APPEND</code> de um novo evento a um \"log de eventos\" (um t\u00f3pico Kafka configurado com reten\u00e7\u00e3o infinita).</li> <li>Ao carregar, o reposit\u00f3rio leria os eventos e os aplicaria para reconstruir o estado da entidade, utilizando o <code>KVStoreProtocol</code> para armazenar snapshots peri\u00f3dicos e otimizar o tempo de leitura.</li> </ol> </li> </ul>"},{"location":"backlog/_ice-box/future-world-first-class-messaging/#4-o-problema-aberto-consistencia-causal","title":"4. O Problema Aberto: Consist\u00eancia Causal","text":"<p>Esta \u00e9 a fronteira te\u00f3rica da computa\u00e7\u00e3o distribu\u00edda, abordando o desafio fundamental de garantir a ordem em sistemas ass\u00edncronos.</p>"},{"location":"backlog/_ice-box/future-world-first-class-messaging/#41-garantindo-a-ordem-causal-com-relogios-logicos","title":"4.1. Garantindo a Ordem Causal com Rel\u00f3gios L\u00f3gicos","text":"<ul> <li>O qu\u00ea? Garantir que, se o Evento A causou o Evento B, todos os servi\u00e7os no sistema processem A antes de B, mesmo que as mensagens cheguem fora de ordem.</li> <li>Por qu\u00ea? Previne bugs de consist\u00eancia de dados extremamente sutis e dif\u00edceis de reproduzir, que surgem de condi\u00e7\u00f5es de corrida entre eventos relacionados em diferentes servi\u00e7os.</li> <li>Proposta para o <code>athomic</code>:<ol> <li>Implementar um mecanismo de Vector Clocks ou Lamport Timestamps.</li> <li>Criar um middleware para o <code>Producer</code> que automaticamente carimba os headers de cada mensagem com o \"tempo l\u00f3gico\" do servi\u00e7o produtor.</li> <li>Criar um middleware no <code>MessageProcessor</code> do consumidor que l\u00ea esses carimbos. O processador poderia pausar o processamento de uma mensagem se detectasse que uma mensagem causalmente anterior (de acordo com o rel\u00f3gio l\u00f3gico) ainda n\u00e3o foi processada, garantindo a ordem causal.</li> </ol> </li> </ul>"},{"location":"backlog/_ice-box/future-world-first-class-messaging/#cheats-praticos-ganhos-rapidos","title":"Cheats pr\u00e1ticos (ganhos r\u00e1pidos)\u26a0\ufe0f","text":"<p>Troque todos json.dumps/loads por orjson no hot path.</p> <p>Pare de re-instanciar AIOKafkaProducer. 1 por processo.</p> <p>Remova middlewares e depend\u00eancias \u201cgen\u00e9ricas\u201d da rota quente; use Depends m\u00ednimos.</p> <p>Use Response e j\u00e1 passe o bytes pronto quando poss\u00edvel.</p> <p>Estruture logs em DEBUG OFF em prod; use sampling para INFO/TRACE.</p>"},{"location":"backlog/_ice-box/future-world-first-class-messaging/#gerenciamento-do-waiting-room-e-risco-de-acumulo-de-memoriaestado","title":"Gerenciamento do \"Waiting Room\" e Risco de Ac\u00famulo de Mem\u00f3ria/Estado:","text":"<p>Quest\u00e3o: Se um \u00fanico evento (ex: sequence_id = 5) se perder ou for extremamente atrasado, todos os eventos subsequentes para aquele aggregate_key (6, 7, 8, ...) ficar\u00e3o retidos na \"sala de espera\" . Como vamos lidar com isso para evitar que a sala de espera cres\u00e7a indefinidamente?</p> <p>Proposta para Debate:</p> <p>TTL no Buffer: Poder\u00edamos definir um TTL (Time-To-Live) para os eventos na sala de espera (ex: 24 horas). Se o evento 5 n\u00e3o chegar dentro desse tempo, o sistema poderia descartar o 5 e todos os eventos subsequentes que dependem dele, talvez movendo-os para uma \"fila de eventos \u00f3rf\u00e3os\" ou simplesmente logando um erro cr\u00edtico.</p> <p>Tamanho M\u00e1ximo do Buffer: Poder\u00edamos impor um limite m\u00e1ximo de eventos em espera por aggregate_key. Ao atingir esse limite, a estrat\u00e9gia poderia ser bloquear o processamento ou descartar o evento mais antigo/novo.</p>"},{"location":"backlog/_ice-box/future-world-first-class-messaging/#atomicidade-na-atualizacao-do-consumidor","title":"Atomicidade na Atualiza\u00e7\u00e3o do Consumidor:","text":"<p>Quest\u00e3o: O fluxo de trabalho do consumidor ordenado \u00e9:</p> <p>Ler a mensagem N.</p> <p>Processar a l\u00f3gica de neg\u00f3cio.</p> <p>Atualizar o last_processed_sequence no KVStore para N.</p> <p>Verificar a \"sala de espera\" por N+1.</p> <p>E se o servi\u00e7o do consumidor falhar entre os passos 2 e 3? A l\u00f3gica de neg\u00f3cio foi executada, mas o estado da sequ\u00eancia n\u00e3o foi atualizado. Na pr\u00f3xima vez que o evento N for reentregue pelo broker, ele ser\u00e1 processado novamente, quebrando a garantia de \"exactly-once processing\".</p> <p>Proposta para Debate:</p> <p>Transa\u00e7\u00e3o de Duas Fases (Emulada): O @ordered_consumer poderia envolver a l\u00f3gica de neg\u00f3cio e a atualiza\u00e7\u00e3o do last_processed_sequence em uma opera\u00e7\u00e3o \"quase-transacional\". Por exemplo, poder\u00edamos usar o padr\u00e3o Transactional Outbox no pr\u00f3prio consumidor para garantir que a atualiza\u00e7\u00e3o do estado da sequ\u00eancia seja at\u00f4mica com a execu\u00e7\u00e3o da l\u00f3gica de neg\u00f3cio. Isso adiciona complexidade, mas oferece a garantia mais forte.</p> <p>Idempot\u00eancia no Handler: Uma alternativa mais simples \u00e9 exigir que a l\u00f3gica de neg\u00f3cio dentro do @ordered_consumer seja idempotente. Se o evento N for reprocessado, ele n\u00e3o deve causar efeitos colaterais indesejados. Podemos refor\u00e7ar isso na documenta\u00e7\u00e3o ou at\u00e9 mesmo fornecer helpers de idempot\u00eancia.</p>"},{"location":"backlog/_ice-box/future-world-first-class-messaging/#observabilidade-da-sala-de-espera","title":"Observabilidade da Sala de Espera:","text":"<p>Quest\u00e3o: Como vamos monitorar a sa\u00fade da \"sala de espera\"? Um aggregate_key com um buffer em constante crescimento \u00e9 um forte indicador de um problema.</p> <p>Proposta para Debate:</p> <p>M\u00e9tricas Dedicadas: Expor m\u00e9tricas do Prometheus como: athomic_ordered_consumer_waiting_room_size (um Gauge com aggregate_key como label) e athomic_ordered_consumer_events_discarded_total (um Counter). Isso permitiria a cria\u00e7\u00e3o de alertas para \"salas de espera\" que est\u00e3o crescendo perigosamente.</p>"},{"location":"backlog/_ice-box/future-world-first-class-messaging/#architectural-evolution-roadmap-scaling-nala-for-future-challenges","title":"Architectural Evolution Roadmap: Scaling Nala for Future Challenges","text":"<p>This roadmap outlines potential evolutionary paths for the Nala architecture. Each horizon builds upon the last, addressing new scales of complexity in distributed systems, data management, and operational resilience.</p>"},{"location":"backlog/_ice-box/future-world-first-class-messaging/#horizon-1-solidify-the-foundation-scale-the-paved-road-next-3-6-months","title":"Horizon 1: Solidify the Foundation &amp; Scale the \"Paved Road\" (Next 3-6 Months)","text":"<p>Focus: Mature the existing architecture and scale its adoption.</p> <ul> <li>1.1. Internal Developer Platform (IDP) Formalization:<ul> <li>Action: Create a <code>cookiecutter</code> template from the current project structure.</li> <li>Goal: Allow developers to bootstrap new, fully-configured services based on the <code>athomic</code> framework in minutes, ensuring consistency and best practices are followed from day one.</li> </ul> </li> </ul>"},{"location":"backlog/_ice-box/future-world-first-class-messaging/#horizon-2-scale-for-distributed-systems-complexity-6-18-months","title":"Horizon 2: Scale for Distributed Systems Complexity (6-18 Months)","text":"<p>Focus: Evolve from a single-service framework to a multi-service ecosystem.</p> <ul> <li> <p>2.1. Adopt Durable Execution for Complex Workflows:</p> <ul> <li>Action: Integrate a durable execution engine like Temporal.io for orchestrating complex, long-running, and mission-critical business processes (e.g., order fulfillment, user onboarding).</li> <li>Goal: Replace complex, stateful message consumers and saga patterns with simple, testable workflow code, drastically improving the resilience and visibility of asynchronous operations. This is the natural evolution of the <code>Outbox</code> and <code>Tasks</code> patterns.</li> </ul> </li> <li> <p>2.2. Externalize Cross-Cutting Concerns with a Service Mesh:</p> <ul> <li>Action: As the number of services grows, begin migrating network-level concerns (retries, circuit breakers, mTLS, traffic shifting) from <code>athomic</code> to a service mesh like Istio or Linkerd.</li> <li>Goal: Simplify the application code by offloading network reliability to the infrastructure layer. This makes <code>athomic</code> leaner and allows for language-agnostic policy enforcement across the entire microservices fleet.</li> </ul> </li> </ul>"},{"location":"backlog/_ice-box/future-world-first-class-messaging/#horizon-3-embrace-the-event-native-paradigm-18-months","title":"Horizon 3: Embrace the Event-Native Paradigm (18+ Months)","text":"<p>Focus: Transition to a data model optimized for auditability, flexibility, and time-travel.*</p> <ul> <li> <p>3.1. Implement CQRS &amp; Event Sourcing for Key Domains:</p> <ul> <li>Action: For new, critical business domains, adopt a full Event Sourcing (ES) and CQRS (Command Query Responsibility Segregation) pattern. Store all changes as an immutable sequence of events.</li> <li>Goal: Achieve perfect auditability, gain the ability to create new read models (projections) from historical events without data migration, and enable powerful temporal queries. This is the ultimate evolution for a truly event-driven system.</li> </ul> </li> <li> <p>3.2. Data Mesh &amp; Decentralized Ownership:</p> <ul> <li>Action: Treat data as a product. Each business domain becomes responsible for its own data pipelines and for exposing its data via well-defined APIs and event streams.</li> <li>Goal: Move away from a centralized data warehouse bottleneck and empower domains to evolve their data products independently, fostering a more scalable and agile data culture.</li> </ul> </li> </ul>"},{"location":"backlog/_ice-box/future-world-first-class-messaging/#5-component-deep-dive","title":"5. Component Deep Dive","text":""},{"location":"backlog/_ice-box/future-world-first-class-messaging/#51-collector-service","title":"5.1. Collector Service","text":"<ul> <li>Framework: Built using our own <code>athomic</code> framework for consistency, resilience, and observability.</li> <li>Logic: Contains a single, highly-available message consumer subscribed to the <code>collector_topic</code>.</li> <li>Handler: The consumer handler will:<ol> <li>Receive and deserialize the <code>LineageEvent</code>.</li> <li>Instantiate a repository class corresponding to the configured storage backend (e.g., <code>MongoLineageRepository</code> or <code>Neo4jLineageRepository</code>).</li> <li>Call the repository's <code>save</code> method to persist the event.</li> </ol> </li> <li>Configuration: The service's configuration (<code>settings.toml</code>) will determine which storage backend to use.</li> </ul>"},{"location":"backlog/_ice-box/future-world-first-class-messaging/#52-storage-layer-configurable","title":"5.2. Storage Layer (Configurable)","text":"<p>The storage backend will be determined by a configuration setting, e.g., <code>LINEAGE_STORAGE_BACKEND=\"document\"</code>.</p>"},{"location":"backlog/_ice-box/future-world-first-class-messaging/#option-a-document-based-backend-document","title":"Option A: Document-Based Backend (<code>document</code>)","text":"<ul> <li>Technology: Elasticsearch or MongoDB.</li> <li>Schema: Each <code>LineageEvent</code> is stored as a single document. The data is indexed by <code>source_service</code>, <code>consumer_service</code>, <code>topic</code>, and <code>source_event_type</code> for efficient searching.</li> <li>Use Cases: Excellent for dashboarding, simple lookups (\"find all consumers of event X\"), and analyzing event volume over time.</li> <li>Pros: Easy to implement, scales well, powerful search capabilities (especially with Elasticsearch).</li> <li>Cons: Analyzing multi-hop, complex dependency chains (\"what is the ultimate downstream impact of a change in Service A?\") can be difficult and require complex application-layer logic.</li> </ul>"},{"location":"backlog/_ice-box/future-world-first-class-messaging/#option-b-graph-based-backend-graph","title":"Option B: Graph-Based Backend (<code>graph</code>)","text":"<ul> <li>Technology: Neo4j or Amazon Neptune.</li> <li>Schema: The <code>LineageEvent</code> is deconstructed into a graph model:<ul> <li>Nodes: <code>(:Service {name: \"Service A\"})</code>, <code>(:Topic {name: \"topic-1\"})</code>, <code>(:EventType {name: \"OrderCreated\"})</code>.</li> <li>Relationships: A single lineage event creates relationships like <code>(ServiceA)-[:PRODUCED]-&gt;(OrderCreated)</code>, <code>(OrderCreated)-[:PUBLISHED_TO]-&gt;(topic-1)</code>, <code>(ServiceC)-[:CONSUMED]-&gt;(OrderCreated)</code>.</li> </ul> </li> <li>Use Cases: Unmatched for impact analysis, discovering hidden dependencies, and visualizing the entire data flow graph.</li> <li>Pros: The ideal model for representing lineage data. Queries for dependency analysis are simple and extremely fast.</li> <li>Cons: Higher initial implementation complexity; requires learning a new query language (Cypher for Neo4j).</li> </ul>"},{"location":"backlog/_ice-box/future-world-first-class-messaging/#6-backlog-implementation-plan","title":"6. Backlog &amp; Implementation Plan","text":""},{"location":"backlog/_ice-box/future-world-first-class-messaging/#milestone-1-mvp-document-based-collector","title":"Milestone 1: MVP - Document-Based Collector","text":"<ul> <li>[Task] Create new repository for the Collector Service (<code>lineage-collector-service</code>).</li> <li>[Task] Implement the <code>LineageEvent</code> consumer using the <code>athomic</code> framework.</li> <li>[Task] Implement <code>DocumentLineageRepository</code> with an Elasticsearch adapter.</li> <li>[Task] Add configuration switch <code>LINEAGE_STORAGE_BACKEND</code> to the service.</li> <li>[Task] Set up a basic Kibana dashboard to visualize event counts per topic and service.</li> <li>[Task] Deploy the service to a development environment.</li> </ul>"},{"location":"backlog/_ice-box/future-world-first-class-messaging/#milestone-2-enhance-with-graph-capabilities","title":"Milestone 2: Enhance with Graph Capabilities","text":"<ul> <li>[Task] Implement <code>GraphLineageRepository</code> with a Neo4j adapter.</li> <li>[Task] Enhance the Collector Service handler to use the configured repository (document or graph) based on the settings.</li> <li>[Task] Write Cypher queries to answer key dependency questions (e.g., \"find all upstream sources for a given service\").</li> <li>[Task] Connect a graph visualization tool (e.g., Neo4j Bloom) to explore the lineage data.</li> </ul>"},{"location":"backlog/_ice-box/future-world-first-class-messaging/#future-work-post-spike","title":"Future Work (Post-Spike)","text":"<ul> <li>[EPIC] Integrate with a standard lineage specification like OpenLineage.</li> <li>[EPIC] Build a dedicated UI for data lineage exploration, tailored to our business needs.</li> </ul>"},{"location":"backlog/todo/TASK-008-outbox-webhook-executor/","title":"\ud83e\udde9 Engineering Task: Implement <code>WebhookExecutor</code> for Agnostic Outbox","text":"<p>Status: To Do Priority: High Parent Task: Generalize Outbox Module to Support Agnostic Executors Linked ADR: ADR - Outbox Module Architecture Definition (Agnostic Outbox Pattern)</p>"},{"location":"backlog/todo/TASK-008-outbox-webhook-executor/#context","title":"\ud83c\udfaf Context","text":"<p>As part of the agnostic outbox pattern, we want to support integration with HTTP endpoints via a new executor: <code>WebhookExecutor</code>.</p> <p>This executor will allow events in the Outbox to be dispatched to configurable webhooks, enabling integrations with external systems (e.g., CRMs, notification services, third-party APIs) through simple HTTP calls with payloads.</p>"},{"location":"backlog/todo/TASK-008-outbox-webhook-executor/#deliverables","title":"\u2705 Deliverables","text":"<ul> <li>[ ] Define and implement <code>WebhookExecutor</code> under <code>nala.athomic.integration.outbox.executors.webhook_executor</code>.</li> <li>[ ] Define configuration contract for the destination URL, headers, and retry behavior (optional).</li> <li>[ ] Support <code>POST</code> method by default with JSON payloads.</li> <li>[ ] Log and trace webhook invocations with request/response status and body (if needed).</li> <li>[ ] Implement retry safety (e.g., avoid retrying on 4xx).</li> <li>[ ] Ensure exceptions are correctly classified as retriable or non-retriable.</li> <li>[ ] Add unit tests with mocked HTTP responses:</li> <li>[ ] Success (200 OK).</li> <li>[ ] Failure (e.g., 500, 404).</li> <li>[ ] Timeout or network error.</li> <li>[ ] Add integration example to <code>/examples</code>.</li> <li>[ ] Document expected <code>payload</code>, <code>destination</code>, and <code>event_type=WEBHOOK</code> usage.</li> </ul>"},{"location":"backlog/todo/TASK-008-outbox-webhook-executor/#technical-notes","title":"\u2699 Technical Notes","text":"<ul> <li>Consider using <code>httpx.AsyncClient()</code> for async support and timeout control.</li> <li>Optionally support dynamic headers through the event payload if explicitly allowed.</li> <li>Ensure the webhook executor can be registered via the router using <code>event_type=WEBHOOK</code>.</li> </ul>"},{"location":"backlog/todo/TASK-008-outbox-webhook-executor/#acceptance-criteria","title":"\ud83d\udee1 Acceptance Criteria","text":"<ul> <li>[ ] Events with <code>event_type=WEBHOOK</code> are routed to <code>WebhookExecutor</code>.</li> <li>[ ] The executor sends HTTP POST requests with the event's payload to the specified <code>destination</code>.</li> <li>[ ] The system logs webhook interactions and handles errors appropriately.</li> <li>[ ] Unit tests and examples validate the executor logic and error handling.</li> </ul>"},{"location":"backlog/todo/TASK-009-outbox-task-executor/","title":"\ud83e\udde9 Engineering Task: Implement <code>TaskExecutor</code> for Internal Async Task Dispatching","text":"<p>Status: To Do Priority: Medium Parent Task: Generalize Outbox Module to Support Agnostic Executors Linked ADR: ADR - Outbox Module Architecture Definition (Agnostic Outbox Pattern)</p>"},{"location":"backlog/todo/TASK-009-outbox-task-executor/#context","title":"\ud83c\udfaf Context","text":"<p>The <code>TaskExecutor</code> will enable the Outbox to dispatch internal Python async functions. This supports use cases where you want the guarantee of execution after persistence without relying on external queues.</p> <p>It is especially useful for async workflows, post-commit hooks, or background processing without adding full-blown messaging infrastructure.</p>"},{"location":"backlog/todo/TASK-009-outbox-task-executor/#deliverables","title":"\u2705 Deliverables","text":"<ul> <li>[ ] Implement <code>TaskExecutor</code> under <code>nala.athomic.integration.outbox.executors.task_executor</code>.</li> <li>[ ] Add support to register and resolve tasks via a <code>TaskRegistry</code>.</li> <li>[ ] The event's <code>destination</code> should reference the task name in the registry.</li> <li>[ ] The payload is passed as a dictionary to the task function.</li> <li>[ ] Tasks must be <code>async def</code> and return <code>None</code> or raise on failure.</li> <li>[ ] Add unit tests:</li> <li>[ ] Successful task execution.</li> <li>[ ] Task not found.</li> <li>[ ] Task raises an exception (retriable vs non-retriable).</li> <li>[ ] Add example usage in <code>/examples</code> directory.</li> <li>[ ] Document usage pattern in README.</li> </ul>"},{"location":"backlog/todo/TASK-009-outbox-task-executor/#technical-notes","title":"\u2699 Technical Notes","text":"<ul> <li>The registry can be a simple dictionary <code>{str: Callable}</code>.</li> <li>Use <code>inspect.iscoroutinefunction()</code> to validate async functions.</li> <li>Avoid importing the registry from business modules; keep the registration local and explicit.</li> <li>Ensure compatibility with the existing <code>OutboxPublisher</code> and router.</li> </ul>"},{"location":"backlog/todo/TASK-009-outbox-task-executor/#acceptance-criteria","title":"\ud83d\udee1 Acceptance Criteria","text":"<ul> <li>[ ] Events with <code>event_type=TASK</code> are routed to <code>TaskExecutor</code>.</li> <li>[ ] The task is resolved and called with the payload.</li> <li>[ ] Exceptions are correctly classified and handled.</li> <li>[ ] Full test coverage is in place.</li> <li>[ ] A working example demonstrates how to register and dispatch a task.</li> </ul>"},{"location":"backlog/todo/TASK-050-DI-containers/","title":"TASK-050: Refactor to a Central Service Provider for Dependency Injection","text":""},{"location":"backlog/todo/TASK-050-DI-containers/#summary","title":"Summary","text":"<p>The current architecture relies heavily on the Factory/Service Locator pattern (e.g., <code>ProducerFactory.create()</code>) for dependency resolution. While effective, it can lead to hidden dependencies and makes it harder to manage the lifecycle of singleton services. The recent refactoring of the <code>DLQHandler</code> to use manual dependency injection proved the value of a more explicit approach.</p> <p>This task proposes the creation of a simple, explicit <code>AthomicServiceProvider</code>. This provider will act as a lightweight, homegrown Dependency Injection (DI) container. The <code>Athomic Facade</code> will be responsible for instantiating key singleton services at startup (like the main message producer, scheduler, etc.) and registering them with the provider. Other services and factories will then fetch these shared instances from the provider instead of creating new ones.</p> <p>This approach formalizes dependency management, centralizes singleton lifecycle control, and improves testability without introducing the complexity and \"magic\" of a full third-party DI container library.</p>"},{"location":"backlog/todo/TASK-050-DI-containers/#acceptance-criteria","title":"Acceptance Criteria","text":""},{"location":"backlog/todo/TASK-050-DI-containers/#1-create-the-athomicserviceprovider-and-singleton-accessor","title":"1. Create the <code>AthomicServiceProvider</code> and Singleton Accessor","text":"<p>A new class will be created to act as the central registry for singleton service instances.</p> <p>Action: Create a new file. Path: <code>src/nala/athomic/services/provider.py</code></p> <p>Code:</p> <pre><code># File: src/nala/athomic/services/provider.py\nfrom typing import Any, Dict\n\nfrom nala.athomic.base.exceptions import ServiceNotFoundError\nfrom nala.athomic.utils.decorators import singleton\n\n@singleton\nclass AthomicServiceProvider:\n    \"\"\"\n    A simple, singleton service provider to manage shared service instances.\n    \"\"\"\n    def __init__(self):\n        self._services: Dict[str, Any] = {}\n\n    def register(self, name: str, instance: Any) -&gt; None:\n        self._services[name] = instance\n\n    def get(self, name: str) -&gt; Any:\n        try:\n            return self._services[name]\n        except KeyError:\n            raise ServiceNotFoundError(f\"Service '{name}' not found in the provider.\")\n\n    def clear(self) -&gt; None:\n        \"\"\"Utility method for test isolation.\"\"\"\n        self._services.clear()\n\n\ndef get_service_provider() -&gt; AthomicServiceProvider:\n    \"\"\"Global accessor for the singleton service provider instance.\"\"\"\n    return AthomicServiceProvider.get_instance()\n</code></pre>"},{"location":"backlog/todo/TASK-050-DI-containers/#2-integrate-the-service-provider-into-the-athomic-facade","title":"2. Integrate the Service Provider into the <code>Athomic Facade</code>","text":"<p>The <code>Athomic</code> class will be responsible for composing the application by creating and registering the core singleton services at startup.</p> <p>Action: Modify the <code>Athomic</code> class. Path: <code>src/nala/athomic/facade.py</code></p> <p>Code:</p> <pre><code># File: src/nala/athomic/facade.py\n# ... other imports ...\nfrom nala.athomic.control.scheduler.factory import SchedulerFactory\nfrom nala.athomic.integration.messaging.producers.factory import ProducerFactory\nfrom nala.athomic.services.provider import get_service_provider\n\nclass Athomic:\n    def __init__(self, ...):\n        # ... (existing init logic) ...\n        self._initialize_service_provider()\n\n    def _initialize_service_provider(self) -&gt; None:\n        \"\"\"\n        Creates and registers the core singleton services for the application.\n        \"\"\"\n        logger.info(\"Initializing Athomic Service Provider...\")\n        provider = get_service_provider()\n        provider.clear() # Ensure clean state for hot-reloads or tests\n\n        # Create and register the default messaging producer\n        if self.settings.integration.messaging.enabled:\n            producer = ProducerFactory.create()\n            provider.register(\"default_messaging_producer\", producer)\n\n        # Create and register the default scheduler\n        if self.settings.control.scheduler and self.settings.control.scheduler.enabled:\n            scheduler = SchedulerFactory.create()\n            provider.register(\"default_scheduler\", scheduler)\n\n        # Add other core singletons here as needed (e.g., default KVStore)\n        logger.success(\"Core services registered in Service Provider.\")\n\n    async def startup(self):\n        # ...\n        # The lifecycle manager will start all registered services,\n        # including the singletons we just created.\n        await self.lifecycle_manager.start_all()\n        # ...\n\n    async def shutdown(self):\n        # ...\n        await self.lifecycle_manager.stop_all()\n        get_service_provider().clear()\n        # ...\n</code></pre>"},{"location":"backlog/todo/TASK-050-DI-containers/#3-refactor-services-to-use-the-provider","title":"3. Refactor Services to Use the Provider","text":"<p>Services and factories that previously created their own dependencies will now fetch them from the <code>AthomicServiceProvider</code>.</p> <p>Action: Modify the <code>SchedulerDelayStrategy</code> as an example. Path: <code>src/nala/athomic/integration/messaging/delay/providers/scheduler_strategy.py</code></p> <p>Code:</p> <pre><code># File: src/nala/athomic/integration/messaging/delay/providers/scheduler_strategy.py\n# ... other imports ...\nfrom nala.athomic.services.provider import get_service_provider\n\n@DelayStrategyRegistry.register(\"scheduler\")\nclass SchedulerDelayStrategy(DelayStrategyProtocol):\n    # ...\n\n    def __init__(self, settings: MessagingSettings):\n        self.settings = settings\n\n        # Instead of creating, we get the globally managed scheduler instance.\n        self.scheduler = get_service_provider().get(\"default_scheduler\")\n\n    # ... (rest of the class is unchanged) ...\n</code></pre>"},{"location":"backlog/todo/TASK-050-DI-containers/#4-update-unit-tests","title":"4. Update Unit Tests","text":"<p>Unit tests that previously patched factories now need to be updated to patch the <code>get_service_provider</code> accessor.</p> <p>Action: Update the unit test for <code>SchedulerDelayStrategy</code>. Path: <code>tests/unit/nala/athomic/integration/messaging/delay/providers/test_scheduler_strategy.py</code></p> <p>Code:</p> <pre><code># File: tests/unit/nala/athomic/integration/messaging/delay/providers/test_scheduler_strategy.py\n# ... other imports ...\n\n# The new path to patch\nSERVICE_PROVIDER_PATH = \"nala.athomic.integration.messaging.delay.providers.scheduler_strategy.get_service_provider\"\n\n@pytest.mark.asyncio\n@patch(SERVICE_PROVIDER_PATH)\nasync def test_scheduler_delay_strategy_schedules_task(mock_get_provider: MagicMock):\n    # ARRANGE\n    mock_scheduler_instance = AsyncMock()\n    mock_provider_instance = MagicMock()\n    mock_provider_instance.get.return_value = mock_scheduler_instance\n    mock_get_provider.return_value = mock_provider_instance\n\n    settings = MessagingSettings(backend=\"kafka\")\n    strategy = SchedulerDelayStrategy(settings=settings)\n\n    # ACT\n    await strategy.publish_delayed(...)\n\n    # ASSERT\n    mock_provider_instance.get.assert_called_once_with(\"default_scheduler\")\n    mock_scheduler_instance.schedule.assert_awaited_once()\n    # ... (rest of the assertions)\n</code></pre>"},{"location":"backlog/todo/TASK-migrations/","title":"Benchmark e Backlog: Sistema de Migra\u00e7\u00f5es Athomic","text":"<p>Data: 12 de Setembro de 2025 Autor: Nala Assistance Status: An\u00e1lise Conclu\u00edda, Backlog Definido</p>"},{"location":"backlog/todo/TASK-migrations/#1-introducao","title":"1. Introdu\u00e7\u00e3o","text":"<p>Este documento apresenta uma an\u00e1lise comparativa (benchmark) entre a biblioteca <code>yoyo-migrations</code> e a solu\u00e7\u00e3o customizada de migra\u00e7\u00f5es que projetamos para o framework Athomic. O objetivo \u00e9 validar nossa abordagem e identificar os pontos-chave a serem implementados para garantir uma ferramenta de alta qualidade, alinhada com os princ\u00edpios do Athomic.</p>"},{"location":"backlog/todo/TASK-migrations/#2-benchmark-de-features-yoyo-migrations-vs-athomic-migration-runner","title":"2. Benchmark de Features: yoyo-migrations vs. Athomic Migration Runner","text":"<p>A biblioteca <code>yoyo-migrations</code> foi usada como refer\u00eancia de mercado para um orquestrador de migra\u00e7\u00f5es agn\u00f3stico e baseado em scripts Python.</p> Feature yoyo-migrations Athomic Migration Runner (Nosso Design) Vantagem Athomic Rastreamento de Migra\u00e7\u00e3o Usa uma tabela dedicada (<code>_yoyo_migrations</code>) para rastrear scripts executados. Usa uma cole\u00e7\u00e3o Beanie (<code>MigrationStatus</code>) com suporte nativo a <code>source</code> para m\u00faltiplos conjuntos de migra\u00e7\u00f5es. \u2705 Superior Suporte a Bancos Agn\u00f3stico. Requer backends espec\u00edficos para cada tipo de banco (SQL, SQLite). N\u00e3o possui backend nativo para NoSQL. Agn\u00f3stico atrav\u00e9s do <code>MigrationBackendProtocol</code>. O <code>MongoBackend</code> \u00e9 a primeira implementa\u00e7\u00e3o. O <code>SqlYoyoBackend</code> (adaptador) permite usar o motor do yoyo para SQL. \u2705 Superior Tipo de Script Suporta arquivos <code>.sql</code> e <code>.py</code> (com fun\u00e7\u00f5es <code>step</code> ou <code>upgrade</code>/<code>downgrade</code>). Foco em <code>.py</code> com <code>upgrade</code>/<code>downgrade</code>. Isso permite que as migra\u00e7\u00f5es usem outros componentes do Athomic (config, logger, etc.) de forma nativa. \u2705 Mais Integrado Suporte <code>asyncio</code> Sim, possui suporte nativo para opera\u00e7\u00f5es ass\u00edncronas. Nativo e Async-first. Toda a arquitetura foi projetada do zero para ser ass\u00edncrona. \u2705 Igual/Superior Interface de Comando (CLI) Fornece uma CLI completa e robusta (<code>apply</code>, <code>rollback</code>, <code>show</code>, etc.). Projetada com <code>Typer</code> para ser uma CLI unificada (<code>athomic db ...</code>), oferecendo uma experi\u00eancia de usu\u00e1rio mais coesa e integrada. \u2705 Superior Gest\u00e3o de Transa\u00e7\u00f5es Excelente para SQL. Executa cada migra\u00e7\u00e3o em uma transa\u00e7\u00e3o, garantindo atomicidade. Ponto de Melhoria para MongoDB. A atomicidade precisa ser gerenciada manualmente nos scripts de migra\u00e7\u00e3o que realizam m\u00faltiplas opera\u00e7\u00f5es. Para SQL, o adaptador para <code>yoyo</code> herda essa funcionalidade. \ud83d\udfe1 Inferior (Mongo) Hooks P\u00f3s-Execu\u00e7\u00e3o Permite configurar \"post-apply hooks\" para executar c\u00f3digo Python ap\u00f3s as migra\u00e7\u00f5es. Solu\u00e7\u00e3o mais robusta via Event Bus. O runner publicar\u00e1 um evento (<code>athomic.migrations.applied</code>), permitindo que qualquer servi\u00e7o reaja de forma desacoplada. \u2705 Superior Manuten\u00e7\u00e3o Gerenciada pela comunidade de c\u00f3digo aberto. Gerenciada pela equipe do Athomic, garantindo total alinhamento com o framework. \u26aa\ufe0f Trade-off"},{"location":"backlog/todo/TASK-migrations/#conclusao-do-benchmark","title":"Conclus\u00e3o do Benchmark","text":"<p>A solu\u00e7\u00e3o customizada para o Athomic, embora demande um esfor\u00e7o inicial de implementa\u00e7\u00e3o, se mostra superior em termos de integra\u00e7\u00e3o, consist\u00eancia e flexibilidade para o ecossistema do framework. Ela adota os melhores conceitos do <code>yoyo-migrations</code> e os eleva ao integr\u00e1-los nativamente com os outros primitivos do Athomic (configura\u00e7\u00e3o, eventos, CLI unificada).</p>"},{"location":"backlog/todo/TASK-migrations/#3-backlog-de-implementacao","title":"3. Backlog de Implementa\u00e7\u00e3o","text":"<p>Com base na an\u00e1lise e no design que finalizamos, aqui est\u00e1 um backlog simples com as tarefas necess\u00e1rias para construir o m\u00f3dulo de migra\u00e7\u00e3o.</p>"},{"location":"backlog/todo/TASK-migrations/#epico-epic-007-implementar-modulo-de-migracao-de-banco-de-dados-agnostico","title":"\u00c9pico: EPIC-007: Implementar M\u00f3dulo de Migra\u00e7\u00e3o de Banco de Dados Agn\u00f3stico","text":""},{"location":"backlog/todo/TASK-migrations/#tarefas-de-implementacao-core","title":"Tarefas de Implementa\u00e7\u00e3o (Core)","text":"<ul> <li>[ ] TASK-MIG-001: Criar o <code>MigrationBackendProtocol</code> para definir a interface dos backends.</li> <li>[ ] TASK-MIG-002: Implementar o schema Beanie <code>MigrationStatus</code> para rastreamento no MongoDB.</li> <li>[ ] TASK-MIG-003: Implementar a classe <code>MongoBackend</code> que satisfaz o protocolo.</li> <li>[ ] TASK-MIG-004: Criar o <code>migration_backend_registry</code> e registrar o <code>MongoBackend</code>.</li> <li>[ ] TASK-MIG-005: Implementar a <code>MigrationBackendFactory</code> com a l\u00f3gica de resolu\u00e7\u00e3o de configura\u00e7\u00e3o (inline vs. <code>config_ref</code>).</li> <li>[ ] TASK-MIG-006: Implementar o <code>MigrationRunner</code> que orquestra o processo lendo as <code>sources</code> e usando a factory.</li> <li>[ ] TASK-MIG-007: Adicionar a publica\u00e7\u00e3o do evento <code>athomic.migrations.applied</code> no Event Bus ao final do processo de <code>upgrade</code>.</li> </ul>"},{"location":"backlog/todo/TASK-migrations/#tarefas-da-cli","title":"Tarefas da CLI","text":"<ul> <li>[ ] TASK-MIG-008: Estruturar a CLI <code>athomic db</code> com <code>Typer</code> no diret\u00f3rio <code>helpers/athomic_cli/commands/</code>.</li> <li>[ ] TASK-MIG-009: Implementar o comando <code>athomic db upgrade</code>.</li> <li>[ ] TASK-MIG-010: Implementar o comando <code>athomic db status</code> com a tabela formatada (<code>rich</code>).</li> <li>[ ] TASK-MIG-011: Implementar o comando <code>athomic db create</code> para gerar templates de migra\u00e7\u00e3o.</li> </ul>"},{"location":"backlog/todo/TASK-migrations/#melhorias-e-proximos-passos-backlog-futuro","title":"Melhorias e Pr\u00f3ximos Passos (Backlog Futuro)","text":"<ul> <li>[ ] TASK-MIG-012: Implementar a l\u00f3gica de <code>downgrade</code> no runner e na CLI.</li> <li>[ ] TASK-MIG-013: Documentar o padr\u00e3o de uso de transa\u00e7\u00f5es do Motor dentro dos scripts de migra\u00e7\u00e3o para MongoDB.</li> <li>[ ] TASK-MIG-014: (Spike) Implementar o <code>SqlYoyoBackend</code> como prova de conceito para o suporte a bancos relacionais.</li> </ul>"},{"location":"backlog/todo/circuit_breaker/","title":"Epic: Circuit Breaker V2 Enhancements","text":"<p>This epic covers a set of features to improve the observability, dynamic configuration, and resilience of the Circuit Breaker module.</p>"},{"location":"backlog/todo/circuit_breaker/#story-1-enhance-circuit-breaker-with-comprehensive-observability","title":"Story 1: Enhance Circuit Breaker with Comprehensive Observability","text":"<p>User Story: As a DevOps/SRE, I want the Circuit Breaker module to emit events and metrics whenever a circuit changes its state, so that I can monitor service health in real-time, create automated alerts, and diagnose failures more effectively.</p> <p>Acceptance Criteria: 1.  A structured log event must be emitted every time a circuit breaker's state changes (e.g., <code>CLOSED -&gt; OPEN</code>). The log should include <code>circuit_name</code>, <code>previous_state</code>, and <code>new_state</code>. 2.  A Prometheus gauge metric named <code>athomic_circuit_breaker_state</code> must be exposed. This metric should have labels for <code>circuit_name</code> and reflect the current state using numerical values (e.g., CLOSED=0, OPEN=1, HALF_OPEN=2). 3.  A Prometheus counter metric named <code>athomic_circuit_breaker_state_changes_total</code> must be incremented on every state change, with labels for <code>circuit_name</code> and the <code>new_state</code>. 4.  This functionality must be enabled/disabled via configuration under the <code>observability</code> settings.</p> <p>Technical Notes: -   Integrate with the <code>nala.athomic.observability</code> module. -   The <code>CircuitBreakerService</code> or the <code>PatchedCircuitRedisStorage</code> are good candidates for instrumenting the state change detection. -   Consider creating a dedicated <code>CircuitBreakerObserver</code> class that subscribes to state changes to decouple observability from the core logic. -   Ensure metric registration is handled correctly to avoid cardinality issues.</p>"},{"location":"backlog/todo/circuit_breaker/#epic-circuit-breaker-v2-enhancements_1","title":"Epic: Circuit Breaker V2 Enhancements","text":"<p>This epic covers a set of features to improve the observability, dynamic configuration, and resilience of the Circuit Breaker module.</p>"},{"location":"backlog/todo/circuit_breaker/#story-1-enhance-circuit-breaker-with-comprehensive-observability_1","title":"Story 1: Enhance Circuit Breaker with Comprehensive Observability","text":"<p>User Story: As a DevOps/SRE, I want the Circuit Breaker module to emit events and metrics whenever a circuit changes its state, so that I can monitor service health in real-time, create automated alerts, and diagnose failures more effectively.</p> <p>Acceptance Criteria: 1.  A structured log event must be emitted every time a circuit breaker's state changes (e.g., <code>CLOSED -&gt; OPEN</code>). The log should include <code>circuit_name</code>, <code>previous_state</code>, and <code>new_state</code>. 2.  A Prometheus gauge metric named <code>athomic_circuit_breaker_state</code> must be exposed. This metric should have labels for <code>circuit_name</code> and reflect the current state using numerical values (e.g., CLOSED=0, OPEN=1, HALF_OPEN=2). 3.  A Prometheus counter metric named <code>athomic_circuit_breaker_state_changes_total</code> must be incremented on every state change, with labels for <code>circuit_name</code> and the <code>new_state</code>. 4.  This functionality must be enabled/disabled via configuration under the <code>observability</code> settings.</p> <p>Technical Notes: -   Integrate with the <code>nala.athomic.observability</code> module. -   The <code>CircuitBreakerService</code> or the <code>PatchedCircuitRedisStorage</code> are good candidates for instrumenting the state change detection. -   Consider creating a dedicated <code>CircuitBreakerObserver</code> class that subscribes to state changes to decouple observability from the core logic. -   Ensure metric registration is handled correctly to avoid cardinality issues.</p>"},{"location":"backlog/todo/circuit_breaker/#story-2-implement-dynamic-configuration-for-circuit-breakers","title":"Story 2: Implement Dynamic Configuration for Circuit Breakers","text":"<p>User Story: As a Platform Engineer, I want the ability to update Circuit Breaker policies (e.g., <code>fail_max</code>, <code>reset_timeout_sec</code>) at runtime without a service restart, so that I can respond to production incidents and changing load patterns dynamically.</p> <p>Acceptance Criteria: 1.  The <code>CircuitBreakerService</code> must listen for configuration changes (e.g., from a live-config provider like Consul). 2.  When a circuit's specific configuration is updated, the active Circuit Breaker instance must be recreated or updated with the new settings. 3.  Existing state information (e.g., failure count) should be preserved if possible when settings are updated. If not, the behavior should be clearly defined (e.g., reset the circuit state). 4.  The update process must be thread-safe and not interrupt ongoing operations protected by the circuit breaker.</p> <p>Technical Notes: -   Requires integration with the <code>nala.athomic.control.live_config</code> module. -   The <code>CircuitBreakerService</code>'s internal cache of breaker instances (<code>self._breakers</code>) needs a mechanism for invalidation. -   When a config change for <code>resilience.circuit_breaker.circuits.&lt;circuit_name&gt;</code> is detected, the corresponding entry in <code>self._breakers</code> should be removed. -   The next call to <code>_get_or_create_breaker</code> for that <code>circuit_name</code> will naturally recreate it with the new settings. -   Need to decide on the state preservation strategy. Resetting the state upon policy change is the simplest and safest initial approach.</p>"},{"location":"backlog/todo/circuit_breaker/#story-2-implement-dynamic-configuration-for-circuit-breakers_1","title":"Story 2: Implement Dynamic Configuration for Circuit Breakers","text":"<p>User Story: As a Platform Engineer, I want the ability to update Circuit Breaker policies (e.g., <code>fail_max</code>, <code>reset_timeout_sec</code>) at runtime without a service restart, so that I can respond to production incidents and changing load patterns dynamically.</p> <p>Acceptance Criteria: 1.  The <code>CircuitBreakerService</code> must listen for configuration changes (e.g., from a live-config provider like Consul). 2.  When a circuit's specific configuration is updated, the active Circuit Breaker instance must be recreated or updated with the new settings. 3.  Existing state information (e.g., failure count) should be preserved if possible when settings are updated. If not, the behavior should be clearly defined (e.g., reset the circuit state). 4.  The update process must be thread-safe and not interrupt ongoing operations protected by the circuit breaker.</p> <p>Technical Notes: -   Requires integration with the <code>nala.athomic.control.live_config</code> module. -   The <code>CircuitBreakerService</code>'s internal cache of breaker instances (<code>self._breakers</code>) needs a mechanism for invalidation. -   When a config change for <code>resilience.circuit_breaker.circuits.&lt;circuit_name&gt;</code> is detected, the corresponding entry in <code>self._breakers</code> should be removed. -   The next call to <code>_get_or_create_breaker</code> for that <code>circuit_name</code> will naturally recreate it with the new settings. -   Need to decide on the state preservation strategy. Resetting the state upon policy change is the simplest and safest initial approach.</p>"},{"location":"backlog/todo/circuit_breaker/#story-3-implement-fail-safe-mode-for-circuit-breaker-storage","title":"Story 3: Implement Fail-Safe Mode for Circuit Breaker Storage","text":"<p>User Story: As an Application Developer, I want the Circuit Breaker to enter a \"fail-safe\" (pass-through) mode if its backend storage (e.g., Redis) is unavailable, so that a failure in the resilience infrastructure does not cause a cascading failure in my application.</p> <p>Acceptance Criteria: 1.  If the <code>CircuitBreakerStorageFactory</code> or the storage client fails to connect or execute a command (e.g., <code>get</code>/<code>set</code> state), the exception must be caught. 2.  When a storage failure is detected, the circuit breaker should immediately default to a <code>CLOSED</code> state, allowing the protected call to execute. 3.  A critical error must be logged, indicating that the circuit breaker is operating in a degraded, fail-safe mode. 4.  The system should periodically attempt to reconnect to the storage backend. Once the connection is restored, the circuit breaker should resume normal operation using the state from the storage.</p> <p>Technical Notes: -   Implement a try/except block within <code>CircuitBreakerService.execute</code> and/or the storage interaction methods. -   The exception handling should be specific to storage-related errors (e.g., <code>redis.ConnectionError</code>). -   This \"fail-safe\" mode can be implemented as a state within the <code>CircuitBreakerService</code> itself or by wrapping the storage object in a resilient proxy. -   A simple reconnection strategy could involve trying again on the next call, avoiding a complex background reconnection loop for the initial implementation.</p>"},{"location":"backlog/todo/circuit_breaker/#story-3-implement-fail-safe-mode-for-circuit-breaker-storage_1","title":"Story 3: Implement Fail-Safe Mode for Circuit Breaker Storage","text":"<p>User Story: As an Application Developer, I want the Circuit Breaker to enter a \"fail-safe\" (pass-through) mode if its backend storage (e.g., Redis) is unavailable, so that a failure in the resilience infrastructure does not cause a cascading failure in my application.</p> <p>Acceptance Criteria: 1.  If the <code>CircuitBreakerStorageFactory</code> or the storage client fails to connect or execute a command (e.g., <code>get</code>/<code>set</code> state), the exception must be caught. 2.  When a storage failure is detected, the circuit breaker should immediately default to a <code>CLOSED</code> state, allowing the protected call to execute. 3.  A critical error must be logged, indicating that the circuit breaker is operating in a degraded, fail-safe mode. 4.  The system should periodically attempt to reconnect to the storage backend. Once the connection is restored, the circuit breaker should resume normal operation using the state from the storage.</p> <p>Technical Notes: -   Implement a try/except block within <code>CircuitBreakerService.execute</code> and/or the storage interaction methods. -   The exception handling should be specific to storage-related errors (e.g., <code>redis.ConnectionError</code>). -   This \"fail-safe\" mode can be implemented as a state within the <code>CircuitBreakerService</code> itself or by wrapping the storage object in a resilient proxy. -   A simple reconnection strategy could involve trying again on the next call, avoiding a complex background reconnection loop for the initial implementation.</p>"},{"location":"backlog/todo/futher_improves/","title":"Futher improves","text":""},{"location":"backlog/todo/futher_improves/#title-add-detailed-exception-logging-in-baseserviceclose","title":"Title: Add Detailed Exception Logging in <code>BaseService.close</code>","text":"<p>Description: The <code>close</code> method in <code>BaseService</code>  currently catches generic <code>Exception</code>s during the shutdown of its background task and the execution of the <code>_close</code> hook. This prevents a failing service from halting the entire application shutdown but can hide the root cause of the failure. The <code>except</code> blocks should be updated to use <code>self.logger.exception(...)</code> to log the full stack trace, which will significantly improve debuggability of shutdown-related issues.</p> <p>Affected Files: * <code>nala/athomic/services/base.py</code> </p> <p>Labels: <code>improvement</code>, <code>observability</code>, <code>logging</code></p> <p>Acceptance Criteria: * The <code>try...except</code> block in the <code>close</code> method must log failures using <code>logger.exception</code>. * Tests should confirm that if a service's <code>_close</code> method raises an exception, it is logged with a full stack trace.</p>"},{"location":"backlog/todo/futher_improves/#title-refactor-publishing-strategies-to-receive-dependencies-directly","title":"Title: Refactor Publishing Strategies to Receive Dependencies Directly","text":"<p>Description: Currently, <code>PublishingStrategyProtocol</code> implementations (<code>DirectPublishStrategy</code>, <code>DelayedPublishStrategy</code>) receive the <code>BaseProducer</code> instance and then access its dependencies like <code>producer.payload_processor</code>. To improve decoupling, follow a more explicit dependency injection pattern. The <code>publish</code> method signature in the protocol and its implementations should be changed to accept <code>payload_processor</code> and <code>key_header_serializer</code> as direct arguments. The <code>BaseProducer</code> will then be responsible for passing them down when calling the strategy.</p> <p>Affected Files: * <code>nala/athomic/integration/messaging/producers/protocol.py</code> * <code>nala/athomic/integration/messaging/producers/base.py</code> * <code>nala/athomic/integration/messaging/producers/strategies/direct.py</code> * <code>nala/athomic/integration/messaging/producers/strategies/delayed.py</code></p> <p>Labels: <code>refactoring</code>, <code>architecture</code>, <code>decoupling</code>, <code>low-priority</code></p> <p>Acceptance Criteria: * The <code>PublishingStrategyProtocol.publish</code> method signature is updated with the new dependencies. * All implementations of the protocol are updated accordingly. * <code>BaseProducer.publish</code> is updated to pass its dependencies to the selected strategy.</p>"},{"location":"backlog/todo/futher_improves/#title-add-documentation-regarding-dynamic-cache-decorator-application-in-httpclientfactory","title":"Title: Add Documentation Regarding Dynamic Cache Decorator Application in HttpClientFactory","text":"<p>Description: The <code>HttpClientFactory</code> dynamically applies the <code>@cache</code> decorator to the <code>get</code> method of a client instance when caching is enabled for that client. This is a powerful and efficient pattern but can be non-obvious to new maintainers. Add a clear and concise code comment in this section of the factory to explain why and how this is done. The comment should clarify that the <code>get</code> method on a cached client instance is a wrapped object, not the original method from the provider class.</p> <p>Affected Files: * <code>nala/athomic/http/factory.py</code> </p> <p>Labels: <code>documentation</code>, <code>maintainability</code>, <code>low-priority</code></p> <p>Acceptance Criteria: * A descriptive comment is added to the <code>if client_settings.cache...</code> block in <code>HttpClientFactory.create</code>.</p> <ul> <li>ID: <code>BK-001</code></li> <li>Title: Refactor Mongo Outbox Repository to Use Atomic Updates</li> <li>Type: <code>Refactor</code></li> <li>Priority: <code>High</code></li> <li>Description: The current implementation in <code>MongoOutboxRepository</code> uses a <code>find_one()</code> followed by a <code>save()</code> for update operations like <code>mark_event_attempted</code>, <code>mark_event_published</code>, and <code>mark_event_failed</code>. This pattern is not atomic and can lead to race conditions under high load. It is also less performant than a single database operation.</li> <li>Acceptance Criteria:<ul> <li>```</li> <li>[ ] The <code>mark_event_attempted</code> method should be refactored to use a single <code>update_one</code> operation with the <code>$inc</code> operator to increment the <code>attempts</code> field and <code>$set</code> to update <code>last_attempt_at</code>.</li> <li>[ ] The <code>mark_event_published</code> method should use a single <code>update_one</code> operation with <code>$set</code> to change the event's <code>status</code> to <code>PUBLISHED</code>.</li> <li>[ ] The <code>mark_event_failed</code> method should use a single <code>update_one</code> operation with <code>$set</code> to update the <code>last_error</code> and potentially the <code>status</code> fields.</li> <li>[ ] The changes must ensure the operations remain atomic and performant.</li> <li>```</li> </ul> </li> <li>Files:<ul> <li><code>nala/athomic/database/outbox/mongo/mongo_outbox_repository.py</code></li> </ul> </li> </ul> <ul> <li>ID: <code>BK-002</code></li> <li>Title: Ensure Consistent Exception Propagation in BaseKVStore</li> <li>Type: <code>Bug</code></li> <li>Priority: <code>Medium</code></li> <li>Description: The <code>delete</code> method in <code>BaseKVStore</code> catches exceptions, records them in the span, but does not re-raise them. This is inconsistent with the <code>get</code> and <code>set</code> methods, which correctly propagate exceptions. This inconsistency can lead to silent failures where callers are unaware that a delete operation failed.</li> <li>Acceptance Criteria:<ul> <li>```</li> <li>[ ] Add a <code>raise</code> statement within the <code>except</code> block of the <code>delete</code> method in <code>BaseKVStore</code>.</li> <li>[ ] Verify that all data operation methods (<code>get</code>, <code>set</code>, <code>delete</code>, <code>exists</code>, <code>clear</code>, etc.) in <code>BaseKVStore</code> have a consistent error handling strategy (i.e., they all propagate exceptions after logging/tracing).</li> <li>```</li> </ul> </li> <li>Files:<ul> <li><code>nala/athomic/database/kvstore/base.py</code></li> </ul> </li> </ul> <ul> <li>ID: <code>BK-003</code></li> <li>Title: Review Saga Compensation Strategy for Critical Failures</li> <li>Type: <code>Chore</code></li> <li>Priority: <code>Medium</code></li> <li>Description: The current <code>OrchestrationSagaExecutor</code> implements a \"fail-fast\" strategy for compensations. If a compensation step fails, the saga is marked as <code>FAILED</code>, and the compensation chain is halted. For critical transactions, this could leave the system in an inconsistent state that requires manual intervention.</li> <li>Acceptance Criteria:<ul> <li>```</li> <li>[ ] The team should discuss the implications of a failing compensation.</li> <li>[ ] Decide if a retry mechanism should be added to compensation steps.</li> <li>[ ] Consider implementing an alerting mechanism (e.g., publishing an event to a \"dead-letter-saga\" topic) when a compensation fails permanently.</li> <li>[ ] Document the chosen strategy and its trade-offs.</li> <li>```</li> </ul> </li> <li>Files:<ul> <li><code>nala/athomic/resilience/sagas/executors/orchestration_executor.py</code></li> </ul> </li> </ul> <ul> <li>ID: <code>BK-004</code></li> <li>Title: Improve Security in RedisKVClient Connection Logging</li> <li>Type: <code>Refactor</code></li> <li>Priority: <code>Low</code></li> <li>Description: The <code>_connect</code> method in <code>RedisKVClient</code> logs the connection URI with a note that credentials have been redacted. While the intention is good, logging the full URI string, even with a note, poses a potential risk of leaking credentials if the redaction process fails or is incomplete. A more secure approach is to never include sensitive parts of the URI in logs in the first place.</li> <li>Acceptance Criteria:<ul> <li>```</li> <li>[ ] The log message in the <code>_connect</code> method should be modified to explicitly exclude the user/password part of the URI.</li> <li>[ ] The log should still clearly indicate the host, port, and database being connected to for debugging purposes.</li> <li>```</li> </ul> </li> <li>Files:<ul> <li><code>nala/athomic/database/kvstore/providers/redis/client.py</code></li> </ul> </li> </ul> <ul> <li>ID: <code>BK-005</code></li> <li>Title: Refactor Athomic Facade <code>__init__</code> for Simpler Default Settings</li> <li>Type: <code>Refactor</code></li> <li>Priority: <code>Low</code></li> <li>Description: In the <code>Athomic</code> class constructor, the <code>settings</code> argument defaults to <code>None</code>, and is then resolved using an <code>or</code> statement. This can be slightly simplified by making <code>get_settings()</code> the default value directly in the method signature.</li> <li>Acceptance Criteria:<ul> <li>```</li> <li>[ ] Change the <code>__init__</code> method signature in <code>nala/athomic/facade.py</code> to <code>def __init__(self, ..., settings: AppSettings = None):</code>.</li> <li>[ ] Update the first line inside the method to <code>self.settings = settings or get_settings()</code>.</li> <li>```</li> </ul> </li> <li>Files:<ul> <li><code>nala/athomic/facade.py</code></li> </ul> </li> </ul>"},{"location":"benchmark/athomic-architecture-report/","title":"\ud83d\udcca Athomic \u2013 Avalia\u00e7\u00e3o Arquitetural por M\u00f3dulo","text":"<p>Este documento consolida a an\u00e1lise t\u00e9cnica e comparativa dos principais m\u00f3dulos do projeto Athomic, com base em frameworks de refer\u00eancia como Spring Boot, Django, FastAPI, Resilience4j, entre outros.</p>"},{"location":"benchmark/athomic-architecture-report/#modulo-1-resilience","title":"\u2705 M\u00f3dulo 1 \u2014 <code>resilience</code>","text":""},{"location":"benchmark/athomic-architecture-report/#pontos-fortes","title":"Pontos Fortes","text":"<ul> <li>Implementa\u00e7\u00e3o s\u00f3lida de retry, fallback, timeout, circuit breaker, bulkhead.</li> <li>Decorators funcionais reutiliz\u00e1veis.</li> <li>Composi\u00e7\u00e3o robusta com trace e m\u00e9tricas.</li> </ul>"},{"location":"benchmark/athomic-architecture-report/#oportunidades","title":"Oportunidades","text":"<ul> <li>Falta lifecycle modular (<code>Retry</code>, <code>Fallback</code> como componentes isolados).</li> <li>N\u00e3o h\u00e1 um registry formal.</li> <li>Falta auto-descoberta ou pluginiza\u00e7\u00e3o.</li> </ul>"},{"location":"benchmark/athomic-architecture-report/#propostas","title":"Propostas","text":"<ul> <li>Registry de <code>ResilienceComponent</code>.</li> <li>Composi\u00e7\u00e3o funcional tipo <code>with_retry(...)</code>.</li> <li>Configura\u00e7\u00e3o din\u00e2mica por servi\u00e7o/m\u00e9todo.</li> </ul>"},{"location":"benchmark/athomic-architecture-report/#modulo-2-config","title":"\u2705 M\u00f3dulo 2 \u2014 <code>config</code>","text":""},{"location":"benchmark/athomic-architecture-report/#pontos-fortes_1","title":"Pontos Fortes","text":"<ul> <li>Dynaconf + Pydantic = estrutura robusta.</li> <li>Contracts por m\u00f3dulo, multi-ambiente, <code>.secrets.toml</code>.</li> </ul>"},{"location":"benchmark/athomic-architecture-report/#oportunidades_1","title":"Oportunidades","text":"<ul> <li>Falta CLI de valida\u00e7\u00e3o (<code>athomic config check</code>).</li> <li>Falta suporte por tenant ou override din\u00e2mico.</li> <li>Falta painel visual.</li> </ul>"},{"location":"benchmark/athomic-architecture-report/#propostas_1","title":"Propostas","text":"<ul> <li>ScopedSettings + registry.</li> <li>CLI + diagnostics endpoint.</li> <li>Plugins registrando seus pr\u00f3prios contratos dinamicamente.</li> </ul>"},{"location":"benchmark/athomic-architecture-report/#modulo-3-observability","title":"\u2705 M\u00f3dulo 3 \u2014 <code>observability</code>","text":""},{"location":"benchmark/athomic-architecture-report/#pontos-fortes_2","title":"Pontos Fortes","text":"<ul> <li>Logging estruturado com maskers.</li> <li>Tracing com contexto.</li> <li>Prometheus integrado.</li> </ul>"},{"location":"benchmark/athomic-architecture-report/#oportunidades_2","title":"Oportunidades","text":"<ul> <li>Falta painel visual de observabilidade.</li> <li>Falta m\u00e9tricas por plugin/modulo.</li> <li>Falta middleware para trace autom\u00e1tico e CLI para log traceado.</li> </ul>"},{"location":"benchmark/athomic-architecture-report/#propostas_2","title":"Propostas","text":"<ul> <li>Decorator <code>@trace_span</code>.</li> <li>Endpoint <code>/diagnostics/logs</code>, <code>/traces</code>.</li> <li>CLI <code>athomic observability trace</code>.</li> </ul>"},{"location":"benchmark/athomic-architecture-report/#modulo-4-security","title":"\u2705 M\u00f3dulo 4 \u2014 <code>security</code>","text":""},{"location":"benchmark/athomic-architecture-report/#pontos-fortes_3","title":"Pontos Fortes","text":"<ul> <li>Vault + fallback.</li> <li>Auth context (JWT, IP, role-aware).</li> <li>IP filtering.</li> <li>Modulariza\u00e7\u00e3o e seguran\u00e7a robusta.</li> </ul>"},{"location":"benchmark/athomic-architecture-report/#oportunidades_3","title":"Oportunidades","text":"<ul> <li>Falta RBAC/ABAC formal.</li> <li>Falta painel de gest\u00e3o e auditoria.</li> <li>Falta suporte a IdPs externos (OAuth2).</li> </ul>"},{"location":"benchmark/athomic-architecture-report/#propostas_3","title":"Propostas","text":"<ul> <li><code>PolicyManager</code> + <code>@requires_policy</code>.</li> <li>CLI <code>athomic security revoke-token</code>.</li> <li>Eventos de auditoria + logs.</li> </ul>"},{"location":"benchmark/athomic-architecture-report/#modulo-5-messaging","title":"\u2705 M\u00f3dulo 5 \u2014 <code>messaging</code>","text":""},{"location":"benchmark/athomic-architecture-report/#pontos-fortes_4","title":"Pontos Fortes","text":"<ul> <li>Interface desacoplada (producers/consumers).</li> <li>Providers para Kafka e RabbitMQ.</li> <li>Testes com mocks, serializa\u00e7\u00e3o configur\u00e1vel.</li> </ul>"},{"location":"benchmark/athomic-architecture-report/#oportunidades_4","title":"Oportunidades","text":"<ul> <li>Falta observabilidade por t\u00f3pico.</li> <li>Falta dead-letter + retry CLI.</li> <li>Falta schema validation (Avro, JSONSchema).</li> </ul>"},{"location":"benchmark/athomic-architecture-report/#propostas_4","title":"Propostas","text":"<ul> <li><code>message_middleware</code>, retry por decorator.</li> <li>M\u00e9tricas Prometheus por t\u00f3pico.</li> <li>CLI: <code>athomic messaging replay</code>.</li> </ul>"},{"location":"benchmark/athomic-architecture-report/#modulo-6-plugins","title":"\u2705 M\u00f3dulo 6 \u2014 <code>plugins</code>","text":""},{"location":"benchmark/athomic-architecture-report/#pontos-fortes_5","title":"Pontos Fortes","text":"<ul> <li>Modulariza\u00e7\u00e3o por dom\u00ednio j\u00e1 bem definida.</li> <li>Uso indireto de registry + decorators.</li> </ul>"},{"location":"benchmark/athomic-architecture-report/#oportunidades_5","title":"Oportunidades","text":"<ul> <li>Falta <code>PluginBase</code> com lifecycle formal.</li> <li>Falta <code>PluginManager</code> com discovery e escopo isolado.</li> <li>Sem CLI de gest\u00e3o (<code>list</code>, <code>enable</code>, <code>disable</code>).</li> </ul>"},{"location":"benchmark/athomic-architecture-report/#propostas_5","title":"Propostas","text":"<ul> <li>Scaffold com <code>PluginManager</code>, <code>plugin_registry</code>, e config por plugin.</li> <li>CLI <code>athomic plugins list</code>.</li> </ul> <p>Relat\u00f3rio gerado com base na arquitetura atual da Athomic e frameworks refer\u00eancia como Spring Boot, Resilience4j, FastAPI, NestJS e Django.</p>"},{"location":"benchmark/athomic-config-module/","title":"\u2699\ufe0f Athomic \u2013 M\u00f3dulo Config (Dynaconf, Contracts, Multi-ambiente)","text":"<p>Este documento descreve a avalia\u00e7\u00e3o do m\u00f3dulo <code>config</code> da Athomic, respons\u00e1vel pela gest\u00e3o robusta e segura de configura\u00e7\u00f5es.</p>"},{"location":"benchmark/athomic-config-module/#o-que-ja-esta-implementado","title":"\u2705 O que j\u00e1 est\u00e1 implementado","text":"<ul> <li>Integra\u00e7\u00e3o com Dynaconf (suporte a <code>.env</code>, <code>.secrets.toml</code>, <code>settings.toml</code>)</li> <li>Valida\u00e7\u00e3o com Pydantic (<code>AppSettings</code>, <code>MessagingSettings</code>, etc.)</li> <li>Suporte multi-ambiente (<code>development</code>, <code>production</code>, etc.)</li> <li>Contratos por m\u00f3dulo e valida\u00e7\u00e3o estruturada</li> <li>Fallbacks seguros e separa\u00e7\u00e3o de secrets</li> </ul>"},{"location":"benchmark/athomic-config-module/#diagnostico-atual","title":"\ud83d\udcca Diagn\u00f3stico Atual","text":"Dimens\u00e3o Status Tipagem e valida\u00e7\u00e3o \ud83d\udfe2 Alta Multi-ambiente \ud83d\udfe2 Alta Fallback e hierarquia de fontes \ud83d\udfe2 Alta Extensibilidade por plugin \ud83d\udfe1 M\u00e9dia Configura\u00e7\u00e3o por tenant \ud83d\udd34 Ausente CLI para valida\u00e7\u00e3o \ud83d\udd34 Ausente Diagn\u00f3stico via API \ud83d\udd34 Ausente"},{"location":"benchmark/athomic-config-module/#oportunidades-de-melhoria","title":"\u26a0\ufe0f Oportunidades de melhoria","text":"<ul> <li>Suporte a <code>ScopedSettings</code> (por tenant, plugin, escopo)</li> <li>CLI para verifica\u00e7\u00e3o: <code>athomic config check</code>, <code>athomic config diff</code></li> <li>Endpoint <code>/diagnostics/settings</code> com mask de dados sens\u00edveis</li> <li>Plugins podendo registrar seus pr\u00f3prios settings dinamicamente</li> </ul>"},{"location":"benchmark/athomic-config-module/#arquitetura-ideal","title":"\ud83d\udcd0 Arquitetura Ideal","text":""},{"location":"benchmark/athomic-config-module/#1-scopedsettings","title":"1. ScopedSettings","text":"<pre><code>scoped_settings.get(\"tenant_x\", \"messaging\").broker_url\n</code></pre>"},{"location":"benchmark/athomic-config-module/#2-cli","title":"2. CLI","text":"<pre><code>athomic config check --env=production\nathomic config diff --from=.env --to=Vault\n</code></pre>"},{"location":"benchmark/athomic-config-module/#3-registry-de-contratos","title":"3. Registry de contratos","text":"<pre><code>settings_registry.register(MessagingSettings)\n</code></pre>"},{"location":"benchmark/athomic-config-module/#4-diagnostico-via-api","title":"4. Diagn\u00f3stico via API","text":"<pre><code>GET /diagnostics/settings\n{\n  \"env\": \"production\",\n  \"messaging.broker_url\": \"***\",\n  ...\n}\n</code></pre>"},{"location":"benchmark/athomic-config-module/#roadmap-sugerido","title":"\ud83c\udfaf Roadmap sugerido","text":"<ul> <li>[ ] ScopedSettings com fallback hier\u00e1rquico</li> <li>[ ] CLI de valida\u00e7\u00e3o e diff</li> <li>[ ] Diagn\u00f3stico via API</li> <li>[ ] Plugins registrando seus pr\u00f3prios contracts</li> <li>[ ] Suporte a m\u00faltiplos tenants no Dynaconf</li> </ul> <p>O m\u00f3dulo config \u00e9 a base de todas as decis\u00f5es e composi\u00e7\u00f5es da Athomic \u2014 robustez aqui \u00e9 refletida em toda a plataforma.</p>"},{"location":"benchmark/athomic-lifecycle-module/","title":"\ud83d\udd01 Athomic \u2013 M\u00f3dulo Lifecycle/Core","text":"<p>Este documento apresenta a avalia\u00e7\u00e3o do m\u00f3dulo <code>lifecycle/core</code> da Athomic, respons\u00e1vel por orquestrar o ciclo de vida da aplica\u00e7\u00e3o e dos m\u00f3dulos.</p>"},{"location":"benchmark/athomic-lifecycle-module/#o-que-ja-esta-implementado","title":"\u2705 O que j\u00e1 est\u00e1 implementado","text":"<ul> <li><code>lifespan</code> com startup/shutdown no FastAPI</li> <li>Inicializa\u00e7\u00e3o de servi\u00e7os externos (Vault, Redis, Kafka)</li> <li>Registro/desregistro em service discovery (Consul)</li> <li>Decorators para controle de ciclo de vida</li> <li>Suporte a fallback se servi\u00e7os estiverem indispon\u00edveis</li> </ul>"},{"location":"benchmark/athomic-lifecycle-module/#diagnostico-atual","title":"\ud83d\udcca Diagn\u00f3stico Atual","text":"Dimens\u00e3o Status Startup/shutdown modular \ud83d\udfe2 Alta Plugin-aware lifecycle \ud83d\udfe1 M\u00e9dia Health indicators \ud83d\udd34 Ausente CLI ou tracing \ud83d\udd34 Ausente Controle de ordem/fallback \ud83d\udfe1 M\u00e9dia"},{"location":"benchmark/athomic-lifecycle-module/#oportunidades-de-melhoria","title":"\u26a0\ufe0f Oportunidades de melhoria","text":"<ul> <li>Plugins ainda n\u00e3o participam formalmente do ciclo de vida</li> <li>Falta controle de ordem de inicializa\u00e7\u00e3o</li> <li>N\u00e3o h\u00e1 retry/backoff na inicializa\u00e7\u00e3o de m\u00f3dulos cr\u00edticos</li> <li>Falta tracing da fase de boot</li> <li>Aus\u00eancia de endpoints <code>/health/startup</code>, <code>/ready</code>, <code>/live</code></li> <li>Aus\u00eancia de CLI <code>athomic health check</code></li> </ul>"},{"location":"benchmark/athomic-lifecycle-module/#arquitetura-ideal","title":"\ud83d\udcd0 Arquitetura Ideal","text":""},{"location":"benchmark/athomic-lifecycle-module/#1-interface-para-ciclo-de-vida","title":"1. Interface para ciclo de vida","text":"<pre><code>class LifecycleParticipant(Protocol):\n    async def on_startup(self): ...\n    async def on_shutdown(self): ...\n</code></pre>"},{"location":"benchmark/athomic-lifecycle-module/#2-lifecycle-manager","title":"2. Lifecycle Manager","text":"<pre><code>lifecycle_manager.register(plugin)\nawait lifecycle_manager.start_all()\nawait lifecycle_manager.stop_all()\n</code></pre>"},{"location":"benchmark/athomic-lifecycle-module/#3-health-endpoints","title":"3. Health endpoints","text":"<ul> <li><code>/health/startup</code> \u2013 sucesso na inicializa\u00e7\u00e3o</li> <li><code>/health/ready</code> \u2013 pronto para requisi\u00e7\u00f5es</li> <li><code>/health/live</code> \u2013 vivo mas n\u00e3o necessariamente pronto</li> </ul>"},{"location":"benchmark/athomic-lifecycle-module/#4-cli","title":"4. CLI","text":"<pre><code>athomic lifecycle show\nathomic health check\n</code></pre>"},{"location":"benchmark/athomic-lifecycle-module/#5-tracing","title":"5. Tracing","text":"<ul> <li>Span por recurso durante startup</li> <li>Prometheus: <code>startup_duration_seconds</code>, <code>lifecycle_errors_total</code></li> </ul>"},{"location":"benchmark/athomic-lifecycle-module/#roadmap-sugerido","title":"\ud83c\udfaf Roadmap sugerido","text":"<ul> <li>[ ] Criar <code>LifecycleParticipant</code> + decorator</li> <li>[ ] Criar <code>LifecycleManager</code> com ordena\u00e7\u00e3o</li> <li>[ ] Integrar plugins com hooks <code>on_startup</code>, <code>on_shutdown</code></li> <li>[ ] Adicionar endpoints <code>/health/{startup, ready, live}</code></li> <li>[ ] CLI de diagn\u00f3stico</li> <li>[ ] Logging/tracing estruturado por fase</li> </ul> <p>Este m\u00f3dulo \u00e9 o n\u00facleo da orquestra\u00e7\u00e3o de todos os componentes da Athomic e fundamental para a robustez em ambientes enterprise.</p>"},{"location":"benchmark/athomic-messaging-module/","title":"\ud83d\udcec Athomic \u2013 M\u00f3dulo Messaging (Kafka, RabbitMQ, Pub/Sub)","text":"<p>Este documento apresenta a avalia\u00e7\u00e3o do m\u00f3dulo <code>messaging</code> da Athomic, respons\u00e1vel pela comunica\u00e7\u00e3o ass\u00edncrona via m\u00faltiplos backends.</p>"},{"location":"benchmark/athomic-messaging-module/#o-que-ja-esta-implementado","title":"\u2705 O que j\u00e1 est\u00e1 implementado","text":"<ul> <li>Interface desacoplada para producers e consumers</li> <li>Providers para Kafka (AIOKafka) e RabbitMQ (aio-pika)</li> <li>Registry de providers e factory central</li> <li>Configura\u00e7\u00e3o isolada por backend (<code>KafkaSettings</code>, <code>LocalMessagingSettings</code>)</li> <li>Suporte a serializa\u00e7\u00e3o customizada (JSON, Avro)</li> <li>Consumo ass\u00edncrono com <code>subscribe</code>, <code>ack</code>, <code>nack</code></li> <li>Testes unit\u00e1rios com mocks para Kafka e Rabbit</li> </ul>"},{"location":"benchmark/athomic-messaging-module/#diagnostico-atual","title":"\ud83d\udcca Diagn\u00f3stico Atual","text":"Dimens\u00e3o Status Interface e provider \ud83d\udfe2 Alta Suporte multi-backend \ud83d\udfe2 Alta Observabilidade \ud83d\udfe1 M\u00e9dia Middleware / intercepta\u00e7\u00e3o \ud83d\udfe1 M\u00e9dia Retry, fallback, dead-letter \ud83d\udfe1 M\u00e9dia Schema registry / valida\u00e7\u00e3o \ud83d\udd34 Ausente Painel e CLI \ud83d\udd34 Ausente"},{"location":"benchmark/athomic-messaging-module/#oportunidades-de-melhoria","title":"\u26a0\ufe0f Oportunidades de melhoria","text":"<ul> <li>Observabilidade por t\u00f3pico/backend (<code>msg_published_total</code>, <code>consume_latency</code>)</li> <li>Tracing propagado por headers (<code>trace_id</code>, <code>span_id</code>)</li> <li>Middleware <code>@message_middleware</code> para logging autom\u00e1tico</li> <li>Suporte a Dead Letter Queue com CLI de replay</li> <li>Schema validation com JSONSchema ou Avro</li> <li>Painel visual ou API para mensagens</li> </ul>"},{"location":"benchmark/athomic-messaging-module/#arquitetura-ideal","title":"\ud83d\udcd0 Arquitetura Ideal","text":""},{"location":"benchmark/athomic-messaging-module/#1-middleware","title":"1. Middleware","text":"<pre><code>@message_middleware\nasync def log_msg(ctx): ...\n</code></pre>"},{"location":"benchmark/athomic-messaging-module/#2-tracing-automatico","title":"2. Tracing autom\u00e1tico","text":"<pre><code>tracer.start_span(\"consume:user_events\", tags={\"tenant\": \"x\"})\n</code></pre>"},{"location":"benchmark/athomic-messaging-module/#3-cli","title":"3. CLI","text":"<pre><code>athomic messaging replay --from=dead-letter\nathomic messaging status --topic=events\n</code></pre>"},{"location":"benchmark/athomic-messaging-module/#4-schema-validation","title":"4. Schema Validation","text":"<pre><code>schema_registry.register(\"user_created\", UserSchema)\n</code></pre>"},{"location":"benchmark/athomic-messaging-module/#roadmap-sugerido","title":"\ud83c\udfaf Roadmap sugerido","text":"<ul> <li>[ ] Middleware de intercepta\u00e7\u00e3o de mensagens</li> <li>[ ] Registro e export de m\u00e9tricas por t\u00f3pico</li> <li>[ ] Tracing com propaga\u00e7\u00e3o autom\u00e1tica</li> <li>[ ] CLI para replay de mensagens e inspe\u00e7\u00e3o de DLQ</li> <li>[ ] Suporte a schema registry</li> <li>[ ] Painel web <code>/diagnostics/messaging</code></li> </ul> <p>Mensageria \u00e9 a funda\u00e7\u00e3o da escalabilidade desacoplada. Este m\u00f3dulo est\u00e1 pronto para ser enterprise-grade com rastreabilidade e controle.</p>"},{"location":"benchmark/athomic-observability-module/","title":"\ud83d\udcc8 Athomic \u2013 M\u00f3dulo Observability (Logging, Metrics, Tracing)","text":"<p>Este documento apresenta a avalia\u00e7\u00e3o do m\u00f3dulo <code>observability</code> da Athomic, respons\u00e1vel por oferecer visibilidade operacional atrav\u00e9s de logs, m\u00e9tricas e tracing distribu\u00eddo.</p>"},{"location":"benchmark/athomic-observability-module/#o-que-ja-esta-implementado","title":"\u2705 O que j\u00e1 est\u00e1 implementado","text":"<ul> <li>Logging estruturado com <code>orjson</code>, <code>Brotli</code> e mascaramento de dados sens\u00edveis</li> <li>Tracing com <code>trace_id</code>, <code>tenant_id</code>, <code>span_id</code></li> <li>Integra\u00e7\u00e3o com Prometheus via <code>prometheus_client</code></li> <li>Decorators e registries de maskers e filtros de log</li> <li>Log context injection e contextualiza\u00e7\u00e3o por requisi\u00e7\u00e3o</li> </ul>"},{"location":"benchmark/athomic-observability-module/#diagnostico-atual","title":"\ud83d\udcca Diagn\u00f3stico Atual","text":"Dimens\u00e3o Status Logging estruturado e filtrado \ud83d\udfe2 Alta Tracing com contexto \ud83d\udfe2 Alta M\u00e9tricas e Prometheus \ud83d\udfe1 M\u00e9dia Painel visual / UI \ud83d\udd34 Ausente Multi-output + fallback \ud83d\udd34 Ausente Suporte modular e por plugin \ud83d\udfe1 M\u00e9dia Sampling / rate-limit de logs \ud83d\udd34 Ausente"},{"location":"benchmark/athomic-observability-module/#oportunidades-de-melhoria","title":"\u26a0\ufe0f Oportunidades de melhoria","text":"<ul> <li>Decorators como <code>@trace_span(\"plugin\")</code> para spans autom\u00e1ticos</li> <li>Registry de m\u00e9tricas por plugin</li> <li>Exposi\u00e7\u00e3o via <code>/diagnostics/logs</code>, <code>/diagnostics/metrics</code></li> <li>CLI <code>athomic observability trace &lt;trace_id&gt;</code></li> <li>Suporte a m\u00faltiplos backends de log com fallback</li> <li>Integra\u00e7\u00e3o com Jaeger, Zipkin ou OTEL para visualiza\u00e7\u00e3o distribu\u00edda</li> </ul>"},{"location":"benchmark/athomic-observability-module/#arquitetura-ideal","title":"\ud83d\udcd0 Arquitetura Ideal","text":""},{"location":"benchmark/athomic-observability-module/#1-trace-automatico","title":"1. Trace autom\u00e1tico","text":"<pre><code>@trace_span(\"messaging\")\ndef publish(...): ...\n</code></pre>"},{"location":"benchmark/athomic-observability-module/#2-registro-de-metricas","title":"2. Registro de m\u00e9tricas","text":"<pre><code>METRICS_REGISTRY.register(\"resilience\", Gauge(\"retry_invoked_total\"))\n</code></pre>"},{"location":"benchmark/athomic-observability-module/#3-diagnostico-via-api","title":"3. Diagn\u00f3stico via API","text":"<ul> <li><code>/diagnostics/metrics</code></li> <li><code>/diagnostics/traces</code></li> <li><code>/diagnostics/logs</code></li> </ul>"},{"location":"benchmark/athomic-observability-module/#4-cli","title":"4. CLI","text":"<pre><code>athomic observability trace &lt;trace_id&gt;\nathomic logs tail --plugin=messaging\n</code></pre>"},{"location":"benchmark/athomic-observability-module/#roadmap-sugerido","title":"\ud83c\udfaf Roadmap sugerido","text":"<ul> <li>[ ] Registry de m\u00e9tricas e spans por m\u00f3dulo</li> <li>[ ] CLI de tracing/logs</li> <li>[ ] Integra\u00e7\u00e3o com ferramentas externas (Jaeger, Tempo, Sentry)</li> <li>[ ] Painel web <code>/diagnostics</code></li> <li>[ ] Log sampling para alta escala</li> </ul> <p>Observabilidade n\u00e3o \u00e9 apenas log \u2014 \u00e9 o senso situacional da Athomic. Investir aqui reduz MTTR e acelera evolu\u00e7\u00e3o segura.</p>"},{"location":"benchmark/athomic-performance-module/","title":"\u2699\ufe0f Athomic \u2013 M\u00f3dulo Performance (Cache, Compress\u00e3o, Locking)","text":"<p>Este documento descreve o m\u00f3dulo <code>performance</code> da Athomic, sua arquitetura atual, pontos fortes, oportunidades de evolu\u00e7\u00e3o e roadmap sugerido.</p>"},{"location":"benchmark/athomic-performance-module/#o-que-ja-esta-implementado","title":"\u2705 O que j\u00e1 est\u00e1 implementado","text":"<ul> <li>Cache com suporte a TTL</li> <li>Providers para mem\u00f3ria e Redis</li> <li>Decorators <code>@cached</code>, <code>@locked</code></li> <li>Compress\u00e3o com Brotli</li> <li>Serializa\u00e7\u00e3o com orjson</li> <li>Locking ass\u00edncrono para concorr\u00eancia</li> </ul>"},{"location":"benchmark/athomic-performance-module/#diagnostico-atual","title":"\ud83d\udcca Diagn\u00f3stico Atual","text":"Dimens\u00e3o Status Cache e locking \ud83d\udfe2 Alta Compress\u00e3o e serializa\u00e7\u00e3o \ud83d\udfe2 Alta Observabilidade de performance \ud83d\udfe1 M\u00e9dia Cache por escopo (tenant, plugin) \ud83d\udfe1 M\u00e9dia Gest\u00e3o via CLI ou API \ud83d\udd34 Ausente Composi\u00e7\u00e3o de decorators (tracing) \ud83d\udd34 Ausente"},{"location":"benchmark/athomic-performance-module/#oportunidades-de-melhoria","title":"\u26a0\ufe0f Oportunidades de melhoria","text":"<ul> <li>Falta observabilidade por cache hit/miss</li> <li>N\u00e3o h\u00e1 dashboard ou endpoint de estat\u00edsticas</li> <li>Falta suporte expl\u00edcito a cache por tenant/plugin</li> <li>Falta CLI para gerenciar e visualizar uso do cache</li> <li>Decorators n\u00e3o est\u00e3o integrados com tracing</li> </ul>"},{"location":"benchmark/athomic-performance-module/#arquitetura-ideal","title":"\ud83d\udcd0 Arquitetura Ideal","text":""},{"location":"benchmark/athomic-performance-module/#interfaces","title":"Interfaces","text":"<pre><code>class BaseCache(Protocol):\n    async def get(self, key: str) -&gt; Any\n    async def set(self, key: str, value: Any, ttl: int): ...\n    async def invalidate(self, key: str): ...\n</code></pre>"},{"location":"benchmark/athomic-performance-module/#registry-de-providers","title":"Registry de providers","text":"<pre><code>cache_registry.register(\"redis\", RedisCache())\ncache_registry.get(\"default\").get(...)\n</code></pre>"},{"location":"benchmark/athomic-performance-module/#cache-com-prefixo-por-tenant","title":"Cache com prefixo por tenant","text":"<pre><code>key = f\"{tenant_id}:get_user:{user_id}\"\n</code></pre>"},{"location":"benchmark/athomic-performance-module/#decorator-com-tracing","title":"Decorator com tracing","text":"<pre><code>@cached_traceable(ttl=60)\nasync def get_user_data(...):\n    ...\n</code></pre>"},{"location":"benchmark/athomic-performance-module/#propostas-de-cli","title":"\ud83d\udd27 Propostas de CLI","text":"<pre><code>athomic cache stats\nathomic cache flush --prefix=tenant_x\nathomic cache prefill --module=user\n</code></pre>"},{"location":"benchmark/athomic-performance-module/#roadmap-sugerido","title":"\ud83c\udfaf Roadmap sugerido","text":"<ul> <li>[ ] Registry de CacheProviders</li> <li>[ ] Decorator <code>@cached_traceable()</code></li> <li>[ ] M\u00e9tricas Prometheus: <code>cache_hit_total</code>, <code>cache_latency_seconds</code></li> <li>[ ] CLI de cache</li> <li>[ ] Endpoint REST para cache flushing seguro</li> <li>[ ] Estrat\u00e9gia de prefill em startup (<code>on_startup_load_cache()</code>)</li> </ul> <p>Este m\u00f3dulo \u00e9 fundamental para escalar aplica\u00e7\u00f5es Athomic com efici\u00eancia e controle de recursos.</p>"},{"location":"benchmark/athomic-resilience-module/","title":"\ud83d\udee1\ufe0f Athomic \u2013 M\u00f3dulo Resilience (Retry, Fallback, Circuit Breaker)","text":"<p>Este documento descreve a avalia\u00e7\u00e3o do m\u00f3dulo <code>resilience</code> da Athomic, respons\u00e1vel por implementar padr\u00f5es de toler\u00e2ncia a falhas.</p>"},{"location":"benchmark/athomic-resilience-module/#o-que-ja-esta-implementado","title":"\u2705 O que j\u00e1 est\u00e1 implementado","text":"<ul> <li>Retry, fallback, circuit breaker, timeout, bulkhead</li> <li>Decorators funcionais para aplicar resili\u00eancia</li> <li>Integra\u00e7\u00e3o com tracing e logging</li> <li>Modulariza\u00e7\u00e3o por fun\u00e7\u00e3o</li> </ul>"},{"location":"benchmark/athomic-resilience-module/#diagnostico-atual","title":"\ud83d\udcca Diagn\u00f3stico Atual","text":"Dimens\u00e3o Status Robustez dos padr\u00f5es \ud83d\udfe2 Alta Modulariza\u00e7\u00e3o e separa\u00e7\u00e3o \ud83d\udfe2 Alta Registry de componentes \ud83d\udd34 Ausente Composi\u00e7\u00e3o funcional \ud83d\udfe1 M\u00e9dia Configura\u00e7\u00e3o din\u00e2mica por m\u00f3dulo \ud83d\udd34 Ausente Tracing por inst\u00e2ncia \ud83d\udfe1 M\u00e9dia Pluginiza\u00e7\u00e3o e extens\u00e3o \ud83d\udd34 Ausente"},{"location":"benchmark/athomic-resilience-module/#oportunidades-de-melhoria","title":"\u26a0\ufe0f Oportunidades de melhoria","text":"<ul> <li>Criar um registry para <code>ResilienceComponent</code></li> <li>Composi\u00e7\u00e3o funcional entre <code>with_retry</code>, <code>with_cb</code>, etc.</li> <li>Suporte a configura\u00e7\u00f5es din\u00e2micas por tipo de erro, servi\u00e7o, ou rota</li> <li>Decorators rastre\u00e1veis com spans e m\u00e9tricas Prometheus</li> <li>Possibilitar aplica\u00e7\u00e3o de m\u00faltiplas estrat\u00e9gias em cadeia</li> </ul>"},{"location":"benchmark/athomic-resilience-module/#arquitetura-ideal","title":"\ud83d\udcd0 Arquitetura Ideal","text":""},{"location":"benchmark/athomic-resilience-module/#1-registry-de-componentes","title":"1. Registry de componentes","text":"<pre><code>resilience_registry.register(\"retry\", RetryStrategy())\n</code></pre>"},{"location":"benchmark/athomic-resilience-module/#2-composicao-fluente","title":"2. Composi\u00e7\u00e3o fluente","text":"<pre><code>@with_resilience([\n    with_retry(max_tries=3),\n    with_circuit_breaker(failure_threshold=0.5)\n])\nasync def process_event(...):\n    ...\n</code></pre>"},{"location":"benchmark/athomic-resilience-module/#3-configuracao-dinamica","title":"3. Configura\u00e7\u00e3o din\u00e2mica","text":"<pre><code>[resilience.services.provider.]\nretry.max_tries = 5\ncircuit_breaker.failure_threshold = 0.4\n</code></pre>"},{"location":"benchmark/athomic-resilience-module/#4-observabilidade-por-componente","title":"4. Observabilidade por componente","text":"<ul> <li><code>retry_invoked_total</code></li> <li><code>circuit_breaker_open_total</code></li> <li><code>resilience_failures_total{strategy=\"retry\"}</code></li> </ul>"},{"location":"benchmark/athomic-resilience-module/#roadmap-sugerido","title":"\ud83c\udfaf Roadmap sugerido","text":"<ul> <li>[ ] Criar <code>ResilienceComponent</code> base + registry</li> <li>[ ] Decorator <code>@with_resilience([...])</code> para composi\u00e7\u00e3o</li> <li>[ ] Suporte a configura\u00e7\u00e3o por servi\u00e7o</li> <li>[ ] M\u00e9tricas por estrat\u00e9gia</li> <li>[ ] CLI: <code>athomic resilience status</code>, <code>athomic resilience test</code></li> </ul> <p>Este m\u00f3dulo \u00e9 a espinha dorsal da robustez da Athomic e ser\u00e1 base para extens\u00f5es futuras.</p>"},{"location":"benchmark/athomic-security-module/","title":"\ud83d\udd10 Athomic \u2013 M\u00f3dulo Security (Auth, Vault, Policies)","text":"<p>Este documento apresenta a avalia\u00e7\u00e3o do m\u00f3dulo <code>security</code> da Athomic, respons\u00e1vel por autentica\u00e7\u00e3o, autoriza\u00e7\u00e3o, prote\u00e7\u00e3o de credenciais e controle de acesso.</p>"},{"location":"benchmark/athomic-security-module/#o-que-ja-esta-implementado","title":"\u2705 O que j\u00e1 est\u00e1 implementado","text":"<ul> <li>Integra\u00e7\u00e3o com Vault e fallback seguro</li> <li>Credential Proxy com m\u00faltiplos providers (Vault, Env, Files)</li> <li>JWT authentication e <code>AuthContext</code> com <code>trace_id</code>, <code>tenant_id</code></li> <li>IP filtering e rota p\u00fablica via <code>@public_route</code></li> <li>Controle de escopo (<code>is_admin</code>, <code>user_id</code>, <code>is_internal</code>)</li> <li><code>SecuritySettings</code> bem estruturado</li> </ul>"},{"location":"benchmark/athomic-security-module/#diagnostico-atual","title":"\ud83d\udcca Diagn\u00f3stico Atual","text":"Dimens\u00e3o Status Gerenciamento de credenciais \ud83d\udfe2 Alta Modularidade e fallback seguro \ud83d\udfe2 Alta JWT e autentica\u00e7\u00e3o b\u00e1sica \ud83d\udfe2 Alta Autoriza\u00e7\u00e3o baseada em roles \ud83d\udfe1 M\u00e9dia Auditoria \ud83d\udd36 Em potencial OAuth2 / IdPs externos \ud83d\udd34 Ausente Painel / CLI de seguran\u00e7a \ud83d\udd34 Ausente"},{"location":"benchmark/athomic-security-module/#oportunidades-de-melhoria","title":"\u26a0\ufe0f Oportunidades de melhoria","text":"<ul> <li>Suporte a RBAC/ABAC com <code>PolicyManager</code></li> <li>Decorator <code>@requires_policy(\"can_edit\")</code></li> <li>CLI <code>athomic security revoke-token</code>, <code>status</code></li> <li>Eventos de auditoria e logs sens\u00edveis</li> <li>Suporte a OAuth2 / SSO / Keycloak</li> <li>Painel de gest\u00e3o de tokens, IPs, credenciais ativas</li> </ul>"},{"location":"benchmark/athomic-security-module/#arquitetura-ideal","title":"\ud83d\udcd0 Arquitetura Ideal","text":""},{"location":"benchmark/athomic-security-module/#1-policymanager","title":"1. PolicyManager","text":"<pre><code>if not policy_manager.check(user_ctx, \"can_delete_user\"):\n    raise ForbiddenException(...)\n</code></pre>"},{"location":"benchmark/athomic-security-module/#2-decorators","title":"2. Decorators","text":"<pre><code>@requires_policy(\"admin:delete\")\ndef delete_user(...): ...\n</code></pre>"},{"location":"benchmark/athomic-security-module/#3-cli","title":"3. CLI","text":"<pre><code>athomic security list-users\nathomic security revoke-token &lt;user_id&gt;\n</code></pre>"},{"location":"benchmark/athomic-security-module/#4-auditoria","title":"4. Auditoria","text":"<ul> <li>Log estruturado:</li> </ul> <pre><code>{\n  \"event\": \"token_revoked\",\n  \"actor\": \"admin_123\",\n  \"target\": \"user_456\",\n  \"ip\": \"201.1.1.1\",\n  \"time\": \"...\"\n}\n</code></pre>"},{"location":"benchmark/athomic-security-module/#roadmap-sugerido","title":"\ud83c\udfaf Roadmap sugerido","text":"<ul> <li>[ ] <code>PolicyManager</code> + suporte a escopos e permiss\u00f5es</li> <li>[ ] CLI de seguran\u00e7a</li> <li>[ ] Eventos de auditoria com log seguro</li> <li>[ ] Suporte a OAuth2 e federated login</li> <li>[ ] Painel de credenciais ativas + permiss\u00f5es</li> </ul> <p>Seguran\u00e7a forte, modular e audit\u00e1vel \u00e9 diferencial. Este m\u00f3dulo j\u00e1 \u00e9 avan\u00e7ado, mas pode evoluir para compliance e enterprise-grade.</p>"},{"location":"benchmark/athomic_cloud_abstraction/","title":"\u2601\ufe0f Athomic \u2013 Benchmark de Componentes Sens\u00edveis \u00e0 Nuvem e Estrat\u00e9gias para Agnosticidade","text":"<p>Este documento apresenta os principais recursos e servi\u00e7os sens\u00edveis a provedores de nuvem utilizados por frameworks modernos e prop\u00f5e uma arquitetura para tornar a Athomic completamente cloud-agnostic.</p>"},{"location":"benchmark/athomic_cloud_abstraction/#benchmark-de-componentes-sensiveis-a-nuvem","title":"\ud83d\udcca Benchmark de Componentes Sens\u00edveis \u00e0 Nuvem","text":"Componente / Servi\u00e7o Athomic Hoje AWS GCP Azure Agnosticidade atual Recomenda\u00e7\u00e3o Secrets Vault, Env Secrets Manager Secret Manager Key Vault \u2705 Criar <code>SecretsProvider</code> Mensageria Kafka, RabbitMQ SQS/SNS Pub/Sub Service Bus \u2705 J\u00e1 usa <code>MessagingProvider</code> Feature Flags Redis/config AppConfig Firebase Remote Config Azure App Configuration \u26a0\ufe0f Criar <code>FeatureFlagProvider</code> Object Storage N/A S3 GCS Blob Storage \u274c Criar <code>StorageProvider</code> File Temp / Transfer Local S3 signed URLs GCS signed URLs Azure SAS \u274c Criar <code>FileStorageProvider</code> Observabilidade Prometheus/OTLP (manual) CloudWatch/X-Ray Stackdriver/Cloud Trace Azure Monitor + AppInsights \u26a0\ufe0f Criar <code>ObservabilityExporter</code> Service Discovery Consul Cloud Map Service Directory Azure App Config \u26a0\ufe0f Criar <code>DiscoveryProvider</code> Auth/OAuth2 JWT Local Cognito Firebase Auth Azure AD \u26a0\ufe0f Criar <code>AuthProvider</code> Configuration Runtime Dynaconf + <code>.env</code> AppConfig / SSM Parameter Config Controller Azure App Configuration \u26a0\ufe0f Criar <code>ConfigProvider</code> opcional Schema Registry N/A (manual JSONSchema) AWS Glue Registry Data Catalog + Pub/Sub Azure Schema Registry \u274c Criar <code>SchemaProvider</code> Job Scheduling Manual / future plans EventBridge Cloud Scheduler Azure Logic Apps \u274c Criar <code>SchedulerProvider</code> Auth Federation (SSO) JWT / IP Check Cognito Firebase Auth Azure AD \u26a0\ufe0f Suporte a OIDC/OAuth2 providers"},{"location":"benchmark/athomic_cloud_abstraction/#estrategia-recomendada-para-agnosticidade","title":"\u2705 Estrat\u00e9gia Recomendada para Agnosticidade","text":""},{"location":"benchmark/athomic_cloud_abstraction/#1-criar-contratos-por-dominio","title":"1. Criar contratos por dom\u00ednio:","text":"<pre><code>class MessagingProvider(Protocol):\n    async def publish(self, topic: str, payload: dict): ...\n\nclass StorageProvider(Protocol):\n    async def upload(self, path: str, content: bytes): ...\n    async def get_url(self, path: str) -&gt; str: ...\n</code></pre>"},{"location":"benchmark/athomic_cloud_abstraction/#2-selecao-de-provider-via-settingsconfig","title":"2. Sele\u00e7\u00e3o de provider via settings/config:","text":"<pre><code>if settings.cloud == \"aws\":\n    provider_registry.register(\"messaging\", SQSMessagingProvider())\n</code></pre>"},{"location":"benchmark/athomic_cloud_abstraction/#3-modularizacao-via-registry-fallback","title":"3. Modulariza\u00e7\u00e3o via registry + fallback:","text":"<pre><code>registry.register(\"secrets\", MultiSecretsProvider([\n    VaultProvider(),\n    EnvFallbackProvider()\n]))\n</code></pre>"},{"location":"benchmark/athomic_cloud_abstraction/#roadmap-para-athomic-agnostic-cloud","title":"\ud83c\udfaf Roadmap para Athomic Agnostic Cloud","text":"<ul> <li>[ ] Criar <code>nala.athomic.cloud.interfaces</code> com os contratos</li> <li>[ ] Implementar backends b\u00e1sicos: Redis, Local, S3</li> <li>[ ] Adicionar suporte a sele\u00e7\u00e3o via <code>settings.toml</code></li> <li>[ ] Documentar arquitetura plug\u00e1vel de providers</li> <li>[ ] Fornecer Terraform templates para m\u00faltiplas nuvens</li> </ul> <p>Com essa abordagem, a Athomic pode se tornar o framework Python mais preparado para ambientes multicloud e h\u00edbridos.</p>"},{"location":"benchmark/athomic_cloud_frameworks_matrix/","title":"\u2601\ufe0f Athomic &amp; Frameworks \u2013 Matriz de Recursos Sens\u00edveis \u00e0 Nuvem","text":"<p>Esta matriz compara como Athomic, Spring Boot, NestJS, Django e FastAPI abordam recursos sens\u00edveis \u00e0 nuvem, com foco em abstra\u00e7\u00e3o, flexibilidade e portabilidade.</p>"},{"location":"benchmark/athomic_cloud_frameworks_matrix/#matriz-de-servicos-e-suporte-multicloud","title":"\ud83d\udcca Matriz de Servi\u00e7os e Suporte Multicloud","text":"Servi\u00e7o / Componente Athomic Spring Boot NestJS Django FastAPI Notas Athomic Secrets Management \u2705 Vault + .env \u2705 Spring Vault \u26a0\ufe0f Lib externa + manual \u26a0\ufe0f Env/boto3 \u26a0\ufe0f boto3/manual J\u00e1 possui providers e fallback Mensageria \u2705 Kafka/RabbitMQ \u2705 Cloud Stream + Binder \u26a0\ufe0f Plugins \u26a0\ufe0f Celery/SQS plugin \u26a0\ufe0f manual pubsub/SQS Registry permite suportar v\u00e1rios Feature Flags \u26a0\ufe0f Redis/manual \u2705 FF4J, Togglz \u26a0\ufe0f LaunchDarkly plugin \u274c \u274c Precisa abstra\u00e7\u00e3o formal (<code>FeatureFlagProvider</code>) Object Storage \u274c (planejado) \u2705 S3, GCS via beans \u26a0\ufe0f Manual + SDK \u26a0\ufe0f boto3/manual \u26a0\ufe0f gcsfs/boto3 Criar <code>StorageProvider</code> File Uploads / Temp \u274c \u2705 S3 Multipart \u26a0\ufe0f middleware \u26a0\ufe0f Manual \u26a0\ufe0f manual Requer abstra\u00e7\u00e3o + assinatura URL Observabilidade (Metrics) \u26a0\ufe0f Prometheus \u2705 Micrometer \u26a0\ufe0f Prom plugins \u26a0\ufe0f Manual exporter \u26a0\ufe0f Manual Expor via OpenMetrics/OTLP Tracing (Spans) \u26a0\ufe0f Custom <code>span_id</code> \u2705 Sleuth / OTEL \u2705 OTEL plugin \u274c \u26a0\ufe0f manual OTEL Suporte parcial \u2014 expandir com OTLP Service Discovery \u2705 Consul \u2705 Eureka/CloudMap \u26a0\ufe0f Custom / hardcoded \u274c \u274c Criar <code>DiscoveryProvider</code> Auth (JWT) \u2705 JWT \u2705 Spring Security \u2705 JWT module \u2705 Django auth \u26a0\ufe0f manual middleware Precisa OAuth2 + IdP Auth (OAuth2 / SSO) \u26a0\ufe0f Em planejamento \u2705 OAuth2 login \u2705 Passport + OIDC \u2705 Django Allauth \u26a0\ufe0f FastAPI Login/OAuth Planejar <code>AuthProvider</code> Config Runtime \u2705 Dynaconf \u2705 AppConfig/Env \u2705 dotenv/config \u26a0\ufe0f Manual settings \u2705 Pydantic Padr\u00e3o forte \u2014 pode adicionar Cloud Provider Job Scheduling \u274c \u2705 Spring Scheduler \u26a0\ufe0f Plugin externo \u2705 Celery Beat \u274c Criar <code>SchedulerProvider</code> Schema Registry \u274c \u2705 Glue/Confluent \u26a0\ufe0f manual JSONSchema \u26a0\ufe0f Django model/docs \u26a0\ufe0f Pydantic, sem registry Criar <code>SchemaProvider</code> opcional Storage Cache \u2705 Redis/Memory \u2705 Redis/Memory \u2705 CacheModule \u2705 Django Cache Framework \u2705 <code>fastapi-cache</code> TTL, Brotli e compress\u00e3o j\u00e1 integrados Deploy/CI/CD \u2705 Docker \u2705 Spring Native/Cloud \u2705 Docker/Nest CLI \u2705 Heroku/CI/CD \u2705 Docker + Deta + Cloud Agn\u00f3stico, suporta m\u00faltiplas estrat\u00e9gias Terraform/Helm compat. \u26a0\ufe0f Sem base ainda \u2705 Amplamente usado \u26a0\ufe0f parcial \u26a0\ufe0f parcial \u26a0\ufe0f manual Planejar <code>infra/</code> com charts/templates"},{"location":"benchmark/athomic_cloud_frameworks_matrix/#conclusoes","title":"\ud83e\udde0 Conclus\u00f5es","text":"<ul> <li>Spring Boot \u00e9 o framework com melhor integra\u00e7\u00e3o nativa com nuvem, por\u00e9m fortemente acoplado \u00e0 AWS/GCP.</li> <li>NestJS oferece plugins para cloud, mas requer configura\u00e7\u00e3o manual.</li> <li>Django/FastAPI dependem fortemente de bibliotecas externas e pr\u00e1ticas manuais.</li> <li>Athomic tem grande potencial para abstra\u00e7\u00e3o verdadeira, com estruturas como <code>registry</code>, <code>provider</code>, e contratos formais \u2014 sendo mais agn\u00f3stica por design.</li> </ul> <p>Com a padroniza\u00e7\u00e3o dos contratos sugeridos (ex: <code>StorageProvider</code>, <code>AuthProvider</code>, <code>MessagingProvider</code>), a Athomic pode se tornar o framework Python mais preparado para ambientes multicloud, edge e h\u00edbridos.</p>"},{"location":"benchmark/athomic_feature_flags_quality_review/","title":"\ud83c\udfc1 Athomic \u2013 Code Quality &amp; Architecture Review: M\u00f3dulo Feature Flags","text":"<p>Este relat\u00f3rio aplica a metodologia de revis\u00e3o arquitetural ao m\u00f3dulo <code>feature_flags</code>, respons\u00e1vel por controle din\u00e2mico de funcionalidades com escopo configur\u00e1vel.</p>"},{"location":"benchmark/athomic_feature_flags_quality_review/#contexto-do-modulo","title":"\ud83d\udce6 Contexto do M\u00f3dulo","text":"<p>O m\u00f3dulo ainda est\u00e1 em est\u00e1gio inicial, com inten\u00e7\u00e3o de permitir ativa\u00e7\u00e3o/desativa\u00e7\u00e3o de funcionalidades por tenant, ambiente, rota, ou condi\u00e7\u00e3o contextual. A ideia \u00e9 integrar tanto flags est\u00e1ticas quanto din\u00e2micas (em tempo de execu\u00e7\u00e3o, via Redis, config, etc).</p>"},{"location":"benchmark/athomic_feature_flags_quality_review/#avaliacao-por-criterio","title":"\u2705 Avalia\u00e7\u00e3o por Crit\u00e9rio","text":"Crit\u00e9rio Status Coment\u00e1rio Design Patterns aplicados \ud83d\udd36 Em constru\u00e7\u00e3o. Planejado uso de Registry, Strategy e Context Injection. SRP (Responsabilidade \u00fanica) \ud83d\udfe1 Estrutura inicial ainda centralizada; precisa dividir controle, storage, l\u00f3gica. Extensibilidade (OCP) \ud83d\udd36 Deve permitir m\u00faltiplos backends (mem\u00f3ria, Redis, config), mas ainda n\u00e3o h\u00e1 abstra\u00e7\u00e3o formal. Contratos claros (Interface) \ud83d\udd34 Falta interface <code>FeatureFlagProvider</code>, ou <code>FeatureFlagResolver</code>. Testabilidade \ud83d\udd36 Vi\u00e1vel com mock de provider, mas dependente da modulariza\u00e7\u00e3o. Observabilidade \ud83d\udd34 Sem m\u00e9tricas ou tracking de hits/uso por flag. Composi\u00e7\u00e3o com escopo \ud83d\udd34 Ainda n\u00e3o h\u00e1 forma padronizada de escopo (user_id, tenant, time, etc). Compara\u00e7\u00e3o com frameworks \ud83d\udd36 Abaixo de LaunchDarkly, Unleash, Flagsmith \u2014 mas com potencial. Clareza e sem\u00e2ntica do c\u00f3digo \ud83d\udfe1 C\u00f3digo direto, mas pouco declarativo para flags."},{"location":"benchmark/athomic_feature_flags_quality_review/#comparacao-com-frameworks-enterprise","title":"\ud83e\uddea Compara\u00e7\u00e3o com frameworks enterprise","text":"Aspecto Athomic (atual) LaunchDarkly / Flagsmith Unleash M\u00faltiplos backends \u274c est\u00e1tico \u2705 Redis, API, SDK \u2705 file, API, Redis Condi\u00e7\u00f5es compostas \u274c \u2705 por regra \u2705 por estrat\u00e9gia Escopo custom \u274c \u2705 userId, context, segment \u2705 environment, context Observabilidade por flag \u274c \u2705 analytics \u2705 usage tracking CLI ou painel \u274c \u2705 dashboard \u2705 dashboard"},{"location":"benchmark/athomic_feature_flags_quality_review/#padroes-de-projeto-recomendados","title":"\u2705 Padr\u00f5es de Projeto recomendados","text":"<ul> <li>Strategy para resolu\u00e7\u00e3o de flag</li> <li>Provider para backend (Config, Redis, etc.)</li> <li>Registry de flags ativos</li> <li>Context Object (<code>FeatureContext</code>)</li> <li>Decorator <code>@requires_feature(\"flag_x\")</code></li> </ul>"},{"location":"benchmark/athomic_feature_flags_quality_review/#oportunidades-de-melhoria","title":"\u26a0\ufe0f Oportunidades de Melhoria","text":"<ul> <li>Criar <code>FeatureFlagProvider</code> com backends plugg\u00e1veis</li> <li>Decorator <code>@requires_feature(\"nova_pagina\")</code></li> <li>Escopo: tenant, user, time, rollout percentual</li> <li>CLI <code>athomic features list</code>, <code>enable</code>, <code>disable</code>, <code>check</code></li> <li>M\u00e9tricas: <code>feature_flag_enabled_total</code>, <code>feature_flag_error_total</code></li> <li>Painel <code>/diagnostics/feature_flags</code></li> </ul>"},{"location":"benchmark/athomic_feature_flags_quality_review/#roadmap-sugerido","title":"\ud83c\udfaf Roadmap sugerido","text":"<ul> <li>[ ] Criar <code>BaseFeatureFlagProvider</code> e <code>FeatureFlagManager</code></li> <li>[ ] Adicionar suporte a escopo contextualizado</li> <li>[ ] CLI de gest\u00e3o de flags</li> <li>[ ] Observabilidade de uso por flag</li> <li>[ ] Registry ou painel visual de status por ambiente</li> </ul> <p>O m\u00f3dulo feature_flags ainda \u00e9 embrion\u00e1rio, mas tem alto potencial estrat\u00e9gico para controle progressivo de funcionalidades, testes A/B e customiza\u00e7\u00f5es por tenant.</p>"},{"location":"benchmark/athomic_final_assessment/","title":"\ud83e\uddfe Parecer Final \u2013 Avalia\u00e7\u00e3o Arquitetural, Benchmark e Estrat\u00e9gia da Athomic","text":"<p>Este parecer consolida todas as an\u00e1lises realizadas sobre a Athomic em termos de arquitetura, qualidade de c\u00f3digo, padr\u00f5es de projeto, compara\u00e7\u00e3o com frameworks de mercado e prontid\u00e3o para ambientes multicloud.</p>"},{"location":"benchmark/athomic_final_assessment/#visao-geral-da-athomic","title":"\u2705 Vis\u00e3o Geral da Athomic","text":"<p>A Athomic \u00e9 uma plataforma modular, extens\u00edvel e com grande ader\u00eancia a boas pr\u00e1ticas de engenharia. Seu design \u00e9 fortemente inspirado por frameworks enterprise como Spring Boot, NestJS e Django, mas adaptado com eleg\u00e2ncia ao ecossistema Python moderno.</p>"},{"location":"benchmark/athomic_final_assessment/#pontos-fortes-identificados","title":"\ud83d\udd0d Pontos Fortes Identificados","text":"\u00c1rea Destaques Arquitetura Modular Separa\u00e7\u00e3o clara por dom\u00ednio (messaging, security, resilience, etc.) Design Patterns Uso extenso de Registry, Decorator, Strategy, Adapter, Context Object Testabilidade Interfaces bem definidas, baixa depend\u00eancia, facilidade de mock Extensibilidade Registry + provider pattern em quase todos os m\u00f3dulos Resili\u00eancia Decorators reutiliz\u00e1veis, fallback handler e planos de melhoria cont\u00ednua Observabilidade Logging estruturado, mascaramento, spans e integra\u00e7\u00e3o Prometheus Seguran\u00e7a JWT, controle de IP, escopo e isolamento contextual Mensageria Interface desacoplada, suporte a Kafka e RabbitMQ com potencial multicloud Performance Cache, compress\u00e3o e locks ass\u00edncronos reutiliz\u00e1veis Configura\u00e7\u00e3o Pydantic + Dynaconf com contratos por dom\u00ednio"},{"location":"benchmark/athomic_final_assessment/#oportunidades-de-evolucao","title":"\u26a0\ufe0f Oportunidades de Evolu\u00e7\u00e3o","text":"\u00c1rea A\u00e7\u00f5es Recomendadas Plugin System Criar <code>PluginBase</code>, <code>PluginManager</code>, ciclo de vida, CLI e status Resilience Composability Introduzir estrat\u00e9gia fluente (<code>@with_resilience([Retry, Circuit])</code>) CLI Padronizado <code>athomic secrets</code>, <code>athomic plugins</code>, <code>athomic config</code>, etc Feature Flags Criar <code>FeatureFlagProvider</code> com escopo por tenant/contexto Schema Registry Adicionar suporte a valida\u00e7\u00e3o de payloads via JSONSchema ou Avro Storage Provider Abstrair uploads para S3/GCS compat\u00edvel com assinaturas e preview OAuth2 &amp; Identity Providers Suporte a Cognito, Azure AD, Firebase Auth Lifecycle Orquestration <code>LifecycleParticipant</code> com ordem, prioridade e retry Observabilidade Expandida Tracing + m\u00e9tricas + eventos por componente / tenant Suporte Multicloud Providers para AWS/GCP/Azure em <code>secrets</code>, <code>messaging</code>, <code>storage</code>, etc"},{"location":"benchmark/athomic_final_assessment/#benchmark-com-frameworks","title":"\ud83c\udf10 Benchmark com Frameworks","text":"<ul> <li>Spring Boot lidera em integra\u00e7\u00e3o com nuvem, mas \u00e9 acoplado.</li> <li>NestJS tem estrutura modular forte, mas requer mais configura\u00e7\u00e3o manual.</li> <li>Django/FastAPI s\u00e3o poderosos, mas n\u00e3o oferecem nativamente os padr\u00f5es de projeto aplicados pela Athomic.</li> <li>Athomic se destaca por ser mais agn\u00f3stica, mais compon\u00edvel e mais estruturalmente limpa para evoluir em dire\u00e7\u00e3o a um produto de n\u00edvel enterprise.</li> </ul>"},{"location":"benchmark/athomic_final_assessment/#conclusao-estrategica","title":"\ud83e\udded Conclus\u00e3o Estrat\u00e9gica","text":"<p>A Athomic j\u00e1 oferece uma base madura para se tornar um framework Python enterprise-grade e cloud-agnostic. Com a consolida\u00e7\u00e3o de seu sistema de plugins, CLI, providers e observabilidade, pode ocupar um espa\u00e7o de destaque entre os frameworks modernos do ecossistema Python \u2014 especialmente para sistemas distribu\u00eddos, SaaS multitenant e arquiteturas plug\u00e1veis.</p>"},{"location":"benchmark/athomic_final_assessment/#proximos-passos-sugeridos","title":"\u2705 Pr\u00f3ximos Passos Sugeridos","text":"<ol> <li>Consolidar interfaces em <code>nala.athomic.cloud.interfaces</code></li> <li>Criar CLI <code>athomic</code> com comandos modulares</li> <li>Finalizar <code>PluginManager</code> com registry, lifecycle e escopo</li> <li>Expandir observabilidade com m\u00e9tricas + tracing + painel</li> <li>Publicar documenta\u00e7\u00e3o e guias para contribuidores</li> <li>Realizar testes de deploy em AWS, GCP e Azure</li> </ol>"},{"location":"devx/development/docker-organization/","title":"\ud83d\udc33 Organiza\u00e7\u00e3o de Arquivos Docker &amp; Docker Compose","text":"<p>Este documento descreve a estrutura recomendada para organizar os arquivos relacionados ao Docker e Docker Compose no projeto <code>athomic-docs</code>. O objetivo \u00e9 manter a clareza, facilitar a manuten\u00e7\u00e3o e seguir pr\u00e1ticas comuns.</p>"},{"location":"devx/development/docker-organization/#1-visao-geral","title":"1. Vis\u00e3o Geral","text":"<p>Usamos o Docker para criar ambientes de desenvolvimento consistentes e para empacotar a aplica\u00e7\u00e3o para diferentes est\u00e1gios (teste, produ\u00e7\u00e3o). O Docker Compose \u00e9 utilizado para orquestrar m\u00faltiplos containers, especialmente no ambiente de desenvolvimento local.</p>"},{"location":"devx/development/docker-organization/#2-estrutura-de-arquivos-recomendada","title":"2. Estrutura de Arquivos Recomendada","text":"<pre><code>athomic-docs/\n\u251c\u2500\u2500 .dockerignore         # &lt;- Na raiz: Ignora arquivos no build context\n\u251c\u2500\u2500 Dockerfile            # &lt;- Na raiz: Build da imagem da aplica\u00e7\u00e3o principal\n\u251c\u2500\u2500 docker-compose.yml    # &lt;- Na raiz: Stack de desenvolvimento principal\n\u251c\u2500\u2500 docker/               # &lt;- (Opcional) Para arquivos auxiliares\n\u2502   \u251c\u2500\u2500 docker-compose.override.yml # (Opcional) Overrides locais n\u00e3o versionados\n\u2502   \u251c\u2500\u2500 docker-compose.prod.yml     # (Opcional) Exemplo para produ\u00e7\u00e3o simplificada\n\u2502   \u251c\u2500\u2500 jaeger/             # (Opcional) Ex: Configs espec\u00edficas do Jaeger\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 mongo/              # (Opcional) Ex: Scripts de init do DB\n\u2502   \u2502   \u2514\u2500\u2500 initdb.d/\n\u2502   \u2502       \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 ...                 # Outras configs/scripts de servi\u00e7os\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 nala/\n\u251c\u2500\u2500 tests/\n\u251c\u2500\u2500 settings/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 poetry.lock\n\u251c\u2500\u2500 Makefile\n\u2514\u2500\u2500 ... (outros arquivos)\n</code></pre>"},{"location":"devx/development/docker-organization/#3-descricao-dos-arquivos-principais","title":"3. Descri\u00e7\u00e3o dos Arquivos Principais","text":""},{"location":"devx/development/docker-organization/#dockerfile-na-raiz","title":"<code>Dockerfile</code> (na raiz)","text":"<ul> <li>Prop\u00f3sito: Cont\u00e9m as instru\u00e7\u00f5es para construir a imagem Docker final da aplica\u00e7\u00e3o <code>athomic-docs</code>.</li> <li>Boas Pr\u00e1ticas: Utilizar multi-stage builds para otimizar o tamanho da imagem final.</li> </ul>"},{"location":"devx/development/docker-organization/#dockerignore-na-raiz","title":"<code>.dockerignore</code> (na raiz)","text":"<ul> <li>Prop\u00f3sito: Ignora arquivos durante o build Docker, acelerando o processo e evitando incluir arquivos sens\u00edveis.</li> </ul>"},{"location":"devx/development/docker-organization/#docker-composeyml-na-raiz","title":"<code>docker-compose.yml</code> (na raiz)","text":"<ul> <li>Prop\u00f3sito: Define e orquestra os servi\u00e7os necess\u00e1rios para o ambiente de desenvolvimento.</li> <li>Servi\u00e7os Comuns: <code>app</code>, <code>mongodb</code>, <code>redis</code>, <code>jaeger</code>, <code>consul</code>, <code>kafka</code>, entre outros.</li> </ul>"},{"location":"devx/development/docker-organization/#docker-diretorio-opcional","title":"<code>docker/</code> (Diret\u00f3rio Opcional)","text":"<ul> <li>Prop\u00f3sito: Armazenar arquivos adicionais como overrides, configura\u00e7\u00f5es espec\u00edficas e scripts auxiliares.</li> </ul>"},{"location":"devx/development/docker-organization/#4-uso-basico","title":"4. Uso B\u00e1sico","text":"<pre><code># Construir a imagem\ndocker build -t athomic-docs:latest .\n\n# Subir o ambiente local\ndocker compose up -d\n\n# Subir ambiente espec\u00edfico\ndocker compose -f docker/docker-compose.prod.yml up -d\n\n# Parar ambiente\ndocker compose down\n</code></pre> <p>Seguir esta estrutura ajuda a manter o projeto organizado e facilita o trabalho com Docker e Compose.</p>"},{"location":"devx/development/athomic-docs-guidelines/","title":"Todo modulo dev ter:","text":"<ul> <li>[] Valida\u00e7\u00e3o de config. (Se poss\u00edvel uma classe settings em cada modulo.)</li> <li>[] log estruturado com replace em dados sens\u00edveis - se estiver ativado - configur\u00e1vel</li> <li>[] retry em chamadas externas configur\u00e1vel</li> <li>[] fallback em chamadas externas - configur\u00e1vel</li> <li>[] cache configur\u00e1vel</li> <li>[] tracing configur\u00e1vel</li> <li>[] rate limiter configur\u00e1vel</li> <li>[] breakout configur\u00e1vel</li> <li>[] observability configur\u00e1vel</li> <li>[] garantir thread safe </li> <li>[] Pool de recursos - quando necess\u00e1rio - configur\u00e1vel</li> <li>[] Singleton - quando necess\u00e1rio - configur\u00e1vel</li> <li>[] Dependency injection - poder receber comportamentos/providers customizados - configur\u00e1vel</li> <li>[] Internacionaliza\u00e7\u00e3o - configur\u00e1vel</li> </ul>"},{"location":"devx/development/athomic-docs-guidelines/#no-desenvolviento","title":"No desenvolviento","text":"<ul> <li>[] Idenficar e aplicar os design patterns apropriados</li> <li>[] Clean code Clean Architecture  </li> <li>[*] Testes unit\u00e1rio</li> <li>[] Testes de integra\u00e7\u00e3o </li> <li>[*] Documenta\u00e7\u00e3o atualizadas</li> <li>[] testes e2e</li> <li>[*] Configura\u00e7\u00e3o por ambiente</li> <li>[] teste de carga/stress</li> <li>[*] teste de seguran\u00e7a -&gt; Acicionamos o bandity</li> <li>[] ferramentas de performance</li> <li>[] plano de deploy/release </li> <li>[] release incremental</li> </ul>"},{"location":"devx/development/athomic-docs-guidelines/#impotante","title":"IMPOTANTE","text":"<ul> <li>[ ] Implementar o case de arquitetura de venda de ingresso do teste do Google -&gt; Criar case de sucesso para apresentar ao mundo</li> </ul>"},{"location":"devx/development/athomic-docs-guidelines/#guideline","title":"guideline","text":"<ul> <li>[ ] sempre que um erro persistir em um arquivo muito grande refatore-o</li> <li>[ ] Sempre verificar o padr\u00e3o do arquivo e mant\u00ea-lo em novas edi\u00e7\u00f5es Ex: \"adicionar nova config no settings.toml\"</li> <li>[ ] sempre avaliar se modulo est\u00e1 fazendo muitos imports, provavelmente est\u00e1 assumindo muitas responsabilidades, o memos serve para classes e metodos.</li> <li>[ ] Sempre perguntar esse m\u00e9todo tem essa responsabilidade? </li> <li>[ ] Sempre verificar se existe logica duplicada - n\u00e3o fazer o que \u00e9 de responsabilidade externa.</li> <li>[ ] Antes de implementar verificar se existe uma lib interna ou externa que j\u00e1 faz a tarefa desejada</li> <li>[ ] Avaliar a complexidade e quebrar em tarefas simples </li> <li>[ ] Verificar poss\u00edveis cen\u00e1rios futuros para decidir estrat\u00e9gia de desenvolvimento</li> <li>[ ] Aten\u00e7\u00e3o ao logs de error ou exeption adequadamente</li> <li>[ ] Sempre verificar o c\u00f3digo para entender se o propblema j\u00e1 foi resolvido integral ou parcialmente e         entender como utilizar outros recursos ex.:         ```         Excelente ideia! Olhar como os provedores s\u00e3o instanciados em outros testes (unit\u00e1rios ou de integra\u00e7\u00e3o) \u00e9 a melhor forma de descobrir o padr\u00e3o correto.             Verifiquei os arquivos de teste no snapshot que voc\u00ea forneceu:             tests/unit/nala/athomic/performance/cache/providers/test_memory_cache_provider.py:             No teste test_memory_cache_provider_set_get_delete, a inst\u00e2ncia \u00e9 criada assim:             Python             provider = MemoryCacheProvider() # Sem argumentos!<pre><code>```\n</code></pre> <ul> <li>[ ] Tentar vairar os parametros de request no modelo quando ficar preso em um problema</li> </ul> </li> </ul>"},{"location":"devx/development/athomic-docs-guidelines/#docs","title":"Docs","text":"<ul> <li>[ ] Para mudan\u00e7as esrtuturais fazer o ADR no padr\u00e3o utilizado </li> <li>[ ] Atualizar/criar documenta\u00e7\u00e3o do modulo/classe </li> <li>[ ] Verificar e validar hiperlinks e referencias em documentos </li> </ul>"},{"location":"devx/development/athomic-docs-guidelines/#exemplode-de-throttling-dinamico-gemini","title":"Exemplode de throttling dinamico gemini:","text":"<p><code>Voc\u00ea est\u00e1 chegando ao limite de uso do Gemini Advanced. Saiba mais.</code></p> <pre><code>O Gemini tem limites de uso?\nO Gemini tem limites de uso para garantir uma \u00f3tima experi\u00eancia para todo mundo. Isso significa que, \u00e0s vezes, podemos limitar seu n\u00famero de comandos e conversas em um per\u00edodo espec\u00edfico. Sua capacidade \u00e9 restaurada regularmente, ent\u00e3o voc\u00ea pode voltar a conversar com o Gemini em breve.\n\nO n\u00famero de comandos que voc\u00ea pode usar antes de atingir o limite varia e depende de fatores como comprimento e complexidade dos comandos, tamanho e n\u00famero de arquivos enviados e dura\u00e7\u00e3o das conversas com o Gemini.\n\nTamb\u00e9m vamos avisar voc\u00ea quando estiver perto de alcan\u00e7ar a capacidade de conversas para um determinado per\u00edodo.\n</code></pre>"},{"location":"devx/development/athomic-docs-guidelines/#flexibilidade-maxima-nos-modulos","title":"Flexibilidade M\u00c1XIMA nos modulos","text":"<p>class FeatureFlagSettings(BaseModel):     # ...     cache: Optional[CacheSettings] = Field(default=None, alias=\"CACHE\")     # ... Avalia\u00e7\u00e3o: Poss\u00edvel, mas provavelmente excessivo. Isso permitiria configurar todo um sistema de cache separado (backend, host, port, prefixo, TTL, etc.) apenas para as feature flags. Pr\u00f3: M\u00e1xima flexibilidade se voc\u00ea realmente precisar de um backend de cache diferente (ex: Memory para flags, Redis para o resto) ou configura\u00e7\u00f5es totalmente distintas. Contra: Aumenta a complexidade da configura\u00e7\u00e3o e potencialmente a sobrecarga de infraestrutura (ex: duas pools de conex\u00e3o Redis). Na maioria dos casos, usar o cache principal da aplica\u00e7\u00e3o (get_cache()) com um TTL espec\u00edfico (como na Sugest\u00e3o 1) \u00e9 suficiente e mais simples.</p>"},{"location":"devx/development/profiling-performance/","title":"\u2699\ufe0f Profiling de Performance","text":"<p>O <code>athomic-docs</code> inclui um modo de profiling integrado para ajudar a diagnosticar gargalos de performance em requisi\u00e7\u00f5es espec\u00edficas. Ele utiliza o m\u00f3dulo <code>cProfile</code> do Python.</p>"},{"location":"devx/development/profiling-performance/#ativando-o-profiling","title":"\ud83d\ude80 Ativando o Profiling","text":"<p>Para ativar o profiling, ajuste as configura\u00e7\u00f5es no seu arquivo <code>.toml</code> (ex: <code>settings/development.toml</code>) ou via vari\u00e1veis de ambiente:</p> <pre><code>[dev.profiling]\nenabled = true           # Mude para true para ativar\nprofiler = \"cprofile\"\noutput_dir = \".profiles\"  # Diret\u00f3rio onde os arquivos .prof ser\u00e3o salvos\n</code></pre> <p>Ou via vari\u00e1veis de ambiente:</p> <pre><code>export NALA_PROFILING__ENABLED=true\nexport NALA_PROFILING__OUTPUT_DIR=\".profiles\"\n</code></pre> <p>\ud83d\udca1 Reinicie a aplica\u00e7\u00e3o ap\u00f3s alterar as configura\u00e7\u00f5es.</p>"},{"location":"devx/development/profiling-performance/#analisando-os-resultados","title":"\ud83d\udcca Analisando os Resultados","text":"<p>Quando o profiling est\u00e1 ativo, para cada requisi\u00e7\u00e3o processada, um arquivo <code>.prof</code> ser\u00e1 salvo no diret\u00f3rio configurado (<code>output_dir</code>, padr\u00e3o <code>.profiles/</code>).</p> <p>O nome do arquivo ser\u00e1 o <code>X-Request-ID</code> da requisi\u00e7\u00e3o (ou um UUID se o header n\u00e3o estiver presente) seguido por <code>.prof</code>.</p> <p>Exemplo:</p> <pre><code>.profiles/d8f3c1a4-5b7e-4a1f-8c3d-9e6a0b2f1d9c.prof\n</code></pre> <p>Voc\u00ea pode usar ferramentas como SnakeViz para visualizar esses arquivos de forma interativa:</p>"},{"location":"devx/development/profiling-performance/#instale-snakeviz","title":"\ud83d\udce6 Instale SnakeViz:","text":"<pre><code>poetry add --group dev  snakeviz\n# Ou: poetry add --group dev snakeviz\n</code></pre>"},{"location":"devx/development/profiling-performance/#visualize-um-arquivo-de-profile","title":"\ud83d\udd0d Visualize um arquivo de profile:","text":"<pre><code>snakeviz .profiles/d8f3c1a4-5b7e-4a1f-8c3d-9e6a0b2f1d9c.prof\n</code></pre> <p>Isso abrir\u00e1 uma visualiza\u00e7\u00e3o no seu navegador, mostrando o tempo gasto em cada fun\u00e7\u00e3o (cumulativo e pr\u00f3prio), o n\u00famero de chamadas, etc.</p>"},{"location":"devx/development/profiling-performance/#limitacoes-cprofile","title":"\u26a0\ufe0f Limita\u00e7\u00f5es (cProfile)","text":"<ul> <li><code>cProfile</code> mede principalmente o tempo de CPU.</li> <li>Em aplica\u00e7\u00f5es ass\u00edncronas com muito I/O (ex: espera de rede, banco de dados), ele pode n\u00e3o refletir com precis\u00e3o o tempo total de espera (wall clock time).</li> <li>Para an\u00e1lises focadas em I/O ou tempo de parede em c\u00f3digo <code>asyncio</code>, considere ferramentas como <code>pprofile</code> ou <code>yappi</code> (implementa\u00e7\u00e3o futura pode ser adicionada ao middleware).</li> </ul>"},{"location":"devx/development/sonarcloud-integration/","title":"\u2601\ufe0f Integra\u00e7\u00e3o com SonarCloud \u2013 athomic-docs","text":""},{"location":"devx/development/sonarcloud-integration/#1-proposito","title":"1. Prop\u00f3sito","text":"<p>O <code>athomic-docs</code> utiliza o SonarCloud como plataforma de inspe\u00e7\u00e3o cont\u00ednua de qualidade e seguran\u00e7a de c\u00f3digo. O objetivo \u00e9:</p> <ul> <li>Manter C\u00f3digo Limpo: Identificar e corrigir \"Code Smells\", bugs potenciais e complexidade desnecess\u00e1ria.</li> <li>Garantir Seguran\u00e7a: Detectar vulnerabilidades conhecidas e \"Security Hotspots\" que precisam de revis\u00e3o.</li> <li>Acompanhar Cobertura de Testes: Manter um hist\u00f3rico da cobertura de testes, garantindo que novas altera\u00e7\u00f5es estejam cobertas e que a cobertura geral n\u00e3o regrida significativamente.</li> <li>Quality Gate: Definir e aplicar um padr\u00e3o m\u00ednimo de qualidade (\"Quality Gate\") que deve ser atendido antes que o c\u00f3digo seja mesclado \u00e0 branch principal.</li> <li>Feedback R\u00e1pido: Fornecer feedback automatizado sobre a qualidade do c\u00f3digo diretamente nos Pull Requests (PRs).</li> </ul>"},{"location":"devx/development/sonarcloud-integration/#2-acessando-o-dashboard","title":"2. Acessando o Dashboard","text":"<p>A an\u00e1lise detalhada, o hist\u00f3rico e a configura\u00e7\u00e3o do projeto podem ser encontrados diretamente na interface do SonarCloud:</p> <p>\u27a1\ufe0f Dashboard do Projeto: <code>https://sonarcloud.io/project/overview?id=&lt;YOUR_PROJECT_KEY&gt;</code></p> <p>(Substitua <code>&lt;YOUR_PROJECT_KEY&gt;</code> pela chave real do seu projeto no SonarCloud. Ex: <code>nalaminds_athomic-docs</code> ou similar)</p> <p>Voc\u00ea pode precisar fazer login com sua conta do GitHub vinculada \u00e0 organiza\u00e7\u00e3o correspondente no SonarCloud.</p>"},{"location":"devx/development/sonarcloud-integration/#3-integracao-com-ci-github-actions","title":"3. Integra\u00e7\u00e3o com CI (GitHub Actions)","text":"<p>A an\u00e1lise do SonarCloud \u00e9 executada automaticamente como parte do nosso workflow de Integra\u00e7\u00e3o Cont\u00ednua definido em <code>.github/workflows/ci.yml</code>.</p> <p>Como funciona:</p> <ol> <li>Trigger: O workflow \u00e9 acionado em cada Pull Request direcionado \u00e0 branch <code>main</code>.</li> <li>Checkout: O c\u00f3digo \u00e9 baixado, incluindo o hist\u00f3rico completo (<code>Workspace-depth: 0</code>) para permitir an\u00e1lises mais ricas.</li> <li>Testes e Cobertura: Os testes s\u00e3o executados com <code>pytest</code>, e um relat\u00f3rio de cobertura em formato XML (<code>coverage.xml</code>) \u00e9 gerado usando <code>pytest-cov</code>.</li> <li>SonarScanner: Uma etapa espec\u00edfica (<code>SonarCloud Scan</code>) utiliza a action <code>SonarSource/sonarcloud-github-action</code>.</li> <li>Autentica\u00e7\u00e3o: A action se autentica no SonarCloud usando um token armazenado como um segredo (<code>SONAR_TOKEN</code>) nas configura\u00e7\u00f5es do reposit\u00f3rio GitHub.</li> <li>An\u00e1lise e Upload: O scanner analisa o c\u00f3digo-fonte e o relat\u00f3rio <code>coverage.xml</code>, enviando os resultados para o projeto configurado no SonarCloud.</li> <li>Feedback no PR: A action tamb\u00e9m utiliza o <code>GITHUB_TOKEN</code> (padr\u00e3o do GitHub Actions) para adicionar um Status Check ao Pull Request, indicando se o Quality Gate passou ou falhou, e potencialmente adicionando coment\u00e1rios sobre novos problemas encontrados.</li> </ol>"},{"location":"devx/development/sonarcloud-integration/#4-interpretando-os-resultados-no-sonarcloud","title":"4. Interpretando os Resultados no SonarCloud","text":"<p>Ao acessar o dashboard do projeto ou a an\u00e1lise de uma branch/PR espec\u00edfica, foque nos seguintes pontos:</p> <ul> <li>Quality Gate Status: Indica se a an\u00e1lise passou (<code>Passed</code>) ou falhou (<code>Failed</code>) de acordo com os crit\u00e9rios definidos (veja se\u00e7\u00e3o 5). Este \u00e9 o indicador mais importante para um PR.</li> <li>Coverage (Cobertura): Mostra a porcentagem de cobertura geral e, crucialmente, a cobertura sobre c\u00f3digo novo (Coverage on New Code). Permite navegar pelos arquivos para ver linhas cobertas e n\u00e3o cobertas.</li> <li>Issues (Problemas): Agrupa problemas encontrados em:<ul> <li>Bugs: Erros prov\u00e1veis no c\u00f3digo.</li> <li>Vulnerabilities: Falhas de seguran\u00e7a que podem ser exploradas.</li> <li>Security Hotspots: Pontos no c\u00f3digo relacionados \u00e0 seguran\u00e7a que exigem revis\u00e3o manual para confirmar se s\u00e3o um problema real.</li> <li>Code Smells: Padr\u00f5es de c\u00f3digo que indicam problemas de manutenibilidade ou design.</li> <li>Filtre por severidade (Blocker, Critical, Major, Minor, Info) e foque principalmente nos problemas introduzidos no \"New Code\".</li> </ul> </li> <li>Measures: Outras m\u00e9tricas como Duplica\u00e7\u00e3o (Duplications), Complexidade (Complexity), Tamanho (Size), etc.</li> </ul>"},{"location":"devx/development/sonarcloud-integration/#5-quality-gate","title":"5. Quality Gate","text":"<p>O Quality Gate \u00e9 um conjunto de condi\u00e7\u00f5es que define se a qualidade do c\u00f3digo est\u00e1 aceit\u00e1vel. O SonarCloud usa um padr\u00e3o (\"Sonar way\") que pode ser customizado. Tipicamente, ele inclui crit\u00e9rios como:</p> <ul> <li>Cobertura sobre c\u00f3digo novo &gt;= 80%</li> <li>Nenhum Bug Blocker ou Critical novo</li> <li>Nenhuma Vulnerabilidade Blocker ou Critical nova</li> <li>Nenhum Security Hotspot novo para revisar (ou uma m\u00e9trica relacionada)</li> <li>Taxa de duplica\u00e7\u00e3o baixa sobre c\u00f3digo novo</li> </ul> <p>O Status Check no Pull Request refletir\u00e1 se essas condi\u00e7\u00f5es foram atendidas. \u00c9 fundamental garantir que o Quality Gate passe antes de fazer o merge de um PR.</p> <p>Usar o SonarCloud consistentemente nos ajuda a manter o <code>athomic-docs</code> limpo, seguro, testado e f\u00e1cil de manter ao longo do tempo.</p>"},{"location":"devx/development/testing-guidelines/","title":"\ud83e\uddea Guia de Testes \u2013 athomic-docs","text":""},{"location":"devx/development/testing-guidelines/#1-filosofia-e-objetivos","title":"1. Filosofia e Objetivos","text":"<p>Testes automatizados s\u00e3o um pilar fundamental do <code>athomic-docs</code>. Nosso objetivo \u00e9 garantir:</p> <ul> <li>Confiabilidade: As funcionalidades devem operar conforme o esperado.</li> <li>Manutenibilidade: Mudan\u00e7as e refatora\u00e7\u00f5es podem ser feitas com seguran\u00e7a, evitando regress\u00f5es.</li> <li>Confian\u00e7a: A su\u00edte de testes robusta nos d\u00e1 confian\u00e7a para fazer releases e evoluir o c\u00f3digo.</li> <li>Documenta\u00e7\u00e3o Viva: Testes servem como exemplos de uso e especifica\u00e7\u00f5es de comportamento.</li> </ul> <p>Buscamos uma alta cobertura de testes e utilizamos ferramentas como SonarCloud para acompanhar a qualidade do c\u00f3digo e o hist\u00f3rico de cobertura ao longo do tempo.</p>"},{"location":"devx/development/testing-guidelines/#2-ferramentas-utilizadas","title":"2. Ferramentas Utilizadas","text":"<ul> <li>Framework Principal: pytest - Pela sua sintaxe concisa, sistema de fixtures e ecossistema de plugins.</li> <li>Testes Ass\u00edncronos: pytest-asyncio - Para testar c\u00f3digo <code>async</code>/<code>await</code>. O modo <code>strict</code> est\u00e1 habilitado (<code>pytest.ini</code>).</li> <li>Mocking: pytest-mock (wrapper para <code>unittest.mock</code>) - Para isolar unidades de teste. Usamos <code>@patch</code>, <code>MagicMock</code>, <code>AsyncMock</code>.</li> <li>Testes de API (Integra\u00e7\u00e3o): httpx com <code>ASGITransport</code> - Para fazer requisi\u00e7\u00f5es HTTP \u00e0 aplica\u00e7\u00e3o FastAPI.</li> <li>Cobertura de Testes: pytest-cov - Para medir a porcentagem de c\u00f3digo coberta pelos testes.</li> <li>Benchmarking: pytest-benchmark - Para medir a performance de trechos de c\u00f3digo.</li> <li>Gera\u00e7\u00e3o de Dados Falsos: Faker (se aplic\u00e1vel, verificar depend\u00eancias) - Para gerar dados de teste realistas.</li> <li>Simula\u00e7\u00e3o de Servi\u00e7os: fakeredis (para <code>aioredis</code>) - Para simular o Redis em testes sem depender de um servidor real.</li> </ul>"},{"location":"devx/development/testing-guidelines/#3-tipos-de-testes","title":"3. Tipos de Testes","text":"<ul> <li>Testes Unit\u00e1rios:<ul> <li>Objetivo: Verificar a l\u00f3gica de uma pequena unidade de c\u00f3digo (fun\u00e7\u00e3o, m\u00e9todo, classe) em total isolamento.</li> <li>Localiza\u00e7\u00e3o: <code>tests/unit/</code> (espelhando a estrutura de <code>src/</code>).</li> <li>Caracter\u00edsticas: R\u00e1pidos, independentes, usam mocks extensivamente.</li> </ul> </li> <li>Testes de Integra\u00e7\u00e3o:<ul> <li>Objetivo: Verificar a intera\u00e7\u00e3o entre m\u00faltiplos componentes ou com servi\u00e7os externos (DB, Redis, APIs, etc.).</li> <li>Localiza\u00e7\u00e3o: <code>tests/integration/</code>.</li> <li>Caracter\u00edsticas: Mais lentos, podem requerer servi\u00e7os externos (idealmente containerizados), usam menos mocks. Marcados com <code>@pytest.mark.integration</code>. Podem ser controlados pela vari\u00e1vel <code>RUN_INTEGRATION</code> no <code>Makefile</code>.</li> </ul> </li> <li>Testes de Benchmark:<ul> <li>Objetivo: Medir a performance (tempo de execu\u00e7\u00e3o) de fun\u00e7\u00f5es ou opera\u00e7\u00f5es espec\u00edficas.</li> <li>Localiza\u00e7\u00e3o: <code>tests/benchmark/</code>.</li> <li>Caracter\u00edsticas: Usam a fixture <code>benchmark</code> do <code>pytest-benchmark</code>. Focam em medir tempo e detectar regress\u00f5es de performance.</li> </ul> </li> </ul>"},{"location":"devx/development/testing-guidelines/#4-estrutura-e-convencoes","title":"4. Estrutura e Conven\u00e7\u00f5es","text":"<ul> <li>Diret\u00f3rio: <code>tests/</code> na raiz do projeto.</li> <li>Subdiret\u00f3rios: <code>unit/</code>, <code>integration/</code>, <code>benchmark/</code> espelhando <code>src/nala/</code>.</li> <li>Nomenclatura:<ul> <li>Arquivos: <code>test_*.py</code>.</li> <li>Fun\u00e7\u00f5es/M\u00e9todos: <code>test_*</code> (para testes funcionais) ou nomes descritivos usando a fixture <code>benchmark</code> (para benchmarks, mas prefixo <code>test_</code> \u00e9 mais seguro para descoberta).</li> </ul> </li> </ul>"},{"location":"devx/development/testing-guidelines/#5-escrevendo-testes","title":"5. Escrevendo Testes","text":"<ul> <li>Clareza (Arrange-Act-Assert): Estruture os testes de forma l\u00f3gica: prepare o cen\u00e1rio (Arrange), execute a a\u00e7\u00e3o a ser testada (Act) e verifique o resultado (Assert).</li> <li>Isolamento (Unit\u00e1rios): Use mocks (<code>@patch</code>, <code>mocker</code>) extensivamente para isolar a unidade de c\u00f3digo testada de suas depend\u00eancias externas (outras classes, fun\u00e7\u00f5es, servi\u00e7os, I/O). O teste unit\u00e1rio deve focar na l\u00f3gica daquela unidade espec\u00edfica.</li> <li>Fixtures (<code>pytest</code>): Utilize fixtures do <code>pytest</code> para configurar objetos ou estados complexos necess\u00e1rios para m\u00faltiplos testes (compartilhadas em <code>conftest.py</code> ou locais no arquivo de teste). Isso melhora a reutiliza\u00e7\u00e3o e a legibilidade. Use <code>@pytest_asyncio.fixture</code> para fixtures ass\u00edncronas.</li> <li>Testes Ass\u00edncronos: Marque fun\u00e7\u00f5es de teste <code>async def</code> com <code>@pytest.mark.asyncio</code>. Use <code>await</code> para chamar c\u00f3digo ass\u00edncrono. Lembre-se que mocks de fun\u00e7\u00f5es/m\u00e9todos ass\u00edncronos devem usar <code>AsyncMock</code> e asser\u00e7\u00f5es como <code>assert_awaited_once_with</code>.</li> <li>Testando Falhas: Use <code>with pytest.raises(ExpectedException, match=\"substring_opcional\"):</code> para verificar se o c\u00f3digo levanta a exce\u00e7\u00e3o correta sob condi\u00e7\u00f5es de erro.</li> <li>Parametriza\u00e7\u00e3o: Use <code>@pytest.mark.parametrize(\"arg1, arg2, esperado\", [(...),(...)])</code> para executar o mesmo teste com diferentes conjuntos de dados, evitando repeti\u00e7\u00e3o de c\u00f3digo.</li> <li>Evitar Testes Fr\u00e1geis - Foco no Comportamento:<ul> <li>Principalmente, teste o comportamento observ\u00e1vel e o contrato da unidade, n\u00e3o os detalhes internos da implementa\u00e7\u00e3o. O que a fun\u00e7\u00e3o retorna? Qual o estado final do objeto? Quais exce\u00e7\u00f5es s\u00e3o levantadas? Quais m\u00e9todos de depend\u00eancias (mocks) s\u00e3o chamados?</li> <li>Cuidado Extremo com Asserts em Logs: Evite basear o sucesso ou falha de um teste unit\u00e1rio na verifica\u00e7\u00e3o de que uma mensagem de log espec\u00edfica foi emitida, especialmente usando <code>caplog.text</code>.<ul> <li>Por qu\u00ea? Mensagens de log mudam frequentemente (corre\u00e7\u00f5es, melhorias), quebrando testes desnecessariamente. Al\u00e9m disso, a captura de logs via <code>caplog</code> pode ser inconsistente, especialmente em projetos que usam <code>Loguru</code> junto com o <code>logging</code> padr\u00e3o. O log \u00e9 geralmente um efeito colateral, n\u00e3o o comportamento principal a ser testado.</li> <li>Alternativas Robustas:<ul> <li>Verifique o valor de retorno da fun\u00e7\u00e3o/m\u00e9todo.</li> <li>Verifique o estado final do objeto ou sistema ap\u00f3s a a\u00e7\u00e3o.</li> <li>Use <code>pytest.raises</code> para verificar exce\u00e7\u00f5es.</li> <li>Verifique intera\u00e7\u00f5es com mocks (<code>assert_called_with</code>, <code>assert_not_called</code>, etc.).</li> </ul> </li> <li>Quando Log \u00e9 Relevante? Se o prop\u00f3sito do c\u00f3digo \u00e9 gerar um log espec\u00edfico, ou se voc\u00ea realmente precisa confirmar que um caminho de erro cr\u00edtico foi atingido, voc\u00ea pode verificar os logs, mas prefira analisar <code>caplog.records</code> (verificando o n\u00edvel, ex: <code>logging.ERROR</code>, e talvez uma substring chave na mensagem) em vez de <code>caplog.text</code>. Mesmo assim, avalie se n\u00e3o h\u00e1 uma forma melhor (exce\u00e7\u00e3o, estado) de verificar o comportamento.</li> </ul> </li> </ul> </li> </ul>"},{"location":"devx/development/testing-guidelines/#6-executando-testes-via-makefile","title":"6. Executando Testes (via <code>Makefile</code>)","text":"<ul> <li>Testes Unit\u00e1rios: <code>make test</code> ou <code>make test-unit</code></li> <li>Testes de Integra\u00e7\u00e3o: <code>make test-integration RUN_INTEGRATION=true</code> (requer flag)</li> <li>Todos os Testes Funcionais: <code>make test-all RUN_INTEGRATION=true</code> (requer flag)</li> <li>Testes Espec\u00edficos (Exemplo): <code>bash     poetry run pytest tests/unit/nala/api/middleware/test_profiling_middleware.py</code></li> </ul>"},{"location":"devx/development/testing-guidelines/#7-cobertura-de-testes-pytest-cov","title":"7. Cobertura de Testes (<code>pytest-cov</code>)","text":"<p>Acompanhar a cobertura nos ajuda a encontrar partes do c\u00f3digo n\u00e3o testadas.</p> <ul> <li>Ver Relat\u00f3rio no Terminal: <code>bash     make coverage</code>     (Mostra o resumo e as linhas n\u00e3o cobertas. Usa <code>--cov=nala --cov-report=term-missing</code>).</li> <li>Gerar Relat\u00f3rio HTML: <code>bash     make coverage-html</code>     (Gera um relat\u00f3rio naveg\u00e1vel na pasta definida por <code>HTML_COV_SAVE_DIR</code>, atualmente <code>.tests_history/htmlcov/</code>. Usa <code>--cov=nala --cov-report=html:...</code>). Lembre-se de adicionar <code>.tests_history/</code> ao <code>.gitignore</code>.</li> <li>Integra\u00e7\u00e3o Cont\u00ednua (CI): O workflow <code>.github/workflows/ci.yml</code> executa os testes com cobertura, gera o <code>coverage.xml</code> e possui uma verifica\u00e7\u00e3o (<code>--cov-fail-under=XX</code>) para falhar o build se a cobertura for muito baixa.</li> <li>Hist\u00f3rico e An\u00e1lise: O SonarCloud est\u00e1 integrado ao CI para receber os relat\u00f3rios de cobertura (<code>coverage.xml</code>), manter o hist\u00f3rico, visualizar tend\u00eancias e analisar a cobertura em Pull Requests. Consulte a documenta\u00e7\u00e3o espec\u00edfica do SonarCloud para mais detalhes. (Sugest\u00e3o: Crie <code>docs/development/sonarcloud_integration.md</code>)</li> </ul>"},{"location":"devx/development/testing-guidelines/#8-benchmarking-pytest-benchmark","title":"8. Benchmarking (<code>pytest-benchmark</code>)","text":"<p>Usamos benchmarking para medir a performance e detectar regress\u00f5es.</p> <ul> <li>Escrevendo Benchmarks: Crie arquivos em <code>tests/benchmark/</code> com fun\u00e7\u00f5es prefixadas <code>test_</code> que aceitem a fixture <code>benchmark</code>. Use <code>benchmark(funcao_a_medir, arg1, arg2)</code> para medir. Para fun\u00e7\u00f5es <code>async</code>, passe a fun\u00e7\u00e3o de corrotina diretamente: <code>benchmark(funcao_async, arg1, arg2)</code>.</li> <li>Executando Benchmarks: <code>bash     make benchmark</code>     (Mostra a tabela de resultados no console).</li> <li>Salvando Resultados:<ul> <li><code>make benchmark-save NAME=&lt;nome_snapshot&gt;</code>: Salva os resultados da execu\u00e7\u00e3o atual sob um nome espec\u00edfico em <code>.tests_history/benchmark/</code>.</li> <li><code>make benchmark-autosave</code>: Salva automaticamente cada resultado em <code>.tests_history/benchmark/</code>.</li> </ul> </li> <li>Comparando Resultados:<ul> <li><code>make benchmark-compare</code>: Compara a execu\u00e7\u00e3o atual com a \u00faltima salva.</li> <li><code>make benchmark-compare COMPARE_NAME=&lt;nome_snapshot&gt;</code>: Compara a execu\u00e7\u00e3o atual com um snapshot espec\u00edfico salvo anteriormente.</li> </ul> </li> <li>Diret\u00f3rio de Resultados: Os resultados salvos ficam na pasta definida por <code>BENCHMARK_SAVE_DIR</code> (atualmente <code>.tests_history/benchmark/</code>). Lembre-se de adicionar <code>.tests_history/</code> ao <code>.gitignore</code>.</li> </ul>"},{"location":"devx/development/testing-guidelines/#9-boas-praticas-gerais","title":"9. Boas Pr\u00e1ticas Gerais","text":"<ul> <li>Mantenha testes r\u00e1pidos (especialmente unit\u00e1rios).</li> <li>Garanta que testes sejam independentes entre si.</li> <li>Teste cen\u00e1rios de sucesso, erro e casos extremos (edge cases).</li> <li>D\u00ea nomes descritivos aos testes.</li> <li>Refatore testes junto com o c\u00f3digo da aplica\u00e7\u00e3o.</li> </ul> <p>Atualizado em: 2025-04-22</p>"},{"location":"devx/helpers/","title":"\ud83d\udee0\ufe0f Helpers Overview","text":"<p>This section contains auxiliary scripts and tools used to maintain and enhance the Athomic Docs development workflow.</p>"},{"location":"devx/helpers/#included-tools","title":"Included Tools","text":"<ul> <li>Snapshot Script: Generates a full project snapshot in text format.</li> <li>Callable Type Fixer: Refactors Python <code>Callable</code> type hints.</li> <li>MyPy Fixer: Auto-inserts type hints based on MyPy output.</li> <li>Test Linter: Validates consistency and hygiene in test files.</li> <li>Coverage Reporter: Lists files with the lowest test coverage.</li> </ul> <p>These tools are located under <code>helpers/</code> and are meant to be run via Poetry or CLI.</p>"},{"location":"devx/helpers/nala-cli-init-module-description/","title":"\ud83d\udce6 <code>nala init module &lt;name&gt;</code>","text":""},{"location":"devx/helpers/nala-cli-init-module-description/#objetivo","title":"\ud83d\udd27 Objetivo","text":"<p>Criar a estrutura base de um novo m\u00f3dulo no projeto <code>athomic-docs</code>, seguindo padr\u00f5es de engenharia robustos e escal\u00e1veis.</p>"},{"location":"devx/helpers/nala-cli-init-module-description/#estrutura-gerada","title":"\ud83d\udcc1 Estrutura Gerada","text":"<p>Exemplo para <code>nala init module rate_limiter</code>:</p> <pre><code>nala/\n\u2514\u2500\u2500 rate_limiter/\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 config/\n    \u2502   \u2514\u2500\u2500 settings.py\n    \u251c\u2500\u2500 core/\n    \u2502   \u2514\u2500\u2500 logic.py\n    \u251c\u2500\u2500 providers/\n    \u2502   \u251c\u2500\u2500 base.py\n    \u2502   \u2514\u2500\u2500 default_provider.py\n    \u251c\u2500\u2500 decorators/\n    \u2502   \u2514\u2500\u2500 tracing.py\n    \u251c\u2500\u2500 utils/\n    \u2502   \u2514\u2500\u2500 helpers.py\n    \u251c\u2500\u2500 observability/\n    \u2502   \u2514\u2500\u2500 metrics.py\n    \u251c\u2500\u2500 exceptions.py\n    \u251c\u2500\u2500 registry.py\n    \u2514\u2500\u2500 interface.py\n</code></pre> <p>E estrutura de testes:</p> <pre><code>tests/\n\u2514\u2500\u2500 unit/nala/rate_limiter/\n    \u251c\u2500\u2500 test_core_logic.py\n    \u251c\u2500\u2500 test_decorators.py\n    \u251c\u2500\u2500 test_providers.py\n    \u2514\u2500\u2500 test_settings.py\n</code></pre>"},{"location":"devx/helpers/nala-cli-init-module-description/#funcionalidades-incluidas","title":"\ud83e\udde0 Funcionalidades Inclu\u00eddas","text":"<ul> <li>[x] Classe <code>Settings</code> com valida\u00e7\u00e3o</li> <li>[x] Logging estruturado com mascaramento</li> <li>[x] Tracing integrado</li> <li>[x] Fallback handler opcional</li> <li>[x] Cache handler opcional</li> <li>[x] Retry handler opcional</li> <li>[x] RateLimiter opcional</li> <li>[x] Circuit Breaker opcional</li> <li>[x] Pool de recursos (Redis, HTTP, etc.)</li> <li>[x] Singleton configur\u00e1vel</li> <li>[x] Interface p\u00fablica clara</li> <li>[x] Base provider + exemplo</li> <li>[x] Registry din\u00e2mico</li> <li>[x] Decorators \u00fateis</li> <li>[x] Testes e mocks b\u00e1sicos</li> <li>[x] Observabilidade com m\u00e9tricas</li> <li>[x] Documenta\u00e7\u00e3o inicial</li> </ul>"},{"location":"devx/helpers/nala-cli-init-module-description/#exemplos","title":"\u2705 Exemplos","text":"<pre><code>nala init module rate_limiter\n</code></pre> <pre><code>nala init module secrets --provider=vault --fallback=true --cache=true\n</code></pre>"},{"location":"devx/helpers/nala-cli-init-module-description/#futuras-flags-sugestoes","title":"\ud83e\udde9 Futuras Flags (sugest\u00f5es)","text":"Flag Descri\u00e7\u00e3o <code>--provider=&lt;name&gt;</code> Cria provider customizado <code>--fallback=true</code> Adiciona fallback encadeado <code>--cache=true</code> Adiciona cache com TTL <code>--singleton=true</code> Usa singleton no m\u00f3dulo <code>--traced=true</code> Inclui tracing decorado <code>--metrics=true</code> Suporte a m\u00e9tricas <code>--registry=true</code> Registry de plugins <code>--secure-logging</code> Logging com mascaramento de dados sens\u00edveis <code>--tests</code> Gera estrutura de testes"},{"location":"devx/helpers/nala-core-agent-instruictions/","title":"\ud83e\udde0 Athomic Docs \u2013 Instru\u00e7\u00f5es para o Agente de IA","text":""},{"location":"devx/helpers/nala-core-agent-instruictions/#objetivo-do-agente","title":"\ud83c\udfaf Objetivo do Agente","text":"<p>Atuar como um especialista em engenharia de software, ajudando no desenvolvimento, manuten\u00e7\u00e3o e expans\u00e3o da <code>athomic-docs</code>. Este projeto fornece a base para APIs modernas, robustas e seguras, com foco em boas pr\u00e1ticas, modularidade e extensibilidade.</p>"},{"location":"devx/helpers/nala-core-agent-instruictions/#contexto-do-projeto","title":"\ud83d\udd0d Contexto do Projeto","text":"<p>A <code>athomic-docs</code> \u00e9 um framework modular para constru\u00e7\u00e3o de APIs modernas e resilientes, com os seguintes pilares:</p> <ul> <li>Seguran\u00e7a: gest\u00e3o de segredos com fallback e rota\u00e7\u00e3o, logging seguro com mascaramento de dados sens\u00edveis.</li> <li>Performance: cache com suporte a m\u00faltiplos providers, rate limiter com controle granular.</li> <li>Configura\u00e7\u00e3o centralizada: baseada em Dynaconf, com suporte a <code>.env</code>, <code>.secrets.toml</code>, e Vault.</li> <li>Observabilidade: logging estruturado, e plano para m\u00e9tricas e tracing.</li> <li>Alta extensibilidade: arquitetura baseada em interfaces e registries, facilitando troca e composi\u00e7\u00e3o de providers.</li> </ul> <p>Este projeto \u00e9 usado como funda\u00e7\u00e3o em m\u00faltiplas aplica\u00e7\u00f5es na Nalaminds e est\u00e1 sendo preparado para uso open-source no futuro.</p>"},{"location":"devx/helpers/nala-core-agent-instruictions/#comportamento-esperado","title":"\ud83e\udde0 Comportamento Esperado","text":"<p>Voc\u00ea \u00e9 um agente t\u00e9cnico que:</p> <ul> <li>Ajuda com debugging, escrita de testes e integra\u00e7\u00e3o de m\u00f3dulos.</li> <li>Garante ades\u00e3o \u00e0s boas pr\u00e1ticas adotadas no projeto.</li> <li>Sugere melhorias e refatora\u00e7\u00f5es conforme o padr\u00e3o de qualidade definido.</li> <li>Documenta e explica decis\u00f5es t\u00e9cnicas quando necess\u00e1rio.</li> </ul>"},{"location":"devx/helpers/nala-core-agent-instruictions/#boas-praticas-e-decisoes-tecnicas","title":"\u2705 Boas Pr\u00e1ticas e Decis\u00f5es T\u00e9cnicas","text":"<ul> <li>Idiomas: C\u00f3digo e coment\u00e1rios em ingl\u00eas. Documenta\u00e7\u00f5es e conversas internas podem estar em portugu\u00eas.</li> <li>Testes: Utiliza\u00e7\u00e3o de <code>pytest</code> com foco em testes unit\u00e1rios e cobertura alta.</li> <li>Modularidade: Cada componente (ex: <code>cache</code>, <code>rate_limiter</code>, <code>secrets</code>) \u00e9 desacoplado e extens\u00edvel.</li> <li>Fallbacks e Resili\u00eancia: Uso do decorator <code>@fallback_handler</code> em m\u00f3dulos cr\u00edticos.</li> <li>Seguran\u00e7a: Uso de <code>safelog</code> para mascarar dados sens\u00edveis em logs. Logging com Loguru.</li> <li>Configura\u00e7\u00e3o: Utiliza\u00e7\u00e3o de <code>Dynaconf</code> com m\u00faltiplas fontes e ambientes (<code>development</code>, <code>production</code>).</li> <li>Rate Limiter: Implementado com o pacote <code>limits</code>, com suporte a m\u00faltiplas estrat\u00e9gias (<code>fixed</code>, <code>moving</code>), fallback e decorator.</li> <li>Cache: Suporte a <code>MemoryCache</code>, <code>RedisCache</code> e <code>FallbackCache</code>. Decorator <code>@cache_result</code> com TTL, jitter, e locking.</li> <li>Extensibilidade: Sistema de <code>registry</code> para registrar dinamicamente novos providers.</li> <li>Observabilidade: Logging completo, plano futuro para m\u00e9tricas/tracing.</li> <li>C\u00f3digo limpo: Uso de <code>type hints</code>, <code>async</code>/<code>await</code>, arquitetura limpa e separa\u00e7\u00e3o de camadas (ex: <code>interfaces</code>, <code>providers</code>, <code>utils</code>, <code>core</code>).</li> </ul>"},{"location":"devx/helpers/nala-core-agent-instruictions/#areas-atuais-mais-sensiveis","title":"\ud83d\udd10 \u00c1reas atuais mais sens\u00edveis","text":"<ul> <li>Secrets: Foco em seguran\u00e7a e fallback.</li> <li>Rate Limiter: Corre\u00e7\u00e3o de comportamentos indesejados com <code>check</code> e <code>hit</code> separados.</li> <li>Logging: Garantia de mascaramento.</li> <li>Resets e fallback: Implementa\u00e7\u00f5es que ainda n\u00e3o possuem suporte completo por chave (<code>reset</code>, <code>clear</code> no <code>limits</code>).</li> </ul>"},{"location":"devx/helpers/nala-core-agent-instruictions/#acoes-esperadas-do-agente","title":"\ud83d\udea7 A\u00e7\u00f5es esperadas do Agente","text":"<ul> <li>Sugerir e escrever testes unit\u00e1rios.</li> <li>Identificar bugs e inconsist\u00eancias.</li> <li>Escrever ou revisar documenta\u00e7\u00e3o (em Markdown).</li> <li>Gerar decorators ou abstra\u00e7\u00f5es reutiliz\u00e1veis.</li> <li>Ajudar a expandir m\u00f3dulos existentes (como <code>rate_limiter</code>, <code>secrets</code>, <code>observability</code>, etc.).</li> <li>Garantir que qualquer novo c\u00f3digo:</li> <li>Seja test\u00e1vel.</li> <li>Tenha fallback (se aplic\u00e1vel).</li> <li>Seja observ\u00e1vel (log ou m\u00e9trica).</li> <li>Siga as boas pr\u00e1ticas do projeto.</li> </ul>"},{"location":"devx/helpers/nala-core-agent-instruictions/#instrucoes-para-agentes-inteligentes-athomic-docs","title":"\ud83e\udd16 Instru\u00e7\u00f5es para Agentes Inteligentes \u2013 Athomic Docs","text":"<p>Este documento define as diretrizes para cria\u00e7\u00e3o e uso de agentes inteligentes dentro do projeto <code>athomic-docs</code>.</p>"},{"location":"devx/helpers/nala-core-agent-instruictions/#principios-gerais","title":"\ud83e\udde0 Princ\u00edpios Gerais","text":"<ol> <li>Modularidade: Cada agente deve ser desacoplado e especializado.</li> <li>Reusabilidade: Compartilhamento de l\u00f3gica comum entre agentes via <code>utils</code> e <code>services</code>.</li> <li>Contextualiza\u00e7\u00e3o: Os agentes devem ser capazes de acessar contexto din\u00e2mico (ex: <code>settings</code>, fallback, cache, retry).</li> <li>Fallback e Resili\u00eancia: Implementar fallback encadeado com o decorator <code>@fallback_handler</code>.</li> <li>Seguran\u00e7a e Logs: Toda intera\u00e7\u00e3o deve ser registrada com <code>SafeLogger</code> e passar por <code>SensitiveDataFilter</code>.</li> </ol>"},{"location":"devx/helpers/nala-core-agent-instruictions/#estrutura-recomendada","title":"\ud83e\udde9 Estrutura Recomendada","text":"<pre><code>nala/\n\u251c\u2500\u2500 agents/\n\u2502   \u251c\u2500\u2500 registry.py               # Registro din\u00e2mico de agentes\n\u2502   \u251c\u2500\u2500 base_agent.py             # Interface/base abstrata\n\u2502   \u251c\u2500\u2500 examples/\n\u2502   \u2502   \u251c\u2500\u2500 agent_secrets.py      # Exemplo de uso com secrets\n\u2502   \u2502   \u251c\u2500\u2500 agent_cache.py        # Exemplo de uso com cache e retry\n\u2502   \u2502   \u2514\u2500\u2500 agent_fallback.py     # Exemplo de fallback com m\u00faltiplas fontes\n</code></pre>"},{"location":"devx/helpers/nala-core-agent-instruictions/#padroes-de-implementacao","title":"\ud83d\udd04 Padr\u00f5es de Implementa\u00e7\u00e3o","text":"<ul> <li>Todos os agentes devem herdar de <code>BaseAgent</code>.</li> <li>Cada agente deve implementar:</li> <li><code>load()</code></li> <li><code>respond(input: AgentInput) -&gt; AgentResponse</code></li> <li>Decorators recomendados:</li> <li><code>@retry_handler</code></li> <li><code>@fallback_handler</code></li> <li><code>@safe_logger</code></li> <li>Utilizar <code>@async_singleton</code> ou <code>@singleton</code> se apropriado.</li> </ul>"},{"location":"devx/helpers/nala-core-agent-instruictions/#exemplo-real-agente-com-cache-e-retry","title":"\u2705 Exemplo Real \u2013 Agente com Cache e Retry","text":"<pre><code>from nala.athomic.decorators import retry_handler\nfrom nala.athomic.performance.cache.interface import CacheProvider\nfrom nala.agents.base_agent import BaseAgent, AgentInput, AgentResponse\n\nclass CachedExampleAgent(BaseAgent):\n    def __init__(self, cache: CacheProvider):\n        self.cache = cache\n\n    @retry_handler\n    async def respond(self, input: AgentInput) -&gt; AgentResponse:\n        cached = await self.cache.get(input.query)\n        if cached:\n            return AgentResponse(text=cached)\n\n        # Processamento fict\u00edcio\n        response = f\"Resposta processada para: {input.query}\"\n        await self.cache.set(input.query, response, ttl=3600)\n        return AgentResponse(text=response)\n</code></pre>"},{"location":"devx/helpers/nala-core-agent-instruictions/#boas-praticas","title":"\ud83d\udea8 Boas Pr\u00e1ticas","text":"\u00c1rea Pr\u00e1tica Recomendada Fallback Usar <code>@fallback_handler</code> com lista de fontes alternativas Seguran\u00e7a Sanitizar logs e entradas com <code>maskers</code> e <code>SafeLogger</code> Cache Usar TTL e estrat\u00e9gia de invalidade Observabilidade Decorar com <code>@trace_handler</code> para OpenTelemetry (planejado) Teste Criar mocks e usar <code>pytest-asyncio</code> nos testes unit\u00e1rios"},{"location":"devx/helpers/nala-core-agent-instruictions/#futuras-extensoes-planejadas","title":"\ud83d\udd27 Futuras extens\u00f5es planejadas","text":"<ul> <li>Integra\u00e7\u00e3o com agentes LLMs externos via LangChain.</li> <li>Suporte a objetivos/miss\u00f5es (agent with goals).</li> <li>Observabilidade e m\u00e9tricas para agentes.</li> </ul>"},{"location":"devx/helpers/nala-helpers-callable-type-fixer/","title":"callable-type-fixer","text":"<p>A simple and safe tool to automatically fix <code>Callable</code> type hints in Python files by explicitly adding <code>[..., Any]</code>, turning <code>Callable</code> into <code>Callable[..., Any]</code>.</p>"},{"location":"devx/helpers/nala-helpers-callable-type-fixer/#why-use-this","title":"\u2728 Why use this?","text":"<p>In Python, using <code>Callable</code> without specifying the argument and return types is discouraged and flagged by type checkers like <code>mypy</code>. This tool automatically updates untyped <code>Callable</code> hints to follow the safer, explicit format:  </p> <pre><code># BEFORE\ndef my_func(cb: Callable): ...\n\n# AFTER\ndef my_func(cb: Callable[..., Any]): ...\n</code></pre> <p>It also ensures <code>Any</code> is imported when needed and avoids modifying runtime type checks like <code>isinstance(..., Callable)</code>.</p>"},{"location":"devx/helpers/nala-helpers-callable-type-fixer/#installation","title":"\ud83d\udce6 Installation","text":"<p>Add it to your project as a development dependency:</p> <pre><code>poetry add --dev callable-type-fixer\n</code></pre>"},{"location":"devx/helpers/nala-helpers-callable-type-fixer/#usage","title":"\ud83d\ude80 Usage","text":""},{"location":"devx/helpers/nala-helpers-callable-type-fixer/#cli","title":"CLI","text":"<pre><code>poetry run fix-callables [PATHS...] [OPTIONS]\n</code></pre>"},{"location":"devx/helpers/nala-helpers-callable-type-fixer/#options","title":"Options","text":"Option Description <code>--check</code> Only scans files and lists those that need fixing. No changes are applied. <code>--diff</code> Shows a git-like diff of what would be changed, without modifying files."},{"location":"devx/helpers/nala-helpers-callable-type-fixer/#what-it-does","title":"\ud83e\udde0 What it Does","text":"<ul> <li>\u2705 Fixes <code>Callable</code> to <code>Callable[..., Any]</code> in:</li> <li>Function signatures</li> <li>Return type hints</li> <li>Type annotations inside variables</li> <li>\u2705 Adds <code>Any</code> to import if it's missing</li> <li>\u2705 Shows readable git-like diff with <code>--diff</code></li> <li>\u2705 Does nothing when:</li> <li><code>Callable</code> is already properly typed</li> <li><code>Callable</code> appears inside <code>isinstance()</code> or <code>issubclass()</code></li> <li><code>Callable[..., Any]</code> already used</li> </ul>"},{"location":"devx/helpers/nala-helpers-callable-type-fixer/#examples","title":"\ud83d\udee1\ufe0f Examples","text":""},{"location":"devx/helpers/nala-helpers-callable-type-fixer/#fix-mode-default","title":"Fix mode (default):","text":"<pre><code>poetry run fix-callables src/ tests/\n</code></pre>"},{"location":"devx/helpers/nala-helpers-callable-type-fixer/#dry-run-with-diff","title":"Dry-run with diff:","text":"<pre><code>poetry run fix-callables src/ tests/ --diff\n</code></pre>"},{"location":"devx/helpers/nala-helpers-callable-type-fixer/#lint-only-mode","title":"Lint-only mode:","text":"<pre><code>poetry run fix-callables src/ tests/ --check\n</code></pre>"},{"location":"devx/helpers/nala-helpers-callable-type-fixer/#testing","title":"\ud83e\uddea Testing","text":"<p>Coming soon: unit tests for line scanners and file fixers.</p>"},{"location":"devx/helpers/nala-helpers-callable-type-fixer/#roadmap","title":"\ud83d\udee0 Roadmap","text":"<ul> <li>[x] CLI with Click</li> <li>[x] Diff-style output</li> <li>[x] Safe skipping of <code>isinstance(..., Callable)</code></li> <li>[ ] Full unit test coverage</li> <li>[ ] Support for Python 3.12's <code>collections.abc.Callable</code></li> </ul>"},{"location":"devx/helpers/nala-helpers-callable-type-fixer/#license","title":"\ud83d\udcc4 License","text":"<p>MIT \u2013 do whatever you want, but attribution is appreciated \u2728</p>"},{"location":"devx/helpers/nala-helpers-mypy-fixer/","title":"<code>mypy_fixer</code> \u2014 Automatic Type Hint Fixer for Python","text":"<p>Automate fixing missing return type annotations in Python code using mypy error logs.</p>"},{"location":"devx/helpers/nala-helpers-mypy-fixer/#purpose","title":"\u2705 Purpose","text":"<p><code>mypy_fixer</code> parses a <code>.mypy_errors.log</code> file and automatically adds missing return type annotations (<code>-&gt; None</code> or <code>-&gt; Any</code>) to Python functions based on whether they return a value or not.</p>"},{"location":"devx/helpers/nala-helpers-mypy-fixer/#features","title":"\ud83d\udce6 Features","text":"<ul> <li>Detects functions with missing return type annotations using a <code>mypy</code> error log.</li> <li>Adds:</li> <li><code>-&gt; None</code> for functions without <code>return</code>.</li> <li><code>-&gt; Any</code> + comment for functions with <code>return</code>.</li> <li>Adds <code>from typing import Any</code> when <code>Any</code> is used but not imported.</li> <li>Shows diff (<code>--diff</code>) or performs dry-run checks (<code>--check</code>).</li> </ul>"},{"location":"devx/helpers/nala-helpers-mypy-fixer/#how-it-works","title":"\ud83e\udde0 How it Works","text":"<ul> <li>Parses the <code>mypy</code> error log line by line.</li> <li>Locates the function using <code>line_number</code> in the file.</li> <li>Scans the following lines to check for <code>return</code> statements.</li> <li>Updates the function signature with the inferred return type.</li> <li>If <code>-&gt; Any</code> is added, ensures the file imports it properly.</li> </ul>"},{"location":"devx/helpers/nala-helpers-mypy-fixer/#usage","title":"\ud83d\udee0\ufe0f Usage","text":"<pre><code># Run and apply fixes\npoetry run mypy-fix src/ --logfile .mypy_errors.log\n\n# Show changes without applying them\npoetry run mypy-fix src/ --logfile .mypy_errors.log --check\n\n# Show the diff of changes to be applied\npoetry run mypy-fix src/ --logfile .mypy_errors.log --diff\n</code></pre>"},{"location":"devx/helpers/nala-helpers-mypy-fixer/#example-mypy_errorslog-entry","title":"\ud83d\udcc1 Example <code>.mypy_errors.log</code> Entry","text":"<pre><code>src/project/module/example.py:10: error: Function is missing a return type annotation  [no-untyped-def]\n</code></pre>"},{"location":"devx/helpers/nala-helpers-mypy-fixer/#example-before-after","title":"\ud83d\udcdd Example Before &amp; After","text":"<p>Before:</p> <pre><code>def process_data(data):\n    return data.upper()\n</code></pre> <p>After:</p> <pre><code>def process_data(data) -&gt; Any:  # TODO: verify return type (mypy)\n    return data.upper()\n</code></pre>"},{"location":"devx/helpers/nala-helpers-mypy-fixer/#limitations","title":"\ud83d\udd0d Limitations","text":"<ul> <li>Only detects up to 5 lines after the function to search for <code>return</code>.</li> <li>Doesn\u2019t analyze control flow or nested functions deeply.</li> <li>Uses <code>Any</code> conservatively when unsure.</li> <li>TODO comments are left for manual verification.</li> </ul>"},{"location":"devx/helpers/nala-helpers-mypy-fixer/#file-structure-suggestion","title":"\ud83d\udccc File Structure Suggestion","text":"<pre><code>mypy_fixer/\n\u2502\n\u251c\u2500\u2500 fixer.py          # Main script\n\u251c\u2500\u2500 __init__.py\n\u2514\u2500\u2500 README.md         # This file\n</code></pre>"},{"location":"devx/helpers/nala-lint/","title":"<code>nala lint</code> - Validador de Conformidade do Framework Nala-IQ","text":"<p>O comando <code>nala lint</code> valida se um m\u00f3dulo da API segue os padr\u00f5es de qualidade definidos no framework Nala-IQ.</p>"},{"location":"devx/helpers/nala-lint/#objetivo","title":"\ud83d\udccc Objetivo","text":"<p>Garantir que todo novo m\u00f3dulo implementado ou modificado siga as boas pr\u00e1ticas de: - Resili\u00eancia (fallback, retry) - Observabilidade (m\u00e9tricas, logs estruturados) - Configura\u00e7\u00e3o (uso de Settings com valida\u00e7\u00e3o) - Manutenibilidade (type hints, docstrings) - Qualidade de c\u00f3digo (testes, modulariza\u00e7\u00e3o)</p>"},{"location":"devx/helpers/nala-lint/#exemplo-de-uso","title":"\u25b6\ufe0f Exemplo de uso","text":"<pre><code>nala lint src/nala/athomic/cache\n</code></pre>"},{"location":"devx/helpers/nala-lint/#checks-implementados-v1","title":"\u2705 Checks Implementados (v1)","text":"Check Descri\u00e7\u00e3o <code>has_settings</code> Verifica se o m\u00f3dulo define <code>Settings</code> herdando de <code>BaseSettings</code>. <code>has_fallback</code> Detecta uso de <code>@fallback_handler</code>. <code>has_observability</code> Verifica uso de m\u00e9tricas ou decorators como <code>@with_metrics</code>. <code>registered_in_registry</code> Verifica se a classe est\u00e1 registrada no <code>registry.py</code>. <code>has_tests</code> Procura testes no diret\u00f3rio <code>tests/unit/&lt;modulo&gt;</code>. <code>has_type_hints</code> Checa se fun\u00e7\u00f5es t\u00eam anota\u00e7\u00f5es de tipo. <code>has_docstrings</code> Confirma se fun\u00e7\u00f5es p\u00fablicas t\u00eam docstrings."},{"location":"devx/helpers/nala-lint/#exemplo-de-saida","title":"\ud83d\udcca Exemplo de sa\u00edda","text":"<pre><code>\ud83d\udd0d Linting m\u00f3dulo: src/nala/athomic/cache\n\n\u2714 has_settings\n\u2714 has_fallback\n\u2714 has_observability\n\u2714 registered_in_registry\n\u2714 has_tests\n\u2714 has_type_hints\n\u2718 has_docstrings -&gt; fun\u00e7\u00e3o `get_cache()` sem docstring\n\n\u2705 Score final: 6/7 (85%)\n</code></pre>"},{"location":"devx/helpers/nala-lint/#futuras-extensoes","title":"\ud83d\ude80 Futuras Extens\u00f5es","text":"<ul> <li>Integra\u00e7\u00e3o com CI/CD (<code>pre-commit</code>, GitHub Actions)</li> <li>Suporte a sa\u00edda JSON para an\u00e1lise automatizada</li> <li>Modo <code>--fix</code> para sugerir ou aplicar corre\u00e7\u00f5es</li> <li>Perfil configur\u00e1vel para checks opcionais</li> </ul>"},{"location":"devx/helpers/nala-lint/#arquitetura-interna","title":"\ud83e\udde9 Arquitetura Interna","text":"<p>O comando ser\u00e1 implementado como um CLI com <code>click</code>, estruturado em validadores independentes para facilitar testes e extens\u00e3o.</p>"},{"location":"devx/helpers/nala-secrets-scaning/","title":"\ud83d\udd10 Secret Scanning with <code>detect-secrets</code>","text":"<p>This project uses <code>detect-secrets</code> to automatically detect and prevent committing secrets (API keys, tokens, passwords, etc.) into the codebase.</p>"},{"location":"devx/helpers/nala-secrets-scaning/#why-use-detect-secrets","title":"\u2705 Why Use <code>detect-secrets</code>?","text":"<ol> <li>Avoid CI false positives by acknowledging safe strings.</li> <li>Version and document previous decisions (audited \"secrets\").</li> <li>Catch real secrets that may be added accidentally in future commits.</li> </ol>"},{"location":"devx/helpers/nala-secrets-scaning/#setup-guide","title":"\ud83d\ude80 Setup Guide","text":""},{"location":"devx/helpers/nala-secrets-scaning/#1-initial-scan-baseline-creation","title":"1. \ud83d\udd0d Initial Scan &amp; Baseline Creation","text":"<p>Run the following to create your first <code>.secrets.baseline</code> file:</p> <pre><code>poetry run detect-secrets scan &gt; .secrets.baseline\ngit add .secrets.baseline\n</code></pre> <p>This file contains a list of potential secrets and marks them as reviewed/unreviewed.</p>"},{"location":"devx/helpers/nala-secrets-scaning/#2-audit-potential-secrets","title":"2. \ud83d\udc40 Audit Potential Secrets","text":"<p>Run this command to review new secrets found:</p> <pre><code>poetry run detect-secrets audit .secrets.baseline\n</code></pre> <ul> <li>\u2705 Mark safe strings as <code>\"is_secret\": false</code></li> <li>\ud83d\udd10 Confirm real secrets as <code>\"is_secret\": true</code> and rotate them if needed.</li> </ul>"},{"location":"devx/helpers/nala-secrets-scaning/#3-update-baseline-after-changes","title":"3. \ud83d\udd01 Update Baseline (After Changes)","text":"<p>If new code adds more strings that look like secrets:</p> <pre><code>poetry run detect-secrets scan --baseline .secrets.baseline &gt; temp.baseline &amp;&amp; mv temp.baseline .secrets.baseline\n</code></pre> <p>Or just recreate it if you're in early development:</p> <pre><code>poetry run detect-secrets scan &gt; .secrets.baseline\n</code></pre>"},{"location":"devx/helpers/nala-secrets-scaning/#4-ci-integration","title":"4. \u2705 CI Integration","text":"<p>Your GitHub Actions workflow already checks for secrets using:</p> <pre><code>- name: \ud83d\udd10 Scan for secrets\n  run: poetry run detect-secrets scan --baseline .secrets.baseline\n</code></pre>"},{"location":"devx/helpers/nala-secrets-scaning/#tips","title":"\ud83d\udccc Tips","text":"<ul> <li>Store <code>.secrets.baseline</code> in version control.</li> <li>Add it to your pre-commit config to block secrets locally:   ```yaml</li> <li>repo: https://github.com/Yelp/detect-secrets       rev: v1.4.0       hooks:         - id: detect-secrets           args: ['--baseline', '.secrets.baseline']   ```</li> </ul>"},{"location":"devx/helpers/nala-secrets-scaning/#references","title":"\ud83d\udcc4 References","text":"<ul> <li>https://github.com/Yelp/detect-secrets</li> <li>https://github.com/Yelp/detect-secrets#pre-commit-hook-usage</li> </ul>"},{"location":"devx/helpers/relatorio-diagnostico-athomic-docs/","title":"\ud83d\udcca Relat\u00f3rio Diagn\u00f3stico \u2013 Projeto Athomic Docs","text":"<p>Avalia\u00e7\u00e3o baseada no framework Nala-IQ com 14 dimens\u00f5es principais.</p> Dimens\u00e3o Evid\u00eancia Ader\u00eancia Seguran\u00e7a Presen\u00e7a de m\u00f3dulos como maskers, logsafe, auth e uso de Dynaconf para segredos. Alta Desenvolvimento e Qualidade de C\u00f3digo Uso de decorators, fallback, retry, cache, estrutura modular. Alta Testes e Garantia de Qualidade tests/unit e tests/integration com estrutura clara e mocks. M\u00e9dia-Alta Escalabilidade e Performance Uso de cache, fallback, decorators e suporte a m\u00faltiplos providers. Alta Robustez e Confiabilidade M\u00f3dulos com retry, fallback, validadores, checks para ambientes ausentes. Alta Persist\u00eancia e Gerenciamento de Dados Presen\u00e7a de settings relacionados a database; c\u00f3digo para MongoDB. M\u00e9dia Processos de Build, Deploy e DevOps .github/workflows/ci.yml com lint e test automatizado. M\u00e9dia-Alta Monitoramento e Observabilidade Suporte parcial a logs estruturados, mas sem OpenTelemetry ou m\u00e9tricas expl\u00edcitas. M\u00e9dia Sustentabilidade e Efici\u00eancia Uso de fallback, modularidade, cache e retry ajudam na efici\u00eancia. Alta Integra\u00e7\u00e3o com Sistemas Externos Integra\u00e7\u00e3o com Vault, Redis, MongoDB, Consul \u2014 suporte bem estruturado. Alta Governan\u00e7a e Ciclo de Vida da API Sem vers\u00e3oing expl\u00edcito ainda, mas estrutura facilita evolu\u00e7\u00e3o. M\u00e9dia Experi\u00eancia do Desenvolvedor (DX) Uso de monkeytype, estrutura clara, tipagem gradual, modularidade. Alta Compliance e Regulamenta\u00e7\u00f5es Ind\u00edcios indiretos com logsafe e maskers, mas sem se\u00e7\u00f5es claras sobre LGPD/GDPR. M\u00e9dia"},{"location":"devx/workflow/bug-report/","title":"\"[BUG] \"","text":"<ul> <li> <p>type: textarea     id: descricao     attributes:       label: Descri\u00e7\u00e3o do Problema       description: Explique claramente o comportamento inesperado.       placeholder: O que aconteceu? O que esperava acontecer?     validations:       required: true</p> </li> <li> <p>type: textarea     id: reproduzir     attributes:       label: Passos para Reproduzir       description: Liste os passos para reproduzir o problema.       placeholder: |         1. V\u00e1 para '...'         2. Clique em '...'         3. Veja o erro     validations:       required: true</p> </li> <li> <p>type: input     id: ambiente     attributes:       label: Ambiente       description: Detalhes sobre seu ambiente (ex: sistema, Python, branch)       placeholder: ex: macOS 14, Python 3.11, branch main     validations:       required: false</p> </li> <li> <p>type: textarea     id: logs     attributes:       label: Logs / Traceback       description: Inclua logs, erros ou mensagens relevantes.       placeholder: Copie e cole logs relevantes aqui.     validations:       required: false</p> </li> </ul>"},{"location":"devx/workflow/feature-request/","title":"\"[FEATURE] \"","text":"<ul> <li> <p>type: textarea     id: proposta     attributes:       label: Descreva a Proposta       description: Qual funcionalidade voc\u00ea gostaria de ver? Por qu\u00ea?       placeholder: Quero poder fazer X porque isso ajuda em Y...     validations:       required: true</p> </li> <li> <p>type: textarea     id: valor     attributes:       label: Valor para o Projeto       description: Explique como isso melhora o projeto, o c\u00f3digo ou a experi\u00eancia.     validations:       required: true</p> </li> <li> <p>type: textarea     id: implementacao     attributes:       label: Ideia de Implementa\u00e7\u00e3o (Opcional)       description: Se tiver sugest\u00f5es de como implementar, escreva aqui.     validations:       required: false</p> </li> </ul>"},{"location":"devx/workflow/labels-sugeridas/","title":"Labels sugeridas para organiza\u00e7\u00e3o do reposit\u00f3rio","text":"<ul> <li> <p>name: bug   description: \"Erro confirmado ou comportamento inesperado\"   color: \"d73a4a\"</p> </li> <li> <p>name: enhancement   description: \"Sugest\u00e3o de nova funcionalidade ou melhoria\"   color: \"a2eeef\"</p> </li> <li> <p>name: documentation   description: \"Mudan\u00e7as relacionadas \u00e0 documenta\u00e7\u00e3o\"   color: \"0075ca\"</p> </li> <li> <p>name: good first issue   description: \"Boa para novos contribuidores\"   color: \"7057ff\"</p> </li> <li> <p>name: help wanted   description: \"Ajuda necess\u00e1ria para essa tarefa\"   color: \"008672\"</p> </li> <li> <p>name: question   description: \"D\u00favida ou discuss\u00e3o t\u00e9cnica\"   color: \"d876e3\"</p> </li> <li> <p>name: refactor   description: \"Refatora\u00e7\u00e3o de c\u00f3digo sem mudan\u00e7a funcional\"   color: \"c2e0c6\"</p> </li> <li> <p>name: test   description: \"Cobertura e melhorias de testes\"   color: \"f9d0c4\"</p> </li> <li> <p>name: chore   description: \"Tarefa interna ou de infraestrutura\"   color: \"fef2c0\"</p> </li> </ul>"},{"location":"devx/workflow/milestones-template/","title":"\ud83d\udccc Template de Milestones \u2013 athomic-docs","text":"<p>Este documento define uma estrutura sugerida de milestones para organizar as entregas do projeto com base em vers\u00f5es (releases). As milestones devem ser usadas para agrupar issues e PRs relacionados a uma mesma entrega significativa.</p>"},{"location":"devx/workflow/milestones-template/#versoes-planejadas","title":"\ud83d\ude80 Vers\u00f5es Planejadas","text":""},{"location":"devx/workflow/milestones-template/#v010-fundacao-tecnica","title":"\u2705 v0.1.0 \u2013 Funda\u00e7\u00e3o T\u00e9cnica","text":"<ul> <li>Estrutura modular e configura\u00e7\u00e3o inicial</li> <li>Integra\u00e7\u00e3o com Dynaconf, Vault e Consul</li> <li>Observabilidade inicial (logs estruturados, tracing b\u00e1sico)</li> <li>M\u00f3dulo de Cache com fallback</li> <li>Circuit Breaker e Retry com testes</li> <li>Testes unit\u00e1rios b\u00e1sicos e CI rodando</li> </ul>"},{"location":"devx/workflow/milestones-template/#v020-seguranca-e-qualidade","title":"\ud83d\udee1\ufe0f v0.2.0 \u2013 Seguran\u00e7a e Qualidade","text":"<ul> <li>Sistema de masking sens\u00edvel (<code>logsafe</code>)</li> <li>Integra\u00e7\u00e3o com detect-secrets</li> <li>Integra\u00e7\u00e3o com Bandit</li> <li>Pol\u00edtica de seguran\u00e7a (<code>SECURITY.md</code>)</li> <li>Linting e tipagem completa</li> <li>Makefile unificado e CLI para tooling</li> </ul>"},{"location":"devx/workflow/milestones-template/#v030-observabilidade-completa","title":"\ud83d\udcca v0.3.0 \u2013 Observabilidade Completa","text":"<ul> <li>Integra\u00e7\u00e3o com Prometheus e m\u00e9tricas customizadas</li> <li>Readiness &amp; health endpoints (/healthz, /status)</li> <li>Exporta\u00e7\u00e3o de logs para ferramenta externa (opcional)</li> <li>Pain\u00e9is de dashboard b\u00e1sico (ex: Grafana template)</li> </ul>"},{"location":"devx/workflow/milestones-template/#v040-agente-dev-automacoes","title":"\ud83e\udd16 v0.4.0 \u2013 Agente Dev &amp; Automa\u00e7\u00f5es","text":"<ul> <li>Agent Dev inicial para gerar m\u00f3dulos via backlog</li> <li>Commit automatizado por agent</li> <li>Execu\u00e7\u00e3o via CLI e integra\u00e7\u00e3o com GitHub</li> <li>Logging do agente</li> <li>Template de backlog suportado por agent</li> </ul>"},{"location":"devx/workflow/milestones-template/#v100-primeira-versao-oficial","title":"\ud83d\udce6 v1.0.0 \u2013 Primeira Vers\u00e3o Oficial","text":"<ul> <li>Documenta\u00e7\u00e3o completa (README, CONTRIBUTING, etc)</li> <li>Testes de carga/stress</li> <li>Auditoria de seguran\u00e7a</li> <li>Refatora\u00e7\u00f5es e polimento</li> <li>Publica\u00e7\u00e3o da vers\u00e3o est\u00e1vel</li> </ul>"},{"location":"devx/workflow/milestones-template/#dicas-de-uso","title":"\ud83e\udde9 Dicas de Uso","text":"<ul> <li>Use milestones para organizar issues e PRs de forma tem\u00e1tica.</li> <li>Atualize os escopos conforme as prioridades do projeto mudarem.</li> <li>Utilize o campo de descri\u00e7\u00e3o do GitHub Milestones para copiar o conte\u00fado relevante de cada se\u00e7\u00e3o acima.</li> </ul> <p>Esse modelo pode ser adaptado conforme o projeto evolui!</p>"},{"location":"devx/workflow/project-board-template/","title":"\ud83e\udded Modelo de Project Board \u2013 athomic-docs","text":"<p>Este \u00e9 um modelo sugerido de organiza\u00e7\u00e3o para gerenciamento visual das tarefas do projeto <code>athomic-docs</code>.</p>"},{"location":"devx/workflow/project-board-template/#colunas","title":"\ud83d\udd04 Colunas","text":""},{"location":"devx/workflow/project-board-template/#backlog","title":"\ud83d\udce5 Backlog","text":"<ul> <li>Tarefas planejadas ou mapeadas</li> <li>Sugest\u00f5es de melhoria e ideias em valida\u00e7\u00e3o</li> </ul>"},{"location":"devx/workflow/project-board-template/#em-andamento","title":"\ud83d\udea7 Em andamento","text":"<ul> <li>Tarefas com algu\u00e9m alocado e execu\u00e7\u00e3o ativa</li> </ul>"},{"location":"devx/workflow/project-board-template/#em-revisao","title":"\ud83e\uddea Em revis\u00e3o","text":"<ul> <li>Pull requests abertos aguardando revis\u00e3o</li> <li>Tarefas que exigem aprova\u00e7\u00e3o t\u00e9cnica</li> </ul>"},{"location":"devx/workflow/project-board-template/#concluido","title":"\u2705 Conclu\u00eddo","text":"<ul> <li>Merge feito, entregue e em produ\u00e7\u00e3o</li> <li>Documentado e testado</li> </ul>"},{"location":"devx/workflow/project-board-template/#dicas","title":"\ud83d\udccc Dicas","text":"<ul> <li>Use labels como <code>enhancement</code>, <code>bug</code>, <code>documentation</code>, <code>good first issue</code> para filtrar</li> <li>Tarefas grandes podem ser quebradas com checklists em issues</li> <li>Utilize milestones para organizar entregas por sprint ou vers\u00e3o</li> </ul>"},{"location":"devx/workflow/pull-request-template/","title":"\ud83d\udce6 Descri\u00e7\u00e3o do Pull Request","text":"<ul> <li>Tipo de mudan\u00e7a: [x] Corre\u00e7\u00e3o de bug | [ ] Nova funcionalidade | [ ] Refatora\u00e7\u00e3o | [ ] Documenta\u00e7\u00e3o | [ ] Outro</li> <li>Descri\u00e7\u00e3o resumida:</li> </ul>"},{"location":"devx/workflow/pull-request-template/#checklist","title":"\u2705 Checklist","text":"<ul> <li>[ ] C\u00f3digo segue o padr\u00e3o de estilo definido (<code>ruff</code>, <code>black</code>)</li> <li>[ ] Testes foram adicionados ou atualizados</li> <li>[ ] Todos os testes est\u00e3o passando (<code>pytest</code>)</li> <li>[ ] Nenhum segredo ou dado sens\u00edvel foi inclu\u00eddo</li> <li>[ ] A documenta\u00e7\u00e3o foi atualizada (se necess\u00e1rio)</li> <li>[ ] Commits seguem o Guia de Commits</li> </ul>"},{"location":"devx/workflow/pull-request-template/#como-testar","title":"\ud83e\uddea Como Testar","text":""},{"location":"devx/workflow/pull-request-template/#relacionamentos","title":"\ud83d\udd17 Relacionamentos","text":"<ul> <li>Closes #[n\u00famero_da_issue] (se aplic\u00e1vel)</li> <li>Refs: [ADR-xxx], [Design Doc], etc</li> </ul>"},{"location":"devx/workflow/pull-request-template/#notas-adicionais","title":"\ud83e\udde0 Notas Adicionais","text":""},{"location":"athomic/compression/","title":"Payload Compression","text":""},{"location":"athomic/compression/#overview","title":"Overview","text":"<p>The Compression module provides an abstraction for compressing and decompressing data. Its primary use case is to act as a step in the Payload Processing Pipeline, where it can transparently compress message payloads before they are sent to a broker and decompress them on the consumer side. This reduces network bandwidth and storage costs.</p>"},{"location":"athomic/compression/#how-it-works","title":"How It Works","text":"<p>The module follows a provider pattern, with the <code>GzipProvider</code> being the default implementation. It is integrated into the <code>PayloadProcessor</code> via the <code>CompressionStepAdapter</code>. When <code>\"compression\"</code> is added as a step to the payload pipeline in your configuration, this adapter is automatically included in the processing chain.</p>"},{"location":"athomic/compression/#usage","title":"Usage","text":"<p>Usage is typically declarative via configuration. See the Payload Processing Pipeline documentation for an example of how to add the <code>compression</code> step to your pipeline.</p>"},{"location":"athomic/compression/#api-reference","title":"API Reference","text":""},{"location":"athomic/compression/#nala.athomic.compression.protocol.CompressionProtocol","title":"<code>nala.athomic.compression.protocol.CompressionProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the contract for a data compression provider.</p> <p>Any class that implements this protocol can be used by Athomic's components, such as the payload processing pipeline, to compress and decompress data. This allows for different compression algorithms (e.g., gzip, brotli) to be used interchangeably.</p>"},{"location":"athomic/compression/#nala.athomic.compression.protocol.CompressionProtocol.compress","title":"<code>compress(data)</code>  <code>async</code>","text":"<p>Compresses the given byte string.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>The raw byte string to be compressed.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The compressed byte string.</p>"},{"location":"athomic/compression/#nala.athomic.compression.protocol.CompressionProtocol.decompress","title":"<code>decompress(data)</code>  <code>async</code>","text":"<p>Decompresses the given byte string.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>The compressed byte string to be decompressed.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The original, decompressed byte string.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the data is corrupt or in an invalid format.</p>"},{"location":"athomic/compression/#nala.athomic.compression.providers.gzip_provider.GzipProvider","title":"<code>nala.athomic.compression.providers.gzip_provider.GzipProvider</code>","text":"<p>               Bases: <code>CompressionProtocol</code></p> <p>A concrete compression provider that uses the gzip algorithm.</p> <p>This class implements the <code>CompressionProtocol</code> by delegating the core compression and decompression logic to a specific algorithm instance obtained from the <code>CompressionAlgorithmFactory</code>. This design decouples the provider from the algorithm's implementation details.</p> <p>Attributes:</p> Name Type Description <code>algorithm</code> <code>CompressionAlgorithmProtocol</code> <p>The algorithm instance that performs the actual compression and decompression.</p>"},{"location":"athomic/compression/#nala.athomic.compression.providers.gzip_provider.GzipProvider.__init__","title":"<code>__init__(algorithm_name='gzip')</code>","text":"<p>Initializes the GzipProvider.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm_name</code> <code>str</code> <p>The name of the compression algorithm to create via the factory. Defaults to \"gzip\".</p> <code>'gzip'</code>"},{"location":"athomic/compression/#nala.athomic.compression.providers.gzip_provider.GzipProvider.compress","title":"<code>compress(data)</code>  <code>async</code>","text":"<p>Compresses data by delegating to the configured algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>The raw byte string to be compressed.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The gzip-compressed byte string.</p>"},{"location":"athomic/compression/#nala.athomic.compression.providers.gzip_provider.GzipProvider.decompress","title":"<code>decompress(data)</code>  <code>async</code>","text":"<p>Decompresses data by delegating to the configured algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>The gzip-compressed byte string to be decompressed.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The original, decompressed byte string.</p>"},{"location":"athomic/config/","title":"Configuration Management","text":""},{"location":"athomic/config/#overview","title":"Overview","text":"<p>The configuration module is a robust, type-safe system that provides the foundation for all configurable components within the Athomic Layer. It is built upon two key libraries:</p> <ul> <li>Dynaconf: For layered configuration loading from multiple sources (files, environment variables, Vault, etc.).</li> <li>Pydantic: For strict, type-safe validation of the configuration structure at startup.</li> </ul> <p>This combination ensures that the application starts with a valid and predictable configuration, failing fast if any required settings are missing or malformed.</p>"},{"location":"athomic/config/#key-features","title":"Key Features","text":"<ul> <li>Layered Loading: Settings are loaded from <code>.toml</code> files and can be individually overridden by environment variables.</li> <li>Environment Switching: Easily switch between environments (e.g., <code>development</code>, <code>production</code>) by setting a single environment variable.</li> <li>Type Safety: The entire configuration is parsed into a strongly-typed Pydantic model (<code>AppSettings</code>), providing auto-completion and static analysis benefits.</li> <li>Integrated Secret Management: Handles sensitive values securely, either by reading them directly or by resolving references to external secret managers like Vault at runtime.</li> </ul>"},{"location":"athomic/config/#how-it-works","title":"How It Works","text":"<p>The configuration is loaded and validated in a two-step process during application startup:</p> <ol> <li>Loading with Dynaconf: The <code>DynaconfLoader</code> reads all specified <code>.toml</code> files, merges them based on the current environment, and then applies any overrides from environment variables.</li> <li>Validation with Pydantic: The raw dictionary loaded by Dynaconf is passed to the root <code>AppSettings</code> Pydantic model. Pydantic recursively validates the entire structure, coercing types and raising a <code>ValidationError</code> if any part of the configuration is invalid.</li> </ol>"},{"location":"athomic/config/#accessing-settings","title":"Accessing Settings","text":"<p>The primary way to access configuration throughout the application is via the <code>get_settings()</code> function. This function is cached, ensuring that the loading and validation process happens only once.</p> <pre><code>from nala.athomic.config import get_settings\n\nsettings = get_settings()\nprint(settings.app_name)\nprint(settings.database.default_document_connection)\n</code></pre>"},{"location":"athomic/config/#configuration-files-environments","title":"Configuration Files &amp; Environments","text":"<p>By default, the application loads configuration from <code>settings.toml</code>. You can create additional files for different environments, like <code>development.toml</code> or <code>production.toml</code>.</p> <p>To activate a specific environment, set the <code>NALA_ENV_FOR_DYNACONF</code> environment variable. Dynaconf will then automatically merge the base <code>settings.toml</code> with the environment-specific file.</p> <pre><code>export NALA_ENV_FOR_DYNACONF=production\n# The application will now load settings from settings.toml and merge them with production.toml\n</code></pre>"},{"location":"athomic/config/#environment-variables-override","title":"Environment Variables Override","text":"<p>Any setting can be overridden using environment variables. This is particularly useful for containerized deployments (e.g., Docker, Kubernetes).</p> <ul> <li>Prefix: All variables must start with <code>NALA_</code>.</li> <li>Separator: Nested keys are separated by a double underscore <code>__</code>.</li> </ul> <p>For example, to override the database name for the default MongoDB connection, you would set the following environment variable:</p> <pre><code>export NALA_DATABASE__DOCUMENTS__DEFAULT_MONGO__PROVIDER__DATABASE_NAME=\"my_production_db\"\n</code></pre>"},{"location":"athomic/config/#secret-management","title":"Secret Management","text":"<p>The configuration system has first-class support for handling secrets securely.</p>"},{"location":"athomic/config/#1-direct-value-from-environment","title":"1. Direct Value (from Environment)","text":"<p>For secrets provided directly as environment variables, you can use Pydantic's <code>SecretStr</code> type in the configuration model. This ensures the value is automatically masked when logged or printed.</p>"},{"location":"athomic/config/#2-secret-reference-from-vault-etc","title":"2. Secret Reference (from Vault, etc.)","text":"<p>For a more secure approach, you can configure a reference to a secret stored in an external manager. This is done using the <code>SecretValue</code> model, which requires a <code>path</code> and a <code>key</code>.</p> <pre><code># In your settings.toml\n[default.database.documents.default_mongo.provider]\n# This does not contain the actual password.\n# It's a reference to a secret stored in Vault at path 'database/mongo' with key 'password'.\npassword = { path = \"database/mongo\", key = \"password\" } # pragma: allowlist secret\n</code></pre> <p>At startup, the <code>SecretsManager</code> traverses the entire configuration and replaces these <code>SecretValue</code> objects with a lazy-loading proxy. The actual secret is only fetched from the provider (e.g., Vault) at the moment it is first needed.</p>"},{"location":"athomic/config/#api-reference","title":"API Reference","text":""},{"location":"athomic/config/#nala.athomic.config.get_settings","title":"<code>nala.athomic.config.get_settings()</code>  <code>cached</code>","text":"<p>Loads, validates, and returns the application settings as a singleton.</p> <p>This is the primary entry point for accessing application configuration. It uses <code>@lru_cache</code> to ensure that the potentially expensive process of loading and validating settings from files and environment variables happens only once.</p> <p>Returns:</p> Name Type Description <code>AppSettings</code> <code>AppSettings</code> <p>A validated Pydantic model containing the entire</p> <code>AppSettings</code> <p>application configuration.</p>"},{"location":"athomic/config/#nala.athomic.config.schemas.app_settings.AppSettings","title":"<code>nala.athomic.config.schemas.app_settings.AppSettings</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The root Pydantic model for the entire application's configuration.</p> <p>This class acts as the central, validated data structure for all settings, populated from configuration files and environment variables. It provides type-safe access to configuration properties across all of Athomic's modules.</p> Model Config <p>extra=\"ignore\": Fields in the configuration source that are not defined     in this model will be ignored, preventing validation errors. validate_assignment=True: The model will re-validate fields if their     values are changed after initialization. populate_by_name=True: Allows populating model fields using their     <code>alias</code> (e.g., an env var <code>APP_NAME</code> populates the <code>app_name</code> field).</p>"},{"location":"athomic/config/#nala.athomic.config.schemas.models.SecretValue","title":"<code>nala.athomic.config.schemas.models.SecretValue</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a structured reference to a secret in an external provider.</p> <p>Instead of hardcoding secrets in configuration, this model is used to point to a secret's location (e.g., in HashiCorp Vault). The <code>SecretsManager</code> resolves these references at runtime to fetch the actual secret value.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>str</code> <p>The path or mount point in the secrets backend where the group of secrets is stored (e.g., 'database/production').</p> <code>key</code> <code>str</code> <p>The specific key within the given path that holds the secret value (e.g., 'password').</p>"},{"location":"athomic/context/","title":"Context Management","text":""},{"location":"athomic/context/#overview","title":"Overview","text":"<p>The Context Management module is the backbone for multi-tenancy, distributed tracing, and enriched logging throughout the Athomic Layer. It provides a safe and reliable way to carry request-scoped state through your application without passing arguments down the call stack.</p> <p>It is built on top of Python's native <code>contextvars</code>, making it fully compatible with <code>asyncio</code> and ensuring that context is never accidentally shared between concurrent requests or tasks.</p>"},{"location":"athomic/context/#core-use-cases","title":"Core Use Cases","text":"<ul> <li>Multi-Tenancy: Automatically isolates data by carrying the <code>tenant_id</code> from the initial request to the database layer.</li> <li>Distributed Tracing: Propagates <code>trace_id</code> and <code>span_id</code> across function calls and even to background tasks.</li> <li>Enriched Logging: Allows log records to be automatically enriched with contextual data like <code>request_id</code> and <code>user_id</code>.</li> <li>Consistent Keying: Provides a <code>ContextualKeyGenerator</code> for creating standardized keys for caches, locks, and rate limiters.</li> </ul>"},{"location":"athomic/context/#core-components","title":"Core Components","text":""},{"location":"athomic/context/#contextvarmanager","title":"<code>ContextVarManager</code>","text":"<p>This is the central registry that manages the lifecycle of all context variables. It handles their creation, default values, and tracks which variables should be propagated to background tasks.</p>"},{"location":"athomic/context/#context_vars-module","title":"<code>context_vars</code> Module","text":"<p>This module defines all the context variables available in the application (e.g., <code>tenant_id</code>, <code>request_id</code>, <code>user_id</code>, <code>locale</code>) and provides simple getter and setter functions for each one. This is the primary API you will interact with.</p> <pre><code># Example of using the getters and setters\nfrom nala.athomic.context import context_vars\n\n# Set a value (typically in a middleware)\ntoken = context_vars.set_tenant_id(\"tenant-123\")\n\n# Get a value anywhere in the call stack\ncurrent_tenant = context_vars.get_tenant_id() # Returns \"tenant-123\"\n\n# Reset the value (restores the previous state)\ncontext_vars.get_tenant_id().reset(token)\n</code></pre>"},{"location":"athomic/context/#executioncontext","title":"<code>ExecutionContext</code>","text":"<p>This is a simple data class that captures a snapshot of all current context variables at a specific moment in time. Its main purpose is to facilitate context propagation.</p>"},{"location":"athomic/context/#context-propagation","title":"Context Propagation","text":"<p>One of the biggest challenges in a microservices architecture is maintaining context across different processes, especially when dispatching background tasks. This module provides a simple and effective solution.</p> <ol> <li><code>capture_context()</code>: Before a background task is sent to a broker, this function is called. It reads all context variables marked for propagation and returns them in a serializable dictionary.</li> <li><code>restore_context()</code>: On the worker side, this function is used as a context manager (<code>with restore_context(...)</code>). It takes the dictionary captured in step 1 and temporarily applies those values to the <code>contextvars</code> for the duration of the task execution.</li> </ol> <p>This ensures that a background task for sending an email, for example, will have the same <code>trace_id</code> and <code>tenant_id</code> as the API request that triggered it.</p>"},{"location":"athomic/context/#example","title":"Example","text":"<pre><code># In your API endpoint\nfrom nala.athomic.context import capture_context\nfrom my_app.tasks import send_welcome_email_task\n\nasync def create_user(user_data: dict):\n    # ... create user ...\n\n    # Capture the current context before enqueuing the task\n    context_to_propagate = capture_context()\n\n    # Pass the context in the task's keyword arguments\n    await send_welcome_email_task.delay(\n        user_id=user.id,\n        _nala_context=context_to_propagate\n    )\n\n# In your worker/task definition\nfrom nala.athomic.context import restore_context\nfrom nala.athomic.integration.tasks import task\n\n@task()\nasync def send_welcome_email_task(user_id: int, _nala_context: dict):\n    # The `run_task_with_context` decorator usually handles this automatically,\n    # but this is how it works internally.\n    with restore_context(_nala_context):\n        # Now, inside this block, get_tenant_id() and get_trace_id()\n        # will return the values from the original API request.\n        await send_email_to(user_id)\n</code></pre> <p>Note: The <code>@run_task_with_context</code> decorator automates this process for you.</p>"},{"location":"athomic/context/#contextual-key-generation","title":"Contextual Key Generation","text":"<p>To ensure consistency in caching, locking, and rate limiting, the <code>ContextualKeyGenerator</code> automatically creates keys that are namespaced and aware of the current context.</p> <p>A key is typically generated with the following structure: <code>static_prefix:context_prefix:namespace:logical_key</code></p> <ul> <li><code>static_prefix</code>: A global prefix (e.g., \"nala\").</li> <li><code>context_prefix</code>: Automatically includes the <code>tenant_id</code> and/or <code>user_id</code> if enabled in the configuration.</li> <li><code>namespace</code>: A logical grouping for the key (e.g., \"cache\", \"ratelimit\").</li> <li><code>logical_key</code>: The specific identifier for the resource.</li> </ul> <pre><code>from nala.athomic.context import ContextualKeyGenerator\n\n# Assume tenant_id='tenant-123' is set in the context\nkey_gen = ContextualKeyGenerator(namespace=\"cache\")\ncache_key = key_gen.generate(\"user-profile\", \"user-456\")\n\n# Result: \"nala:tenant-123:cache:user-profile:user-456\"\n</code></pre>"},{"location":"athomic/context/#api-reference","title":"API Reference","text":""},{"location":"athomic/context/#nala.athomic.context.context_vars","title":"<code>nala.athomic.context.context_vars</code>","text":""},{"location":"athomic/context/#nala.athomic.context.context_vars.clear_all_context","title":"<code>clear_all_context()</code>","text":"<p>Clears all context variables to their default values.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.clear_correlation_id","title":"<code>clear_correlation_id()</code>","text":"<p>Clears the correlation_id context variable.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.clear_feature_flags","title":"<code>clear_feature_flags()</code>","text":"<p>Resets the feature_flags context variable to an empty dict.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.clear_locale","title":"<code>clear_locale()</code>","text":"<p>Resets the locale context variable to its default ('pt-BR').</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.clear_request_id","title":"<code>clear_request_id()</code>","text":"<p>Clears the request_id context variable.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.clear_role","title":"<code>clear_role()</code>","text":"<p>Clears the role context variable.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.clear_session_id","title":"<code>clear_session_id()</code>","text":"<p>Clears the session_id context variable.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.clear_source_ip","title":"<code>clear_source_ip()</code>","text":"<p>Clears the source_ip context variable.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.clear_span_id","title":"<code>clear_span_id()</code>","text":"<p>Clears the span_id context variable.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.clear_tenant_id","title":"<code>clear_tenant_id()</code>","text":"<p>Clears the tenant_id context variable.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.clear_timeout_cancelled","title":"<code>clear_timeout_cancelled()</code>","text":"<p>Resets the timeout_cancelled flag to False.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.clear_timeout_deadline","title":"<code>clear_timeout_deadline()</code>","text":"<p>Clears the timeout_deadline context variable.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.clear_trace_id","title":"<code>clear_trace_id()</code>","text":"<p>Clears the trace_id context variable.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.clear_user_id","title":"<code>clear_user_id()</code>","text":"<p>Clears the user_id context variable.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.get_correlation_id","title":"<code>get_correlation_id()</code>","text":"<p>Retrieves the correlation ID for linking related events across services.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.get_current_context_dic","title":"<code>get_current_context_dic()</code>","text":"<p>Returns a dictionary with the state of all context variables.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.get_feature_flags","title":"<code>get_feature_flags()</code>","text":"<p>Retrieves the dictionary of active feature flags.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.get_locale","title":"<code>get_locale()</code>","text":"<p>Retrieves the locale/language code for the current request.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.get_request_id","title":"<code>get_request_id()</code>","text":"<p>Retrieves the unique identifier for the current request context.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.get_role","title":"<code>get_role()</code>","text":"<p>Retrieves the role of the authenticated user for the current context.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.get_session_id","title":"<code>get_session_id()</code>","text":"<p>Retrieves the session identifier for the current context.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.get_source_ip","title":"<code>get_source_ip()</code>","text":"<p>Retrieves the source IP address of the client making the request.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.get_span_id","title":"<code>get_span_id()</code>","text":"<p>Retrieves the ID for the current span within the distributed trace.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.get_tenant_id","title":"<code>get_tenant_id()</code>","text":"<p>Retrieves the tenant ID for the current multi-tenancy context.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.get_timeout_deadline","title":"<code>get_timeout_deadline()</code>","text":"<p>Retrieves the absolute Unix timestamp of when the current operation must complete.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.get_trace_id","title":"<code>get_trace_id()</code>","text":"<p>Retrieves the distributed trace ID for the current context.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.get_user_id","title":"<code>get_user_id()</code>","text":"<p>Retrieves the authenticated user ID for the current context.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.is_timeout_cancelled","title":"<code>is_timeout_cancelled()</code>","text":"<p>Checks if the current operation has been marked as cancelled due to timeout.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.set_correlation_id","title":"<code>set_correlation_id(value)</code>","text":"<p>Sets the correlation ID for the current context, returning a token to reset the change.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.set_feature_flags","title":"<code>set_feature_flags(value)</code>","text":"<p>Sets the feature flags dictionary for the current context, returning a token to reset the change.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.set_locale","title":"<code>set_locale(value)</code>","text":"<p>Sets the locale for the current context, returning a token to reset the change.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.set_request_id","title":"<code>set_request_id(value)</code>","text":"<p>Sets the request ID for the current context, returning a token to reset the change.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.set_role","title":"<code>set_role(value)</code>","text":"<p>Sets the user role for the current context, returning a token to reset the change.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.set_session_id","title":"<code>set_session_id(value)</code>","text":"<p>Sets the session ID for the current context, returning a token to reset the change.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.set_source_ip","title":"<code>set_source_ip(value)</code>","text":"<p>Sets the source IP for the current context, returning a token to reset the change.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.set_span_id","title":"<code>set_span_id(value)</code>","text":"<p>Sets the span ID for the current context, returning a token to reset the change.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.set_tenant_id","title":"<code>set_tenant_id(value)</code>","text":"<p>Sets the tenant ID for the current context, returning a token to reset the change.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.set_timeout_cancelled","title":"<code>set_timeout_cancelled(value)</code>","text":"<p>Sets the timeout cancelled flag for the current context, returning a token to reset the change.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.set_timeout_deadline","title":"<code>set_timeout_deadline(value)</code>","text":"<p>Sets the timeout deadline for the current context, returning a token to reset the change.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.set_trace_id","title":"<code>set_trace_id(value)</code>","text":"<p>Sets the trace ID for the current context, returning a token to reset the change.</p>"},{"location":"athomic/context/#nala.athomic.context.context_vars.set_user_id","title":"<code>set_user_id(value)</code>","text":"<p>Sets the user ID for the current context, returning a token to reset the change.</p>"},{"location":"athomic/context/#nala.athomic.context.propagation","title":"<code>nala.athomic.context.propagation</code>","text":""},{"location":"athomic/context/#nala.athomic.context.propagation.capture_context","title":"<code>capture_context()</code>","text":"<p>Captures the current values of all context variables marked for propagation.</p> <p>This function iterates through the centrally registered context variables in the <code>context_var_manager</code>, collects the values of those flagged with <code>propagate=True</code>, and returns them as a dictionary. This dictionary is suitable for serialization and passing to background tasks or events.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary containing the current context values to be propagated.</p>"},{"location":"athomic/context/#nala.athomic.context.propagation.restore_context","title":"<code>restore_context(context_dict)</code>","text":"<p>A context manager to temporarily set context variables from a dictionary.</p> <p>This is used on the worker side (e.g., in a background task or event handler) to re-establish the context that existed when the job was enqueued. It safely resets all variables to their previous state upon exiting the <code>with</code> block.</p> <p>Parameters:</p> Name Type Description Default <code>context_dict</code> <code>Optional[Dict[str, Any]]</code> <p>The dictionary of context values captured before the           task was enqueued.</p> required"},{"location":"athomic/context/#nala.athomic.context.key_resolver.ContextualKeyGenerator","title":"<code>nala.athomic.context.key_resolver.ContextualKeyGenerator</code>","text":"<p>A centralized service for creating consistent and context-sensitive keys.</p> <p>This class standardizes key generation for features like caching, locking, and rate limiting. It automatically prepends keys with a static prefix and contextual information (e.g., tenant ID) based on the application's configuration, ensuring key consistency and multi-tenancy support.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <code>ContextSettings</code> <p>The context configuration settings.</p> <code>static_prefix</code> <code>str</code> <p>A global prefix for all generated keys (e.g., 'nala').</p> <code>use_tenant</code> <code>bool</code> <p>If True, the tenant ID is included in the key.</p> <code>use_user</code> <code>bool</code> <p>If True, the user ID is included in the key.</p> <code>separator</code> <code>str</code> <p>The character used to separate key parts.</p> <code>namespace</code> <code>str</code> <p>The specific namespace for this key generator instance.</p>"},{"location":"athomic/context/#nala.athomic.context.key_resolver.ContextualKeyGenerator.__init__","title":"<code>__init__(*, namespace=None, settings=None)</code>","text":"<p>Initializes the ContextualKeyGenerator.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>Optional[str]</code> <p>A specific namespace for the keys generated by this instance. Overrides the default namespace from settings if provided.</p> <code>None</code> <code>settings</code> <code>Optional[ContextSettings]</code> <p>The context configuration settings. If not provided, global settings are used.</p> <code>None</code>"},{"location":"athomic/context/#nala.athomic.context.key_resolver.ContextualKeyGenerator.generate","title":"<code>generate(*key_parts)</code>","text":"<p>Generates a final key from multiple parts with context and namespace.</p> <p>The final key structure is: <code>static_prefix:context_prefix:namespace:part1:part2:...</code></p> <p>Parameters:</p> Name Type Description Default <code>*key_parts</code> <code>Any</code> <p>A variable number of logical parts to be appended to the key.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The fully constructed, context-aware key.</p>"},{"location":"athomic/context/#nala.athomic.context.key_resolver.ContextualKeyGenerator.generate_for_function","title":"<code>generate_for_function(func, args, kwargs)</code>","text":"<p>Generates a unique and deterministic key for a function call.</p> <p>This method is ideal for function caching decorators. It creates a stable key by combining the function's qualified name with a SHA256 hash of its serialized arguments, ensuring that the same call always produces the same key.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>The function being called.</p> required <code>args</code> <code>Tuple</code> <p>The positional arguments passed to the function.</p> required <code>kwargs</code> <code>Dict</code> <p>The keyword arguments passed to the function.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A unique key representing the specific function call.</p>"},{"location":"athomic/context/#nala.athomic.context.execution.ExecutionContext","title":"<code>nala.athomic.context.execution.ExecutionContext</code>","text":"<p>A data class that provides a snapshot of the current execution context.</p> <p>This class consolidates all context variables (e.g., tenant_id, request_id, trace_id) from the <code>context_vars</code> module into a single, cohesive object. Its primary purpose is to offer a clean way to access context information and to capture the context at a specific point in time for propagation to background tasks or asynchronous events.</p> <p>Attributes:</p> Name Type Description <code>tenant_id</code> <code>Optional[str]</code> <p>The identifier for the current tenant.</p> <code>request_id</code> <code>Optional[str]</code> <p>The unique ID for the current request.</p> <code>trace_id</code> <code>Optional[str]</code> <p>The ID for the distributed trace.</p> <code>span_id</code> <code>Optional[str]</code> <p>The ID for the current span within a trace.</p> <code>user_id</code> <code>Optional[str]</code> <p>The identifier for the authenticated user.</p> <code>role</code> <code>Optional[str]</code> <p>The role of the authenticated user.</p> <code>locale</code> <code>Optional[str]</code> <p>The locale/language for the current request.</p> <code>source_ip</code> <code>Optional[str]</code> <p>The IP address of the original client.</p> <code>session_id</code> <code>Optional[str]</code> <p>The session identifier.</p> <code>correlation_id</code> <code>Optional[str]</code> <p>An ID to correlate logs and events across services.</p> <code>feature_flags</code> <code>Optional[Dict[str, bool]]</code> <p>A dictionary of active feature flags.</p>"},{"location":"athomic/context/#nala.athomic.context.execution.ExecutionContext.__init__","title":"<code>__init__(tenant_id=None, request_id=None, trace_id=None, span_id=None, user_id=None, role=None, locale=None, source_ip=None, session_id=None, correlation_id=None, feature_flags=None)</code>","text":"<p>Initializes the ExecutionContext.</p> <p>If a value for a parameter is not explicitly provided, the constructor will fetch the current value from the global <code>context_vars</code>. This allows for both creating a snapshot of the current context (<code>ExecutionContext()</code>) and creating a custom context for specific purposes (like testing or background jobs).</p> <p>Parameters:</p> Name Type Description Default <code>tenant_id</code> <code>Optional[str]</code> <p>The unique identifier for the tenant.</p> <code>None</code> <code>request_id</code> <code>Optional[str]</code> <p>The unique ID for the request.</p> <code>None</code> <code>trace_id</code> <code>Optional[str]</code> <p>The distributed trace ID.</p> <code>None</code> <code>span_id</code> <code>Optional[str]</code> <p>The current span ID.</p> <code>None</code> <code>user_id</code> <code>Optional[str]</code> <p>The identifier for the user.</p> <code>None</code> <code>role</code> <code>Optional[str]</code> <p>The user's role.</p> <code>None</code> <code>locale</code> <code>Optional[str]</code> <p>The request's locale.</p> <code>None</code> <code>source_ip</code> <code>Optional[str]</code> <p>The client's IP address.</p> <code>None</code> <code>session_id</code> <code>Optional[str]</code> <p>The session ID.</p> <code>None</code> <code>correlation_id</code> <code>Optional[str]</code> <p>The correlation ID.</p> <code>None</code> <code>feature_flags</code> <code>Optional[Dict[str, bool]]</code> <p>A dictionary of feature flags.</p> <code>None</code>"},{"location":"athomic/context/#nala.athomic.context.execution.ExecutionContext.__repr__","title":"<code>__repr__()</code>","text":"<p>Returns a string representation of the object.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A developer-friendly string representation of the context.</p>"},{"location":"athomic/context/#nala.athomic.context.execution.ExecutionContext.to_dict","title":"<code>to_dict()</code>","text":"<p>Serializes the context object to a dictionary.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary representation of the execution context,</p> <code>Dict[str, Any]</code> <p>suitable for logging or propagating to other services.</p>"},{"location":"athomic/credentials/","title":"Credential Resolution","text":""},{"location":"athomic/credentials/#overview","title":"Overview","text":"<p>The Credentials module provides the core components for the \"just-in-time\" secret resolution system used throughout the Athomic Layer. Its purpose is to securely handle sensitive values that might be raw strings, protected Pydantic <code>SecretStr</code> types, or lazy-loading <code>CredentialProxy</code> objects.</p>"},{"location":"athomic/credentials/#key-components","title":"Key Components","text":"<ul> <li><code>CredentialProxy</code>: A lightweight object that replaces a secret reference in the configuration at startup. It holds a function that knows how to fetch the real secret from a provider but doesn't execute it until the secret is actually needed.</li> <li><code>CredentialResolve</code>: A mixin class that provides the <code>_resolve_credential</code> and <code>_decode_credential_to_str</code> helper methods. Any service that needs to handle a potentially unresolved secret (like a database provider needing a password) inherits from this mixin to safely get the final secret value.</li> </ul> <p>For a more detailed explanation, see the Secrets Management documentation.</p>"},{"location":"athomic/credentials/#api-reference","title":"API Reference","text":""},{"location":"athomic/credentials/#nala.athomic.credentials.proxy.CredentialProxy","title":"<code>nala.athomic.credentials.proxy.CredentialProxy</code>","text":"<p>A lazy-loading proxy for a secret value.</p> <p>It holds a reference to the secrets provider and the secret's location (path/key). It fetches the secret's actual value \"just-in-time\" only when its <code>get()</code> method is explicitly awaited. This ensures that the application always uses the most up-to-date credential, supporting secret rotation.</p>"},{"location":"athomic/credentials/#nala.athomic.credentials.proxy.CredentialProxy.get","title":"<code>get()</code>  <code>async</code>","text":"<p>Fetches the secret's value just-in-time from the secrets provider.</p> <p>Returns:</p> Type Description <code>Optional[Any]</code> <p>A Resolved secret value, or None if not found.</p>"},{"location":"athomic/credentials/#nala.athomic.credentials.base.CredentialResolve","title":"<code>nala.athomic.credentials.base.CredentialResolve</code>","text":"<p>A mixin class providing standardized methods for resolving credentials.</p> <p>This class is intended to be inherited by services or providers that need to handle sensitive data. It centralizes the logic for unwrapping various credential types\u2014such as raw strings, Pydantic's <code>SecretStr</code>, or a lazy-loading <code>CredentialProxy</code>\u2014into their final, usable form.</p>"},{"location":"athomic/database/","title":"Database Connection Management","text":""},{"location":"athomic/database/#overview","title":"Overview","text":"<p>The <code>athomic.database</code> module provides a centralized, lifecycle-managed system for all data store connections within the application. Its primary component, the <code>ConnectionManager</code>, acts as a registry and orchestrator for all configured data stores, including Document Databases (like MongoDB) and Key-Value Stores (like Redis).</p> <p>This system is crucial for building robust applications as it ensures that:</p> <ul> <li>All database connections are established and checked for readiness during application startup.</li> <li>Dependencies are respected, so services that require a database connection only start after the connection is ready.</li> <li>All connections are gracefully closed during application shutdown.</li> </ul>"},{"location":"athomic/database/#core-components","title":"Core Components","text":""},{"location":"athomic/database/#connectionmanager","title":"<code>ConnectionManager</code>","text":"<p>The <code>ConnectionManager</code> is the heart of the database module. It is a <code>BaseService</code> whose lifecycle is managed by the main <code>LifecycleManager</code>. Its responsibilities are:</p> <ul> <li>Reading Configuration: It inspects the <code>[database]</code> section of your settings file to discover all configured connections.</li> <li>Creating Providers: For each configured connection, it uses a dedicated factory (e.g., <code>DocumentsDatabaseFactory</code>, <code>KVStoreFactory</code>) to instantiate the correct provider client.</li> <li>Managing Lifecycle: It concurrently starts (<code>connect</code>) and checks the readiness (<code>wait_ready</code>) of all provider instances during startup, and gracefully closes them during shutdown.</li> <li>Acting as a Registry: It provides getter methods (<code>get_document_db</code>, <code>get_kv_store</code>) for other services to retrieve active and ready-to-use database clients.</li> </ul>"},{"location":"athomic/database/#connectionmanagerfactory","title":"<code>ConnectionManagerFactory</code>","text":"<p>This is a singleton factory responsible for creating the single instance of the <code>ConnectionManager</code>. Any component in the application that needs access to a database connection should use this factory to get the <code>ConnectionManager</code>.</p>"},{"location":"athomic/database/#how-it-works","title":"How It Works","text":""},{"location":"athomic/database/#startup-sequence","title":"Startup Sequence","text":"<ol> <li>During application startup, the main <code>LifecycleManager</code> starts the <code>ConnectionManager</code> service.</li> <li>The <code>ConnectionManager</code> reads the settings and identifies all connections defined under <code>[database.documents]</code> and <code>[database.kvstore]</code>.</li> <li>It then concurrently calls the <code>connect()</code> and <code>wait_ready()</code> methods on each of these connection providers.</li> <li>The <code>ConnectionManager</code> itself becomes \"ready\" only after all its managed connections are successfully established.</li> </ol>"},{"location":"athomic/database/#retrieving-a-connection","title":"Retrieving a Connection","text":"<p>A service that needs to interact with a database follows these steps:</p> <ol> <li>Get the singleton <code>ConnectionManager</code> instance from the <code>ConnectionManagerFactory</code>.</li> <li>Call the appropriate getter method with the name of the desired connection, for example: <code>connection_manager.get_document_db(\"default_mongo\")</code>.</li> </ol> <p>This call will return a client instance that is guaranteed to be connected and ready to use.</p>"},{"location":"athomic/database/#configuration","title":"Configuration","text":"<p>You can define multiple connections for both document databases and key-value stores in your <code>settings.toml</code> file. This is useful for connecting to different databases for different purposes (e.g., one for application data, another for caching).</p> <pre><code>[default.database]\n# Define the names of the default connections to be used by other modules.\ndefault_document_connection = \"default_mongo\"\ndefault_kvstore_connection = \"default_redis\"\n\n# A dictionary of all available document database connections.\n[default.database.documents]\n  [default.database.documents.default_mongo]\n  enabled = true\n  backend = \"mongo\"\n    [default.database.documents.default_mongo.provider]\n    url = \"mongodb://user:pass@localhost:27017\" # pragma: allowlist secret\n    database_name = \"main_db\"\n\n# A dictionary of all available key-value store connections.\n[default.database.kvstore]\n  [default.database.kvstore.default_redis]\n  enabled = true\n  namespace = \"cache\"\n    [default.database.kvstore.default_redis.provider]\n    backend = \"redis\"\n    uri = \"redis://localhost:6379/0\"\n\n  [default.database.kvstore.another_redis]\n  enabled = true\n  namespace = \"sessions\"\n    [default.database.kvstore.another_redis.provider]\n    backend = \"redis\"\n    uri = \"redis://localhost:6379/1\"\n</code></pre>"},{"location":"athomic/database/#api-reference","title":"API Reference","text":""},{"location":"athomic/database/#nala.athomic.database.manager.ConnectionManager","title":"<code>nala.athomic.database.manager.ConnectionManager</code>","text":"<p>               Bases: <code>BaseService</code></p> <p>A central service for managing the lifecycle of all database connections.</p> <p>This class acts as a registry and lifecycle orchestrator for all configured data stores (e.g., Document Databases, Key-Value Stores). It is responsible for initializing all providers based on the application's configuration, starting them concurrently during startup, and closing them gracefully during shutdown. Other services should use this manager to retrieve active database client instances.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <p>The validated database configuration settings.</p>"},{"location":"athomic/database/#nala.athomic.database.manager.ConnectionManager.__init__","title":"<code>__init__(settings)</code>","text":"<p>Initializes the ConnectionManager.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[DatabaseSettings]</code> <p>The database settings object. If not provided, it will be loaded from the global application settings. This allows for dependency injection, especially during testing.</p> required"},{"location":"athomic/database/#nala.athomic.database.manager.ConnectionManager.add_document_db","title":"<code>add_document_db(name, instance, is_default=False)</code>","text":"<p>Adds or overwrites a document database instance in the manager.</p> <p>This method is primarily used for dependency injection in tests or for programmatically registering a database provider at runtime.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The unique name to assign to the database instance.</p> required <code>instance</code> <code>DocumentsDatabaseProtocol</code> <p>The database provider instance to add.</p> required <code>is_default</code> <code>Optional[bool]</code> <p>If True, sets this instance as the default (currently not used).</p> <code>False</code>"},{"location":"athomic/database/#nala.athomic.database.manager.ConnectionManager.add_kv_store","title":"<code>add_kv_store(name, instance, is_default=False)</code>","text":"<p>Adds or overwrites a KV store instance in the manager.</p> <p>This method is primarily used for dependency injection in tests or for programmatically registering a KV store provider at runtime.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The unique name to assign to the KV store instance.</p> required <code>instance</code> <code>KVStoreProtocol</code> <p>The KV store provider instance to add.</p> required <code>is_default</code> <code>bool</code> <p>If True, sets this instance as the default (currently not used).</p> <code>False</code>"},{"location":"athomic/database/#nala.athomic.database.manager.ConnectionManager.get_document_db","title":"<code>get_document_db(name)</code>","text":"<p>Gets a connected document database instance by its configured name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The unique name of the document database connection as defined in the configuration.</p> required <p>Returns:</p> Type Description <code>DocumentsDatabaseProtocol</code> <p>The requested document database provider instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the connection name is not found or not configured.</p>"},{"location":"athomic/database/#nala.athomic.database.manager.ConnectionManager.get_kv_store","title":"<code>get_kv_store(name)</code>","text":"<p>Gets a connected KV store instance by its configured name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The unique name of the KV store connection as defined in the configuration.</p> required <p>Returns:</p> Type Description <code>KVStoreProtocol</code> <p>The requested KV store provider instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the connection name is not found or not configured.</p>"},{"location":"athomic/database/#nala.athomic.database.factory.ConnectionManagerFactory","title":"<code>nala.athomic.database.factory.ConnectionManagerFactory</code>","text":""},{"location":"athomic/database/#nala.athomic.database.factory.ConnectionManagerFactory.clear","title":"<code>clear()</code>  <code>async</code> <code>classmethod</code>","text":"<p>Clears the singleton instance of the ConnectionManager.</p>"},{"location":"athomic/database/#nala.athomic.database.factory.ConnectionManagerFactory.create","title":"<code>create(settings=None)</code>  <code>classmethod</code>","text":"<p>Creates and returns the singleton instance of the ConnectionManager.</p>"},{"location":"athomic/database/#nala.athomic.database.protocol.DatabaseClientProtocol","title":"<code>nala.athomic.database.protocol.DatabaseClientProtocol</code>","text":"<p>               Bases: <code>BaseServiceProtocol</code>, <code>Protocol</code></p> <p>Protocol for a document database provider.</p> <p>Defines the contract for managing a database connection and providing access to the native database instance (e.g., a Motor/Beanie database object). It inherits from ConnectionServiceProtocol to be managed by the application's lifecycle.</p>"},{"location":"athomic/database/#nala.athomic.database.protocol.DatabaseClientProtocol.get_database","title":"<code>get_database()</code>  <code>async</code>","text":"<p>Returns the native database instance required by repositories to perform operations.</p>"},{"location":"athomic/events/","title":"Internal Event Bus","text":""},{"location":"athomic/events/#overview","title":"Overview","text":"<p>The Internal Event Bus provides a lightweight, in-process publish/subscribe mechanism that allows different components within your application to communicate without being directly coupled to one another. It is a powerful tool for building modular and maintainable systems.</p> <p>Important Distinction: The Internal Event Bus is designed for intra-service communication (communication within a single service instance or across instances of the same service if using Redis). For durable, guaranteed, inter-service communication (between different microservices), you should use the more robust Messaging Module.</p>"},{"location":"athomic/events/#key-features","title":"Key Features","text":"<ul> <li>Decoupled Communication: Allows components to react to events happening elsewhere in the application without needing a direct reference.</li> <li>Automatic Context Propagation: This is a critical feature. The event bus automatically captures the <code>ExecutionContext</code> (<code>trace_id</code>, <code>tenant_id</code>, etc.) when an event is published and restores it when the subscriber callback is executed. This ensures perfect continuity for tracing and multi-tenancy, even in an event-driven flow.</li> <li>Multiple Backends: Supports both an in-memory provider for single-instance applications and a Redis Pub/Sub provider for multi-instance scenarios.</li> </ul>"},{"location":"athomic/events/#how-it-works","title":"How It Works","text":"<p>The module implements the classic Publish/Subscribe pattern:</p> <ol> <li>Subscribe: A component (the \"subscriber\") registers an asynchronous callback function for a specific event name (e.g., <code>\"user.created\"</code>).</li> <li>Publish: Another component (the \"publisher\") publishes an event with that name and a data payload.</li> <li>Dispatch: The event bus finds all registered callbacks for that event name and executes them concurrently, passing the payload to each one.</li> </ol>"},{"location":"athomic/events/#available-providers","title":"Available Providers","text":"<ul> <li><code>LocalEventBus</code>: The default, in-memory provider. It is extremely fast and requires no external dependencies. It is the perfect choice for decoupling components within a single service instance.</li> <li><code>RedisEventBus</code>: This provider uses Redis Pub/Sub as its backend. This allows multiple instances of the same service to share the same event bus, enabling simple real-time communication across processes without the overhead of a full message broker like Kafka.</li> </ul>"},{"location":"athomic/events/#usage-example","title":"Usage Example","text":"<p>Imagine a <code>UserService</code> that needs to notify an <code>AuditService</code> whenever a new user is created, without the two services knowing about each other.</p>"},{"location":"athomic/events/#subscriber-auditservice","title":"Subscriber (<code>AuditService</code>)","text":"<pre><code># In your_app/services/audit_service.py\nfrom nala.athomic.events import get_event_bus\nfrom nala.athomic.observability import get_logger\n\nlogger = get_logger(__name__)\n\nclass AuditService:\n    def __init__(self):\n        # Get the singleton event bus instance\n        event_bus = get_event_bus()\n        # Subscribe the `on_user_created` method to the \"user.created\" event\n        event_bus.subscribe(\"user.created\", self.on_user_created)\n\n    async def on_user_created(self, payload: dict):\n        # This method is executed when the event is published.\n        # It runs with the same context (trace_id, etc.) as the publisher.\n        user_id = payload.get(\"id\")\n        logger.info(f\"Auditing creation of user {user_id}\")\n</code></pre>"},{"location":"athomic/events/#publisher-userservice","title":"Publisher (<code>UserService</code>)","text":"<pre><code># In your_app/services/user_service.py\nfrom nala.athomic.events import get_event_bus\n\nclass UserService:\n    def __init__(self):\n        self.event_bus = get_event_bus()\n\n    async def create_user(self, name: str) -&gt; dict:\n        user = {\"id\": \"user-123\", \"name\": name}\n\n        # ... logic to save the user to the database ...\n\n        # Publish an event with the user data as the payload\n        await self.event_bus.publish(\"user.created\", payload=user)\n\n        return user\n</code></pre>"},{"location":"athomic/events/#configuration","title":"Configuration","text":"<p>The event bus is configured under the <code>[events]</code> section in your <code>settings.toml</code>.</p>"},{"location":"athomic/events/#local-provider-default","title":"Local Provider (Default)","text":"<pre><code>[default.events]\nenabled = true\n\n  [default.events.provider]\n  backend = \"local\"\n</code></pre>"},{"location":"athomic/events/#redis-provider","title":"Redis Provider","text":"<pre><code>[default.events]\nenabled = true\n\n  [default.events.provider]\n  backend = \"redis\"\n\n    # The redis provider reuses a KVStore connection configuration\n    [default.events.provider.provider]\n    # The key_prefix is used to namespace the Redis channels\n    key_prefix = \"app_events\"\n\n      [default.events.provider.provider.provider]\n      backend = \"redis\"\n      uri = \"redis://localhost:6379/2\"\n</code></pre>"},{"location":"athomic/events/#api-reference","title":"API Reference","text":""},{"location":"athomic/events/#nala.athomic.events.protocol.EventBusProtocol","title":"<code>nala.athomic.events.protocol.EventBusProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the standard interface for an internal event bus.</p> <p>Implementations of this protocol handle publishing events and managing subscriptions within the application, enabling decoupled, in-process (or cross-process with backends like Redis) communication.</p>"},{"location":"athomic/events/#nala.athomic.events.protocol.EventBusProtocol.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Closes any open connections or releases resources.</p>"},{"location":"athomic/events/#nala.athomic.events.protocol.EventBusProtocol.publish","title":"<code>publish(event_name, payload)</code>  <code>async</code>","text":"<p>Publishes an event to all registered subscribers for that event name.</p> <p>Implementations should automatically capture and include the current ExecutionContext within the payload (e.g., under a '_nala_context' key) before dispatching to subscribers.</p> <p>Parameters:</p> Name Type Description Default <code>event_name</code> <code>str</code> <p>The string identifier of the event being published.</p> required <code>payload</code> <code>Dict[str, Any]</code> <p>A dictionary containing the event data.</p> required"},{"location":"athomic/events/#nala.athomic.events.protocol.EventBusProtocol.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Initializes the event bus backend if necessary (e.g., connections).</p>"},{"location":"athomic/events/#nala.athomic.events.protocol.EventBusProtocol.subscribe","title":"<code>subscribe(event_name, callback)</code>","text":"<p>Registers an async callback to be executed when an event is published.</p> <p>Parameters:</p> Name Type Description Default <code>event_name</code> <code>str</code> <p>The name of the event to subscribe to.</p> required <code>callback</code> <code>Callable[[Dict[str, Any]], Awaitable[None]]</code> <p>An async function that will receive the event payload (including context). Callbacks should handle their own exceptions to prevent disrupting the event bus.</p> required"},{"location":"athomic/events/#nala.athomic.events.protocol.EventBusProtocol.unsubscribe","title":"<code>unsubscribe(event_name, callback)</code>","text":"<p>Unregisters a previously subscribed callback for a specific event.</p> <p>Parameters:</p> Name Type Description Default <code>event_name</code> <code>str</code> <p>The name of the event from which to unsubscribe.</p> required <code>callback</code> <code>Callable</code> <p>The specific callback function instance to remove.</p> required"},{"location":"athomic/events/#nala.athomic.events.factory.get_event_bus","title":"<code>nala.athomic.events.factory.get_event_bus()</code>","text":"<p>Convenience function that delegates to the factory.</p>"},{"location":"athomic/events/#nala.athomic.events.providers.local_events_provider.LocalEventBus","title":"<code>nala.athomic.events.providers.local_events_provider.LocalEventBus</code>","text":"<p>               Bases: <code>EventBusBase</code></p> <p>An in-memory event bus implementation for in-process communication.</p> <p>This provider executes subscribed callbacks immediately and concurrently upon publication, avoiding the need for a separate continuous polling loop (<code>_run_loop</code>). It is designed for high-speed, decoupled communication within a single application instance, making it ideal for testing and development environments.</p>"},{"location":"athomic/events/#nala.athomic.events.providers.local_events_provider.LocalEventBus.__init__","title":"<code>__init__(settings)</code>","text":"<p>Initializes the LocalEventBus.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>EventsSettings</code> <p>The event bus configuration settings.</p> required"},{"location":"athomic/events/#nala.athomic.events.providers.redis_events_provider.RedisEventBus","title":"<code>nala.athomic.events.providers.redis_events_provider.RedisEventBus</code>","text":"<p>               Bases: <code>EventBusBase</code></p> <p>An event bus provider implementation using Redis Pub/Sub for cross-process communication.</p> <p>This provider enables multiple application instances to share the same event bus. It manages the connection to Redis, dynamically subscribes to channels based on active subscribers, and processes incoming events in a continuous background loop.</p> <p>Attributes:</p> Name Type Description <code>redis_events_settings</code> <code>RedisEventsSettings</code> <p>The Redis-specific configuration model.</p> <code>kvstore_settings</code> <code>KVStoreSettings</code> <p>The underlying KVStore configuration used to connect to Redis.</p> <code>kvstore</code> <code>KVStoreProtocol</code> <p>The instance of the KVStore client.</p> <code>_redis_client</code> <code>Optional[Redis]</code> <p>The raw asynchronous Redis client instance.</p> <code>_pubsub_client</code> <code>Optional[PubSub]</code> <p>The Redis PubSub client used for listening to channels.</p>"},{"location":"athomic/events/#nala.athomic.events.providers.redis_events_provider.RedisEventBus.__init__","title":"<code>__init__(settings, kv_store=None)</code>","text":"<p>Initializes the RedisEventBus.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>EventsSettings</code> <p>The event bus configuration settings.</p> required <code>kv_store</code> <code>Optional[KVStoreProtocol]</code> <p>Optional pre-configured KV store instance for dependency injection.</p> <code>None</code>"},{"location":"athomic/events/#nala.athomic.events.providers.redis_events_provider.RedisEventBus.before_start","title":"<code>before_start()</code>  <code>async</code>","text":"<p>Establishes the connection to Redis and initializes the PubSub client.</p> <p>This hook is called by the <code>BaseService</code> lifecycle before the main processing loop starts.</p> <p>Raises:</p> Type Description <code>EventBusError</code> <p>If the connection to Redis fails.</p>"},{"location":"athomic/events/#nala.athomic.events.providers.redis_events_provider.RedisEventBus.before_stop","title":"<code>before_stop()</code>  <code>async</code>","text":"<p>Performs cleanup before the service stops. Currently a no-op as connection closure is handled by the KVStore client if it's managed globally.</p>"},{"location":"athomic/http/","title":"Resilient HTTP Client","text":""},{"location":"athomic/http/#overview","title":"Overview","text":"<p>The HTTP module provides a factory for creating pre-configured, resilient, and observable HTTP clients for communicating with external services. Instead of creating <code>httpx</code> or <code>requests</code> clients directly in your code, you define named clients in your configuration, and Athomic assembles them with a rich set of features.</p> <p>This approach centralizes the configuration of external services and ensures that all outgoing HTTP traffic from your application is handled in a consistent, resilient, and observable way.</p>"},{"location":"athomic/http/#key-features","title":"Key Features","text":"<ul> <li>Centralized Configuration: Define all your external service clients (base URL, authentication, timeouts) in one place.</li> <li>Built-in Resilience: Automatically apply Retry and Circuit Breaker policies to your HTTP calls simply by referencing them in the configuration.</li> <li>Automatic Observability: Every request made through a Athomic client is automatically instrumented with Prometheus metrics (latency, request count, error rate) and OpenTelemetry traces.</li> <li>Automatic Context Propagation: Critical context headers like <code>x-request-id</code>, <code>x-trace-id</code>, and <code>x-tenant-id</code> are automatically injected into every outgoing request.</li> <li>Declarative Caching: Enable and configure caching for <code>GET</code> requests directly in your settings.</li> <li>Extensible Authentication: Built-in support for Bearer Token and API Key authentication.</li> </ul>"},{"location":"athomic/http/#how-it-works","title":"How It Works","text":"<p>You don't instantiate clients directly. Instead, you use the singleton <code>HttpClientFactory</code> to get a pre-configured client by name.</p> <p>When you request a client (e.g., <code>HttpClientFactory.create(\"payment_api\")</code>), the factory performs the following steps:</p> <ol> <li>Reads the configuration for the <code>\"payment_api\"</code> client from your <code>settings.toml</code>.</li> <li>Instantiates the base provider (e.g., <code>HttpxProvider</code>).</li> <li>Creates and attaches the configured authentication strategy (e.g., <code>BearerTokenAuth</code>).</li> <li>Builds a <code>ResilienceOrchestrator</code> and injects the configured <code>Retry</code> and <code>Circuit Breaker</code> handlers.</li> <li>If caching is enabled for the client, it wraps the <code>get</code> method with the <code>@cache</code> decorator.</li> <li>Returns a fully assembled, ready-to-use client instance (and caches it for future calls).</li> </ol>"},{"location":"athomic/http/#usage-example","title":"Usage Example","text":"<p>First, define your client in <code>settings.toml</code> (see Configuration section below). Then, get and use the client in your code:</p> <pre><code>from nala.athomic.http import HttpClientFactory\n\n# Get the pre-configured client for the \"payment_api\"\npayment_api_client = HttpClientFactory.create(\"payment_api\")\n\nasync def process_payment(payment_data: dict):\n    try:\n        # This call is automatically resilient (retries, circuit breaker)\n        # and observable (traces, metrics).\n        response = await payment_api_client.post(\"/v1/payments\", json=payment_data)\n        return response\n    except Exception as e:\n        # Handle specific HTTP errors if needed\n        print(f\"Failed to process payment: {e}\")\n        raise\n</code></pre>"},{"location":"athomic/http/#configuration","title":"Configuration","text":"<p>This module's power comes from its declarative configuration. You define all clients under the <code>[http.clients]</code> section in your <code>settings.toml</code>.</p> <pre><code>[default.http]\nenabled = true\n\n  # Define a dictionary of named clients\n  [default.http.clients]\n\n    # --- Example 1: A client for an external payment API ---\n    [default.http.clients.payment_api]\n    base_url = \"[https://api.paymentservice.com](https://api.paymentservice.com)\"\n    timeout = 15.0 # seconds\n\n      # Use Bearer Token authentication\n      [default.http.clients.payment_api.auth]\n      method = \"bearer\"\n      # Token is a secret reference\n      token = { path = \"services/payment\", key = \"api_token\" }\n\n      # Apply resilience policies defined in the [resilience] section\n      retry_policy_name = \"default\"\n      circuit_breaker_policy_name = \"external_services\"\n\n    # --- Example 2: A client for an internal service with caching ---\n    [default.http.clients.user_service]\n    base_url = \"[http://user-service.internal:8000](http://user-service.internal:8000)\"\n    timeout = 5.0\n\n      # Use API Key authentication\n      [default.http.clients.user_service.auth]\n      method = \"api_key\"\n      header_name = \"X-Internal-API-Key\"\n      key = { path = \"services/user\", key = \"api_key\" }\n\n      # Enable and configure caching for GET requests\n      [default.http.clients.user_service.cache]\n      enabled = true\n      key_prefix = \"user_service_cache\"\n\n        # Uses a KVStore connection named \"cache_redis\"\n        [default.http.clients.user_service.cache.kv_store_connection_name]\n        name = \"cache_redis\"\n\n        # Apply a default TTL wrapper to the cache\n        [[default.http.clients.user_service.cache.kv_store_connection_name.wrappers]]\n        name = \"default_ttl\"\n          [default.http.clients.user_service.cache.kv_store_connection_name.wrappers.config]\n          default_ttl_seconds = 300 # 5 minutes\n</code></pre>"},{"location":"athomic/http/#api-reference","title":"API Reference","text":""},{"location":"athomic/http/#nala.athomic.http.protocol.HttpClientProtocol","title":"<code>nala.athomic.http.protocol.HttpClientProtocol</code>","text":"<p>               Bases: <code>BaseServiceProtocol</code>, <code>Protocol</code></p> <p>Defines the contract for a resilient, lifecycle-managed HTTP client.</p> <p>Any class that implements this protocol can be used by the framework to make external HTTP requests. Inheriting from <code>BaseServiceProtocol</code> ensures that the client's lifecycle (e.g., connection pooling) is managed by Athomic's central <code>LifecycleManager</code>.</p>"},{"location":"athomic/http/#nala.athomic.http.protocol.HttpClientProtocol.delete","title":"<code>delete(url, **kwargs)</code>  <code>async</code>","text":"<p>Sends an HTTP DELETE request.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL path for the request, relative to the client's base URL.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to be passed to the underlying HTTP client library (e.g., <code>params</code>, <code>headers</code>).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The parsed response body.</p> <p>Raises:</p> Type Description <code>HTTPRequestError</code> <p>If the server responds with a 4xx or 5xx status.</p> <code>HTTPTimeoutError</code> <p>If the request times out.</p> <code>ServiceNotReadyError</code> <p>If the client has not been started.</p>"},{"location":"athomic/http/#nala.athomic.http.protocol.HttpClientProtocol.get","title":"<code>get(url, **kwargs)</code>  <code>async</code>","text":"<p>Sends an HTTP GET request.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL path for the request, relative to the client's base URL.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to be passed to the underlying HTTP client library (e.g., <code>params</code>, <code>headers</code>).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The parsed response body.</p> <p>Raises:</p> Type Description <code>HTTPRequestError</code> <p>If the server responds with a 4xx or 5xx status.</p> <code>HTTPTimeoutError</code> <p>If the request times out.</p> <code>ServiceNotReadyError</code> <p>If the client has not been started.</p>"},{"location":"athomic/http/#nala.athomic.http.protocol.HttpClientProtocol.patch","title":"<code>patch(url, **kwargs)</code>  <code>async</code>","text":"<p>Sends an HTTP PATCH request.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL path for the request, relative to the client's base URL.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to be passed to the underlying HTTP client library (e.g., <code>json</code>, <code>data</code>, <code>headers</code>).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The parsed response body.</p> <p>Raises:</p> Type Description <code>HTTPRequestError</code> <p>If the server responds with a 4xx or 5xx status.</p> <code>HTTPTimeoutError</code> <p>If the request times out.</p> <code>ServiceNotReadyError</code> <p>If the client has not been started.</p>"},{"location":"athomic/http/#nala.athomic.http.protocol.HttpClientProtocol.post","title":"<code>post(url, **kwargs)</code>  <code>async</code>","text":"<p>Sends an HTTP POST request.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL path for the request, relative to the client's base URL.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to be passed to the underlying HTTP client library (e.g., <code>json</code>, <code>data</code>, <code>headers</code>).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The parsed response body.</p> <p>Raises:</p> Type Description <code>HTTPRequestError</code> <p>If the server responds with a 4xx or 5xx status.</p> <code>HTTPTimeoutError</code> <p>If the request times out.</p> <code>ServiceNotReadyError</code> <p>If the client has not been started.</p>"},{"location":"athomic/http/#nala.athomic.http.protocol.HttpClientProtocol.put","title":"<code>put(url, **kwargs)</code>  <code>async</code>","text":"<p>Sends an HTTP PUT request.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL path for the request, relative to the client's base URL.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to be passed to the underlying HTTP client library (e.g., <code>json</code>, <code>data</code>, <code>headers</code>).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The parsed response body.</p> <p>Raises:</p> Type Description <code>HTTPRequestError</code> <p>If the server responds with a 4xx or 5xx status.</p> <code>HTTPTimeoutError</code> <p>If the request times out.</p> <code>ServiceNotReadyError</code> <p>If the client has not been started.</p>"},{"location":"athomic/http/#nala.athomic.http.factory.HttpClientFactory","title":"<code>nala.athomic.http.factory.HttpClientFactory</code>","text":"<p>Factory to create configured and resilient HTTP client instances.</p> <p>This factory orchestrates the creation of <code>HttpClientProtocol</code> instances. It assembles a complete client by: 1. Loading the client's specific configuration by name. 2. Instantiating the correct backend provider (e.g., Httpx). 3. Creating and attaching the configured authentication strategy. 4. Building a resilience pipeline with Retry and Circuit Breaker handlers. 5. Applying a cache decorator to the <code>get</code> method if configured.</p> <p>Created instances are cached by name to ensure a single client instance per configuration is used throughout the application.</p>"},{"location":"athomic/http/#nala.athomic.http.factory.HttpClientFactory.create","title":"<code>create(name)</code>  <code>classmethod</code>","text":"<p>Creates or retrieves a named, fully-configured HTTP client instance.</p> <p>This is the main factory method. It builds a complete client, including its provider, authentication, resilience (retry, circuit breaker), and caching layers, based on the application's configuration.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The unique name of the client configuration to create, as defined in the settings file.</p> required <p>Returns:</p> Type Description <code>HttpClientProtocol</code> <p>A fully initialized and resilient HTTP client instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the client configuration for the given <code>name</code> is not found.</p> <code>RuntimeError</code> <p>If the configured provider class is not found in the registry.</p>"},{"location":"athomic/http/#nala.athomic.http.base.HTTPClientBase","title":"<code>nala.athomic.http.base.HTTPClientBase</code>","text":"<p>               Bases: <code>BaseService</code>, <code>HttpClientProtocol</code></p> <p>Abstract base class for resilient HTTP Client providers.</p> <p>This class implements the Template Method pattern, providing a shared, robust framework for making HTTP requests. It handles common concerns like service lifecycle, resilience orchestration (via <code>ResilienceOrchestrator</code>), automatic context propagation in headers, and comprehensive observability (metrics and tracing). Concrete subclasses must only implement the <code>_request</code> method to handle the specifics of the underlying HTTP client library.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <code>HttpClientSettings</code> <p>The configuration for this specific client instance.</p> <code>resilience_orchestrator</code> <p>The orchestrator that applies resilience patterns like retry and circuit breaker to the request.</p>"},{"location":"athomic/http/#nala.athomic.http.base.HTTPClientBase.__init__","title":"<code>__init__(settings, service_name, resilience_orchestrator)</code>","text":"<p>Initializes the HTTPClientBase.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>HttpClientSettings</code> <p>The configuration for this specific HTTP client.</p> required <code>service_name</code> <code>str</code> <p>A unique name for the service, used for observability.</p> required <code>resilience_orchestrator</code> <code>ResilienceOrchestrator</code> <p>An orchestrator instance to apply resilience patterns to the outgoing requests.</p> required"},{"location":"athomic/http/#nala.athomic.http.base.HTTPClientBase.delete","title":"<code>delete(url, **kwargs)</code>  <code>async</code>","text":"<p>Sends an HTTP DELETE request.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL path for the request, relative to the client's base URL.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the HTTP request.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The parsed response body.</p>"},{"location":"athomic/http/#nala.athomic.http.base.HTTPClientBase.get","title":"<code>get(url, **kwargs)</code>  <code>async</code>","text":"<p>Sends an HTTP GET request.</p> <p>This is a convenience method that delegates to the main <code>_execute_request</code> orchestrator.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL path for the request, relative to the client's base URL.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the HTTP request.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The parsed response body.</p>"},{"location":"athomic/http/#nala.athomic.http.base.HTTPClientBase.patch","title":"<code>patch(url, **kwargs)</code>  <code>async</code>","text":"<p>Sends an HTTP PATCH request.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL path for the request, relative to the client's base URL.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the HTTP request.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The parsed response body.</p>"},{"location":"athomic/http/#nala.athomic.http.base.HTTPClientBase.post","title":"<code>post(url, **kwargs)</code>  <code>async</code>","text":"<p>Sends an HTTP POST request.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL path for the request, relative to the client's base URL.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the HTTP request.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The parsed response body.</p>"},{"location":"athomic/http/#nala.athomic.http.base.HTTPClientBase.put","title":"<code>put(url, **kwargs)</code>  <code>async</code>","text":"<p>Sends an HTTP PUT request.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL path for the request, relative to the client's base URL.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the HTTP request.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The parsed response body.</p>"},{"location":"athomic/lifecycle/","title":"Lifecycle Management","text":""},{"location":"athomic/lifecycle/#overview","title":"Overview","text":"<p>The <code>LifecycleManager</code> is the central orchestrator responsible for managing the startup and shutdown sequences of all registered <code>BaseService</code> instances in the application.</p>"},{"location":"athomic/lifecycle/#how-it-works","title":"How It Works","text":"<ol> <li>Registration: Services are registered with the <code>LifecycleManager</code>, optionally declaring dependencies on other services.</li> <li>Startup: When <code>manager.start_all()</code> is called, it builds a dependency graph and starts the services in the correct topological order. It uses <code>asyncio.gather</code> to start independent services concurrently, optimizing startup time.</li> <li>Readiness: The manager waits for all services to become \"ready\" by calling their <code>wait_ready()</code> method. The application is only considered fully started after all essential services are ready.</li> <li>Shutdown: When <code>manager.stop_all()</code> is called, it shuts down all services in the reverse order of their startup, ensuring a graceful teardown.</li> </ol> <p>This system guarantees that a service that depends on a database connection will only start after the database connection manager is ready, preventing race conditions and startup errors.</p>"},{"location":"athomic/lifecycle/#api-reference","title":"API Reference","text":""},{"location":"athomic/lifecycle/#nala.athomic.lifecycle.manager.LifecycleManager","title":"<code>nala.athomic.lifecycle.manager.LifecycleManager</code>","text":"<p>Orchestrates the application lifecycle for all Athomic and Domain services.</p> <p>This class is the central coordinator for service startup and shutdown. It ensures that dependencies are respected by starting and stopping services based on a priority system. It first registers all framework-level infrastructure services and then allows the application to register its own domain-specific services before managing their execution order.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <code>AppSettings</code> <p>The application's configuration settings.</p>"},{"location":"athomic/lifecycle/#nala.athomic.lifecycle.manager.LifecycleManager.__init__","title":"<code>__init__(domain_initializers_register=None, settings=None)</code>","text":"<p>Initializes the LifecycleManager.</p> <p>This constructor registers all of Athomic's internal infrastructure services and then executes the provided domain initializer callback to allow the application to register its own services into the lifecycle registry.</p> <p>Parameters:</p> Name Type Description Default <code>domain_initializers_register</code> <code>Optional[Callable[[], None]]</code> <p>An optional function that, when called, registers all domain-specific initializers and services.</p> <code>None</code> <code>settings</code> <code>Optional[AppSettings]</code> <p>The application settings instance. If not provided, global settings will be used.</p> <code>None</code>"},{"location":"athomic/lifecycle/#nala.athomic.lifecycle.manager.LifecycleManager.shutdown","title":"<code>shutdown()</code>  <code>async</code>","text":"<p>Executes the graceful shutdown sequence for all registered services.</p> <p>It retrieves services and stops them in reverse priority order to correctly handle dependencies during shutdown. A timeout is applied to each <code>stop</code> operation, and any errors are logged without halting the shutdown of other services.</p>"},{"location":"athomic/lifecycle/#nala.athomic.lifecycle.manager.LifecycleManager.startup","title":"<code>startup()</code>  <code>async</code>","text":"<p>Executes the startup sequence for all registered services.</p> <p>It retrieves all services from the registry, sorted by their priority, and starts each one sequentially. A timeout is applied to each service's startup to prevent the application from hanging.</p>"},{"location":"athomic/lifecycle/#nala.athomic.lifecycle.registry.LifecycleRegistry","title":"<code>nala.athomic.lifecycle.registry.LifecycleRegistry</code>","text":"<p>               Bases: <code>BaseInstanceRegistry[BaseServiceProtocol]</code></p> <p>A specialized registry for services with a manageable lifecycle.</p> <p>This class extends BaseInstanceRegistry by adding a priority system to control the startup and shutdown order of services. Services with a lower priority number are started first and stopped last, which is essential for managing dependencies (e.g., ensuring a database service is available before a repository service that uses it).</p> <p>Attributes:</p> Name Type Description <code>_registry_with_priority</code> <code>List[Tuple[int, str, BaseServiceProtocol]]</code> <p>An internal list that stores services along with their priority, sorted on insertion.</p>"},{"location":"athomic/lifecycle/#nala.athomic.lifecycle.registry.LifecycleRegistry.__init__","title":"<code>__init__(protocol)</code>","text":"<p>Initializes the LifecycleRegistry.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>BaseServiceProtocol</code> <p>The protocol that all registered items must conform to.</p> required"},{"location":"athomic/lifecycle/#nala.athomic.lifecycle.registry.LifecycleRegistry.clear","title":"<code>clear()</code>  <code>async</code>","text":"<p>Stops all services and completely clears the registry.</p> <p>This method overrides the parent <code>clear</code> to also purge the internal priority list, ensuring a complete reset for testing or re-initialization.</p>"},{"location":"athomic/lifecycle/#nala.athomic.lifecycle.registry.LifecycleRegistry.get_services_by_priority","title":"<code>get_services_by_priority()</code>","text":"<p>Returns all registered services, sorted by startup priority.</p> <p>Returns:</p> Type Description <code>List[BaseServiceProtocol]</code> <p>List[BaseServiceProtocol]: A list of service instances ordered from</p> <code>List[BaseServiceProtocol]</code> <p>lowest to highest priority, ready for startup.</p>"},{"location":"athomic/lifecycle/#nala.athomic.lifecycle.registry.LifecycleRegistry.register","title":"<code>register(name, item_instance, priority=100)</code>","text":"<p>Registers a service instance with a specific priority.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The unique string name for the service instance.</p> required <code>item_instance</code> <code>BaseServiceProtocol</code> <p>The service object to register.</p> required <code>priority</code> <code>Optional[int]</code> <p>The startup priority. Lower numbers are started earlier and stopped later. Defaults to 100.</p> <code>100</code>"},{"location":"athomic/lineage/","title":"Data Lineage","text":""},{"location":"athomic/lineage/#overview","title":"Overview","text":"<p>The Data Lineage module provides tools for generating and storing lineage events that comply with the OpenLineage standard. Data lineage is the process of tracking the origin, movement, and transformation of data, which is crucial for data governance, impact analysis, and debugging complex data pipelines.</p>"},{"location":"athomic/lineage/#how-it-works","title":"How It Works","text":"<p>The <code>LineageProcessor</code> is the central orchestrator. When a message consumer successfully processes a message, it calls <code>lineage_processor.record_consumption()</code>. The processor then: 1.  Extracts metadata from the message headers (e.g., source service, original message ID). 2.  Combines it with consumer information (service name, consumer group). 3.  Builds an internal <code>LineageEvent</code> model. 4.  Translates this model into the standard OpenLineage event format. 5.  Delegates the persistence of the event to a configured <code>LineageStorageProtocol</code> provider.</p>"},{"location":"athomic/lineage/#storage-backends","title":"Storage Backends","text":"<ul> <li><code>LoggingLineageStore</code>: Writes the lineage event as a structured JSON log entry.</li> <li><code>MessagingLineageStore</code>: Publishes the lineage event to a dedicated message broker topic.</li> <li><code>OpenLineageStore</code>: Sends the event directly to an OpenLineage-compatible HTTP API (like Marquez).</li> </ul>"},{"location":"athomic/lineage/#api-reference","title":"API Reference","text":""},{"location":"athomic/lineage/#nala.athomic.lineage.processor.LineageProcessor","title":"<code>nala.athomic.lineage.processor.LineageProcessor</code>","text":"<p>Constructs an internal LineageEvent, translates it to the standard OpenLineage format, and delegates its persistence to a configured store.</p> <p>This class acts as the central orchestrator for the lineage feature. It ensures that the event format is standardized before being sent to any storage backend.</p>"},{"location":"athomic/lineage/#nala.athomic.lineage.processor.LineageProcessor.record_consumption","title":"<code>record_consumption(*, topic, group_id, headers)</code>  <code>async</code>","text":"<p>Constructs, translates, and initiates the saving of a LineageEvent.</p> <p>This method is designed to be safe; it logs all exceptions and does not propagate them, ensuring that lineage tracing failures never impact the primary application logic.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>The topic the message was consumed from.</p> required <code>group_id</code> <code>str</code> <p>The consumer group ID that processed the message.</p> required <code>headers</code> <code>Dict[str, Any]</code> <p>The deserialized headers of the consumed message as a dictionary.</p> required"},{"location":"athomic/lineage/#nala.athomic.lineage.models.LineageEvent","title":"<code>nala.athomic.lineage.models.LineageEvent</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A structured event that captures the end-to-end consumption of a message by a service.</p> <p>This schema serves as the canonical definition for data lineage records within the Athomic framework, essential for tracing causality and understanding data flow in a microservices architecture.</p>"},{"location":"athomic/notification/","title":"Notifications","text":""},{"location":"athomic/notification/#overview","title":"Overview","text":"<p>The Notification module provides a reliable, extensible, and provider-agnostic system for sending notifications, with an initial focus on email. It is designed to decouple your application's business logic from the specific details of how notifications are delivered.</p>"},{"location":"athomic/notification/#key-features","title":"Key Features","text":"<ul> <li>Multi-Provider Support: Comes with built-in providers for SMTP and a console logger, and can be easily extended to support others like SendGrid or Twilio.</li> <li>Resilient by Default: Network-based providers like the <code>SMTPProvider</code> are automatically configured with a retry policy, making them resilient to transient failures.</li> <li>Structured Payloads: Uses Pydantic models like <code>EmailPayload</code> to ensure that all necessary data for a notification is provided in a structured and validated way.</li> <li>Centralized Configuration: All provider settings, including credentials and retry policies, are managed in a single configuration section.</li> </ul>"},{"location":"athomic/notification/#how-it-works","title":"How It Works","text":"<p>The module is built around a few key abstractions:</p> <ol> <li><code>NotificationProtocol</code>: A simple contract that all providers must implement. It defines a primary <code>send(payload)</code> method.</li> <li><code>NotificationPayload</code>: A set of Pydantic models that define the structure for different types of notifications (e.g., <code>EmailPayload</code>).</li> <li><code>NotificationFactory</code>: A singleton factory that is the main entry point for the application. It reads the configuration and instantiates the correct provider (e.g., <code>SMTPProvider</code>). If notifications are disabled in the settings, it safely returns a <code>NullNotificationProvider</code> that performs no action, preventing application errors.</li> <li>Provider Creators: For providers with complex dependencies, like the <code>SMTPProvider</code> which requires a <code>ResilienceOrchestrator</code>, a dedicated creator class handles the dependency injection, ensuring the provider is properly assembled.</li> </ol>"},{"location":"athomic/notification/#available-providers","title":"Available Providers","text":"<ul> <li><code>SMTPProvider</code>: Sends emails using a standard SMTP server. This provider is automatically wrapped with a retry mechanism.</li> <li><code>ConsoleNotificationProvider</code>: Does not send any real notifications. Instead, it prints the content of the notification payload to the console, making it perfect for local development and testing.</li> <li><code>NullNotificationProvider</code>: A \"no-op\" provider that does absolutely nothing. It is used as a safe fallback when the notification module is disabled in the configuration.</li> </ul>"},{"location":"athomic/notification/#usage-example","title":"Usage Example","text":"<p>Sending an email is a simple, two-step process: build the payload and send it using the factory.</p> <pre><code>from nala.athomic.notification import NotificationFactory\nfrom nala.athomic.notification.payloads import EmailPayload\n\nasync def send_welcome_email(user_email: str, user_name: str):\n    # 1. Create the structured payload\n    email = EmailPayload(\n        to=[user_email],\n        subject=f\"Welcome, {user_name}!\",\n        body=\"Thank you for joining our platform.\",\n        html_body=\"&lt;p&gt;Thank you for joining our platform.&lt;/p&gt;\"\n    )\n\n    try:\n        # 2. Get the configured provider and send the payload\n        notifier = NotificationFactory.create()\n        success = await notifier.send(email)\n\n        if success:\n            print(\"Welcome email sent successfully!\")\n        else:\n            print(\"Failed to send welcome email.\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n</code></pre>"},{"location":"athomic/notification/#configuration","title":"Configuration","text":"<p>Notifications are configured under the <code>[notifications]</code> section in your <code>settings.toml</code>.</p>"},{"location":"athomic/notification/#console-provider-example-for-development","title":"Console Provider Example (for Development)","text":"<pre><code>[default.notifications]\nenabled = true\ndefault_from_email = \"dev@nala.com\"\n\n  [default.notifications.provider]\n  backend = \"console\"\n</code></pre>"},{"location":"athomic/notification/#smtp-provider-example-for-production","title":"SMTP Provider Example (for Production)","text":"<pre><code>[default.notifications]\nenabled = true\ndefault_from_email = \"noreply@nala.com\"\n\n  # Configure a retry policy for sending emails\n  [default.notifications.retry]\n  attempts = 3\n  wait_min_seconds = 1.0\n  backoff = 2.0\n\n  # Configure the SMTP provider\n  [default.notifications.provider]\n  backend = \"smtp\"\n  host = \"smtp.mailprovider.com\"\n  port = 587\n  username = \"my-user\"\n  # The password should always be a secret reference\n  password = { path = \"smtp/prod\", key = \"password\" } # pragma: allowlist secret\n  use_tls = true\n</code></pre>"},{"location":"athomic/notification/#api-reference","title":"API Reference","text":""},{"location":"athomic/notification/#nala.athomic.notification.protocol.NotificationProtocol","title":"<code>nala.athomic.notification.protocol.NotificationProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol defining the standard interface for notification providers.</p>"},{"location":"athomic/notification/#nala.athomic.notification.protocol.NotificationProtocol.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Closes connections or releases resources, if applicable.</p>"},{"location":"athomic/notification/#nala.athomic.notification.protocol.NotificationProtocol.is_available","title":"<code>is_available()</code>  <code>async</code>","text":"<p>Checks if the provider is configured and ready.</p>"},{"location":"athomic/notification/#nala.athomic.notification.protocol.NotificationProtocol.send","title":"<code>send(payload)</code>  <code>async</code>","text":"<p>Sends a notification based on the provided payload.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>NotificationPayload</code> <p>A structured data object (e.g., EmailPayload, SMSPayload)      containing all necessary information for the notification.</p> required <p>Returns:     bool: True if sending was successful, False otherwise.</p>"},{"location":"athomic/notification/#nala.athomic.notification.factory.NotificationFactory","title":"<code>nala.athomic.notification.factory.NotificationFactory</code>","text":"<p>Factory responsible for creating and managing the singleton instance of the configured NotificationProtocol provider.</p> <p>It uses a Registry for provider discovery and implements a fallback to NullProvider if the notifications module is disabled in settings.</p>"},{"location":"athomic/notification/#nala.athomic.notification.factory.NotificationFactory.clear","title":"<code>clear()</code>  <code>classmethod</code>","text":"<p>Clears the singleton instance.</p> <p>Primarily used for test isolation to ensure fresh setup between tests.</p>"},{"location":"athomic/notification/#nala.athomic.notification.factory.NotificationFactory.create","title":"<code>create(settings=None)</code>  <code>classmethod</code>","text":"<p>Retrieves or creates the singleton instance of the NotificationProtocol.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[NotificationSettings]</code> <p>Optional explicit settings to override application configuration.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no provider creator is registered for the configured backend.</p> <code>RuntimeError</code> <p>If initialization of the concrete provider fails.</p> <p>Returns:</p> Name Type Description <code>NotificationProtocol</code> <code>NotificationProtocol</code> <p>The shared singleton instance.</p>"},{"location":"athomic/notification/#nala.athomic.notification.payloads.EmailPayload","title":"<code>nala.athomic.notification.payloads.EmailPayload</code>","text":"<p>               Bases: <code>BasePayload</code></p> <p>Specific payload structure required for sending email notifications.</p>"},{"location":"athomic/notification/#nala.athomic.notification.providers.smtp_provider.SMTPProvider","title":"<code>nala.athomic.notification.providers.smtp_provider.SMTPProvider</code>","text":"<p>               Bases: <code>BaseNotification</code>, <code>CredentialResolve</code></p> <p>SMTPProvider implementation specialized for EmailPayload, using a resilience orchestrator.</p>"},{"location":"athomic/notification/#nala.athomic.notification.providers.smtp_provider.SMTPProvider.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Closes any open resources. (No-op for smtplib as connection is per-send).</p>"},{"location":"athomic/notification/#nala.athomic.notification.providers.smtp_provider.SMTPProvider.is_available","title":"<code>is_available()</code>  <code>async</code>","text":"<p>Verifies SMTP availability by sending a NOOP command.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the SMTP connection was successful, False otherwise.</p>"},{"location":"athomic/payload/","title":"Payload Processing Pipeline","text":""},{"location":"athomic/payload/#overview","title":"Overview","text":"<p>The Payload Processing module provides a flexible and powerful implementation of the Pipes and Filters architectural pattern. Its core component, the <code>PayloadProcessor</code>, orchestrates a configurable pipeline of transformation steps that are applied to data payloads, primarily within the messaging system.</p> <p>This architecture allows complex processing sequences\u2014such as serialization, encryption, compression, and large message handling (Claim Check)\u2014to be composed declaratively in your configuration file. It decouples the messaging producers and consumers from the specifics of the payload format, making the system highly extensible.</p>"},{"location":"athomic/payload/#how-the-pipeline-works","title":"How The Pipeline Works","text":"<p>The <code>PayloadProcessor</code> manages an ordered list of \"steps,\" where each step is a component that implements the <code>ProcessingStepProtocol</code> (defining <code>encode</code> and <code>decode</code> methods).</p>"},{"location":"athomic/payload/#encoding-flow-on-publish","title":"Encoding Flow (On Publish)","text":"<p>When a message is published, the pipeline is executed in the forward order as defined in your configuration. The output of one step becomes the input for the next.</p> <ul> <li><code>Domain Object</code> -&gt; [Serializer] -&gt; <code>Bytes</code> -&gt; [Crypto] -&gt; <code>Encrypted Bytes</code> -&gt; [Compression] -&gt; <code>Compressed Bytes</code> -&gt; <code>Message Broker</code></li> </ul>"},{"location":"athomic/payload/#decoding-flow-on-consume","title":"Decoding Flow (On Consume)","text":"<p>When a message is consumed, the pipeline is executed in reverse order.</p> <ul> <li><code>Message Broker</code> -&gt; <code>Compressed Bytes</code> -&gt; [Decompression] -&gt; <code>Encrypted Bytes</code> -&gt; [Decryption] -&gt; <code>Bytes</code> -&gt; [Deserializer] -&gt; <code>Domain Object</code></li> </ul> <p>This symmetric design ensures that transformations are applied and reverted correctly and transparently.</p>"},{"location":"athomic/payload/#available-pipeline-steps","title":"Available Pipeline Steps","text":"<p>Athomic provides several built-in steps that you can include in your payload pipeline:</p> <ul> <li><code>serializer</code>: (Required first step for encoding). Converts Python objects (like Pydantic models) into a byte representation (e.g., JSON, Protobuf) and vice-versa.</li> <li><code>crypto</code>: Encrypts the byte payload using the configured cryptographic provider (e.g., Fernet) to ensure end-to-end security.</li> <li><code>compression</code>: Compresses the byte payload to reduce its size, saving network bandwidth and storage costs.</li> <li><code>claim_check</code>: Implements the Claim Check pattern. If a payload exceeds a size threshold, it's uploaded to a file storage provider (like GCS), and a small reference is sent through the broker instead.</li> </ul>"},{"location":"athomic/payload/#configuration","title":"Configuration","text":"<p>You configure the pipeline in your <code>settings.toml</code> file under the <code>[integration.messaging.payload]</code> section. The <code>pipeline</code> is a list of tables, and their order matters.</p>"},{"location":"athomic/payload/#example-full-pipeline","title":"Example: Full Pipeline","text":"<p>This example configures a pipeline that first serializes the object to JSON, then encrypts it, and finally compresses it.</p> <pre><code>[default.integration.messaging.payload]\n\n# The pipeline is a list of steps. Order is crucial.\n# Encoding runs top-to-bottom. Decoding runs bottom-to-top.\n[[default.integration.messaging.payload.pipeline]]\nstep = \"serializer\"\n  # Optional settings for this specific step can be provided.\n  [default.integration.messaging.payload.pipeline.settings]\n  backend = \"orjson\"\n\n[[default.integration.messaging.payload.pipeline]]\nstep = \"crypto\"\n  # No settings needed here; it will use the global [security.crypto] config.\n\n[[default.integration.messaging.payload.pipeline]]\nstep = \"compression\"\n</code></pre>"},{"location":"athomic/payload/#extending-the-pipeline-custom-steps","title":"Extending the Pipeline (Custom Steps)","text":"<p>The pipeline is fully extensible. To add your own custom processing step (e.g., for data validation or transformation):</p> <ol> <li>Create a class that implements the <code>ProcessingStepProtocol</code>.</li> <li>Create a factory class that implements the <code>StepCreatorProtocol</code>.</li> <li>Register an instance of your creator in the <code>payload_step_registry</code> with a unique name.</li> </ol> <p>Once registered, your custom step can be added to the <code>pipeline</code> in your configuration just like the built-in ones.</p>"},{"location":"athomic/payload/#api-reference","title":"API Reference","text":""},{"location":"athomic/payload/#nala.athomic.payload.processor.PayloadProcessor","title":"<code>nala.athomic.payload.processor.PayloadProcessor</code>","text":"<p>An orchestrator that executes a dynamic pipeline of processing steps (Pipes and Filters pattern).</p> <p>This class is agnostic to the nature of the steps (serialization, encryption, compression), focusing only on the correct order of execution for both outbound (encode) and inbound (decode) flows.</p>"},{"location":"athomic/payload/#nala.athomic.payload.processor.PayloadProcessor.__init__","title":"<code>__init__(pipeline_steps)</code>","text":"<p>Initializes the processor with an ordered list of pipeline steps.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_steps</code> <code>List[ProcessingStepProtocol]</code> <p>An ordered list of steps to be executed during encoding.</p> required"},{"location":"athomic/payload/#nala.athomic.payload.processor.PayloadProcessor.decode","title":"<code>decode(raw_data, **kwargs)</code>  <code>async</code>","text":"<p>Executes the decoding pipeline in reverse order (backward).</p> <p>The flow is typically: Raw Bytes -&gt; [Decompress -&gt; Decrypt -&gt; Deserialize] -&gt; Domain Object.</p> <p>Parameters:</p> Name Type Description Default <code>raw_data</code> <code>bytes</code> <p>The initial payload (transport bytes).</p> required <code>**kwargs</code> <code>Any</code> <p>Contextual arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The final domain object or data structure.</p>"},{"location":"athomic/payload/#nala.athomic.payload.processor.PayloadProcessor.encode","title":"<code>encode(data_object, **kwargs)</code>  <code>async</code>","text":"<p>Executes the encoding pipeline in the defined order (forward).</p> <p>The flow is typically: Domain Object -&gt; [Serialize -&gt; Encrypt -&gt; Compress] -&gt; Raw Bytes.</p> <p>Parameters:</p> Name Type Description Default <code>data_object</code> <code>Any</code> <p>The initial payload (usually a domain object or model).</p> required <code>**kwargs</code> <code>Any</code> <p>Contextual arguments to be passed to each step.</p> <code>{}</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the final output of the pipeline is not bytes.</p> <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The final, transport-ready payload.</p>"},{"location":"athomic/payload/#nala.athomic.payload.protocol.ProcessingStepProtocol","title":"<code>nala.athomic.payload.protocol.ProcessingStepProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the contract for a single, bidirectional (encode/decode) step in a payload processing pipeline.</p> <p>Examples of steps include serialization (JSON/Protobuf), compression (Gzip), or encryption (AES).</p>"},{"location":"athomic/payload/#nala.athomic.payload.protocol.ProcessingStepProtocol.decode","title":"<code>decode(data, **kwargs)</code>  <code>async</code>","text":"<p>Processes data in the inbound flow (e.g., preparing data for consumption).</p> <p>The flow direction is typically from raw bytes/transport format to the final domain object expected by the consumer.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The received payload (can be bytes or a transport format).</p> required <code>**kwargs</code> <code>Any</code> <p>Contextual arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The decoded payload (usually the final domain object).</p>"},{"location":"athomic/payload/#nala.athomic.payload.protocol.ProcessingStepProtocol.encode","title":"<code>encode(data, **kwargs)</code>  <code>async</code>","text":"<p>Processes data in the outbound flow (e.g., preparing data for transmission).</p> <p>The flow direction is typically from domain object to raw bytes/transport format.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The payload to be processed (can be an object or bytes).</p> required <code>**kwargs</code> <code>Any</code> <p>Contextual arguments (e.g., schema version, compression level).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The processed payload (usually bytes for intermediate steps).</p>"},{"location":"athomic/payload/#nala.athomic.payload.factory.PayloadProcessorFactory","title":"<code>nala.athomic.payload.factory.PayloadProcessorFactory</code>","text":"<p>A Singleton Factory responsible for assembling the final PayloadProcessor.</p> <p>It dynamically retrieves step creators from a central registry based on a structured pipeline configuration (Pipes and Filters pattern), ensuring the pipeline assembly logic runs only once.</p>"},{"location":"athomic/payload/#nala.athomic.payload.factory.PayloadProcessorFactory.clear","title":"<code>clear()</code>  <code>classmethod</code>","text":"<p>Clears the singleton instance. Used primarily for test isolation.</p>"},{"location":"athomic/payload/#nala.athomic.payload.factory.PayloadProcessorFactory.create","title":"<code>create(settings)</code>  <code>classmethod</code>","text":"<p>Retrieves or creates the singleton instance of the fully assembled PayloadProcessor pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>PayloadSettings</code> <p>The configuration specifying the order and type of steps       in the processing pipeline.</p> required <p>Returns:</p> Name Type Description <code>PayloadProcessor</code> <code>PayloadProcessor</code> <p>The shared, single instance of the processor.</p>"},{"location":"athomic/payload/#nala.athomic.payload.creators_protocol.StepCreatorProtocol","title":"<code>nala.athomic.payload.creators_protocol.StepCreatorProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the contract for an object (a specialized Factory) that knows how to create a specific ProcessingStepProtocol implementation.</p> <p>This protocol is used by the PayloadProcessorFactory to dynamically construct the processing pipeline based on configuration, enabling Dependency Injection for complex steps.</p>"},{"location":"athomic/payload/#nala.athomic.payload.creators_protocol.StepCreatorProtocol.create","title":"<code>create(settings)</code>","text":"<p>Creates and returns a concrete instance of a ProcessingStepProtocol.</p> <p>The creation logic within the implementer should handle the necessary setup and dependency injection (e.g., retrieving a key for an EncryptionStep).</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[Dict[str, Any]]</code> <p>The specific configuration dictionary for this step       from the main PayloadSettings.</p> required <p>Returns:</p> Name Type Description <code>ProcessingStepProtocol</code> <code>ProcessingStepProtocol</code> <p>The fully initialized processing step.</p>"},{"location":"athomic/plugins/","title":"Plugin System","text":""},{"location":"athomic/plugins/#overview","title":"Overview","text":"<p>The Athomic Plugin System provides a powerful and clean way to extend the core functionality of the application without modifying its source code. Plugins can introduce new API routes, register custom services into the lifecycle, or execute logic at specific points in the application's startup sequence.</p> <p>The system is built on Python's native <code>importlib.metadata</code> and entry points, making it a standard and robust way to create a pluggable architecture.</p>"},{"location":"athomic/plugins/#how-it-works","title":"How It Works","text":"<ol> <li>Entry Point: A plugin is a separate, installable Python package that defines an entry point in its <code>pyproject.toml</code> under the <code>[project.entry-points.\"nala.plugins\"]</code> group.</li> <li>Discovery: At startup, the <code>PluginManager</code> discovers all installed packages that declare this entry point.</li> <li>Hooks: Plugins define functions decorated with <code>@hookimpl</code>. The <code>PluginManager</code> registers these functions as implementations for specific, predefined hooks.</li> <li>Execution: When the core application reaches a certain point, it calls <code>plugin_manager.call_hook(\"hook_name\")</code>, which executes all registered implementations for that hook.</li> </ol>"},{"location":"athomic/plugins/#available-hooks","title":"Available Hooks","text":"<ul> <li><code>on_athomic_startup(athomic: Athomic)</code>: Called early in the startup sequence. Ideal for registering custom services with the <code>LifecycleRegistry</code>.</li> <li><code>get_api_routers() -&gt; List[APIRouter]</code>: Called by the web server layer. Each plugin can return a list of FastAPI <code>APIRouter</code> objects, which will be automatically mounted into the main application.</li> </ul>"},{"location":"athomic/plugins/#creating-a-plugin","title":"Creating a Plugin","text":"<p>See the <code>plugins/nala-plugin-greetings</code> directory for a complete example of a simple plugin.</p>"},{"location":"athomic/plugins/#api-reference","title":"API Reference","text":""},{"location":"athomic/plugins/#nala.athomic.plugins.hooks.hookimpl","title":"<code>nala.athomic.plugins.hooks.hookimpl(func)</code>","text":"<p>Decorator to explicitly mark a function as a hook implementation.</p> <p>This metadata allows the PluginManager to discover and register the implementation with the corresponding hook specification.</p>"},{"location":"athomic/plugins/#nala.athomic.plugins.manager.PluginManager","title":"<code>nala.athomic.plugins.manager.PluginManager</code>","text":"<p>               Bases: <code>PluginManagerProtocol</code></p> <p>Mediates the interaction between the Athomic core and external plugins.</p> <p>It implements the Mediator, Observer, and Registry patterns to handle plugin discovery, registration, and resilient hook execution.</p>"},{"location":"athomic/plugins/#nala.athomic.plugins.manager.PluginManager.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the PluginManager and its internal hook registry.</p>"},{"location":"athomic/plugins/#nala.athomic.plugins.manager.PluginManager.call_hook","title":"<code>call_hook(hook_name, **kwargs)</code>  <code>async</code>","text":"<p>Executes all registered implementations for a given hook with resilience, observability (tracing/metrics), and failure notification.</p>"},{"location":"athomic/plugins/#nala.athomic.plugins.manager.PluginManager.discover_and_load","title":"<code>discover_and_load()</code>  <code>async</code>","text":"<p>Discovers plugins using Python's entry points (group=\"nala.plugins\"), loads their modules, and registers all functions marked with @hookimpl.</p>"},{"location":"athomic/plugins/#nala.athomic.plugins.manager.PluginManager.get_api_routers","title":"<code>get_api_routers()</code>  <code>async</code>","text":"<p>Public contract method for the API layer. It calls the 'get_api_routers' hook, aggregates the results (a list of lists), and flattens them into a single list of router objects.</p>"},{"location":"athomic/serializer/","title":"Serializer","text":""},{"location":"athomic/serializer/#overview","title":"Overview","text":"<p>The Serializer module is a core component of the Athomic Layer responsible for the crucial task of converting Python objects into a byte representation (serialization) and back (deserialization). This functionality is essential for any operation that involves data persistence or network transmission, such as:</p> <ul> <li>Storing data in a Key-Value store (e.g., Redis).</li> <li>Publishing messages to a message broker (e.g., Kafka).</li> <li>Caching function results.</li> </ul> <p>The module is designed with extensibility in mind, built upon a protocol-centric architecture. This allows developers to easily add new serialization formats without modifying the core components that rely on it.</p>"},{"location":"athomic/serializer/#core-concepts","title":"Core Concepts","text":"<p>The Serializer module is built on three key components that work together:</p>"},{"location":"athomic/serializer/#1-serializerprotocol","title":"1. <code>SerializerProtocol</code>","text":"<p>This is the abstract contract that every serializer provider must implement. It defines the standard interface for all serialization and deserialization operations, ensuring that any component can use any serializer interchangeably. The key methods include:</p> <ul> <li><code>serialize_value(value)</code>: Converts a Python object into bytes.</li> <li><code>deserialize_value(value)</code>: Converts bytes back into a Python object.</li> <li><code>serialize_key(key)</code>: A dedicated method for serializing message keys.</li> <li><code>serialize_headers(headers)</code>: A dedicated method for serializing message headers.</li> </ul>"},{"location":"athomic/serializer/#2-serializerregistry","title":"2. <code>SerializerRegistry</code>","text":"<p>This is a central registry that maps string identifiers (backend names) to the concrete serializer classes that implement the <code>SerializerProtocol</code>. For example, it maps the name <code>\"orjson\"</code> to the <code>OrjsonSerializer</code> class. This allows the factory to be decoupled from the implementations.</p>"},{"location":"athomic/serializer/#3-serializerfactory","title":"3. <code>SerializerFactory</code>","text":"<p>This is the primary entry point for the rest of the application to obtain a serializer instance. It reads the application's configuration, finds the configured backend name in the <code>SerializerRegistry</code>, and instantiates the correct class. It manages a singleton instance to ensure the same serializer is reused throughout the application.</p>"},{"location":"athomic/serializer/#available-providers","title":"Available Providers","text":"<p>The Athomic Layer comes with several built-in serializer providers:</p> <ul> <li><code>JsonPydanticSerializer</code>: A robust JSON serializer that enhances deserialization by validating the incoming data against a target Pydantic model. This is great for ensuring data integrity at the boundaries of your application.</li> <li><code>OrjsonSerializer</code>: A high-performance JSON serializer that uses the <code>orjson</code> library for significant speed improvements over the standard <code>json</code> library.</li> <li><code>ProtobufSerializer</code>: A highly efficient, schema-based serializer for Google Protocol Buffers. It can optionally integrate with a Schema Registry to handle schema validation and wire format encapsulation, making it ideal for high-throughput messaging systems.</li> </ul>"},{"location":"athomic/serializer/#configuration","title":"Configuration","text":"<p>The serializer is configured under the <code>serializer</code> section in your settings file (e.g., <code>settings.toml</code>). You can select the backend and provide specific configurations for features like Schema Registry.</p> <pre><code>[default.serializer]\n# The name of the specific serializer implementation to use (e.g., 'json_pydantic', 'protobuf', 'orjson').\nbackend = \"orjson\"\n\n# The fundamental data format that this serializer handles.\nformat = \"json\"\n\n# --- Schema Registry (for Protobuf/Avro) ---\nschema_validation_enabled = false\nschema_registry_url = \"http://localhost:8081\"\nhandler = \"confluent_protobuf\"\n</code></pre>"},{"location":"athomic/serializer/#api-reference","title":"API Reference","text":"<p>The following sections are auto-generated from the source code docstrings.</p>"},{"location":"athomic/serializer/#serializerprotocol","title":"<code>SerializerProtocol</code>","text":""},{"location":"athomic/serializer/#nala.athomic.serializer.protocol.SerializerProtocol","title":"<code>nala.athomic.serializer.protocol.SerializerProtocol</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Defines the bidirectional contract for message serializers.</p> <p>This protocol handles the conversion of payloads, keys, and headers between Python objects/dictionaries and the raw byte format suitable for transport-specific backends (e.g., Kafka, Pub/Sub) or storage layers (e.g., Cache, Outbox). It is a critical component of the message processing pipeline.</p>"},{"location":"athomic/serializer/#nala.athomic.serializer.protocol.SerializerProtocol.__init__","title":"<code>__init__(settings)</code>","text":"<p>Initializes the serializer with its required configuration settings.</p>"},{"location":"athomic/serializer/#nala.athomic.serializer.protocol.SerializerProtocol.deserialize_headers","title":"<code>deserialize_headers(headers, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Deserializes the message headers from the backend's raw format into a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>headers</code> <code>Optional[Any]</code> <p>The serialized headers from the broker,                      which may be a list of tuples or a raw object.</p> required <code>**kwargs</code> <code>Any</code> <p>Contextual arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[Dict[str, str]]</code> <p>Optional[Dict[str, str]]: The deserialized headers as a dictionary of key-value strings, or None.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If deserialization fails.</p>"},{"location":"athomic/serializer/#nala.athomic.serializer.protocol.SerializerProtocol.deserialize_key","title":"<code>deserialize_key(key, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Deserializes the raw byte message key back into a string or object.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>bytes</code> <p>The serialized message key as bytes.</p> required <code>**kwargs</code> <code>Any</code> <p>Contextual arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: The deserialized key (typically a string), or None.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If deserialization fails.</p>"},{"location":"athomic/serializer/#nala.athomic.serializer.protocol.SerializerProtocol.deserialize_value","title":"<code>deserialize_value(value, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Deserializes the raw byte message payload back into a Python object.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>bytes</code> <p>The serialized message payload as bytes.</p> required <code>**kwargs</code> <code>Any</code> <p>Contextual arguments, potentially including a <code>target_type</code>             (e.g., a Pydantic model) for validation.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The deserialized message value (e.g., dictionary, Pydantic model instance).</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If deserialization fails (e.g., invalid JSON/Protobuf format, schema mismatch).</p>"},{"location":"athomic/serializer/#nala.athomic.serializer.protocol.SerializerProtocol.serialize_headers","title":"<code>serialize_headers(headers, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Serializes the message headers into a list of backend-compatible byte tuples.</p> <p>Parameters:</p> Name Type Description Default <code>headers</code> <code>Optional[Dict[str, str]]</code> <p>Optional dictionary of string-based headers.</p> required <code>**kwargs</code> <code>Any</code> <p>Contextual arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[list[tuple[str, bytes]]]</code> <p>Optional[list[tuple[str, bytes]]]: A list of (key, value_bytes) tuples, or None.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If serialization fails (e.g., non-string key or value).</p>"},{"location":"athomic/serializer/#nala.athomic.serializer.protocol.SerializerProtocol.serialize_key","title":"<code>serialize_key(key, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Serializes the message key for partitioning or routing.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Optional[Any]</code> <p>The optional message key.</p> required <code>**kwargs</code> <code>Any</code> <p>Contextual arguments (e.g., topic name).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[bytes]</code> <p>Optional[bytes]: The serialized key as raw bytes, or None.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If serialization fails.</p>"},{"location":"athomic/serializer/#nala.athomic.serializer.protocol.SerializerProtocol.serialize_value","title":"<code>serialize_value(value, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Serializes the message payload for outbound transmission or storage.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The original message payload (e.g., Pydantic model, dictionary).</p> required <code>**kwargs</code> <code>Any</code> <p>Contextual arguments (e.g., topic name, schema version).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[bytes]</code> <p>Optional[bytes]: The serialized payload as raw bytes, or None if the input was None.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If serialization fails (e.g., unhandled data type, schema incompatibility).</p>"},{"location":"athomic/serializer/#serializerfactory","title":"<code>SerializerFactory</code>","text":""},{"location":"athomic/serializer/#nala.athomic.serializer.factory.SerializerFactory","title":"<code>nala.athomic.serializer.factory.SerializerFactory</code>","text":"<p>Factory for instantiating message serializers based on messaging backend. Falls back to BaseSerializer if no specific implementation is registered. Caches instances by backend name for singleton-like behavior per backend.</p>"},{"location":"athomic/serializer/#nala.athomic.serializer.factory.SerializerFactory.clear","title":"<code>clear()</code>  <code>classmethod</code>","text":"<p>Clears the singleton cache of serializer instances.</p>"},{"location":"athomic/serializer/#nala.athomic.serializer.factory.SerializerFactory.create","title":"<code>create(settings=None)</code>  <code>classmethod</code>","text":"<p>Creates and returns a singleton instance of the configured SerializerProtocol by delegating to a registered creator.</p>"},{"location":"athomic/serializer/#jsonpydanticserializer","title":"<code>JsonPydanticSerializer</code>","text":""},{"location":"athomic/serializer/#nala.athomic.serializer.providers.json_pydantic_serializer.JsonPydanticSerializer","title":"<code>nala.athomic.serializer.providers.json_pydantic_serializer.JsonPydanticSerializer</code>","text":"<p>               Bases: <code>BaseSerializer</code></p> <p>A JSON serializer implementation optimized for data validation using Pydantic models.</p> <p>This serializer inherits the core functionality for header and key handling, as well as default serialization (<code>serialize_value</code>), from <code>BaseSerializer</code>. Its specialized role is to enhance the deserialization process by enforcing the schema defined by a target Pydantic model.</p>"},{"location":"athomic/serializer/#nala.athomic.serializer.providers.json_pydantic_serializer.JsonPydanticSerializer.deserialize_value","title":"<code>deserialize_value(value, target_type=dict, **kwargs)</code>  <code>async</code>","text":"<p>Deserializes a JSON payload from bytes and validates it against a target Pydantic model.</p> <p>If a <code>target_type</code> (that is a subclass of <code>BaseModel</code>) is provided, the data is validated against that schema before being returned.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>bytes | None</code> <p>The raw JSON payload as bytes.</p> required <code>target_type</code> <code>Any</code> <p>The expected class for the deserialized result, typically                a <code>pydantic.BaseModel</code> subclass. Defaults to <code>dict</code>.</p> <code>dict</code> <p>Returns:</p> Type Description <code>Any | None</code> <p>Any | None: The validated <code>target_type</code> instance or the raw data (<code>dict</code>/<code>list</code>).</p> <p>Raises:</p> Type Description <code>DeserializationError</code> <p>If JSON decoding fails or if Pydantic validation fails.</p>"},{"location":"athomic/serializer/#orjsonserializer","title":"<code>OrjsonSerializer</code>","text":""},{"location":"athomic/serializer/#nala.athomic.serializer.providers.orjson_serializer.OrjsonSerializer","title":"<code>nala.athomic.serializer.providers.orjson_serializer.OrjsonSerializer</code>","text":"<p>               Bases: <code>BaseSerializer</code></p> <p>A high-performance serializer that uses the <code>orjson</code> library to encode and decode JSON.</p> <p>This class provides a fast, dedicated JSON implementation for message payloads, inheriting all common features (logging, tracing, header/key handling) from <code>BaseSerializer</code>.</p>"},{"location":"athomic/serializer/#nala.athomic.serializer.providers.orjson_serializer.OrjsonSerializer.__init__","title":"<code>__init__(settings)</code>","text":"<p>Initializes the OrjsonSerializer.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>SerializerSettings</code> <p>The configuration for this serializer instance.</p> required"},{"location":"athomic/serializer/#nala.athomic.serializer.providers.orjson_serializer.OrjsonSerializer.deserialize_value","title":"<code>deserialize_value(data, **kwargs)</code>  <code>async</code>","text":"<p>Deserializes a JSON payload from bytes back into a Python object (dict/list).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The serialized message payload, expected as bytes.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The deserialized Python object.</p> <p>Raises:</p> Type Description <code>DeserializationError</code> <p>If orjson fails to decode the payload (e.g., invalid JSON structure).</p>"},{"location":"athomic/serializer/#nala.athomic.serializer.providers.orjson_serializer.OrjsonSerializer.serialize_value","title":"<code>serialize_value(value, **kwargs)</code>  <code>async</code>","text":"<p>Serializes a value into a JSON byte string using orjson.</p> <p>It uses the custom default encoder to handle complex types like Pydantic models.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The original message payload.</p> required <p>Returns:</p> Type Description <code>Optional[bytes]</code> <p>Optional[bytes]: The serialized payload as raw bytes.</p> <p>Raises:</p> Type Description <code>SerializationError</code> <p>If orjson fails to serialize the object (e.g., non-serializable type).</p>"},{"location":"athomic/serializer/#protobufserializer","title":"<code>ProtobufSerializer</code>","text":""},{"location":"athomic/serializer/#nala.athomic.serializer.providers.protobuf_serializer.ProtobufSerializer","title":"<code>nala.athomic.serializer.providers.protobuf_serializer.ProtobufSerializer</code>","text":"<p>               Bases: <code>BaseSerializer</code></p> <p>Serializes and deserializes messages using Google Protocol Buffers.</p> <p>This serializer is optimized for performance and type safety. It conditionally delegates schema validation and wire format encapsulation (Confluent Wire Format) to a specialized Schema Handler (e.g., Avro, Protobuf, etc.) when configured. This design keeps the class decoupled (SRP) and focused on Protobuf operations.</p> <p>Attributes:</p> Name Type Description <code>handler</code> <code>Optional[SchemaHandlerProtocol]</code> <p>The component responsible for                                        Schema Registry interaction and wire format handling.</p>"},{"location":"athomic/serializer/#nala.athomic.serializer.providers.protobuf_serializer.ProtobufSerializer.__init__","title":"<code>__init__(settings)</code>","text":"<p>Initializes the serializer.</p> <p>It attempts to inject a schema handler if schema validation is enabled in the configuration.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>SerializerSettings</code> <p>The specific configuration for this serializer instance.</p> required"},{"location":"athomic/serializer/#nala.athomic.serializer.providers.protobuf_serializer.ProtobufSerializer.deserialize_value","title":"<code>deserialize_value(data, **kwargs)</code>  <code>async</code>","text":"<p>Deserializes raw bytes into a Protobuf message instance.</p> <p>If the payload is in the Confluent Wire Format (starts with MAGIC_BYTE), it delegates the process to the schema handler for schema ID lookup. Otherwise, it attempts standard Protobuf deserialization.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The raw message data, expected as bytes.</p> required <code>**kwargs</code> <code>Any</code> <p>Contextual arguments (must include 'target_type' and 'topic' if schema handler is active).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The deserialized Protobuf message instance.</p> <p>Raises:</p> Type Description <code>DeserializationError</code> <p>If 'target_type' is missing, data is malformed,                   or standard deserialization fails.</p>"},{"location":"athomic/serializer/#nala.athomic.serializer.providers.protobuf_serializer.ProtobufSerializer.serialize_value","title":"<code>serialize_value(value, **kwargs)</code>  <code>async</code>","text":"<p>Serializes a Protobuf message into raw bytes.</p> <p>Delegates to the schema handler if configured to include the wire format (magic byte + schema ID). Otherwise, performs standard Protobuf serialization.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The Protobuf message instance to serialize.</p> required <code>**kwargs</code> <code>Any</code> <p>Contextual arguments (must include 'topic' if schema handler is active).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[bytes]</code> <p>Optional[bytes]: The serialized payload.</p> <p>Raises:</p> Type Description <code>SerializationError</code> <p>If the value is not a Protobuf message or if the                 required 'topic' is missing when a schema handler is used.</p>"},{"location":"athomic/services/","title":"Base Service &amp; Lifecycle","text":""},{"location":"athomic/services/#overview","title":"Overview","text":"<p>The <code>BaseService</code> is a fundamental building block within the Athomic Layer. It provides a standardized protocol (<code>BaseServiceProtocol</code>) for all long-running or stateful components that have a defined lifecycle (e.g., a database connection manager, a message consumer, a background poller).</p>"},{"location":"athomic/services/#key-lifecycle-methods","title":"Key Lifecycle Methods","text":"<ul> <li><code>start()</code>: Initializes the service and its dependencies.</li> <li><code>stop()</code>: Gracefully shuts down the service and releases resources.</li> <li><code>wait_ready()</code>: An awaitable method that resolves only when the service is fully initialized and ready to be used.</li> <li><code>is_ready()</code>: A boolean property to check the current readiness state.</li> </ul> <p>This consistent interface allows the Lifecycle Management system to orchestrate the application's startup and shutdown sequences in a reliable and predictable order.</p>"},{"location":"athomic/storage/","title":"File Storage Abstraction","text":""},{"location":"athomic/storage/#overview","title":"Overview","text":"<p>The Storage module provides a unified, protocol-based interface for interacting with various file and object storage backends, such as the local filesystem or cloud providers like Google Cloud Storage (GCS).</p> <p>This abstraction is essential for decoupling application logic from the underlying storage technology. Its primary use cases include: -   Handling large message payloads via the Claim Check pattern. -   General application file storage (e.g., user uploads, generated reports).</p>"},{"location":"athomic/storage/#core-concepts","title":"Core Concepts","text":""},{"location":"athomic/storage/#storageprotocol","title":"<code>StorageProtocol</code>","text":"<p>This is the abstract contract that all storage providers must implement. It defines a standard set of asynchronous operations for object storage: -   <code>upload(source_path, destination_path)</code> -   <code>download(source_path, destination_path)</code> -   <code>delete(path)</code> -   <code>get_url(path)</code> -   <code>exists(path)</code></p>"},{"location":"athomic/storage/#storagefactory","title":"<code>StorageFactory</code>","text":"<p>A singleton factory that creates the configured storage provider instance. It ensures that a single, shared client is used throughout the application, managing resources efficiently.</p>"},{"location":"athomic/storage/#available-providers","title":"Available Providers","text":""},{"location":"athomic/storage/#localstorageprovider","title":"<code>LocalStorageProvider</code>","text":"<p>This provider stores files on the local filesystem. It is ideal for local development and testing. A key feature is its built-in path traversal protection, which ensures that all file operations are securely contained within the configured <code>base_path</code>, preventing security vulnerabilities.</p>"},{"location":"athomic/storage/#gcsstorageprovider","title":"<code>GcsStorageProvider</code>","text":"<p>This is a production-ready provider for Google Cloud Storage (GCS). It handles authentication using either a service account JSON file or Application Default Credentials (ADC). It also supports generating temporary, secure signed URLs for accessing private objects.</p>"},{"location":"athomic/storage/#integration-with-messaging-the-claim-check-pattern","title":"Integration with Messaging: The Claim Check Pattern","text":"<p>A powerful feature built on the Storage module is the implementation of the Claim Check pattern for the messaging system. This pattern solves the problem of sending large messages through brokers that have size limits.</p> <p>Here\u2019s how it works: 1.  You add the <code>\"claim_check\"</code> step to your messaging payload processing pipeline in your configuration. 2.  When a producer sends a message, the <code>ClaimCheckStepAdapter</code> intercepts it. 3.  If the message's size exceeds a configured threshold (e.g., 250KB), the adapter automatically uploads the large payload to the configured storage provider (e.g., GCS). 4.  The original message payload is replaced with a small JSON object\u2014the \"claim check\"\u2014that contains a reference to the storage path. 5.  On the consumer side, the <code>ClaimCheckStepAdapter</code> detects the claim check, downloads the original large payload from storage, and transparently passes it to the next step in the processing pipeline.</p> <p>This entire process is seamless to the developer and provides a robust solution for handling large payloads.</p>"},{"location":"athomic/storage/#configuration","title":"Configuration","text":"<p>The storage provider is configured under the <code>[storage]</code> section in your <code>settings.toml</code>.</p>"},{"location":"athomic/storage/#local-storage-example","title":"Local Storage Example","text":"<pre><code>[default.storage]\nenabled = true\n\n  [default.storage.provider]\n  backend = \"local\"\n  # The base directory where files will be stored.\n  base_path = \"./.storage\"\n</code></pre>"},{"location":"athomic/storage/#google-cloud-storage-gcs-example","title":"Google Cloud Storage (GCS) Example","text":"<pre><code>[default.storage]\nenabled = true\n\n  [default.storage.provider]\n  backend = \"gcs\"\n  bucket_name = \"my-gcs-bucket-name\"\n\n  # Optional: Path to your service account JSON key file.\n  # If omitted, Application Default Credentials (ADC) are used.\n  credentials_path = \"path/to/your/gcs-credentials.json\"\n</code></pre>"},{"location":"athomic/storage/#api-reference","title":"API Reference","text":""},{"location":"athomic/storage/#nala.athomic.storage.protocol.StorageProtocol","title":"<code>nala.athomic.storage.protocol.StorageProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol defining the standard interface for storage operations. Implementations will handle interaction with specific backends like local filesystem, S3, GCS, Azure Blob, etc.</p>"},{"location":"athomic/storage/#nala.athomic.storage.protocol.StorageProtocol.delete","title":"<code>delete(path)</code>  <code>async</code>","text":"<p>Deletes an object from the storage backend.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path (key or blob name) of the object to delete.</p> required <p>Raises:</p> Type Description <code>StorageOperationError</code> <p>If the deletion fails. (Often idempotent if object doesn't exist)</p>"},{"location":"athomic/storage/#nala.athomic.storage.protocol.StorageProtocol.download","title":"<code>download(source_path, destination_path)</code>  <code>async</code>","text":"<p>Downloads an object from the storage backend to a local destination path.</p> <p>Parameters:</p> Name Type Description Default <code>source_path</code> <code>str</code> <p>The path (key or blob name) of the object in the storage backend.</p> required <code>destination_path</code> <code>str</code> <p>The local path where the file should be saved.</p> required <p>Raises:</p> Type Description <code>ObjectNotFoundError</code> <p>If the source_path does not exist in the storage.</p> <code>StorageOperationError</code> <p>If the download fails for other reasons.</p>"},{"location":"athomic/storage/#nala.athomic.storage.protocol.StorageProtocol.exists","title":"<code>exists(path)</code>  <code>async</code>","text":"<p>Checks if an object exists at the specified path in the storage backend.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path (key or blob name) to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the object exists, False otherwise.</p> <p>Raises:</p> Type Description <code>StorageOperationError</code> <p>If the check fails due to connection or permission issues.</p>"},{"location":"athomic/storage/#nala.athomic.storage.protocol.StorageProtocol.get_url","title":"<code>get_url(path, expires_in=3600)</code>  <code>async</code>","text":"<p>Generates a public or pre-signed URL to access an object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path (key or blob name) of the object.</p> required <code>expires_in</code> <code>Optional[int]</code> <p>Optional duration (in seconds) for which the URL should be valid         (relevant for pre-signed URLs). Defaults to 1 hour.</p> <code>3600</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The accessible URL as a string, or None if the provider cannot generate URLs</p> <code>Optional[str]</code> <p>or the object doesn't exist.</p> <p>Raises:</p> Type Description <code>StorageOperationError</code> <p>If URL generation fails.</p>"},{"location":"athomic/storage/#nala.athomic.storage.protocol.StorageProtocol.upload","title":"<code>upload(source_path, destination_path, metadata=None)</code>  <code>async</code>","text":"<p>Uploads a file from a local source path to a destination path in the storage backend.</p> <p>Parameters:</p> Name Type Description Default <code>source_path</code> <code>str</code> <p>The local path of the file to upload.</p> required <code>destination_path</code> <code>str</code> <p>The target path (key or blob name) in the storage backend.</p> required <code>metadata</code> <code>Optional[Dict[str, str]]</code> <p>Optional dictionary of metadata to associate with the object.</p> <code>None</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the source_path does not exist.</p> <code>StorageOperationError</code> <p>If the upload fails for other reasons (permissions, connection, etc.).</p>"},{"location":"athomic/storage/#nala.athomic.storage.factory.StorageFactory","title":"<code>nala.athomic.storage.factory.StorageFactory</code>","text":"<p>A factory responsible for creating and managing the singleton instance of the configured <code>StorageProvider</code>.</p> <p>This factory ensures that only one instance of the storage provider (e.g., <code>LocalStorageProvider</code>, <code>GcsStorageProvider</code>) is created during the application's lifecycle, managing resource efficiency and centralized configuration access.</p>"},{"location":"athomic/storage/#nala.athomic.storage.factory.StorageFactory.clear","title":"<code>clear()</code>  <code>classmethod</code>","text":"<p>Clears the cached singleton instance. Intended for use in test cleanup.</p>"},{"location":"athomic/storage/#nala.athomic.storage.factory.StorageFactory.create","title":"<code>create(settings=None)</code>  <code>classmethod</code>","text":"<p>Creates or retrieves the singleton instance of the configured storage provider.</p> <p>The creation process validates settings, retrieves the correct provider class from the <code>storage_registry</code>, and instantiates it with the corresponding configuration.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[StorageSettings]</code> <p>Optional explicit settings to override                                   global configuration.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>StorageProtocol</code> <code>StorageProtocol</code> <p>The fully initialized singleton instance of the storage provider.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the storage module is disabled or configuration is missing/invalid.</p> <code>ValueError</code> <p>If the configured storage backend is not registered.</p>"},{"location":"athomic/storage/#nala.athomic.storage.providers.local_storage.LocalStorageProvider","title":"<code>nala.athomic.storage.providers.local_storage.LocalStorageProvider</code>","text":"<p>               Bases: <code>StorageBase</code></p> <p>A storage provider implementation that saves and retrieves files from the local filesystem.</p> <p>This provider is primarily intended for development, testing, and single-instance deployments. It inherits base lifecycle management and observability from <code>StorageBase</code>. All synchronous file I/O operations are wrapped using <code>asyncio.to_thread</code> to prevent blocking the asynchronous event loop.</p> <p>It includes critical path traversal defense to ensure all operations are contained within the configured <code>base_path</code>.</p> <p>Attributes:</p> Name Type Description <code>base_path</code> <code>Path</code> <p>The resolved absolute path to the root storage directory.</p>"},{"location":"athomic/storage/#nala.athomic.storage.providers.local_storage.LocalStorageProvider.__init__","title":"<code>__init__(config)</code>","text":"<p>Initializes the provider and ensures the base storage directory exists.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>LocalStorageProviderSettings</code> <p>The configuration settings for the local storage.</p> required <p>Raises:</p> Type Description <code>StorageOperationError</code> <p>If the base directory cannot be created.</p>"},{"location":"athomic/storage/#nala.athomic.storage.providers.gcs_storage.GcsStorageProvider","title":"<code>nala.athomic.storage.providers.gcs_storage.GcsStorageProvider</code>","text":"<p>               Bases: <code>StorageBase</code></p> <p>A storage provider implementation for Google Cloud Storage (GCS).</p> <p>This provider uses the synchronous GCS client (<code>google.cloud.storage</code>) and wraps all blocking I/O operations with <code>asyncio.to_thread</code> to maintain compatibility with the asynchronous event loop. It handles initialization with service account credentials or Application Default Credentials (ADC).</p> <p>Attributes:</p> Name Type Description <code>bucket_name</code> <code>str</code> <p>The name of the target GCS bucket.</p> <code>client</code> <code>Client</code> <p>The underlying synchronous GCS client instance.</p> <code>bucket</code> <code>Bucket</code> <p>The reference to the configured GCS bucket.</p>"},{"location":"athomic/storage/#nala.athomic.storage.providers.gcs_storage.GcsStorageProvider.__init__","title":"<code>__init__(config)</code>","text":"<p>Initializes the GcsStorageProvider.</p> <p>It attempts to create the synchronous GCS client and resolve the target bucket.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>GcsStorageProviderSettings</code> <p>The configuration settings for GCS.</p> required <p>Raises:</p> Type Description <code>StorageOperationError</code> <p>If the GCS client fails to initialize (e.g., due to invalid credentials).</p>"},{"location":"athomic/storage/#nala.athomic.storage.payload.adapter.ClaimCheckStepAdapter","title":"<code>nala.athomic.storage.payload.adapter.ClaimCheckStepAdapter</code>","text":"<p>               Bases: <code>ProcessingStepProtocol</code></p> <p>An adapter that implements the Claim Check pattern as a payload processing step.</p> <p>This component is integrated into the messaging payload pipeline to handle messages whose size exceeds a configurable threshold. It stores the large payload in external blob storage (<code>StorageProtocol</code>) and replaces the original message body with a small \"claim check\" (a reference to the storage location).</p> <p>Attributes:</p> Name Type Description <code>settings</code> <p>Configuration for the Claim Check pattern, including the size threshold.</p> <code>storage</code> <p>The configured instance of the <code>StorageProtocol</code> (e.g., GCS, S3).</p>"},{"location":"athomic/storage/#nala.athomic.storage.payload.adapter.ClaimCheckStepAdapter.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the adapter and resolves the storage dependency and configuration settings.</p>"},{"location":"athomic/storage/#nala.athomic.storage.payload.adapter.ClaimCheckStepAdapter.decode","title":"<code>decode(data, **kwargs)</code>  <code>async</code>","text":"<p>Executes the Claim Check logic for inbound messages.</p> <p>If the message contains the claim check header and payload structure, it downloads the original, large payload from storage.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>The raw payload bytes received from the message broker.</p> required <code>**kwargs</code> <code>Any</code> <p>Contextual arguments (must include <code>headers</code>).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The original, large payload bytes.</p> <p>Raises:</p> Type Description <code>StorageOperationError</code> <p>If the download fails or the claim check path is missing.</p>"},{"location":"athomic/storage/#nala.athomic.storage.payload.adapter.ClaimCheckStepAdapter.encode","title":"<code>encode(data, **kwargs)</code>  <code>async</code>","text":"<p>Executes the Claim Check logic for outbound messages.</p> <p>If the payload size exceeds the threshold, the data is uploaded to storage, and the original payload is replaced by a JSON envelope containing the path.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>The raw payload bytes from the previous pipeline step.</p> required <code>**kwargs</code> <code>Any</code> <p>Contextual arguments (must include <code>headers</code>).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The original payload or the small claim check JSON envelope.</p> <p>Raises:</p> Type Description <code>StorageOperationError</code> <p>If the upload operation fails.</p>"},{"location":"athomic/config/schemas/","title":"Configuration Schemas","text":""},{"location":"athomic/config/schemas/#overview","title":"Overview","text":"<p>The <code>config/schemas</code> directory is the heart of the Athomic Layer's type-safe configuration system. It contains a collection of Pydantic models that precisely define the structure, data types, default values, and validation rules for every setting in the application.</p> <p>This approach provides several key benefits: -   Type Safety: Eliminates configuration errors at startup by validating the entire settings structure. -   Auto-Completion: Developers get full auto-completion and static analysis for configuration objects in their IDE. -   Clear Structure: The directory structure of the schemas directly mirrors the structure of the <code>settings.toml</code> file, making the configuration intuitive to navigate.</p> <p>The root of this system is the <code>AppSettings</code> model, which composes all other schema models into a single, unified configuration object.</p>"},{"location":"athomic/config/schemas/#schema-module-structure","title":"Schema Module Structure","text":"<p>Below is a list of the primary modules within the <code>schemas</code> directory. Each module corresponds to a major feature or component of the Athomic Layer.</p> <ul> <li> <p><code>app_settings.py</code>: The main Pydantic model that aggregates all other configuration models.</p> </li> <li> <p><code>context/</code>:</p> <ul> <li><code>context_config.py</code>: Defines settings for the Context Management module, including which variables to propagate.</li> </ul> </li> <li> <p><code>control/</code>:</p> <ul> <li><code>control_config.py</code>: Root model for runtime control features.</li> <li><code>feature_flags_config.py</code>: Schemas for the Feature Flags system.</li> <li><code>live_config.py</code>: Schemas for the Live Configuration watcher and state services.</li> <li><code>scheduler_config.py</code>: Schemas for the Task Scheduler.</li> </ul> </li> <li> <p><code>database/</code>:</p> <ul> <li><code>database_config.py</code>: Main model for all Data &amp; Persistence connections.</li> <li><code>document/</code>: Schemas for Document Store providers (e.g., MongoDB).</li> <li><code>kvstore/</code>: Schemas for Key-Value Store providers (e.g., Redis) and their wrappers.</li> <li><code>migrations_config.py</code>: Schemas for the Database Migrations engine.</li> </ul> </li> <li> <p><code>events/</code>:</p> <ul> <li><code>events_config.py</code>: Schemas for the Internal Event Bus.</li> </ul> </li> <li> <p><code>http/</code>:</p> <ul> <li><code>http_config.py</code>: Schemas for the Resilient HTTP Client.</li> </ul> </li> <li> <p><code>integration/</code>:</p> <ul> <li><code>integration_config.py</code>: Root model for all external integrations.</li> <li><code>consul_config.py</code>: Schemas for the Consul Client.</li> <li><code>discovery_config.py</code>: Schemas for the Service Discovery system.</li> <li><code>messaging/</code>: A rich set of schemas for the Messaging module, including producer, consumer, DLQ, and republisher settings.</li> <li><code>tasks/</code>: Schemas for Background Task brokers.</li> </ul> </li> <li> <p><code>observability/</code>:</p> <ul> <li><code>observability_config.py</code>: Main model for the observability stack.</li> <li><code>log/logging_config.py</code>: Schemas for Structured Logging.</li> <li><code>metrics/metrics_config.py</code>: Schemas for Prometheus Metrics.</li> </ul> </li> <li> <p><code>performance/</code>:</p> <ul> <li><code>performance_config.py</code>: Root model for performance features.</li> <li><code>cache_config.py</code>: Schemas for the Caching system.</li> <li><code>compression_config.py</code>: Schemas for HTTP Compression.</li> </ul> </li> <li> <p><code>resilience/</code>:</p> <ul> <li><code>resilience_config.py</code>: The main model that aggregates all resilience patterns.</li> <li>Contains individual schema files for each pattern (e.g., <code>retry_config.py</code>, <code>circuit_breaker_config.py</code>).</li> </ul> </li> <li> <p><code>security/</code>:</p> <ul> <li><code>security_config.py</code>: The main model for all security features.</li> <li><code>auth_config.py</code>: Schemas for Authentication (JWT, API Key).</li> <li><code>crypto_config.py</code>: Schemas for Cryptography.</li> <li><code>secrets/</code>: Schemas for Secrets Management.</li> </ul> </li> <li> <p><code>serializer/</code>:</p> <ul> <li><code>serializer_config.py</code>: Schemas for the Serializer module.</li> </ul> </li> <li> <p><code>storage/</code>:</p> <ul> <li><code>storage_config.py</code>: Schemas for the File Storage abstraction.</li> </ul> </li> </ul>"},{"location":"athomic/control/feature_flags/","title":"Feature Flags","text":"<p>feature_flags# Feature Flags</p>"},{"location":"athomic/control/feature_flags/#overview","title":"Overview","text":"<p>The Feature Flags module provides a powerful system for dynamically enabling or disabling features in your application at runtime, without requiring a code deployment. This is an essential tool for continuous delivery, A/B testing, and safely rolling out new functionality.</p>"},{"location":"athomic/control/feature_flags/#key-features","title":"Key Features","text":"<ul> <li>Multiple Backends: Supports fetching flags from a local configuration dictionary, Redis, or Consul KV.</li> <li>Declarative Usage: Protect an entire function or code path with the simple <code>@feature_enabled</code> decorator.</li> <li>Contextual Evaluation: Flags can be evaluated based on the current <code>ExecutionContext</code> (e.g., enable a feature only for a specific <code>tenant_id</code>).</li> <li>Multivariate Flags: Supports variants, which are flags that can return a string, number, or JSON object instead of just true/false.</li> </ul>"},{"location":"athomic/control/feature_flags/#how-it-works","title":"How It Works","text":"<p>The system is orchestrated by the <code>FeatureFlagFactory</code>, which creates a provider instance based on your configuration. The primary way to interact with the system is through two utility functions: -   <code>is_feature_enabled(flag_key)</code>: Checks if a boolean flag is active. -   <code>get_feature_variant(flag_key)</code>: Retrieves the value of a multivariate flag.</p> <p>For convenience, the <code>@feature_enabled</code> decorator wraps this logic, protecting an entire function.</p>"},{"location":"athomic/control/feature_flags/#usage-example","title":"Usage Example","text":"<pre><code>from nala.athomic.control import feature_enabled, is_feature_enabled, get_feature_variant\nfrom nala.athomic.context import context_vars\n\n# --- Using the decorator ---\n@feature_enabled(flag_key=\"new-dashboard-feature\", context_kwargs=[\"tenant_id\"])\nasync def show_new_dashboard(tenant_id: str):\n    # This code will only run if the \"new-dashboard-feature\" is enabled.\n    # The check can be contextual based on the tenant_id.\n    return {\"dashboard\": \"v2\"}\n\n# --- Using the utility functions ---\nasync def get_user_limit() -&gt; int:\n    # Get a variant value, with a default of 100\n    limit = await get_feature_variant(\"user-concurrent-limit\", default=100)\n    return limit\n</code></pre>"},{"location":"athomic/control/feature_flags/#configuration","title":"Configuration","text":"<pre><code>[default.control.feature_flags]\nenabled = true\n# The primary backend to fetch flags from.\nbackend = \"consul\"\n# TTL in seconds for caching flags from external providers (Redis, Consul).\ncache_ttl_seconds = 60\n\n  [default.control.feature_flags.provider]\n  backend = \"consul\"\n  # The base path in Consul KV where flags are stored.\n  kv_prefix = \"config/flags\"\n</code></pre>"},{"location":"athomic/control/live_config/","title":"Live Configuration (Hot-Reloading)","text":""},{"location":"athomic/control/live_config/#overview","title":"Overview","text":"<p>The Live Configuration system provides a powerful mechanism to change application settings at runtime without requiring a restart or deployment. This is a critical feature for operational agility, allowing you to tune parameters, toggle features, or respond to incidents in real-time.</p>"},{"location":"athomic/control/live_config/#key-features","title":"Key Features","text":"<ul> <li>Dynamic Updates: Settings can be changed in a central source (like Consul KV) and are automatically reflected in the application.</li> <li>Seamless Integration: Works through a special Pydantic model, <code>LiveConfigModel</code>, which makes accessing dynamic values transparent to the developer.</li> <li>Event-Driven: Uses the internal Event Bus for efficient, decoupled communication between the watcher and the state service.</li> </ul>"},{"location":"athomic/control/live_config/#how-it-works-a-two-part-system","title":"How It Works: A Two-Part System","text":"<ol> <li> <p><code>ConfigWatcherService</code>: This is a background service that continuously long-polls an external configuration provider (e.g., Consul) for changes under a specific key prefix.</p> <ul> <li>When a change is detected, it compares the new values with its last known state.</li> <li>For each key that has changed, it publishes a <code>config.updated</code> event on the internal event bus with the key and the new value.</li> </ul> </li> <li> <p><code>LiveConfigService</code>: This service subscribes to the <code>config.updated</code> event. It maintains an in-memory, flattened dictionary of the application's most current configuration state. When it receives an update event, it updates its internal state.</p> </li> <li> <p><code>LiveConfigModel</code>: Your Pydantic settings models can inherit from <code>LiveConfigModel</code>. When you access an attribute on such a model, it automatically intercepts the call and queries the <code>LiveConfigService</code> for the latest value, falling back to the statically configured value if none is found.</p> </li> </ol>"},{"location":"athomic/control/live_config/#api-reference","title":"API Reference","text":""},{"location":"athomic/control/live_config/#nala.athomic.control.live_config.service.LiveConfigService","title":"<code>nala.athomic.control.live_config.service.LiveConfigService</code>","text":"<p>               Bases: <code>BaseService</code></p> <p>Maintains the current state of the application's live configuration.</p> <p>This service acts as an in-memory cache for configuration values that can be updated at runtime. It subscribes to internal <code>config.updated</code> events, which are published by the <code>ConfigWatcherService</code>, and updates its internal state accordingly. Other components, like <code>LiveConfigModel</code>, query this service to get the most up-to-date configuration values.</p> <p>Attributes:</p> Name Type Description <code>event_bus</code> <code>EventBusProtocol</code> <p>The internal event bus used to receive update notifications.</p> <code>_config_state</code> <code>Dict[str, Any]</code> <p>The in-memory dictionary holding the flattened, most current configuration state.</p>"},{"location":"athomic/control/live_config/#nala.athomic.control.live_config.service.LiveConfigService.__init__","title":"<code>__init__(settings=None, event_bus=None)</code>","text":"<p>Initializes the LiveConfigService.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[LiveConfigSettings]</code> <p>The configuration for the live config system. If not provided, global settings are used.</p> <code>None</code> <code>event_bus</code> <code>Optional[EventBusProtocol]</code> <p>An instance of the event bus. If not provided, the global instance is retrieved.</p> <code>None</code>"},{"location":"athomic/control/live_config/#nala.athomic.control.live_config.service.LiveConfigService.get","title":"<code>get(key, default=None)</code>","text":"<p>Gets the latest value of a configuration key from the in-memory state.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The configuration key in flattened format (e.g., 'resilience.retry.attempts').</p> required <code>default</code> <code>Optional[Any]</code> <p>The value to return if the key is not found.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The current configuration value or the default.</p>"},{"location":"athomic/control/live_config/#nala.athomic.control.live_config.watcher_service.ConfigWatcherService","title":"<code>nala.athomic.control.live_config.watcher_service.ConfigWatcherService</code>","text":"<p>               Bases: <code>BaseService</code></p> <p>Watches a configuration prefix in an external provider for changes.</p> <p>This background service actively monitors a specified key prefix in a dynamic configuration backend (e.g., Consul KV). When it detects a change, it compares the new values with its last known state and publishes <code>config.updated</code> events on the internal event bus for each changed key.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <code>LiveConfigSettings</code> <p>The settings for the live config system.</p> <code>config_provider</code> <code>ConfigProvidersProtocol</code> <p>The client for the external configuration source.</p> <code>event_bus</code> <code>EventBusProtocol</code> <p>The internal event bus for publishing updates.</p>"},{"location":"athomic/control/live_config/#nala.athomic.control.live_config.watcher_service.ConfigWatcherService.__init__","title":"<code>__init__(settings=None, config_provider=None, event_bus=None)</code>","text":"<p>Initializes the ConfigWatcherService.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[LiveConfigSettings]</code> <p>The configuration for the live config system.</p> <code>None</code> <code>config_provider</code> <code>Optional[ConfigProvidersProtocol]</code> <p>The provider for the external configuration source.</p> <code>None</code> <code>event_bus</code> <code>Optional[EventBusProtocol]</code> <p>The internal event bus for publishing updates.</p> <code>None</code>"},{"location":"athomic/control/live_config/#nala.athomic.config.schemas.live.LiveConfigModel","title":"<code>nala.athomic.config.schemas.live.LiveConfigModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A Pydantic BaseModel that provides transparent dynamic configuration.</p> <p>Subclasses of <code>LiveConfigModel</code> can have their attributes updated at runtime. When an attribute is accessed, this class intercepts the call and attempts to fetch the latest value from the central <code>LiveConfigService</code>. This enables hot-reloading of configuration without an application restart.</p> <p>The mapping of dynamic fields is controlled in two ways, with configuration from files taking precedence:</p> <ol> <li> <p>Via Configuration File (<code>settings.toml</code>):     In the TOML section for your schema, add a <code>_live_keys_config</code> map:     <code>toml     [my_module]     _live_keys_config = { pydantic_attr_name = \"live.config.key\" }     PYDANTIC_ATTR_NAME = \"static_default\"</code></p> </li> <li> <p>Via Code (<code>model_config</code>):     In the Pydantic schema class, define a <code>live_keys</code> dictionary in <code>model_config</code>.</p> </li> </ol> <p>Attributes:</p> Name Type Description <code>_instance_live_keys</code> <code>Dict[str, str]</code> <p>A dictionary mapping attribute names in this model to their corresponding keys in the live configuration backend. This map is populated from the <code>_live_keys_config</code> section in the TOML configuration file.</p>"},{"location":"athomic/control/live_config/#nala.athomic.config.schemas.live.LiveConfigModel.__getattribute__","title":"<code>__getattribute__(name)</code>","text":"<p>Overrides attribute access to fetch dynamic values from the live config service.</p> <p>This method implements the core logic for live configuration: 1. Retrieves the static value (loaded at application startup). 2. Checks if the requested attribute is mapped as a \"live key\". 3. If it is, queries the <code>LiveConfigService</code> for the most up-to-date value. 4. Returns the live value if found; otherwise, the static value is used    as a fallback. 5. If the attribute is not a live key, the static value is returned directly.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the attribute being accessed.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The latest value of the attribute from the live config service,</p> <code>Any</code> <p>or the statically configured value as a fallback.</p>"},{"location":"athomic/control/live_config/#nala.athomic.config.schemas.live.LiveConfigModel.__init__","title":"<code>__init__(**data)</code>","text":"<p>Initializes the model and extracts the live key mapping.</p> <p>This constructor intercepts the <code>_live_keys_config</code> dictionary from the incoming data before passing the rest to the standard Pydantic validator. This allows the model to store the live configuration mapping without treating it as a standard model field.</p> <p>Parameters:</p> Name Type Description Default <code>**data</code> <code>Any</code> <p>The raw data dictionary used to populate the model, which may include the <code>_live_keys_config</code> metadata key.</p> <code>{}</code>"},{"location":"athomic/control/scheduler/","title":"Task Scheduler","text":""},{"location":"athomic/control/scheduler/#overview","title":"Overview","text":"<p>The Scheduler module provides a persistent, distributed mechanism for scheduling tasks to be executed at a specific time in the future or after a certain delay. It is distinct from the main Background Tasks module, which is for immediate asynchronous execution.</p> <p>The scheduler is essential for features like: -   Implementing the Delayed Messages strategy. -   Scheduling recurring batch jobs. -   Triggering future actions (e.g., ending a trial period).</p>"},{"location":"athomic/control/scheduler/#how-it-works","title":"How It Works","text":"<p>The system is composed of a persistent storage backend and a background worker. 1.  Scheduling: When you call <code>scheduler.schedule()</code>, a <code>GenericTask</code> object (containing the handler path and payload) is created. This object is stored in a KV store (like Redis), and its execution time is added as a score to a sorted set. 2.  Polling (Worker): The <code>SchedulerWorkerService</code> is a background service that periodically polls the sorted set for tasks whose execution time has passed. 3.  Dispatching: The worker atomically fetches and removes a due task from the schedule and then enqueues it into the main application Task Broker for actual execution by a standard worker.</p> <p>This design decouples scheduling from execution, making the system highly scalable and resilient.</p>"},{"location":"athomic/control/scheduler/#api-reference","title":"API Reference","text":""},{"location":"athomic/control/scheduler/#nala.athomic.control.scheduler.protocol.SchedulerProtocol","title":"<code>nala.athomic.control.scheduler.protocol.SchedulerProtocol</code>","text":"<p>               Bases: <code>BaseServiceProtocol</code>, <code>Protocol</code></p> <p>Defines the contract for a task scheduling system. Abstracts the logic of delaying or scheduling the execution of a task, whether in memory, via broker, or in persistent storage.</p>"},{"location":"athomic/control/scheduler/#nala.athomic.control.scheduler.protocol.SchedulerProtocol.cancel","title":"<code>cancel(task_id)</code>  <code>async</code>","text":"<p>Attempts to cancel a scheduled task.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>The ID of the task to be canceled.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the cancellation was successful, False otherwise.</p>"},{"location":"athomic/control/scheduler/#nala.athomic.control.scheduler.protocol.SchedulerProtocol.schedule","title":"<code>schedule(task, *, delay, task_id=None)</code>  <code>async</code>","text":"<p>Schedules a task to be executed after a certain delay.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Any</code> <p>The task object to be scheduled (can be a Pydantic model, dict, etc.).</p> required <code>delay</code> <code>timedelta</code> <p>The waiting time from now.</p> required <code>task_id</code> <code>str | None</code> <p>An optional ID for the task. If not provided, one will be generated.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The ID of the scheduled task.</p>"},{"location":"athomic/control/scheduler/#nala.athomic.control.scheduler.protocol.SchedulerProtocol.schedule_at","title":"<code>schedule_at(task, *, execution_time, task_id=None)</code>  <code>async</code>","text":"<p>Schedules a task to be executed at an exact moment in the future.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Any</code> <p>The task object to be scheduled.</p> required <code>execution_time</code> <code>datetime</code> <p>The exact date and time (UTC) for execution.</p> required <code>task_id</code> <code>str | None</code> <p>An optional ID for the task.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The ID of the scheduled task.</p>"},{"location":"athomic/control/scheduler/#nala.athomic.control.scheduler.schemas.GenericTask","title":"<code>nala.athomic.control.scheduler.schemas.GenericTask</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a generic, serializable task for the scheduler.</p> <p>This model acts as a standard envelope for any task that needs to be scheduled for delayed or future execution. It decouples the scheduler from the task's implementation by referencing the handler as a string and encapsulating all necessary data within the <code>payload</code>.</p> <p>Attributes:</p> Name Type Description <code>task_id</code> <code>UUID</code> <p>A unique identifier for this specific task instance.</p> <code>handler</code> <code>str</code> <p>The string identifier of the callable handler that will execute the task's logic.</p> <code>payload</code> <code>Dict[str, Any]</code> <p>A dictionary containing the data required by the handler to perform the task.</p>"},{"location":"athomic/control/scheduler/#nala.athomic.control.scheduler.worker.SchedulerWorkerService","title":"<code>nala.athomic.control.scheduler.worker.SchedulerWorkerService</code>","text":"<p>               Bases: <code>BaseService</code></p> <p>A background service that polls a persistent schedule and dispatches due tasks.</p> <p>This worker is the execution engine for the custom KV-based scheduler. It periodically polls a sorted set in a KV store (e.g., Redis) to find tasks whose scheduled execution time has passed. Once a task is claimed, it is dispatched to the primary application task broker (e.g., Taskiq) for actual execution. It uses an exponential backoff strategy to reduce polling frequency during idle periods.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <p>The worker-specific configuration settings.</p> <code>kv_store</code> <code>Optional[KVStoreProtocol]</code> <p>The client for the KV store that holds the schedule.</p> <code>task_broker</code> <code>Optional[TaskBrokerProtocol]</code> <p>The client for the main task broker where tasks are dispatched.</p> <code>tasks_key</code> <code>str</code> <p>The key for the sorted set of scheduled tasks.</p> <code>details_prefix</code> <code>str</code> <p>The key prefix for storing task details.</p>"},{"location":"athomic/control/scheduler/#nala.athomic.control.scheduler.worker.SchedulerWorkerService.__init__","title":"<code>__init__(settings)</code>","text":"<p>Initializes the SchedulerWorkerService.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>SchedulerSettings</code> <p>The main scheduler configuration, from which worker-specific settings are extracted.</p> required"},{"location":"athomic/database/documents/","title":"Document Stores","text":""},{"location":"athomic/database/documents/#overview","title":"Overview","text":"<p>The <code>athomic.database.documents</code> module provides the abstraction layer for connecting to and managing document-oriented databases within the Athomic Layer. It follows a provider-based pattern, allowing different backends like MongoDB to be used interchangeably without affecting the services that depend on them.</p> <p>The primary responsibility of this module is to provide a lifecycle-managed, ready-to-use database client instance that can be passed to data repositories or Object-Document Mappers (ODMs) like Beanie.</p>"},{"location":"athomic/database/documents/#how-it-works","title":"How It Works","text":"<p>The components of this module are designed to work together seamlessly with the central <code>ConnectionManager</code>:</p> <ol> <li>Configuration: In your <code>settings.toml</code>, you define one or more connections under <code>[database.documents]</code>. Each connection specifies a <code>backend</code> (e.g., \"mongo\").</li> <li>Factory Creation: The <code>ConnectionManager</code> calls the <code>DocumentsDatabaseFactory</code> for each configured connection.</li> <li>Registry Lookup: The factory uses the <code>backend</code> name to look up the corresponding provider class (e.g., <code>MongoProvider</code>) in the <code>DocumentDatabaseRegistry</code>.</li> <li>Instantiation: The factory instantiates the provider class with its specific configuration.</li> <li>Lifecycle Management: The <code>ConnectionManager</code> then manages the <code>start</code> and <code>stop</code> lifecycle of the created provider instance.</li> </ol> <p>All providers must adhere to the <code>DocumentsDatabaseProtocol</code>, which guarantees a consistent interface for obtaining the native database client.</p>"},{"location":"athomic/database/documents/#available-providers","title":"Available Providers","text":""},{"location":"athomic/database/documents/#mongoprovider","title":"<code>MongoProvider</code>","text":"<p>This is the primary, production-ready provider for connecting to a MongoDB server or replica set. It is built on top of the official asynchronous driver <code>motor</code>. The provider handles: -   Securely resolving the connection URI. -   Establishing the connection and verifying it with a <code>ping</code> command. -   Providing the native <code>AsyncIOMotorDatabase</code> object that can be used directly by libraries like Beanie.</p>"},{"location":"athomic/database/documents/#localdbprovider","title":"<code>LocalDBProvider</code>","text":"<p>A simple, in-memory provider designed for testing and development. It simulates the connection lifecycle (<code>start</code>, <code>stop</code>, <code>wait_ready</code>) without requiring any external database dependencies. This is extremely useful for fast and isolated unit tests.</p>"},{"location":"athomic/database/documents/#configuration","title":"Configuration","text":"<p>Below is an example of how to configure a MongoDB connection in your <code>settings.toml</code>. You can define multiple connections with different names (e.g., <code>default_mongo</code>, <code>logs_mongo</code>).</p> <pre><code>[default.database.documents.default_mongo]\n# Enables or disables this specific document database connection.\nenabled = true\n\n# The name of the backend. This is the key used in the registry.\nbackend = \"mongo\"\n\n  # Provider-specific settings, validated by a Pydantic model.\n  [default.database.documents.default_mongo.provider]\n  # The connection URI for the MongoDB instance.\n  # This can be a direct value or a secret reference.\n  url = \"mongodb://user:pass@localhost:27017\" # pragma: allowlist secret\n\n  # The name of the database to use.\n  database_name = \"main_app_db\"\n</code></pre>"},{"location":"athomic/database/documents/#api-reference","title":"API Reference","text":""},{"location":"athomic/database/documents/#nala.athomic.database.documents.protocol.DocumentsDatabaseProtocol","title":"<code>nala.athomic.database.documents.protocol.DocumentsDatabaseProtocol</code>","text":"<p>               Bases: <code>BaseServiceProtocol</code>, <code>Protocol</code></p> <p>Protocol for a document database provider.</p> <p>Defines the contract for managing a database connection and providing access to the native database instance (e.g., a Motor/Beanie database object). It inherits from ConnectionServiceProtocol to be managed by the application's lifecycle.</p>"},{"location":"athomic/database/documents/#nala.athomic.database.documents.protocol.DocumentsDatabaseProtocol.get_database","title":"<code>get_database()</code>  <code>async</code>","text":"<p>Returns the native database instance required by repositories to perform operations.</p>"},{"location":"athomic/database/documents/#nala.athomic.database.documents.factory.DocumentsDatabaseFactory","title":"<code>nala.athomic.database.documents.factory.DocumentsDatabaseFactory</code>","text":"<p>Factory responsible for creating a singleton instance of the configured document database provider.</p>"},{"location":"athomic/database/documents/#nala.athomic.database.documents.factory.DocumentsDatabaseFactory.create","title":"<code>create(settings)</code>  <code>classmethod</code>","text":"<p>Creates and returns a singleton instance of the document database provider.</p>"},{"location":"athomic/database/documents/#nala.athomic.database.documents.providers.mongo_provider.MongoProvider","title":"<code>nala.athomic.database.documents.providers.mongo_provider.MongoProvider</code>","text":"<p>               Bases: <code>DocumentsDatabaseBase</code>, <code>CredentialResolve</code></p> <p>A Document Database Provider implementation for MongoDB.</p> <p>This class manages the connection lifecycle, state, and client access for a MongoDB instance using the asynchronous <code>motor.motor_asyncio</code> library. It integrates credential resolution to securely handle the connection URI. It inherits all core lifecycle and state management logic from <code>DocumentsDatabaseBase</code>.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <code>MongoSettings</code> <p>The MongoDB-specific configuration settings.</p> <code>client</code> <code>AsyncIOMotorClient | None</code> <p>The underlying Motor client instance.</p> <code>_db</code> <code>AsyncIOMotorDatabase | None</code> <p>The active database instance.</p>"},{"location":"athomic/database/documents/#nala.athomic.database.documents.providers.mongo_provider.MongoProvider.__init__","title":"<code>__init__(settings=None)</code>","text":"<p>Initializes the MongoProvider.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[DocumentsSettings]</code> <p>The document database configuration settings.</p> <code>None</code>"},{"location":"athomic/database/kvstore/","title":"Key-Value Stores","text":""},{"location":"athomic/database/kvstore/#overview","title":"Overview","text":"<p>The Key-Value (KV) Store module provides a powerful and extensible abstraction layer for interacting with key-value data stores like Redis. It is a fundamental building block used by many other Athomic modules, including:</p> <ul> <li>Caching: For storing the results of expensive operations.</li> <li>Rate Limiting: For tracking request counts.</li> <li>Feature Flags: For fetching flag configurations.</li> <li>Distributed Locking: For managing lock states.</li> <li>Sagas &amp; Schedulers: For persisting state and scheduled tasks.</li> </ul> <p>The architecture is designed to be highly flexible, featuring a protocol-based design, multiple backend providers, and a powerful wrapper system that allows for adding cross-cutting functionality declaratively.</p>"},{"location":"athomic/database/kvstore/#core-concepts","title":"Core Concepts","text":""},{"location":"athomic/database/kvstore/#kvstoreprotocol","title":"<code>KVStoreProtocol</code>","text":"<p>This is the contract that all KV store providers must implement. It defines a rich set of asynchronous operations, including not only basic CRUD (<code>get</code>, <code>set</code>, <code>delete</code>, <code>exists</code>) but also advanced commands for sorted sets (<code>zadd</code>, <code>zrem</code>, <code>zpopbyscore</code>), which are critical for implementing components like the task scheduler.</p>"},{"location":"athomic/database/kvstore/#kvstorefactory","title":"<code>KVStoreFactory</code>","text":"<p>This is the main entry point for creating a KV store client. The factory is responsible for: 1.  Reading the configuration for a named connection. 2.  Instantiating the correct base provider (e.g., <code>RedisKVClient</code>) using a registry. 3.  Applying a chain of wrappers to the base provider, adding functionality like multi-tenancy and default TTLs.</p>"},{"location":"athomic/database/kvstore/#wrapper-decorator-pattern","title":"Wrapper (Decorator) Pattern","text":"<p>A key architectural feature of the KV Store module is its use of the Decorator pattern. You can \"wrap\" a base client with additional functionality defined in your configuration. This makes the system extremely flexible. For example, you can add multi-tenant key resolution to any KV store (Redis, in-memory, or a future DynamoDB provider) just by adding a wrapper to the configuration.</p>"},{"location":"athomic/database/kvstore/#available-providers","title":"Available Providers","text":"<ul> <li><code>RedisKVClient</code>: The primary, production-ready provider for Redis. It uses the high-performance <code>redis-py</code> async client and supports the full <code>KVStoreProtocol</code>, including atomic sorted set operations implemented with Lua scripting for maximum safety.</li> <li><code>LocalKVClient</code>: A simple, in-memory provider that simulates a KV store using Python dictionaries. It is perfect for fast, dependency-free unit and integration testing.</li> </ul>"},{"location":"athomic/database/kvstore/#available-wrappers","title":"Available Wrappers","text":"<ul> <li><code>KeyResolvingKVClient</code>: This wrapper automatically prepends contextual prefixes to all keys before they hit the database. It uses the <code>ContextualKeyGenerator</code> to include the <code>namespace</code>, <code>tenant_id</code>, and <code>user_id</code> (if configured), making multi-tenancy transparent.</li> <li><code>DefaultTTLKvClient</code>: This wrapper automatically applies a default Time-To-Live (TTL) to all <code>set</code> operations if one is not explicitly provided in the method call. This is useful for ensuring that cache keys, for example, always expire.</li> </ul>"},{"location":"athomic/database/kvstore/#configuration","title":"Configuration","text":"<p>The true power of this module is visible in its configuration. You define a base provider and then apply a list of wrappers to it.</p> <pre><code># In settings.toml, under [default.database.kvstore]\n\n[default.database.kvstore.default_redis]\nenabled = true\n# The namespace is used by the KeyResolvingKVClient wrapper\nnamespace = \"cache\"\n\n  # 1. Define the base provider (e.g., Redis)\n  [default.database.kvstore.default_redis.provider]\n  backend = \"redis\"\n  uri = \"redis://localhost:6379/0\"\n\n  # 2. Define an ordered list of wrappers to apply\n  [[default.database.kvstore.default_redis.wrappers]]\n  name = \"default_ttl\" # This must match a name in the KVStoreWrapperRegistry\n  enabled = true\n    # Custom settings for this wrapper instance\n    [default.database.kvstore.default_redis.wrappers.config]\n    default_ttl_seconds = 3600 # 1 hour\n\n  [[default.database.kvstore.default_redis.wrappers]]\n  name = \"key_resolver\"\n  enabled = true\n    # The 'namespace' from the top level will be used by this wrapper\n    [default.database.kvstore.default_redis.wrappers.config]\n    # No extra config needed here, it uses the parent's namespace\n</code></pre> <p>In this example, when a <code>set(\"my-key\", \"value\")</code> call is made, the final key in Redis will look something like <code>nala:tenant-123:cache:my-key</code>, and it will have a default TTL of 1 hour.</p>"},{"location":"athomic/database/kvstore/#api-reference","title":"API Reference","text":""},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol","title":"<code>nala.athomic.database.kvstore.protocol.KVStoreProtocol</code>","text":"<p>               Bases: <code>BaseServiceProtocol</code>, <code>Protocol</code></p> <p>Defines the contract for an asynchronous Key-Value store provider.</p> <p>This protocol specifies the standard interface for all key-value store implementations within the framework. It includes basic CRUD operations, sorted set commands, and lifecycle management. Any class that conforms to this protocol can be used as a KV store backend for features like caching, rate limiting, and feature flags.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.clear","title":"<code>clear()</code>  <code>async</code>","text":"<p>Removes all keys from the current database.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Closes the connection to the backend service.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.connect","title":"<code>connect()</code>  <code>async</code>","text":"<p>Establishes the connection to the backend service.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.delete","title":"<code>delete(key)</code>  <code>async</code>","text":"<p>Deletes a key from the store.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to delete.</p> required"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.exists","title":"<code>exists(key)</code>  <code>async</code>","text":"<p>Checks if a key exists in the store.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the key exists, False otherwise.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.get","title":"<code>get(key)</code>  <code>async</code>","text":"<p>Retrieves a value by its key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the item to retrieve.</p> required <p>Returns:</p> Type Description <code>Optional[Any]</code> <p>The deserialized value, or None if the key is not found.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.get_final_client","title":"<code>get_final_client()</code>  <code>async</code>","text":"<p>Returns the underlying raw asynchronous client instance.</p> <p>This provides an escape hatch to access provider-specific features not covered by the protocol (e.g., calling a unique Redis command).</p> <p>Returns:</p> Type Description <code>Any</code> <p>The raw provider-specific async client instance (e.g., <code>redis.asyncio.Redis</code>).</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.get_sync_client","title":"<code>get_sync_client()</code>","text":"<p>Returns an underlying raw synchronous client instance, if applicable.</p> <p>Returns:</p> Type Description <code>Any</code> <p>The raw provider-specific sync client instance (e.g., <code>redis.Redis</code>).</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If a synchronous client is not available or supported by the provider.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.is_available","title":"<code>is_available()</code>  <code>async</code>","text":"<p>Checks if the provider is connected and ready to accept operations.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the service is ready, False otherwise.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.set","title":"<code>set(key, value, ttl=None, nx=False)</code>  <code>async</code>","text":"<p>Sets a key-value pair, with an optional TTL.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the item to set.</p> required <code>value</code> <code>Any</code> <p>The value to store. It will be serialized by the provider.</p> required <code>ttl</code> <code>Optional[int]</code> <p>Optional time-to-live for the key in seconds.</p> <code>None</code> <code>nx</code> <code>bool</code> <p>If True, set the key only if it does not already exist.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the operation was successful.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.zadd","title":"<code>zadd(key, mapping)</code>  <code>async</code>","text":"<p>Adds one or more members with scores to a sorted set.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>mapping</code> <code>Mapping[str, float]</code> <p>A dictionary of member-score pairs to add.</p> required"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.zpopbyscore","title":"<code>zpopbyscore(key, max_score)</code>  <code>async</code>","text":"<p>Atomically removes and returns the member with the lowest score.</p> <p>The member returned is the one with the lowest score that is less than or equal to the <code>max_score</code>.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>max_score</code> <code>float</code> <p>The maximum score to consider for popping a member.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The member name as a string, or None if no qualifying members exist.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.zrangebyscore","title":"<code>zrangebyscore(key, min_score, max_score)</code>  <code>async</code>","text":"<p>Returns members of a sorted set within a score range.</p> <p>Retrieves all members in a sorted set with scores between <code>min_score</code> and <code>max_score</code> (inclusive).</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>min_score</code> <code>float</code> <p>The minimum score of the range.</p> required <code>max_score</code> <code>float</code> <p>The maximum score of the range.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of member names.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.protocol.KVStoreProtocol.zrem","title":"<code>zrem(key, members)</code>  <code>async</code>","text":"<p>Removes one or more members from a sorted set.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>members</code> <code>list[str]</code> <p>A list of members to remove.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of members that were removed from the set.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.factory.KVStoreFactory","title":"<code>nala.athomic.database.kvstore.factory.KVStoreFactory</code>","text":"<p>               Bases: <code>FactoryProtocol</code></p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.factory.KVStoreFactory.create","title":"<code>create(settings)</code>  <code>classmethod</code>","text":"<p>Factory that creates a KVStore client based on the provided KVStoreSettings. Applies wrappers as defined INSIDE the kv_config.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.providers.redis.client.RedisKVClient","title":"<code>nala.athomic.database.kvstore.providers.redis.client.RedisKVClient</code>","text":"<p>               Bases: <code>BaseKVStore</code>, <code>CredentialResolve</code></p> <p>A concrete KVStoreProvider implementation for Redis using the aioredis library.</p> <p>This class manages the asynchronous connection lifecycle, handles low-level Redis commands (including atomic operations via Lua scripts), and securely resolves connection credentials.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <code>RedisSettings</code> <p>The Redis-specific configuration settings.</p> <code>_client</code> <code>Redis | None</code> <p>The underlying asynchronous Redis client instance.</p> <code>_claim_script_sha</code> <code>Optional[str]</code> <p>The SHA hash of the Lua script used for ZPOPBYSCORE.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.providers.redis.client.RedisKVClient.__init__","title":"<code>__init__(settings=None, serializer=None)</code>","text":"<p>Initializes the RedisKVClient.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[KVStoreSettings]</code> <p>The configuration for this KV store instance.</p> <code>None</code> <code>serializer</code> <code>Optional[SerializerProtocol]</code> <p>The serializer used for values.</p> <code>None</code>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.providers.redis.client.RedisKVClient.get_final_client","title":"<code>get_final_client()</code>  <code>async</code>","text":"<p>Returns the raw asynchronous Redis client instance (Public Contract).</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.providers.redis.client.RedisKVClient.get_sync_client","title":"<code>get_sync_client()</code>","text":"<p>Creates and returns a NEW instance of the synchronous Redis client (Public Contract).</p> <p>This instance is not managed by the provider's lifecycle.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the Redis URI is not available for connection string construction.</p> <code>RuntimeError</code> <p>If the synchronous client creation fails.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.key_resolver_kv_client.KeyResolvingKVClient","title":"<code>nala.athomic.database.kvstore.wrappers.key_resolver_kv_client.KeyResolvingKVClient</code>","text":"<p>               Bases: <code>WrapperBase</code></p> <p>A decorator (Wrapper) for the KVStore client that dynamically resolves keys using a ContextualKeyGenerator.</p> <p>This wrapper applies a contextual prefix (e.g., namespace, tenant ID, user ID) to the logical key provided by the caller before forwarding the operation to the underlying KV store. This is essential for supporting multi-tenancy and avoiding key collisions in shared storage systems.</p> <p>Attributes:</p> Name Type Description <code>resolver</code> <code>ContextualKeyGenerator</code> <p>The utility used to generate context-aware keys.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.key_resolver_kv_client.KeyResolvingKVClient.__init__","title":"<code>__init__(client, settings=None, wrapper_settings=None, context_settings=None)</code>","text":"<p>Initializes the KeyResolvingKVClient and sets up the ContextualKeyGenerator.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>KVStoreProtocol</code> <p>The underlying KVStore client being wrapped.</p> required <code>settings</code> <code>Optional[KVStoreSettings]</code> <p>The global KVStore settings.</p> <code>None</code> <code>wrapper_settings</code> <code>Optional[KeyResolvingWrapperSettings]</code> <p>Specific settings for this wrapper.</p> <code>None</code> <code>context_settings</code> <code>Optional[ContextSettings]</code> <p>Optional settings for the key generator context.</p> <code>None</code>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.key_resolver_kv_client.KeyResolvingKVClient.delete","title":"<code>delete(key)</code>  <code>async</code>","text":"<p>Deletes a key after resolving the contextual key.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.key_resolver_kv_client.KeyResolvingKVClient.exists","title":"<code>exists(key)</code>  <code>async</code>","text":"<p>Checks for key existence after resolving the contextual key.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.key_resolver_kv_client.KeyResolvingKVClient.get","title":"<code>get(key)</code>  <code>async</code>","text":"<p>Retrieves a value after resolving the contextual key.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.key_resolver_kv_client.KeyResolvingKVClient.set","title":"<code>set(key, value, ttl=None, nx=False)</code>  <code>async</code>","text":"<p>Sets a value after resolving the contextual key.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.key_resolver_kv_client.KeyResolvingKVClient.zadd","title":"<code>zadd(key, mapping)</code>  <code>async</code>","text":"<p>Delegates the zadd operation after resolving the contextual key.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.key_resolver_kv_client.KeyResolvingKVClient.zpopbyscore","title":"<code>zpopbyscore(key, max_score)</code>  <code>async</code>","text":"<p>Delegates the zpopbyscore operation after resolving the contextual key.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.key_resolver_kv_client.KeyResolvingKVClient.zrangebyscore","title":"<code>zrangebyscore(key, min_score, max_score)</code>  <code>async</code>","text":"<p>Delegates the zrangebyscore operation after resolving the contextual key.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.key_resolver_kv_client.KeyResolvingKVClient.zrem","title":"<code>zrem(key, members)</code>  <code>async</code>","text":"<p>Delegates the zrem operation after resolving the contextual key.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.default_ttl_kv_client.DefaultTTLKvClient","title":"<code>nala.athomic.database.kvstore.wrappers.default_ttl_kv_client.DefaultTTLKvClient</code>","text":"<p>               Bases: <code>WrapperBase</code></p> <p>A decorator (Wrapper) for the KVStore client that automatically applies a default Time-To-Live (TTL).</p> <p>This class intercepts the <code>set</code> operation and injects a configured default TTL if no explicit TTL is provided by the caller. It ensures that keys are automatically expired, preventing cache overflow and maintaining consistency.</p> <p>Attributes:</p> Name Type Description <code>default_ttl</code> <code>Optional[int]</code> <p>The default TTL in seconds extracted from configuration, or None.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.default_ttl_kv_client.DefaultTTLKvClient.__init__","title":"<code>__init__(client, settings=None, wrapper_settings=None)</code>","text":"<p>Initializes the wrapper, resolving the configured default TTL value.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>KVStoreProtocol</code> <p>The underlying KVStore client being wrapped.</p> required <code>settings</code> <code>Optional[KVStoreSettings]</code> <p>Optional global KVStore settings.</p> <code>None</code> <code>wrapper_settings</code> <code>Optional[DefaultTTLWrapperSettings]</code> <p>Specific settings for this wrapper instance.</p> <code>None</code>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.default_ttl_kv_client.DefaultTTLKvClient.set","title":"<code>set(key, value, ttl=None, nx=False)</code>  <code>async</code>","text":"<p>Sets a key-value pair, applying the default TTL if no explicit TTL is given.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the item to set.</p> required <code>value</code> <code>Any</code> <p>The value to store.</p> required <code>ttl</code> <code>Optional[int]</code> <p>Explicit time-to-live in seconds provided by the caller.</p> <code>None</code> <code>nx</code> <code>bool</code> <p>If True, set the key only if it does not already exist.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the operation was successful.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.default_ttl_kv_client.DefaultTTLKvClient.zadd","title":"<code>zadd(key, mapping)</code>  <code>async</code>","text":"<p>Delegates the zadd operation to the wrapped client.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.default_ttl_kv_client.DefaultTTLKvClient.zpopbyscore","title":"<code>zpopbyscore(key, max_score)</code>  <code>async</code>","text":"<p>Delegates the zpopbyscore operation to the wrapped client.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.default_ttl_kv_client.DefaultTTLKvClient.zrangebyscore","title":"<code>zrangebyscore(key, min_score, max_score)</code>  <code>async</code>","text":"<p>Delegates the zrangebyscore operation to the wrapped client.</p>"},{"location":"athomic/database/kvstore/#nala.athomic.database.kvstore.wrappers.default_ttl_kv_client.DefaultTTLKvClient.zrem","title":"<code>zrem(key, members)</code>  <code>async</code>","text":"<p>Delegates the zrem operation to the wrapped client.</p>"},{"location":"athomic/database/migrations/","title":"Database Migrations","text":""},{"location":"athomic/database/migrations/#overview","title":"Overview","text":"<p>The Database Migrations module provides a programmatic, version-controlled way to manage your database schema and data transformations over time. It is a lightweight but powerful engine designed to be database-agnostic, with a built-in implementation for MongoDB.</p> <p>When the application starts, the <code>MigrationRunner</code> automatically detects and applies any pending migrations, ensuring that your database schema is always in the state required by the current version of the code.</p>"},{"location":"athomic/database/migrations/#how-it-works","title":"How It Works","text":"<p>The migration process is orchestrated by the <code>MigrationRunner</code> service:</p> <ol> <li>Discovery: At startup, the runner inspects the module <code>paths</code> defined in your configuration to find all migration script files (e.g., <code>001_initial.py</code>).</li> <li>State Check: It connects to the database and queries a special history collection (e.g., <code>athomic_migration_status</code>) to determine the revision number of the last migration that was successfully applied.</li> <li>Execution: The runner compares the list of discovered scripts with the database's history. For every script with a <code>revision</code> number greater than the last applied one, it executes the script's <code>upgrade</code> function in sequential order.</li> <li>Recording: After each script is executed successfully, a new record is added to the history collection, marking the new revision as applied.</li> </ol> <p>This entire process is wrapped with observability, providing detailed logs, metrics, and tracing for each migration operation.</p>"},{"location":"athomic/database/migrations/#how-to-create-a-migration-script","title":"How to Create a Migration Script","text":"<p>Creating a new migration is straightforward.</p>"},{"location":"athomic/database/migrations/#1-create-the-file","title":"1. Create the File","text":"<p>In your application's migrations directory (e.g., <code>src/your_app/migrations/versions/</code>), create a new Python file. The filename is important and must follow this format:</p> <p><code>[NNN]_[description].py</code></p> <ul> <li><code>[NNN]</code>: A three-digit, zero-padded revision number (e.g., <code>001</code>, <code>002</code>).</li> <li><code>[description]</code>: A short, descriptive name for the migration (e.g., <code>create_user_indexes</code>).</li> </ul> <p>Example: <code>001_create_user_indexes.py</code></p>"},{"location":"athomic/database/migrations/#2-define-the-script-content","title":"2. Define the Script Content","text":"<p>Each migration script must contain two key elements:</p> <ul> <li>A <code>revision</code> variable holding the integer revision number.</li> <li>An <code>async def upgrade(db)</code> function that receives the native database client instance (for MongoDB, this is a <code>motor.core.AgnosticDatabase</code>).</li> </ul>"},{"location":"athomic/database/migrations/#example-migration-script","title":"Example Migration Script","text":"<p>This example creates a unique index on the <code>users</code> collection.</p> <pre><code># In: src/your_app/migrations/versions/001_create_user_indexes.py\nfrom motor.core import AgnosticDatabase\nfrom pymongo import ASCENDING, IndexModel\n\n# The sequential revision number for this migration.\nrevision = 1\n\nasync def upgrade(db: AgnosticDatabase) -&gt; None:\n    \"\"\"\n    Applies the migration to create a unique index on the 'email' field\n    of the 'users' collection.\n    \"\"\"\n    users_collection = db.get_collection(\"users\")\n\n    email_index = IndexModel(\n        [(\"email\", ASCENDING)],\n        name=\"user_email_unique_idx\",\n        unique=True\n    )\n\n    await users_collection.create_indexes([email_index])\n\n# (Optional) You can also define an `async def downgrade(db)` function\n# to support reverting the migration, although this is not currently used by the runner.\n</code></pre>"},{"location":"athomic/database/migrations/#configuration","title":"Configuration","text":"<p>To enable and configure the migration runner, you need to add a <code>[database.migrations]</code> section to your <code>settings.toml</code>.</p> <pre><code>[default.database.migrations]\n# A master switch to enable or disable the migration engine.\nenabled = true\n\n  # A dictionary of named database connections to run migrations against.\n  [default.database.migrations.connections.my_app_db]\n  # The migration engine to use.\n  engine = \"mongodb\"\n\n  # The name of the connection defined under '[database.documents]' to use.\n  db_connection_name = \"default_mongo\"\n\n  # The collection/table used to store migration history.\n  version_collection = \"my_app_migrations\"\n\n  # A list of Python module paths where your migration scripts are located.\n  # The runner will search these directories for migration files.\n  paths = [\"my_app.migrations.versions\"]\n</code></pre>"},{"location":"athomic/database/migrations/#api-reference","title":"API Reference","text":""},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.runner.MigrationRunner","title":"<code>nala.athomic.database.migrations.runner.MigrationRunner</code>","text":"<p>Orchestrates the database migration process.</p> <p>This class is the core engine for applying database migrations. It is database-agnostic and delegates all database-specific operations (like fetching the current version or executing a script) to a configured backend provider (e.g., <code>MongoDbBackend</code>). Its main responsibility is to discover migration scripts, determine which ones need to be run, and execute them in the correct order for all configured database connections.</p> <p>Attributes:</p> Name Type Description <code>app_settings</code> <p>The root application settings object.</p> <code>migrations_settings</code> <p>The specific configuration for the migration engine.</p> <code>logger</code> <p>A pre-configured logger instance for the runner.</p> <code>tracer</code> <p>An OpenTelemetry tracer instance for the runner.</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.runner.MigrationRunner.__init__","title":"<code>__init__(settings)</code>","text":"<p>Initializes the MigrationRunner.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>AppSettings</code> <p>The application's root configuration object.</p> required"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.runner.MigrationRunner.upgrade","title":"<code>upgrade()</code>  <code>async</code>","text":"<p>Runs the 'upgrade' process for all configured database connections.</p> <p>This is the main entry point for applying migrations. It iterates through each database connection defined in the migration settings and calls <code>_upgrade_connection</code> to process its migrations.</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.protocol.MigrationBackendProtocol","title":"<code>nala.athomic.database.migrations.protocol.MigrationBackendProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the contract for a database-specific migration backend.</p> <p>This allows the MigrationRunner to be agnostic of the underlying database. All I/O operations are designed to be asynchronous.</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.protocol.MigrationBackendProtocol.apply_migration","title":"<code>apply_migration(source_name, revision, path)</code>  <code>async</code>","text":"<p>Loads and executes the 'upgrade' function from a migration script.</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.protocol.MigrationBackendProtocol.connect","title":"<code>connect()</code>  <code>async</code>","text":"<p>Initializes the connection to the database.</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.protocol.MigrationBackendProtocol.disconnect","title":"<code>disconnect()</code>  <code>async</code>","text":"<p>Closes the connection to the database.</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.protocol.MigrationBackendProtocol.get_applied_migrations","title":"<code>get_applied_migrations()</code>  <code>async</code>","text":"<p>Gets the latest applied revision number for each migration source.</p> <p>Returns:</p> Name Type Description <code>Dict[str, int]</code> <p>A dictionary mapping a source_name to its latest revision_number.</p> <code>Example</code> <code>Dict[str, int]</code> <p>{\"athomic\": 2, \"my_app_billing\": 1}</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.protocol.MigrationBackendProtocol.record_migration","title":"<code>record_migration(source_name, revision, description)</code>  <code>async</code>","text":"<p>Records that a migration has been successfully applied.</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.protocol.MigrationBackendProtocol.remove_migration_record","title":"<code>remove_migration_record(source_name, revision)</code>  <code>async</code>","text":"<p>Removes the record of a migration after it has been reverted.</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.protocol.MigrationBackendProtocol.revert_migration","title":"<code>revert_migration(source_name, revision, path)</code>  <code>async</code>","text":"<p>Loads and executes the 'downgrade' function from a migration script.</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.providers.mongodb.MongoDbBackend","title":"<code>nala.athomic.database.migrations.providers.mongodb.MongoDbBackend</code>","text":"<p>               Bases: <code>BaseMigrationBackend</code></p> <p>A migration backend for managing schema changes in a MongoDB database.</p> <p>This class implements the <code>MigrationBackendProtocol</code> for MongoDB, handling database connection retrieval, migration script execution, and version history persistence using MongoDB's aggregation and document operations. It supports dependency injection for testing.</p> <p>Attributes:</p> Name Type Description <code>db_connection_name</code> <code>str</code> <p>The logical name of the database connection to use.</p> <code>connection_manager</code> <code>ConnectionManager</code> <p>The manager for obtaining database providers.</p> <code>db_provider</code> <code>DocumentsDatabaseProtocol</code> <p>The active MongoDB client provider.</p> <code>migration_status_model</code> <code>Type[Document]</code> <p>The Beanie Document model used for storing migration history.</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.providers.mongodb.MongoDbBackend.__init__","title":"<code>__init__(settings, version_target, connection_name, connection_manager=None, db_provider=None, migration_status_model=MigrationStatus)</code>","text":"<p>Initializes the MongoDB Migration Backend, resolving dependencies via factories or injection.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>MigrationConnectionSettings</code> <p>Specific configuration for this migration connection.</p> required <code>version_target</code> <code>str</code> <p>The name of the collection to store version history (deprecated in favor of <code>migration_status_model</code>).</p> required <code>connection_name</code> <code>str</code> <p>The logical name of this migration connection.</p> required <code>connection_manager</code> <code>Optional[ConnectionManager]</code> <p>Optional injected connection manager.</p> <code>None</code> <code>db_provider</code> <code>Optional[DocumentsDatabaseProtocol]</code> <p>Optional injected database provider.</p> <code>None</code> <code>migration_status_model</code> <code>Type[Document]</code> <p>The model used for migration history persistence.</p> <code>MigrationStatus</code>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.providers.mongodb.MongoDbBackend.connect","title":"<code>connect()</code>  <code>async</code>","text":"<p>Ensures the MongoDB provider is connected and ready for migration operations.</p> <p>Raises:</p> Type Description <code>MigrationDatabaseError</code> <p>If the configured database connection cannot be retrieved or fails to connect.</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.providers.mongodb.MongoDbBackend.disconnect","title":"<code>disconnect()</code>  <code>async</code>","text":"<p>Performs a non-operation (no-op) as the MongoDB connection lifecycle is managed by the central ConnectionManager.</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.providers.mongodb.MongoDbBackend.get_applied_migrations","title":"<code>get_applied_migrations()</code>  <code>async</code>","text":"<p>Fetches the latest applied revision number for each migration source from the database.</p> <p>Uses a MongoDB aggregation pipeline to find the maximum <code>revision</code> grouped by <code>source_name</code>.</p> <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>Dict[str, int]: A dictionary mapping each <code>source_name</code> to its latest applied <code>revision</code> number.</p> <p>Raises:</p> Type Description <code>MigrationDatabaseError</code> <p>If the database query fails.</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.providers.mongodb.MongoDbBackend.record_migration","title":"<code>record_migration(source_name, revision, description)</code>  <code>async</code>","text":"<p>Records a successfully applied migration in the version tracking collection.</p> <p>Parameters:</p> Name Type Description Default <code>source_name</code> <code>str</code> <p>The source of the migration (e.g., 'athomic', 'app_billing').</p> required <code>revision</code> <code>int</code> <p>The revision number of the applied migration.</p> required <code>description</code> <code>str</code> <p>A brief description of the migration.</p> required <p>Raises:</p> Type Description <code>MigrationDatabaseError</code> <p>If the record insertion fails.</p>"},{"location":"athomic/database/migrations/#nala.athomic.database.migrations.providers.mongodb.MongoDbBackend.remove_migration_record","title":"<code>remove_migration_record(source_name, revision)</code>  <code>async</code>","text":"<p>Removes the record of a migration after it has been reverted (downgraded).</p> <p>Parameters:</p> Name Type Description Default <code>source_name</code> <code>str</code> <p>The source of the migration.</p> required <code>revision</code> <code>int</code> <p>The revision number to remove.</p> required <p>Raises:</p> Type Description <code>MigrationDatabaseError</code> <p>If the deletion operation fails.</p>"},{"location":"athomic/database/outbox/","title":"Transactional Outbox Pattern","text":""},{"location":"athomic/database/outbox/#overview","title":"Overview","text":"<p>The Transactional Outbox pattern is a critical resilience mechanism that ensures reliable, at-least-once delivery of events in a distributed system. It solves the classic \"dual-write\" problem, where an application needs to both commit a change to its database and publish a corresponding event to a message broker.</p> <p>Without the outbox pattern, if the database commit succeeds but the message broker publish fails, the system becomes inconsistent. The Athomic outbox implementation guarantees that an event is published if and only if the business transaction that created it is successfully committed.</p>"},{"location":"athomic/database/outbox/#how-it-works","title":"How It Works","text":"<p>The pattern is implemented in two distinct parts: saving the event atomically and publishing it reliably.</p>"},{"location":"athomic/database/outbox/#1-saving-the-event-the-write","title":"1. Saving the Event (The \"Write\")","text":"<p>Instead of publishing an event directly to a message broker, the application saves the event record to a dedicated <code>outbox_events</code> collection within the same database and transaction as the business data.</p> <p>The primary way to achieve this is by using the <code>@publish_on_success</code> decorator on your service methods. This decorator wraps your business logic and ensures that the outbox event is only saved to the database if your function completes without raising an exception.</p>"},{"location":"athomic/database/outbox/#example-usage","title":"Example Usage","text":"<pre><code># In your application's service layer\nfrom nala.athomic.database.outbox import OutboxEventType, publish_on_success\n\nclass DocumentService:\n    @publish_on_success(\n        event_name=\"DocumentCreated\",\n        event_type=OutboxEventType.MESSAGING,\n        destination_topic=\"document.events.v1\",\n        # The payload_mapper receives the result of the decorated function\n        payload_mapper=lambda result, **kwargs: result.model_dump()\n    )\n    async def create_document(self, content: str, user_id: str, db_session) -&gt; Document:\n        \"\"\"\n        Creates a document and ensures a 'DocumentCreated' event is reliably published.\n\n        The `db_session` is passed by the repository and used by the decorator\n        to ensure the document and the outbox event are saved in the same transaction.\n        \"\"\"\n        new_doc = Document(content=content, owner_id=user_id)\n        await self.repository.save(new_doc, session=db_session)\n        return new_doc\n</code></pre>"},{"location":"athomic/database/outbox/#2-publishing-the-event-the-poll","title":"2. Publishing the Event (The \"Poll\")","text":"<p>A separate, continuously running background service called the <code>OutboxPublisher</code> polls the <code>outbox_events</code> collection for new, <code>PENDING</code> events.</p> <p>When the publisher finds pending events, it: 1.  Publishes them to the actual message broker (e.g., Kafka). 2.  Upon successful publication, it updates the event's status in the database to <code>PUBLISHED</code>.</p> <p>This polling mechanism ensures that even if the application crashes immediately after the database transaction commits, the event will eventually be found and published when the service restarts.</p>"},{"location":"athomic/database/outbox/#guaranteed-event-ordering-fifo","title":"Guaranteed Event Ordering (FIFO)","text":"<p>The outbox pattern in Athomic also supports guaranteed First-In, First-Out (FIFO) processing for events that belong to the same logical entity (or \"aggregate\"). This is essential for use cases where the order of events matters (e.g., <code>OrderCreated</code>, <code>OrderUpdated</code>, <code>OrderShipped</code>).</p> <p>To enable ordering, you simply need to provide an <code>aggregate_key</code> when creating the event. The <code>MongoOutboxRepository</code> will then atomically generate a <code>sequence_id</code> for each event within that aggregate. The <code>OutboxPublisher</code> is designed to process events for the same aggregate key in strict sequential order.</p>"},{"location":"athomic/database/outbox/#configuration","title":"Configuration","text":"<p>The <code>OutboxPublisher</code> is configured under the <code>[integration.outbox_publisher]</code> section in your <code>settings.toml</code>. Here you can enable the service, define the polling interval, and configure its storage backend.</p> <pre><code>[default.integration.outbox_publisher]\nenabled = true\npoll_interval_seconds = 5.0\nfetch_limit = 100 # Max number of unordered events to fetch per cycle\n\n  [default.integration.outbox_publisher.storage]\n  backend = \"mongo\"\n  max_attempts = 3\n    [default.integration.outbox_publisher.storage.mongo]\n    collection_name = \"outbox_events\"\n    db_connection_name = \"default_mongo\"\n</code></pre>"},{"location":"athomic/database/outbox/#api-reference","title":"API Reference","text":""},{"location":"athomic/database/outbox/#nala.athomic.integration.outbox.decorators.publish_on_success","title":"<code>nala.athomic.integration.outbox.decorators.publish_on_success(event_name, event_type, destination_topic, payload_mapper, key_mapper=None, include_context=True, storage=None)</code>","text":"<p>Decorator implementing the Transactional Outbox pattern. It ensures that an event is saved to the persistent Outbox storage (and thus will be published later) only if the decorated function completes without raising an exception.</p> <p>The storage operation is intended to be executed within the same database transaction as the main function's work.</p> <p>Parameters:</p> Name Type Description Default <code>event_name</code> <code>str</code> <p>A descriptive name for the domain event (e.g., 'DocumentCreated').</p> required <code>event_type</code> <code>OutboxEventType</code> <p>The type of the event (e.g., Command, Event, Query).</p> required <code>destination_topic</code> <code>Optional[str]</code> <p>The messaging topic where the event will eventually be published.</p> required <code>payload_mapper</code> <code>Callable[..., Dict[str, Any]]</code> <p>A callable that accepts the decorated function's result,             args, and kwargs, and returns the event payload (Dict[str, Any]).</p> required <code>key_mapper</code> <code>Optional[Callable[..., Optional[str]]]</code> <p>An optional callable to generate a message key (str) for         partitioning purposes.</p> <code>None</code> <code>include_context</code> <code>Optional[bool]</code> <p>If True, automatically embeds the current ExecutionContext              into the event payload under the '_nala_context' key. Defaults to True.</p> <code>True</code> <code>storage</code> <code>Optional[OutboxStorageProtocol]</code> <p>An optional pre-configured OutboxStorageProtocol instance. If None,      the OutboxStorageFactory is used to create one.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable[P, Coroutine[Any, R, Any]]</code> <p>The decorated asynchronous wrapper function.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.protocol.OutboxStorageProtocol","title":"<code>nala.athomic.database.outbox.protocol.OutboxStorageProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol defining the storage operations for the Outbox pattern.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.protocol.OutboxStorageProtocol.delete_processed_event","title":"<code>delete_processed_event(event_id)</code>  <code>async</code>","text":"<p>(Optional but recommended for cleanup) Deletes an event that has been successfully published or marked as permanently failed (e.g., after reaching max attempts).</p> <p>Parameters:</p> Name Type Description Default <code>event_id</code> <code>UUID</code> <p>The unique ID (UUID) of the event to delete.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the event was found and deleted, False otherwise.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.protocol.OutboxStorageProtocol.get_distinct_pending_aggregate_keys","title":"<code>get_distinct_pending_aggregate_keys()</code>  <code>async</code>","text":"<p>Retrieves a list of unique aggregate_keys that have events in the PENDING state. This is used by the publisher to shard and process aggregates independently.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of unique aggregate key strings.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.protocol.OutboxStorageProtocol.get_hot_aggregates","title":"<code>get_hot_aggregates(top_n=10)</code>  <code>async</code>","text":"<p>Retrieves the top N aggregate keys with the highest number of pending events.</p> <p>Parameters:</p> Name Type Description Default <code>top_n</code> <code>int</code> <p>The number of hot aggregates to retrieve.</p> <code>10</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list of dictionaries, each containing 'aggregate_key' and 'count'.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.protocol.OutboxStorageProtocol.get_pending_events","title":"<code>get_pending_events(limit=100)</code>  <code>async</code>","text":"<p>Retrieves a batch of events currently in the PENDING state, ordered by creation time (oldest first).</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>The maximum number of events to retrieve.</p> <code>100</code> <p>Returns:</p> Type Description <code>List[OutboxEventData]</code> <p>A list of pending OutboxEventData objects.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.protocol.OutboxStorageProtocol.get_pending_events_for_aggregate","title":"<code>get_pending_events_for_aggregate(aggregate_key)</code>  <code>async</code>","text":"<p>Retrieves all PENDING events for a specific aggregate_key, ordered by sequence_id.</p> <p>Parameters:</p> Name Type Description Default <code>aggregate_key</code> <code>str</code> <p>The aggregate key to fetch events for.</p> required <p>Returns:</p> Type Description <code>List[OutboxEventData]</code> <p>A list of pending OutboxEventData objects for the given aggregate, correctly ordered.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.protocol.OutboxStorageProtocol.mark_event_attempted","title":"<code>mark_event_attempted(event_id)</code>  <code>async</code>","text":"<p>Marks an event as attempted by incrementing its attempt counter and updating the last attempt timestamp.</p> <p>Parameters:</p> Name Type Description Default <code>event_id</code> <code>UUID</code> <p>The unique ID (UUID) of the event to mark.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the event was found and marked, False otherwise.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.protocol.OutboxStorageProtocol.mark_event_failed","title":"<code>mark_event_failed(event_id, error_message)</code>  <code>async</code>","text":"<p>Marks an event as failed, increments the attempt counter, and stores the error message.</p> <p>Parameters:</p> Name Type Description Default <code>event_id</code> <code>UUID</code> <p>The unique ID (UUID) of the event.</p> required <code>error_message</code> <code>str</code> <p>A description of the error encountered during publishing.</p> required <p>Returns:</p> Type Description <code>Optional[int]</code> <p>The new attempt count if the event was found and marked, None otherwise.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.protocol.OutboxStorageProtocol.mark_event_published","title":"<code>mark_event_published(event_id)</code>  <code>async</code>","text":"<p>Marks a specific event as successfully published by updating its status.</p> <p>Parameters:</p> Name Type Description Default <code>event_id</code> <code>UUID</code> <p>The unique ID (UUID) of the event to mark.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the event was found and marked, False otherwise.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.protocol.OutboxStorageProtocol.save_event","title":"<code>save_event(event, db_session=None)</code>  <code>async</code>","text":"<p>Saves an event to the outbox storage within the provided database session/transaction. Supports agnostic usage (messaging, webhooks, tasks...).</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>OutboxEventData</code> <p>An OutboxEventData object containing the event details.</p> required <p>Returns:</p> Type Description <code>OutboxEventData</code> <p>The saved OutboxEventData object.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.domain_models.OutboxEventData","title":"<code>nala.athomic.database.outbox.domain_models.OutboxEventData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A pure domain model for an outbox event.</p> <p>This Pydantic model represents a single event to be processed reliably. It is designed to be independent of any specific database or ORM, containing all necessary data for the Outbox Publisher to process it. It supports both unordered events and ordered (FIFO) events via the <code>aggregate_key</code> and <code>sequence_id</code> fields.</p> <p>Attributes:</p> Name Type Description <code>event_id</code> <code>UUID</code> <p>The unique identifier for this specific event instance.</p> <code>event_name</code> <code>str</code> <p>A human-readable name identifying the event (e.g., \"user.created\").</p> <code>event_type</code> <code>Optional[OutboxEventType]</code> <p>The category of the event, determining how it's handled.</p> <code>payload</code> <code>Dict[str, Any]</code> <p>The actual data or body of the event.</p> <code>status</code> <code>OutboxEventStatus</code> <p>The current processing status of the event.</p> <code>destination_topic</code> <code>Optional[str]</code> <p>The target topic or queue for 'messaging' type events.</p> <code>message_key</code> <code>Optional[str]</code> <p>The partitioning key for 'messaging' type events.</p> <code>aggregate_key</code> <code>Optional[str]</code> <p>The identifier used to group events to ensure ordered processing. All events with the same <code>aggregate_key</code> are processed in strict FIFO order.</p> <code>sequence_id</code> <code>Optional[int]</code> <p>A monotonically increasing number for events within the same <code>aggregate_key</code> to enforce ordering.</p> <code>attempts</code> <code>int</code> <p>The number of times processing has been attempted for this event.</p> <code>last_attempt_at</code> <code>Optional[datetime]</code> <p>The timestamp of the last processing attempt.</p> <code>last_error</code> <code>Optional[str]</code> <p>The error message from the last failed attempt.</p> <code>created_at</code> <code>datetime</code> <p>The UTC timestamp of when the event was created.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.mongo.mongo_outbox_repository.MongoOutboxRepository","title":"<code>nala.athomic.database.outbox.mongo.mongo_outbox_repository.MongoOutboxRepository</code>","text":"<p>               Bases: <code>OutboxStorageProtocol</code></p> <p>A MongoDB-backed repository for storing and managing outbox events.</p> <p>This class provides a concrete implementation of the <code>OutboxStorageProtocol</code> using MongoDB as the persistence layer. It leverages <code>beanie</code> for object-document mapping and uses atomic operations and aggregation pipelines for efficient and reliable event handling.</p> <p>Attributes:</p> Name Type Description <code>storage_settings</code> <p>Configuration specific to the outbox storage.</p> <code>db_provider</code> <p>The database client provider for MongoDB connections.</p> <code>logger</code> <p>A pre-configured logger instance.</p> <code>service_name</code> <p>The identifier for this service in observability systems.</p> <code>tracer</code> <p>An OpenTelemetry tracer instance.</p> <code>sequence_generator</code> <p>A component for generating sequential IDs for ordered events.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.mongo.mongo_outbox_repository.MongoOutboxRepository.__init__","title":"<code>__init__(storage_settings=None, db_provider=None, sequence_generator=None)</code>","text":"<p>Initializes the MongoOutboxRepository.</p> <p>Parameters:</p> Name Type Description Default <code>storage_settings</code> <code>Optional[OutboxStorageSettings]</code> <p>Configuration for the outbox storage layer.</p> <code>None</code> <code>db_provider</code> <code>Optional[DatabaseClientProtocol]</code> <p>A required database client provider instance.</p> <code>None</code> <code>sequence_generator</code> <code>Optional[SequenceGeneratorProtocol]</code> <p>An optional custom sequence generator.</p> <code>None</code>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.mongo.mongo_outbox_repository.MongoOutboxRepository.delete_processed_event","title":"<code>delete_processed_event(event_id)</code>  <code>async</code>","text":"<p>Deletes a processed event from the collection for cleanup purposes.</p> <p>Parameters:</p> Name Type Description Default <code>event_id</code> <code>UUID</code> <p>The unique ID of the event to delete.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the event was found and deleted, False otherwise.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.mongo.mongo_outbox_repository.MongoOutboxRepository.get_distinct_pending_aggregate_keys","title":"<code>get_distinct_pending_aggregate_keys()</code>  <code>async</code>","text":"<p>Finds all unique aggregate keys that have pending events.</p> <p>Uses MongoDB's <code>distinct</code> command to efficiently get a list of all <code>aggregate_key</code> values that have events in the 'PENDING' state. This is used by the publisher to shard the processing workload.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of unique aggregate key strings.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.mongo.mongo_outbox_repository.MongoOutboxRepository.get_hot_aggregates","title":"<code>get_hot_aggregates(top_n=10)</code>  <code>async</code>","text":"<p>Finds the top N aggregate keys with the most pending events.</p> <p>This method executes a MongoDB aggregation pipeline to group events by <code>aggregate_key</code>, count them, sort by the count, and return the top results. This is primarily used for monitoring to identify potential bottlenecks.</p> <p>Parameters:</p> Name Type Description Default <code>top_n</code> <code>int</code> <p>The number of hot aggregates to retrieve.</p> <code>10</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list of dictionaries, each containing 'aggregate_key' and 'count'.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.mongo.mongo_outbox_repository.MongoOutboxRepository.get_pending_events","title":"<code>get_pending_events(limit=100)</code>  <code>async</code>","text":"<p>Retrieves a batch of pending, unordered events.</p> <p>This method queries MongoDB for events that are in the 'PENDING' state and do not have an <code>aggregate_key</code>, sorted by their creation time to ensure older events are processed first.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>The maximum number of events to retrieve.</p> <code>100</code> <p>Returns:</p> Type Description <code>List[OutboxEventData]</code> <p>A list of pending <code>OutboxEventData</code> objects.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.mongo.mongo_outbox_repository.MongoOutboxRepository.get_pending_events_for_aggregate","title":"<code>get_pending_events_for_aggregate(aggregate_key)</code>  <code>async</code>","text":"<p>Retrieves all pending events for a specific aggregate, sorted by sequence.</p> <p>Parameters:</p> Name Type Description Default <code>aggregate_key</code> <code>str</code> <p>The aggregate key to fetch events for.</p> required <p>Returns:</p> Type Description <code>List[OutboxEventData]</code> <p>A list of pending <code>OutboxEventData</code> objects for the given aggregate.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.mongo.mongo_outbox_repository.MongoOutboxRepository.mark_event_attempted","title":"<code>mark_event_attempted(event_id)</code>  <code>async</code>","text":"<p>Increments an event's attempt counter and updates its last attempt timestamp.</p> <p>Parameters:</p> Name Type Description Default <code>event_id</code> <code>UUID</code> <p>The unique ID of the event to update.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the event was found and updated, False otherwise.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.mongo.mongo_outbox_repository.MongoOutboxRepository.mark_event_failed","title":"<code>mark_event_failed(event_id, exception)</code>  <code>async</code>","text":"<p>Marks an event as failed, storing the error and updating its status.</p> <p>This method updates the event with the error message. It checks if the exception is non-retriable or if the maximum attempt count has been reached to decide whether to move the event to the final 'FAILED' state or keep it as 'PENDING' for a future retry.</p> <p>Parameters:</p> Name Type Description Default <code>event_id</code> <code>UUID</code> <p>The unique ID of the event.</p> required <code>exception</code> <code>Exception</code> <p>The exception that caused the failure.</p> required <p>Returns:</p> Type Description <code>Optional[int]</code> <p>The new attempt count if the event was found and marked, None otherwise.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.mongo.mongo_outbox_repository.MongoOutboxRepository.mark_event_published","title":"<code>mark_event_published(event_id)</code>  <code>async</code>","text":"<p>Marks a specific event's status as PUBLISHED.</p> <p>Parameters:</p> Name Type Description Default <code>event_id</code> <code>UUID</code> <p>The unique ID of the event to mark as published.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the event was found and updated, False otherwise.</p>"},{"location":"athomic/database/outbox/#nala.athomic.database.outbox.mongo.mongo_outbox_repository.MongoOutboxRepository.save_event","title":"<code>save_event(event, db_session=None)</code>  <code>async</code>","text":"<p>Saves an event to the outbox collection within a transaction.</p> <p>If the event includes an <code>aggregate_key</code>, this method first atomically generates a new <code>sequence_id</code> before inserting the event document into the database.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>OutboxEventData</code> <p>The domain model of the event to save.</p> required <code>db_session</code> <code>Optional[ClientSession]</code> <p>An optional MongoDB client session for transactional operations.</p> <code>None</code> <p>Returns:</p> Type Description <code>OutboxEventData</code> <p>The saved event, now potentially populated with a <code>sequence_id</code>.</p>"},{"location":"athomic/integration/config_providers/","title":"Dynamic Configuration Providers","text":""},{"location":"athomic/integration/config_providers/#overview","title":"Overview","text":"<p>The Dynamic Configuration Providers module contains the client-side logic for fetching configuration values from external, centralized sources like HashiCorp Consul's Key-Value (KV) store. This is the engine that powers the Live Configuration feature.</p>"},{"location":"athomic/integration/config_providers/#key-features","title":"Key Features","text":"<ul> <li>Provider-Based: Built on a protocol, allowing for different backends to be supported.</li> <li>Resilient Fallback: Can be configured with a chain of providers. If the primary provider (e.g., Consul) is unavailable, it can fall back to a secondary one.</li> <li>Long Polling: Efficiently watches for changes using long-polling HTTP requests, enabling near real-time configuration updates.</li> </ul>"},{"location":"athomic/integration/config_providers/#how-it-works","title":"How It Works","text":"<p>The <code>ConfigurationProviderFactory</code> creates a singleton instance of the configured provider. The primary implementation, <code>ConsulConfigProvider</code>, uses the shared <code>ConsulClient</code> to interact with the Consul KV API.</p> <p>This provider is primarily consumed by the <code>ConfigWatcherService</code>, which uses its <code>watch_prefix</code> method to monitor for changes and publish update events to the internal event bus.</p>"},{"location":"athomic/integration/config_providers/#api-reference","title":"API Reference","text":""},{"location":"athomic/integration/config_providers/#nala.athomic.integration.config_providers.protocol.ConfigProvidersProtocol","title":"<code>nala.athomic.integration.config_providers.protocol.ConfigProvidersProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the contract for any remote/dynamic configuration provider.</p> <p>Implementations will handle fetching configuration values from external sources like Consul, Vault, or other configuration services.</p>"},{"location":"athomic/integration/config_providers/#nala.athomic.integration.config_providers.protocol.ConfigProvidersProtocol.get","title":"<code>get(key, default=None)</code>  <code>async</code>","text":"<p>Fetches a configuration value for a given key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The identifier of the configuration value to fetch.</p> required <code>default</code> <code>Optional[Any]</code> <p>The default value to return if the key is not found      or if the provider is unavailable.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The configuration value, which could be a string, number, dict, or list.</p>"},{"location":"athomic/integration/config_providers/#nala.athomic.integration.config_providers.protocol.ConfigProvidersProtocol.watch","title":"<code>watch(key, last_known_index=0)</code>  <code>async</code>","text":"<p>Watches a key for changes using long polling.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to watch.</p> required <code>last_known_index</code> <code>int</code> <p>The last known 'ModifyIndex' from Consul for this key.</p> <code>0</code> <p>Returns:</p> Type Description <code>int</code> <p>A tuple containing (new_index, new_value).</p> <code>Any</code> <p>The call will block until there is a change or a timeout occurs.</p>"},{"location":"athomic/integration/config_providers/#nala.athomic.integration.config_providers.protocol.ConfigProvidersProtocol.watch_prefix","title":"<code>watch_prefix(prefix, last_known_index=0)</code>  <code>async</code>","text":"<p>Watches a key prefix for any changes recursively using long polling.</p> <p>Returns:</p> Type Description <code>int</code> <p>A tuple containing (new_index, list_of_changed_items).</p> <code>Optional[List[Dict[str, Any]]]</code> <p>Each item in the list is a dictionary like {'Key': ..., 'Value': ...}.</p>"},{"location":"athomic/integration/config_providers/#nala.athomic.integration.config_providers.factory.ConfigurationProviderFactory","title":"<code>nala.athomic.integration.config_providers.factory.ConfigurationProviderFactory</code>","text":"<p>Factory for the singleton instance of the dynamic configuration provider.</p> <p>This factory is responsible for building and providing a singleton instance of the <code>ConfigProvidersProtocol</code>. It intelligently assembles the provider based on the application's configuration. If a single provider is configured, it creates a direct instance. If multiple providers are configured, it creates a <code>FallbackConfigProvider</code> that wraps them in a resilient chain.</p>"},{"location":"athomic/integration/config_providers/#nala.athomic.integration.config_providers.factory.ConfigurationProviderFactory.clear","title":"<code>clear()</code>  <code>classmethod</code>","text":"<p>Clears the cached singleton instance. Useful for tests.</p>"},{"location":"athomic/integration/config_providers/#nala.athomic.integration.config_providers.factory.ConfigurationProviderFactory.create","title":"<code>create(settings=None)</code>  <code>classmethod</code>","text":"<p>Creates or retrieves the singleton instance of the configuration provider.</p> <p>This method orchestrates the creation of the dynamic configuration provider. It reads the settings, determines if a single provider or a fallback chain is needed, and uses the <code>config_provider_registry</code> to instantiate the appropriate provider class(es). The created instance is cached for subsequent calls.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[ConfigurationSettings]</code> <p>The configuration for the dynamic providers. If None, global settings are used.</p> <code>None</code> <p>Returns:</p> Type Description <code>ConfigProvidersProtocol</code> <p>A fully configured configuration provider instance, which may be</p> <code>ConfigProvidersProtocol</code> <p>a single provider or a <code>FallbackConfigProvider</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no providers are configured or if a configured backend name is not found in the registry.</p>"},{"location":"athomic/integration/config_providers/#nala.athomic.integration.config_providers.providers.consul_provider.ConsulConfigProvider","title":"<code>nala.athomic.integration.config_providers.providers.consul_provider.ConsulConfigProvider</code>","text":"<p>               Bases: <code>ConfigProvidersProtocol</code></p> <p>Fetches configuration values from Consul's Key-Value store.</p> <p>This class implements the <code>ConfigProvidersProtocol</code> to provide dynamic configuration values from a HashiCorp Consul KV backend. It uses the application's central, lifecycle-managed <code>ConsulClient</code> for all interactions with the Consul agent. It supports fetching single keys and long-polling for changes on keys or prefixes.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <code>ConsulSettings</code> <p>The configuration for connecting to the Consul agent.</p>"},{"location":"athomic/integration/config_providers/#nala.athomic.integration.config_providers.providers.consul_provider.ConsulConfigProvider.consul_client","title":"<code>consul_client</code>  <code>property</code>","text":"<p>Lazily resolves and returns the shared ConsulClient instance.</p> <p>This ensures that the client is only fetched when needed and that it's the same instance managed by the application's lifecycle.</p> <p>Returns:</p> Type Description <code>ConsulClient</code> <p>The shared, application-wide Consul client instance.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If a valid <code>ConsulClient</code> cannot be retrieved.</p>"},{"location":"athomic/integration/config_providers/#nala.athomic.integration.config_providers.providers.consul_provider.ConsulConfigProvider.__init__","title":"<code>__init__(settings=None, consul_client=None)</code>","text":"<p>Initializes the ConsulConfigProvider.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[ConsulSettings]</code> <p>The Consul connection settings.</p> <code>None</code> <code>consul_client</code> <code>Optional[ConsulClient]</code> <p>An optional, pre-initialized Consul client, primarily for testing and dependency injection.</p> <code>None</code>"},{"location":"athomic/integration/config_providers/#nala.athomic.integration.config_providers.providers.consul_provider.ConsulConfigProvider.get","title":"<code>get(key, default=None)</code>  <code>async</code>","text":"<p>Fetches a configuration value for a given key from Consul.</p> <p>It safely handles connection issues by returning the default value. It attempts to parse the retrieved value as JSON, falling back to a plain string if parsing fails.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The identifier of the configuration value to fetch.</p> required <code>default</code> <code>Optional[Any]</code> <p>The value to return if the key is not found or if the provider is unavailable.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The parsed configuration value or the provided default.</p>"},{"location":"athomic/integration/config_providers/#nala.athomic.integration.config_providers.providers.consul_provider.ConsulConfigProvider.watch","title":"<code>watch(key, last_known_index=0)</code>  <code>async</code>","text":"<p>Watches a single key in Consul for changes using long-polling.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to watch.</p> required <code>last_known_index</code> <code>int</code> <p>The last <code>ModifyIndex</code> returned by Consul for this key. The request will block until the index changes.</p> <code>0</code> <p>Returns:</p> Type Description <code>int</code> <p>A tuple containing the new index and the new value. The value is</p> <code>Any</code> <p>None if the key was deleted or not found.</p>"},{"location":"athomic/integration/config_providers/#nala.athomic.integration.config_providers.providers.consul_provider.ConsulConfigProvider.watch_prefix","title":"<code>watch_prefix(prefix, last_known_index=0)</code>  <code>async</code>","text":"<p>Watches a key prefix recursively in Consul for changes using long-polling.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>The key prefix (folder) to watch.</p> required <code>last_known_index</code> <code>int</code> <p>The last <code>ModifyIndex</code> returned by Consul for this prefix. The request will block until the index changes.</p> <code>0</code> <p>Returns:</p> Type Description <code>int</code> <p>A tuple containing the new index and a list of all key-value</p> <code>Optional[List[Dict[str, Any]]]</code> <p>data under the prefix.</p>"},{"location":"athomic/integration/consul/","title":"Consul Client","text":""},{"location":"athomic/integration/consul/#overview","title":"Overview","text":"<p>The Consul module provides a centralized, lifecycle-managed client for interacting with a HashiCorp Consul agent. This shared client is a fundamental dependency for several other modules that rely on Consul for their operation, including:</p> <ul> <li>Service Discovery</li> <li>Dynamic Configuration Providers</li> <li>Feature Flags (when using the Consul backend)</li> </ul>"},{"location":"athomic/integration/consul/#how-it-works","title":"How It Works","text":"<p>The <code>ConsulClient</code> is a <code>BaseService</code> that wraps the <code>consul.aio.Consul</code> library. The <code>ConsulClientFactory</code> creates a singleton instance of this client at startup, managing its connection and readiness state.</p> <p>By using a shared singleton, all other modules are guaranteed to use the same connection pool and configuration, making the system efficient and consistent. The client's lifecycle is managed by the main <code>LifecycleManager</code>, ensuring it connects at startup and disconnects gracefully.</p>"},{"location":"athomic/integration/consul/#api-reference","title":"API Reference","text":""},{"location":"athomic/integration/consul/#nala.athomic.integration.consul.client.ConsulClient","title":"<code>nala.athomic.integration.consul.client.ConsulClient</code>","text":"<p>               Bases: <code>BaseService</code></p> <p>An asynchronous, lifecycle-managed client for interacting with Consul.</p> <p>This class wraps the <code>consul.aio.Consul</code> client, integrating it into the Athomic service lifecycle. It handles connection setup, readiness checks by verifying a cluster leader, and graceful shutdown. This centralized client is intended to be used as a singleton by other Athomic components that need to interact with Consul, such as service discovery and dynamic configuration providers.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <code>ConsulSettings</code> <p>The configuration for connecting to the Consul agent.</p> <code>client</code> <code>Optional[Consul]</code> <p>The underlying <code>consul.aio.Consul</code> client instance, available after a successful connection.</p>"},{"location":"athomic/integration/consul/#nala.athomic.integration.consul.client.ConsulClient.__init__","title":"<code>__init__(settings)</code>","text":"<p>Initializes the ConsulClient.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>ConsulSettings</code> <p>The Pydantic model containing Consul connection details.</p> required"},{"location":"athomic/integration/consul/#nala.athomic.integration.consul.factory.ConsulClientFactory","title":"<code>nala.athomic.integration.consul.factory.ConsulClientFactory</code>","text":"<p>A singleton factory to create and provide the <code>ConsulClient</code> instance.</p> <p>This factory manages a single, shared instance of the <code>ConsulClient</code>. It ensures that the entire application uses the same client, preventing redundant connections and centralizing its creation logic.</p>"},{"location":"athomic/integration/consul/#nala.athomic.integration.consul.factory.ConsulClientFactory.clear","title":"<code>clear()</code>  <code>classmethod</code>","text":"<p>Clears the cached singleton instance.</p> <p>This method resets the factory, allowing a new <code>ConsulClient</code> instance to be created on the next <code>create()</code> call. This is essential for ensuring test isolation.</p>"},{"location":"athomic/integration/consul/#nala.athomic.integration.consul.factory.ConsulClientFactory.create","title":"<code>create(settings=None)</code>  <code>classmethod</code>","text":"<p>Creates or retrieves the singleton instance of <code>ConsulClient</code>.</p> <p>On the first call, this method instantiates the client using global or provided settings. Subsequent calls return the cached instance, ensuring a single connection manager is used throughout the application's lifecycle.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[ConsulSettings]</code> <p>Optional Consul settings to use for instantiation, primarily for dependency injection during testing. If None, global settings are used.</p> <code>None</code> <p>Returns:</p> Type Description <code>ConsulClient</code> <p>The singleton <code>ConsulClient</code> instance.</p>"},{"location":"athomic/integration/discovery/","title":"Service Discovery","text":""},{"location":"athomic/integration/discovery/#overview","title":"Overview","text":"<p>The Service Discovery module provides a mechanism for services to find and communicate with each other in a dynamic, distributed environment. Instead of hardcoding IP addresses or hostnames, services register themselves with a central registry (like HashiCorp Consul) and query it to discover the locations of other services.</p> <p>This is a fundamental pattern for building scalable and resilient microservice architectures. Athomic uses this module primarily for the Distributed Workload Sharding pattern.</p>"},{"location":"athomic/integration/discovery/#how-it-works","title":"How It Works","text":"<p>The <code>ServiceDiscoveryProtocol</code> defines the contract for any provider, with <code>register</code>, <code>deregister</code>, and <code>discover</code> methods. The <code>ConsulDiscoveryProvider</code> is the default implementation.</p> <p>At startup, a service instance calls <code>discovery_service.register_self()</code>, which registers its service name, instance ID, and address with Consul. On shutdown, it calls <code>deregister_self()</code>. Other services can then call <code>discovery_service.discover(\"service-name\")</code> to get a list of all healthy, active instances of that service.</p>"},{"location":"athomic/integration/discovery/#api-reference","title":"API Reference","text":""},{"location":"athomic/integration/discovery/#nala.athomic.integration.discovery.protocol.ServiceDiscoveryProtocol","title":"<code>nala.athomic.integration.discovery.protocol.ServiceDiscoveryProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the abstract contract for any service discovery provider.</p> <p>This protocol adheres to the Dependency Inversion Principle (DIP), decoupling service consumers (like the Sharding or HTTP Client modules) from the concrete implementation (e.g., Consul, ZooKeeper).</p>"},{"location":"athomic/integration/discovery/#nala.athomic.integration.discovery.protocol.ServiceDiscoveryProtocol.deregister","title":"<code>deregister(service_id)</code>  <code>async</code>","text":"<p>Removes this service instance from the discovery provider.</p> <p>Parameters:</p> Name Type Description Default <code>service_id</code> <code>str</code> <p>The unique ID of the service instance to deregister.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if deregistration was successful.</p>"},{"location":"athomic/integration/discovery/#nala.athomic.integration.discovery.protocol.ServiceDiscoveryProtocol.discover","title":"<code>discover(name)</code>  <code>async</code>","text":"<p>Discovers all healthy instances of a service by its logical name.</p> <p>The provider should only return services that have a passing health check.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The logical name of the service to discover.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries, where each dictionary                    represents a healthy service instance (containing address, port, etc.).</p>"},{"location":"athomic/integration/discovery/#nala.athomic.integration.discovery.protocol.ServiceDiscoveryProtocol.register","title":"<code>register(name, service_id, port, address=None, tags=None, health_check=None)</code>  <code>async</code>","text":"<p>Registers the current service instance with the discovery provider.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The logical name of the service (e.g., \"user-service\").</p> required <code>service_id</code> <code>str</code> <p>A unique ID for this specific instance of the service.</p> required <code>port</code> <code>int</code> <p>The port on which the service is running.</p> required <code>address</code> <code>Optional[str]</code> <p>The address of the service. If None, the agent's address is used.</p> <code>None</code> <code>tags</code> <code>Optional[List[str]]</code> <p>A list of tags to associate with the service.</p> <code>None</code> <code>health_check</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary defining a health check (e.g., HTTP, TCP, TTL).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if registration was successful, False otherwise.</p>"},{"location":"athomic/integration/discovery/#nala.athomic.integration.discovery.providers.consul_provider.ConsulDiscoveryProvider","title":"<code>nala.athomic.integration.discovery.providers.consul_provider.ConsulDiscoveryProvider</code>","text":"<p>               Bases: <code>ServiceDiscoveryProtocol</code></p> <p>Service Discovery implementation using HashiCorp Consul as the backend.</p> <p>This provider fulfills the <code>ServiceDiscoveryProtocol</code> contract by delegating I/O operations (register, deregister, discover) to the central, lifecycle-managed <code>ConsulClient</code> instance. All operations are wrapped for resilience via <code>@cancellable_operation</code>.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <code>DiscoverySettings</code> <p>The service discovery configuration.</p> <code>consul_client</code> <code>ConsulClient</code> <p>The shared, active Consul client instance.</p>"},{"location":"athomic/integration/discovery/#nala.athomic.integration.discovery.providers.consul_provider.ConsulDiscoveryProvider.__init__","title":"<code>__init__(settings=None, consul_client=None)</code>","text":"<p>Initializes the provider, resolving the shared ConsulClient dependency.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[DiscoverySettings]</code> <p>Configuration for the discovery module.</p> <code>None</code> <code>consul_client</code> <code>Optional[ConsulClient]</code> <p>Optional injected Consul client instance for testing.</p> <code>None</code>"},{"location":"athomic/integration/discovery/#nala.athomic.integration.discovery.providers.consul_provider.ConsulDiscoveryProvider.deregister","title":"<code>deregister(service_id)</code>  <code>async</code>","text":"<p>Removes the service instance from the Consul agent.</p> <p>Parameters:</p> Name Type Description Default <code>service_id</code> <code>str</code> <p>The unique ID of the service instance to deregister.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if deregistration was successful.</p>"},{"location":"athomic/integration/discovery/#nala.athomic.integration.discovery.providers.consul_provider.ConsulDiscoveryProvider.discover","title":"<code>discover(name)</code>  <code>async</code>","text":"<p>Discovers all currently healthy instances of a service by its logical name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The logical name of the service to discover.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries, each representing a healthy service instance.</p>"},{"location":"athomic/integration/discovery/#nala.athomic.integration.discovery.providers.consul_provider.ConsulDiscoveryProvider.register","title":"<code>register(name, service_id, port, address=None, tags=None, health_check=None)</code>  <code>async</code>","text":"<p>Registers the current service instance with the Consul agent.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The logical name of the service (e.g., \"user-service\").</p> required <code>service_id</code> <code>str</code> <p>A unique ID for this specific service instance.</p> required <code>port</code> <code>int</code> <p>The port on which the service is running.</p> required <code>address</code> <code>Optional[str]</code> <p>The routable address of the service.</p> <code>None</code> <code>tags</code> <code>Optional[List[str]]</code> <p>A list of tags for categorization.</p> <code>None</code> <code>health_check</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary defining a health check.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if registration was successful, False otherwise.</p>"},{"location":"athomic/integration/openlineage/","title":"OpenLineage Integration","text":""},{"location":"athomic/integration/openlineage/#overview","title":"Overview","text":"<p>The OpenLineage integration module provides a concrete storage backend for the Data Lineage system. It is responsible for sending structured lineage events to an OpenLineage-compatible HTTP API, such as Marquez.</p>"},{"location":"athomic/integration/openlineage/#how-it-works","title":"How It Works","text":"<p>The <code>OpenLineageStore</code> implements the <code>LineageStorageProtocol</code>. When the <code>LineageProcessor</code> has constructed a valid OpenLineage event, it passes it to this store's <code>save</code> method.</p> <p>The store then uses a pre-configured, resilient HTTP Client to send the event as a <code>POST</code> request to the configured OpenLineage collector endpoint. The operation is designed to be fail-safe; if the API call fails, the error is logged, but it will not interrupt the main application flow.</p>"},{"location":"athomic/integration/openlineage/#api-reference","title":"API Reference","text":""},{"location":"athomic/integration/openlineage/#nala.athomic.integration.openlineage.store.OpenLineageStore","title":"<code>nala.athomic.integration.openlineage.store.OpenLineageStore</code>","text":"<p>               Bases: <code>LineageStorageProtocol</code></p> <p>A concrete implementation of the LineageStorageProtocol that publishes lineage events to an external OpenLineage-compatible HTTP API (e.g., Marquez).</p> <p>It leverages Athomic's internal HttpClientFactory to ensure the connection inherits global network policies, resilience, and observability features.</p>"},{"location":"athomic/integration/openlineage/#nala.athomic.integration.openlineage.store.OpenLineageStore.__init__","title":"<code>__init__(settings)</code>","text":"<p>Initializes the store by fetching global client configuration and creating the underlying HTTP client.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>OpenLineageStoreSettings</code> <p>Instance-specific settings for the OpenLineage store,       including endpoint overrides.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the required global configuration block         ('integration.openlineage') is missing.</p>"},{"location":"athomic/integration/openlineage/#nala.athomic.integration.openlineage.store.OpenLineageStore.save","title":"<code>save(event_payload)</code>  <code>async</code>","text":"<p>Sends the pre-formatted lineage event payload to the configured OpenLineage API endpoint via an HTTP POST request.</p> <p>If the HTTP request fails, the error is logged but deliberately not re-raised (swallowed) to prevent lineage errors from blocking the main application flow.</p> <p>Parameters:</p> Name Type Description Default <code>event_payload</code> <code>Dict</code> <p>The fully structured lineage event data dictionary.</p> required"},{"location":"athomic/integration/outbox_publisher/","title":"Outbox Publisher","text":""},{"location":"athomic/integration/outbox_publisher/#overview","title":"Overview","text":"<p>The Outbox Publisher is the second half of the Transactional Outbox Pattern. It is a highly resilient, distributed background service responsible for polling the outbox database table, publishing the events to the message broker, and marking them as processed.</p>"},{"location":"athomic/integration/outbox_publisher/#key-features","title":"Key Features","text":"<ul> <li>Guaranteed Delivery: Continuously polls for new events to ensure they are eventually published.</li> <li>Horizontally Scalable: You can run multiple instances of the publisher service to increase throughput.</li> <li>Distributed Coordination: Leverages other Athomic resilience patterns to coordinate work between instances:<ul> <li>Sharding: To distribute the processing of different <code>aggregate_key</code>s across all active workers.</li> <li>Leasing: To ensure only one worker is processing events for a specific <code>aggregate_key</code> at a time, guaranteeing order.</li> <li>Backpressure: To temporarily stop polling for a key that is causing repeated publishing failures.</li> <li>Backoff: To reduce polling frequency when no events are found.</li> </ul> </li> </ul>"},{"location":"athomic/integration/outbox_publisher/#how-it-works","title":"How It Works","text":"<p>The <code>OutboxPublisher</code> is a <code>BaseService</code> managed by the application's <code>LifecycleManager</code>. In its main run loop, it performs the following steps: 1.  Queries the database for all distinct <code>aggregate_key</code>s with pending events. 2.  Uses the <code>ShardingService</code> to filter that list down to only the keys this specific instance is responsible for. 3.  Uses the <code>BackpressureManager</code> to further filter out any keys that are temporarily throttled. 4.  For each remaining key, it acquires a lease and then processes all pending events for that key in strict sequential order. 5.  It also polls for and processes any unordered (non-aggregate) events.</p>"},{"location":"athomic/integration/outbox_publisher/#api-reference","title":"API Reference","text":""},{"location":"athomic/integration/outbox_publisher/#nala.athomic.integration.outbox.publishers.base_publisher.OutboxPublisherBase","title":"<code>nala.athomic.integration.outbox.publishers.base_publisher.OutboxPublisherBase</code>","text":"<p>               Bases: <code>BaseService</code></p> <p>Base implementation for the Outbox Publisher (Poller) Service.</p> <p>This service is a high-resilience component that polls the Outbox storage, applies sharding, backpressure, and leasing mechanisms to ensure atomic, ordered, and distributed processing of events. It inherits lifecycle management from BaseService.</p>"},{"location":"athomic/integration/outbox_publisher/#nala.athomic.integration.outbox.publishers.base_publisher.OutboxPublisherBase.__init__","title":"<code>__init__(storage=None, router=None, settings=None, lease_manager=None, sharding_manager=None, backpressure_manager=None)</code>","text":"<p>Initializes the publisher, establishing all necessary resilience and data access dependencies, prioritizing explicit DI over factory creation.</p>"},{"location":"athomic/integration/tasks/","title":"Background Tasks","text":""},{"location":"athomic/integration/tasks/#overview","title":"Overview","text":"<p>The Tasks module provides a high-level abstraction for enqueuing and executing background tasks asynchronously. It decouples the function call from its execution, allowing you to offload long-running or non-critical operations to a separate pool of workers.</p>"},{"location":"athomic/integration/tasks/#key-features","title":"Key Features","text":"<ul> <li>Simple Decorator API: Convert any <code>async</code> function into a background task with the <code>@task</code> decorator.</li> <li>Multiple Backends: Supports <code>Taskiq</code> and <code>RQ</code> (Redis Queue) as distributed backends, as well as a local in-memory backend for testing.</li> <li>Automatic Context Propagation: The <code>ExecutionContext</code> (<code>trace_id</code>, <code>tenant_id</code>, etc.) from the caller is automatically captured and restored on the worker, ensuring seamless tracing and multi-tenancy.</li> <li>Resilient Fallback Chain: Configure a chain of providers (e.g., try <code>Taskiq</code> first, fall back to <code>RQ</code> if it's down) for high availability.</li> </ul>"},{"location":"athomic/integration/tasks/#how-to-define-and-enqueue-a-task","title":"How to Define and Enqueue a Task","text":"<pre><code># In your_app/tasks.py\nfrom nala.athomic.integration.tasks import task\n\n@task(queue=\"emails\")\nasync def send_welcome_email(user_id: str, email: str):\n    # This function will be executed by a worker process.\n    # The context from the original API request will be available here.\n    await email_service.send(to=email, ...)\n\n# In your API endpoint or service\nfrom your_app.tasks import send_welcome_email\n\nasync def create_user(user_data: dict):\n    # Instead of calling the function directly...\n    # send_welcome_email(user.id, user.email)\n\n    # ...you enqueue it for background execution using .delay()\n    await send_welcome_email.delay(user_id=user.id, email=user.email)\n</code></pre>"},{"location":"athomic/integration/tasks/#api-reference","title":"API Reference","text":""},{"location":"athomic/integration/tasks/#nala.athomic.integration.tasks.decorator.task","title":"<code>nala.athomic.integration.tasks.decorator.task(queue=None)</code>","text":"<p>Decorator used to register a standard asynchronous function as a background task.</p> <p>The decorated function is wrapped in a TaskWrapper, giving it the <code>.delay()</code> method for asynchronous execution scheduling.</p> <p>Parameters:</p> Name Type Description Default <code>queue</code> <code>Optional[str]</code> <p>Optional name of the queue where the task should be placed.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable[[Callable], TaskWrapper]</code> <p>The decorator function that returns a TaskWrapper instance.</p>"},{"location":"athomic/integration/tasks/#nala.athomic.integration.tasks.protocol.TaskBrokerProtocol","title":"<code>nala.athomic.integration.tasks.protocol.TaskBrokerProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the standard, agnostic interface (contract) for integrating with background task queue systems.</p> <p>This protocol serves as an abstraction layer (DIP) over concrete backends like RQ, Taskiq, or internal local executors, decoupling application logic from the task execution infrastructure.</p>"},{"location":"athomic/integration/tasks/#nala.athomic.integration.tasks.protocol.TaskBrokerProtocol.enqueue_task","title":"<code>enqueue_task(task_name, *args, **kwargs)</code>  <code>async</code>","text":"<p>Enqueues a task for eventual asynchronous execution by a dedicated worker process.</p> <p>Parameters:</p> Name Type Description Default <code>task_name</code> <code>str</code> <p>A string identifier for the task function        (e.g., 'send_welcome_email'). The format depends on the        specific task broker backend (e.g., function path, registered name).</p> required <code>*args</code> <code>Any</code> <p>Positional arguments to be passed to the task function.    These arguments must be safely serializable by the backend broker.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to be passed to the task function.       Must also be safely serializable.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>An optional string representing the unique ID assigned to the enqueued</p> <code>Optional[str]</code> <p>task by the backend (if available). Returns None if the backend</p> <code>Optional[str]</code> <p>does not provide an ID or enqueues silently.</p> <p>Raises:</p> Type Description <code>TaskEnqueueError</code> <p>If the system fails to communicate with the broker               or if a critical serialization failure prevents               the task from being successfully placed in the queue.</p>"},{"location":"athomic/integration/tasks/#nala.athomic.integration.tasks.worker.run_task_with_context","title":"<code>nala.athomic.integration.tasks.worker.run_task_with_context(func)</code>","text":"<p>A decorator wrapper applied to task functions on the worker side.</p> <p>This wrapper performs the critical function of: 1. Extracting the serialized execution context from task arguments. 2. Restoring the context (tracing IDs, tenant info, etc.) for the worker's thread/async loop. 3. Applying tracing and metrics before and after task execution. 4. Handling exceptions and logging status.</p>"},{"location":"athomic/integration/messaging/","title":"Messaging (Producers &amp; Consumers)","text":""},{"location":"athomic/integration/messaging/#overview","title":"Overview","text":"<p>The Messaging module provides the core abstractions and implementations for producing and consuming messages from external, distributed message brokers like Apache Kafka. It is designed for durable, reliable, and observable inter-service communication, forming the backbone of an event-driven microservices architecture.</p> <p>Reminder: For intra-service communication (within a single service), use the lightweight Internal Event Bus.</p>"},{"location":"athomic/integration/messaging/#key-features","title":"Key Features","text":"<ul> <li>Producer/Consumer Protocols: Built on clean <code>ProducerProtocol</code> and <code>ConsumerProtocol</code> interfaces, decoupling your business logic from the specific message broker technology.</li> <li>Decorator-Based Consumers: Define your message consumers declaratively and elegantly with a simple <code>@subscribe_to</code> decorator, keeping your code clean and focused.</li> <li>Extensible Payload Pipeline: Leverages the Payload Processing Pipeline for transparent serialization, encryption, and compression of messages.</li> <li>Built-in Resilience: Deep integration with resilience patterns like Retry/DLQ and the Transactional Outbox to ensure messages are not lost.</li> <li>Full Observability: Every message produced and consumed is automatically instrumented with distributed tracing, detailed Prometheus metrics, and data lineage events.</li> </ul>"},{"location":"athomic/integration/messaging/#how-it-works","title":"How It Works","text":"<ul> <li> <p>Producers: You obtain a singleton <code>Producer</code> instance from the <code>ProducerFactory</code>. When you call <code>producer.publish()</code>, the message passes through the configured payload processing pipeline (e.g., serialize, encrypt) before being sent to the broker by a backend-specific provider like <code>KafkaProducer</code>.</p> </li> <li> <p>Consumers: At application startup, the <code>MessagingLifecycleManager</code> discovers all functions that have been decorated with <code>@subscribe_to</code>. For each one, it uses a <code>ConsumerFactory</code> to build a complete consumer service, assembling all necessary components like the message processor, DLQ handler, and the backend-specific client (e.g., <code>KafkaConsumer</code>). These services are then managed by the application's main lifecycle.</p> </li> </ul>"},{"location":"athomic/integration/messaging/#deeper-dive","title":"Deeper Dive","text":"<p>Explore the specific components of the messaging system in more detail:</p> <ul> <li>Producer Guide: Learn how to publish messages and use advanced features like delayed publishing.</li> <li>Consumer Guide: Learn how to write consumer handlers using the <code>@subscribe_to</code> decorator.</li> <li>Retry &amp; DLQ: Understand the automatic retry mechanism and the Dead Letter Queue for handling failed messages.</li> <li>Delayed Messages: Learn how to publish messages that should only be processed after a delay.</li> <li>Context Propagation: See how tracing and multi-tenancy context flows seamlessly across the message bus.</li> </ul>"},{"location":"athomic/integration/messaging/#api-reference","title":"API Reference","text":""},{"location":"athomic/integration/messaging/#nala.athomic.integration.messaging.decorators.subscribe_to","title":"<code>nala.athomic.integration.messaging.decorators.subscribe_to(topic, *, target_type=None, group_id=None, batch_size=None, batch_timeout_ms=None, max_batch_linger_ms=None, max_batch_bytes=None)</code>","text":"<p>A decorator used to register an async function as a message consumer for a specific topic(s).</p> <p>This function collects the configuration metadata and registers it with the global <code>decorated_consumer_registry</code>, enabling the <code>ConsumerFactory</code> to assemble the concrete consumer at application startup.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>Union[str, List[str]]</code> <p>The topic name (str) or a list of topic names (List[str]) to subscribe to.</p> required <code>target_type</code> <code>Optional[Type[Any]]</code> <p>Optional Pydantic model or class used to deserialize the message payload.</p> <code>None</code> <code>group_id</code> <code>Optional[str]</code> <p>Optional consumer group identifier. If None, the consumer backend must derive a default.</p> <code>None</code> <code>batch_size</code> <code>Optional[int]</code> <p>If provided and &gt; 0, enables batch processing mode and defines the maximum number of messages per batch.</p> <code>None</code> <code>batch_timeout_ms</code> <code>Optional[int]</code> <p>Optional override for the maximum time (in milliseconds) the consumer should wait for a record to arrive when in batch mode.</p> <code>None</code> <code>max_batch_linger_ms</code> <code>Optional[int]</code> <p>Optional override for the maximum time (in milliseconds) to wait for a full batch before processing a partial one.</p> <code>None</code> <code>max_batch_bytes</code> <code>Optional[int]</code> <p>Optional override for the maximum size of a batch in bytes.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>The decorator function itself, which registers the configuration and returns the original handler function unchanged.</p>"},{"location":"athomic/integration/messaging/#nala.athomic.integration.messaging.producers.factory.ProducerFactory","title":"<code>nala.athomic.integration.messaging.producers.factory.ProducerFactory</code>","text":"<p>A Factory class responsible for creating and caching concrete Producer instances.</p> <p>It implements the Singleton Pattern via an internal cache (<code>_cache</code>) to ensure that only one instance of the producer (per backend) exists across the application, optimizing connection resources. It also coordinates the assembly of all required strategies and processors.</p>"},{"location":"athomic/integration/messaging/#nala.athomic.integration.messaging.producers.factory.ProducerFactory.clear","title":"<code>clear()</code>  <code>classmethod</code>","text":"<p>Clears the producer cache.</p> <p>This method should be used primarily for testing or when a full reinitialization of the messaging system is required.</p>"},{"location":"athomic/integration/messaging/#nala.athomic.integration.messaging.producers.factory.ProducerFactory.create","title":"<code>create(settings=None, *, ignore_cache=False)</code>  <code>classmethod</code>","text":"<p>Creates a producer instance, fetching from the cache if available.</p> <p>The creation process involves: 1. Resolving the concrete Producer class from the registry. 2. Creating the PayloadProcessor based on pipeline configuration. 3. Resolving all available PublishingStrategies. 4. Injecting all dependencies into the producer instance.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[MessagingSettings]</code> <p>Optional explicit messaging settings. If None, global settings are used.</p> <code>None</code> <code>ignore_cache</code> <code>bool</code> <p>If True, forces the creation of a new producer instance.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>ProducerProtocol</code> <code>ProducerProtocol</code> <p>A fully assembled and configured producer instance.</p> <p>Raises:</p> Type Description <code>InvalidProducerConfigurationError</code> <p>If the backend setting is missing.</p> <code>MessagingConnectionError</code> <p>If any underlying component fails during creation.</p>"},{"location":"athomic/integration/messaging/#nala.athomic.integration.messaging.consumers.factory.ConsumerFactory","title":"<code>nala.athomic.integration.messaging.consumers.factory.ConsumerFactory</code>","text":"<p>Builds a fully configured consumer instance by assembling components retrieved from registries based on configuration.</p> <p>This factory acts as the main assembler for the messaging consumer subsystem. It adheres to the Open/Closed Principle (OCP) by retrieving concrete classes (Consumer, Strategies) from registries rather than using explicit conditional logic (e.g., if-elif-else on backend type).</p>"},{"location":"athomic/integration/messaging/#nala.athomic.integration.messaging.consumers.factory.ConsumerFactory.create","title":"<code>create(settings, handler_config, connection_manager=None, serializer=None, dlq_handler=None)</code>  <code>staticmethod</code>","text":"<p>Assembles and returns a fully configured concrete consumer instance.</p> <p>The assembly process involves resolving the Consumer class, the DLQ Handler, the Serializer, the Consumption Strategy, and the Message Processor before injecting them into the final BaseConsumer implementation.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>MessagingSettings</code> <p>The global messaging configuration.</p> required <code>handler_config</code> <code>ConsumerHandlerConfig</code> <p>The configuration for the specific handler (topic, group, batch mode).</p> required <code>connection_manager</code> <code>Optional[ConnectionManager]</code> <p>Optional manager for databases/KV stores, required for ordering logic.</p> <code>None</code> <code>serializer</code> <code>Optional[SerializerProtocol]</code> <p>Optional pre-resolved serializer instance.</p> <code>None</code> <code>dlq_handler</code> <code>Optional[DLQHandler]</code> <p>Optional pre-resolved Dead Letter Queue handler.</p> <code>None</code> <p>Returns:</p> Type Description <code>BaseConsumer</code> <p>A concrete instance of <code>BaseConsumer</code> (e.g., KafkaConsumer).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If ordering is enabled but <code>connection_manager</code> is not provided or connected.</p> <code>KeyError</code> <p>If the specified backend or strategy creator is not found in the registries.</p>"},{"location":"athomic/integration/messaging/consumer/","title":"Consuming Messages (Consumer)","text":""},{"location":"athomic/integration/messaging/consumer/#overview","title":"Overview","text":"<p>The Message Consumer is the component responsible for subscribing to topics on a message broker, receiving messages, and delegating them to your application's business logic for processing.</p> <p>In the Athomic Layer, defining a consumer is an extremely simple and declarative process thanks to the <code>@subscribe_to</code> decorator. This decorator handles all the complex boilerplate of connecting to the broker, managing the consumer lifecycle, and integrating with resilience and observability systems, allowing you to focus purely on your handler logic.</p>"},{"location":"athomic/integration/messaging/consumer/#defining-a-consumer","title":"Defining a Consumer","text":"<p>To create a message consumer, you simply decorate an <code>async</code> function with <code>@subscribe_to</code>. This function becomes your handler, which will be executed for each message received from the specified topic.</p>"},{"location":"athomic/integration/messaging/consumer/#basic-single-message-consumer","title":"Basic (Single Message) Consumer","text":"<p>By default, your handler will receive one message at a time. The handler should accept the deserialized message payload as its first argument.</p> <pre><code>from nala.athomic.integration.messaging import subscribe_to\nfrom your_app.schemas import UserCreatedEvent\n\n@subscribe_to(\n    topic=\"user.events.v1\",\n    group_id=\"user-event-processor-group\",\n    target_type=UserCreatedEvent # Pydantic model for deserialization and validation\n)\nasync def handle_user_created_event(message: UserCreatedEvent):\n    \"\"\"\n    This handler will be called for each message on the 'user.events.v1' topic.\n    The incoming JSON payload will be automatically deserialized and validated\n    into a `UserCreatedEvent` instance before this function is called.\n    \"\"\"\n    print(f\"Processing new user: {message.user_id}\")\n    await user_service.send_welcome_email(message.email)\n</code></pre>"},{"location":"athomic/integration/messaging/consumer/#batch-consumer","title":"Batch Consumer","text":"<p>For high-throughput scenarios, you can process messages in batches to improve efficiency. To enable batch mode, simply set the <code>batch_size</code> argument in the decorator. Your handler function will then receive a list of deserialized messages.</p> <pre><code>@subscribe_to(\n    topic=\"tracking.events.v1\",\n    group_id=\"tracking-batch-processor\",\n    target_type=TrackingEvent,\n    batch_size=100,\n    max_batch_linger_ms=5000\n)\nasync def handle_tracking_events_in_batch(messages: list[TrackingEvent]):\n    \"\"\"\n    This handler will receive a list of up to 100 TrackingEvent objects.\n    \"\"\"\n    await tracking_repo.save_many([event.data for event in messages])\n</code></pre>"},{"location":"athomic/integration/messaging/consumer/#advanced-ordered-consumption-fifo-with-ordered_consumer","title":"Advanced: Ordered Consumption (FIFO) with <code>@ordered_consumer</code>","text":"<p>This is an innovative feature of the Athomic Layer. The <code>@ordered_consumer</code> decorator guarantees that messages belonging to the same logical entity (e.g., all events for <code>order-123</code>) are processed in strict First-In, First-Out (FIFO) order, even in a distributed environment with multiple consumer instances.</p>"},{"location":"athomic/integration/messaging/consumer/#how-it-works","title":"How It Works","text":"<p>This strategy requires two fields in your message payload: -   An <code>aggregate_key_field</code>: The identifier for the entity (e.g., <code>\"order_id\"</code>). -   A <code>sequence_id_field</code>: A monotonically increasing number for that entity.</p> <p>The <code>OrderedOrchestrationStrategy</code> uses two KV stores behind the scenes: 1.  A State Store to track the last successfully processed <code>sequence_id</code> for each <code>aggregate_key</code>. 2.  A Waiting Room (a Redis Sorted Set) to temporarily buffer messages that arrive out of order.</p> <p>When a message arrives, the strategy checks its <code>sequence_id</code> against the last processed ID for its <code>aggregate_key</code>. If it's the next message in the sequence, it's processed. If it's a future message, it's placed in the waiting room. After processing a message, the strategy checks the waiting room to process any subsequent messages that have now come into order.</p>"},{"location":"athomic/integration/messaging/consumer/#usage-example","title":"Usage Example","text":"<pre><code>from nala.athomic.integration.messaging.decorators import ordered_consumer\nfrom your_app.schemas import OrderEvent\n\n@ordered_consumer(\n    topic=\"order.events.v1\",\n    group_id=\"ordered-order-processor\",\n    target_type=OrderEvent,\n    # Specify the fields in your message payload for ordering\n    aggregate_key_field=\"order_id\",\n    sequence_id_field=\"event_sequence\"\n)\nasync def handle_ordered_order_event(message: OrderEvent):\n    \"\"\"\n    This handler will process events for the same order_id in strict\n    sequence based on the event_sequence number.\n    \"\"\"\n    print(f\"Processing event #{message.event_sequence} for order {message.order_id}\")\n    await order_service.update_status(message.order_id, message.new_status)\n</code></pre>"},{"location":"athomic/integration/messaging/consumer/#api-reference","title":"API Reference","text":""},{"location":"athomic/integration/messaging/consumer/#nala.athomic.integration.messaging.decorators.subscribe_to","title":"<code>nala.athomic.integration.messaging.decorators.subscribe_to(topic, *, target_type=None, group_id=None, batch_size=None, batch_timeout_ms=None, max_batch_linger_ms=None, max_batch_bytes=None)</code>","text":"<p>A decorator used to register an async function as a message consumer for a specific topic(s).</p> <p>This function collects the configuration metadata and registers it with the global <code>decorated_consumer_registry</code>, enabling the <code>ConsumerFactory</code> to assemble the concrete consumer at application startup.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>Union[str, List[str]]</code> <p>The topic name (str) or a list of topic names (List[str]) to subscribe to.</p> required <code>target_type</code> <code>Optional[Type[Any]]</code> <p>Optional Pydantic model or class used to deserialize the message payload.</p> <code>None</code> <code>group_id</code> <code>Optional[str]</code> <p>Optional consumer group identifier. If None, the consumer backend must derive a default.</p> <code>None</code> <code>batch_size</code> <code>Optional[int]</code> <p>If provided and &gt; 0, enables batch processing mode and defines the maximum number of messages per batch.</p> <code>None</code> <code>batch_timeout_ms</code> <code>Optional[int]</code> <p>Optional override for the maximum time (in milliseconds) the consumer should wait for a record to arrive when in batch mode.</p> <code>None</code> <code>max_batch_linger_ms</code> <code>Optional[int]</code> <p>Optional override for the maximum time (in milliseconds) to wait for a full batch before processing a partial one.</p> <code>None</code> <code>max_batch_bytes</code> <code>Optional[int]</code> <p>Optional override for the maximum size of a batch in bytes.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>The decorator function itself, which registers the configuration and returns the original handler function unchanged.</p>"},{"location":"athomic/integration/messaging/consumer/#nala.athomic.integration.messaging.decorators.ordered_consumer.ordered_consumer","title":"<code>nala.athomic.integration.messaging.decorators.ordered_consumer.ordered_consumer(topic, *, group_id, aggregate_key_field, sequence_id_field, target_type=None)</code>","text":"<p>A decorator to register an async function as an ordered message consumer.</p> <p>Messages consumed by handlers decorated with this will be processed in strict sequence based on the combination of <code>aggregate_key_field</code> and <code>sequence_id_field</code>.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>Union[str, List[str]]</code> <p>The topic name(s) to subscribe to.</p> required <code>group_id</code> <code>str</code> <p>The mandatory consumer group identifier for state tracking.</p> required <code>aggregate_key_field</code> <code>str</code> <p>The field name in the deserialized message payload that identifies the stream or aggregate instance to be ordered.</p> required <code>sequence_id_field</code> <code>str</code> <p>The field name in the deserialized message payload that provides the sequential ID of the message within the aggregate.</p> required <code>target_type</code> <code>Optional[Type[Any]]</code> <p>Optional Pydantic model or class for message deserialization.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>The decorator function.</p>"},{"location":"athomic/integration/messaging/consumer/#nala.athomic.integration.messaging.consumers.protocol.ConsumerProtocol","title":"<code>nala.athomic.integration.messaging.consumers.protocol.ConsumerProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for consuming messages from a messaging system.</p> <p>Focuses on a subscription pattern where received messages are processed by an asynchronous callback function.</p>"},{"location":"athomic/integration/messaging/consumer/#nala.athomic.integration.messaging.consumers.protocol.ConsumerProtocol.consume","title":"<code>consume()</code>  <code>async</code>","text":"<p>Starts the continuous message consumption process.</p> <p>Subscribes to topics/queues and passes received messages to the provided callback function for processing. This function typically blocks execution (or runs in a background task) until <code>close()</code> is called or an unrecoverable error occurs.</p> <p>Raises:</p> Type Description <code>ConsumeError</code> <p>If an unrecoverable error occurs during consumption (e.g., persistent authentication failure, error in the consume loop).</p> <code>MessagingConnectionError</code> <p>If there are initial connection problems with the broker when trying to start consumption.</p> <code>ValueError</code> <p>If essential arguments (like <code>topics</code> for Kafka) are not provided.</p>"},{"location":"athomic/integration/messaging/consumer/#nala.athomic.integration.messaging.consumers.base.BaseConsumer","title":"<code>nala.athomic.integration.messaging.consumers.base.BaseConsumer</code>","text":"<p>               Bases: <code>BaseService</code>, <code>ABC</code>, <code>ConsumerProtocol</code></p> <p>Abstract base class for message consumers.</p> <p>This class serves as the core lifecycle manager for any message consumer implementation (e.g., Kafka, Redis, SQS). It enforces the required dependencies and delegates all message-specific logic to injected protocols, thereby adhering strictly to the Dependency Inversion Principle (DIP).</p> <p>Responsibilities include: - Managing the connection lifecycle to the message broker (via BaseService). - Holding the configuration and essential dependencies. - Delegating the message fetching loop to a <code>ConsumptionStrategy</code>. - Delegating message processing to a <code>Processor</code>.</p>"},{"location":"athomic/integration/messaging/consumer/#nala.athomic.integration.messaging.consumers.base.BaseConsumer.group_id","title":"<code>group_id</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>The consumer group identifier for this consumer instance.</p> <p>Returns:</p> Type Description <code>str</code> <p>The unique string identifying the consumer group.</p>"},{"location":"athomic/integration/messaging/consumer/#nala.athomic.integration.messaging.consumers.base.BaseConsumer.__init__","title":"<code>__init__(settings, handler_config, message_processor, consumption_strategy)</code>","text":"<p>Initializes the base consumer with all necessary dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>MessagingSettings</code> <p>The general messaging configuration settings.</p> required <code>handler_config</code> <code>ConsumerHandlerConfig</code> <p>The configuration metadata for the consumer handler, typically extracted from a decorator.</p> required <code>message_processor</code> <code>ProcessorProtocol</code> <p>The strategy responsible for the entire message processing pipeline (deserialization, execution, DLQ).</p> required <code>consumption_strategy</code> <code>ConsumptionStrategyProtocol</code> <p>The strategy defining how messages are fetched from the broker (e.g., individual or batch consumption).</p> required <p>Raises:</p> Type Description <code>InvalidConsumerConfigurationError</code> <p>If <code>handler_config</code> is not provided.</p>"},{"location":"athomic/integration/messaging/consumer/#nala.athomic.integration.messaging.consumers.base.BaseConsumer.get_batch","title":"<code>get_batch(max_records, timeout_ms)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Fetches a batch of raw messages from the broker in a single operation.</p> <p>This method must be implemented by concrete consumer providers to enable batch processing strategies.</p> <p>Parameters:</p> Name Type Description Default <code>max_records</code> <code>int</code> <p>The maximum number of messages to attempt to fetch.</p> required <code>timeout_ms</code> <code>int</code> <p>The maximum time (in milliseconds) to wait for records to arrive.</p> required <p>Returns:</p> Type Description <code>List[RawMessage]</code> <p>A list of <code>RawMessage</code> objects, which may be empty.</p>"},{"location":"athomic/integration/messaging/consumer/#nala.athomic.integration.messaging.consumers.base.BaseConsumer.get_message_iterator","title":"<code>get_message_iterator()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Returns an async generator that yields raw messages one by one from the broker.</p> <p>This method must be implemented by concrete consumer providers (e.g., Kafka, SQS) to handle the broker-specific low-level message fetching.</p> <p>Yields:</p> Name Type Description <code>RawMessage</code> <code>AsyncGenerator[RawMessage, None]</code> <p>The next raw message fetched from the broker.</p>"},{"location":"athomic/integration/messaging/context-propagation/","title":"Context Propagation in Messaging","text":""},{"location":"athomic/integration/messaging/context-propagation/#overview","title":"Overview","text":"<p>A fundamental challenge in distributed, event-driven architectures is maintaining a consistent \"context\" as a request flows from one service to another through a message broker. This context includes critical observability data like the <code>trace_id</code> for distributed tracing and application-level data like the <code>tenant_id</code> for multi-tenancy.</p> <p>The Athomic messaging module solves this problem with automatic context propagation. It ensures that when you consume a message in a background worker, you have access to the same execution context that existed in the service that originally published the message.</p>"},{"location":"athomic/integration/messaging/context-propagation/#how-it-works","title":"How It Works","text":"<p>The entire process is automated and built upon the Context Management module.</p>"},{"location":"athomic/integration/messaging/context-propagation/#injection-producer-side","title":"Injection (Producer Side)","text":"<ol> <li>When you call <code>producer.publish()</code>, the <code>BaseProducer</code> automatically invokes the <code>MessagingTelemetryAdapter</code>.</li> <li>This adapter calls the <code>capture_context()</code> function, which serializes all context variables marked for propagation (like <code>trace_id</code>, <code>tenant_id</code>, <code>request_id</code>, etc.) into a dictionary.</li> <li>The W3C Trace Context (<code>traceparent</code>, <code>tracestate</code>) is injected into this dictionary for OpenTelemetry.</li> <li>Finally, this context dictionary is injected as a special header into the message before it is sent to the broker.</li> </ol>"},{"location":"athomic/integration/messaging/context-propagation/#extraction-consumer-side","title":"Extraction (Consumer Side)","text":"<ol> <li>When a consumer receives a message, the <code>BaseMessageProcessor</code> receives the raw message with its headers.</li> <li>It uses the <code>MessagingTelemetryAdapter</code> to extract the W3C Trace Context, which creates and activates a new OpenTelemetry span that is correctly parented to the producer's span.</li> <li>The <code>@run_task_with_context</code> decorator (for background tasks) or internal consumer logic calls <code>restore_context()</code>, which takes the context dictionary from the message headers and applies its values to the <code>contextvars</code> of the current execution scope.</li> </ol> <p>The result is seamless. When your consumer handler function is executed, a call to <code>context_vars.get_tenant_id()</code> will return the same <code>tenant_id</code> from the original publisher, and your logs and traces will be perfectly correlated.</p>"},{"location":"athomic/integration/messaging/context-propagation/#what-is-propagated","title":"What is Propagated?","text":"<p>By default, the following context variables are automatically propagated in message headers:</p> <ul> <li><code>request_id</code></li> <li><code>trace_id</code></li> <li><code>span_id</code></li> <li><code>tenant_id</code></li> <li><code>user_id</code></li> <li><code>role</code></li> <li><code>locale</code></li> <li><code>correlation_id</code></li> <li><code>feature_flags</code></li> </ul> <p>This list is fully configurable in your <code>settings.toml</code> file under the <code>[context]</code> section.</p>"},{"location":"athomic/integration/messaging/context-propagation/#api-reference","title":"API Reference","text":""},{"location":"athomic/integration/messaging/context-propagation/#nala.athomic.context.propagation.capture_context","title":"<code>nala.athomic.context.propagation.capture_context()</code>","text":"<p>Captures the current values of all context variables marked for propagation.</p> <p>This function iterates through the centrally registered context variables in the <code>context_var_manager</code>, collects the values of those flagged with <code>propagate=True</code>, and returns them as a dictionary. This dictionary is suitable for serialization and passing to background tasks or events.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary containing the current context values to be propagated.</p>"},{"location":"athomic/integration/messaging/context-propagation/#nala.athomic.context.propagation.restore_context","title":"<code>nala.athomic.context.propagation.restore_context(context_dict)</code>","text":"<p>A context manager to temporarily set context variables from a dictionary.</p> <p>This is used on the worker side (e.g., in a background task or event handler) to re-establish the context that existed when the job was enqueued. It safely resets all variables to their previous state upon exiting the <code>with</code> block.</p> <p>Parameters:</p> Name Type Description Default <code>context_dict</code> <code>Optional[Dict[str, Any]]</code> <p>The dictionary of context values captured before the           task was enqueued.</p> required"},{"location":"athomic/integration/messaging/context-propagation/#nala.athomic.observability.telemetry.adapters.messaging_adapter.MessagingTelemetryAdapter","title":"<code>nala.athomic.observability.telemetry.adapters.messaging_adapter.MessagingTelemetryAdapter</code>","text":"<p>               Bases: <code>TelemetryAdapterProtocol</code></p> <p>A concrete Telemetry Adapter for messaging systems.</p> <p>This adapter handles the injection and extraction of W3C Trace Context into and from the Athomic MessageHeaders object, enabling end-to-end tracing across message broker boundaries.</p>"},{"location":"athomic/integration/messaging/context-propagation/#nala.athomic.observability.telemetry.adapters.messaging_adapter.MessagingTelemetryAdapter.activate_from_headers","title":"<code>activate_from_headers(headers)</code>","text":"<p>A context manager that extracts the trace context from incoming message headers and activates it for the duration of the message processing.</p> <p>This is the primary method used by consumer implementations to continue the distributed trace from the producer.</p>"},{"location":"athomic/integration/messaging/context-propagation/#nala.athomic.observability.telemetry.adapters.messaging_adapter.MessagingTelemetryAdapter.extract_context","title":"<code>extract_context(headers)</code>","text":"<p>Extracts the trace context from an incoming MessageHeaders object and returns the active OpenTelemetry Context.</p>"},{"location":"athomic/integration/messaging/context-propagation/#nala.athomic.observability.telemetry.adapters.messaging_adapter.MessagingTelemetryAdapter.inject_context","title":"<code>inject_context(headers)</code>","text":"<p>Injects the current trace context and a unique 'message_id' into a MessageHeaders object for an outgoing message.</p>"},{"location":"athomic/integration/messaging/delay/","title":"Delayed Messages","text":""},{"location":"athomic/integration/messaging/delay/#overview","title":"Overview","text":"<p>The Messaging module provides a robust mechanism for publishing messages that should only be processed after a specified delay. This is a common requirement in many applications, for example:</p> <ul> <li>Sending a follow-up email 24 hours after a user signs up.</li> <li>Retrying a failed operation with an exponential backoff delay.</li> <li>Scheduling a task to be executed at a specific time in the future.</li> </ul> <p>Athomic implements this using a Delay Topics Strategy, which is a reliable and scalable pattern for handling message delays in high-throughput systems like Kafka.</p>"},{"location":"athomic/integration/messaging/delay/#how-the-delay-topics-strategy-works","title":"How the Delay Topics Strategy Works","text":"<p>Instead of relying on a scheduler or timers within the application, this strategy uses a set of dedicated, intermediary topics in the message broker.</p> <ol> <li> <p>Configuration: You define a set of \"delay buckets\" in your configuration\u2014a mapping of a delay duration in seconds to a specific topic name.</p> <p>```toml</p> </li> <li> <p>Publishing with Delay: When you call <code>producer.publish(..., delay_seconds=X)</code>, the <code>DelayedPublishStrategy</code> is activated. It finds the smallest delay bucket that is greater than or equal to your requested delay.</p> </li> <li> <p>Envelope Wrapping: The producer wraps your original message (including its payload, headers, and final destination topic) inside a special \"envelope\" message.</p> </li> <li> <p>Publish to Delay Topic: This envelope is then published to the chosen delay topic (e.g., <code>platform.internal.delay-5m.v1</code>).</p> </li> <li> <p>The Republisher Service: Athomic runs a dedicated, internal consumer service (the \"republisher\"). This service is automatically configured to subscribe to all of your configured delay topics.</p> </li> <li> <p>Unwrap and Republish: When the republisher consumes a message from a delay topic, it unwraps the envelope, extracts the original message and its metadata, and publishes it to its final destination topic.</p> </li> </ol> <p>This approach is highly scalable and resilient because it leverages the underlying message broker's infrastructure for storing and delivering the delayed messages.</p>"},{"location":"athomic/integration/messaging/delay/#in-settingstoml","title":"In settings.toml","text":"<p>[default.integration.messaging.republisher.delay_topics] 300 = \"platform.internal.delay-5m.v1\"    # 5-minute bucket 900 = \"platform.internal.delay-15m.v1\"   # 15-minute bucket 3600 = \"platform.internal.delay-1h.v1\"   # 1-hour bucket ```</p>"},{"location":"athomic/integration/messaging/delay/#usage","title":"Usage","text":"<p>Using the delayed message feature is as simple as adding the <code>delay_seconds</code> parameter to a standard <code>publish</code> call.</p> <pre><code>from nala.athomic.integration.messaging import ProducerFactory\n\nproducer = ProducerFactory.create()\n\nasync def schedule_follow_up_email(user_id: str):\n    # This message will be processed in approximately 1 hour.\n    await producer.publish(\n        topic=\"user.follow-ups.v1\",\n        message={\"user_id\": user_id, \"email_type\": \"onboarding_d1\"},\n        key=user_id,\n        delay_seconds=3600 # 1 hour\n    )\n</code></pre>"},{"location":"athomic/integration/messaging/delay/#configuration","title":"Configuration","text":"<p>The delayed message system is configured under the <code>[integration.messaging.republisher]</code> section.</p> <pre><code>[default.integration.messaging.republisher]\nenabled = true\n\n# This must be set to use the delay topics strategy.\ndelay_strategy_backend = \"kafka_topic_delay_strategy\"\n\n  # The consumer and producer settings for the internal republisher service.\n  # This service needs its own configuration to connect to Kafka.\n  [default.integration.messaging.republisher.consumer]\n  bootstrap_servers = [\"localhost:9092\"]\n  group_id = \"nala-global-republisher-group\"\n\n  [default.integration.messaging.republisher.producer]\n  bootstrap_servers = [\"localhost:9092\"]\n\n  # The mapping of delay durations (in seconds) to topic names.\n  [default.integration.messaging.republisher.delay_topics]\n  60 = \"platform.internal.delay-1m.v1\"\n  300 = \"platform.internal.delay-5m.v1\"\n  3600 = \"platform.internal.delay-1h.v1\"\n</code></pre>"},{"location":"athomic/integration/messaging/delay/#api-reference","title":"API Reference","text":""},{"location":"athomic/integration/messaging/delay/#nala.athomic.integration.messaging.producers.strategies.delayed.DelayedPublishStrategy","title":"<code>nala.athomic.integration.messaging.producers.strategies.delayed.DelayedPublishStrategy</code>","text":"<p>               Bases: <code>BasePublishingStrategy</code>, <code>PublishingStrategyProtocol</code></p> <p>Implements the strategy for publishing a message with a delay.</p> <p>This strategy works by: 1. Finding the closest matching delay topic bucket based on <code>delay_seconds</code>. 2. Wrapping the original message and its metadata (target topic, key, headers)    in an envelope. 3. Publishing the envelope to the intermediary delay topic.</p> <p>A separate consumer service (the republisher) will later unwrap and send the original message to the final destination after the delay.</p>"},{"location":"athomic/integration/messaging/delay/#nala.athomic.integration.messaging.producers.strategies.delayed.DelayedPublishStrategy.publish","title":"<code>publish(producer, message, topic, key, headers, delay_seconds)</code>  <code>async</code>","text":"<p>Processes the message for delayed delivery and dispatches it to the appropriate delay topic.</p> <p>Parameters:</p> Name Type Description Default <code>producer</code> <code>BaseProducer</code> <p>The producer instance used for transport.</p> required <code>message</code> <code>Any</code> <p>The original message payload.</p> required <code>topic</code> <code>str</code> <p>The final destination topic.</p> required <code>key</code> <code>Optional[Any]</code> <p>The message key.</p> required <code>headers</code> <code>Optional[MessageHeaders]</code> <p>The message headers.</p> required <code>delay_seconds</code> <code>Optional[int]</code> <p>The duration of the delay.</p> required <p>Returns:</p> Name Type Description <code>PublishingOutcome</code> <code>PublishingOutcome</code> <p>SUCCESS_DELAYED if the message was successfully published to the delay topic.</p>"},{"location":"athomic/integration/messaging/delay/#nala.athomic.integration.messaging.delay.republisher.DelayedMessageHandler","title":"<code>nala.athomic.integration.messaging.delay.republisher.DelayedMessageHandler</code>","text":"<p>A callable class that acts as a consumer callback for delayed messages.</p> <p>It unwraps the message envelope created by the <code>DelayedExecutionStrategy</code> and republishes the original payload to its final destination topic. This component ensures that the original message integrity (payload, headers, and key) is maintained during the delay process.</p>"},{"location":"athomic/integration/messaging/delay/#nala.athomic.integration.messaging.delay.republisher.DelayedMessageHandler.producer","title":"<code>producer</code>  <code>property</code>","text":"<p>Lazily creates a producer instance if one was not injected at initialization.</p> <p>Returns:</p> Name Type Description <code>ProducerProtocol</code> <code>ProducerProtocol</code> <p>The producer client instance.</p>"},{"location":"athomic/integration/messaging/delay/#nala.athomic.integration.messaging.delay.republisher.DelayedMessageHandler.__call__","title":"<code>__call__(message, message_key, raw_headers)</code>  <code>async</code>","text":"<p>Handles a message from a delay topic by republishing it to its final destination.</p> <p>This method is the entry point for the consumer. It deserializes the envelope, rehydrates the original headers, and uses the producer to send the message to its target topic.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Dict[str, Any]</code> <p>The message payload from the delay topic, containing the      original message wrapped in an envelope.</p> required <code>message_key</code> <code>Optional[str]</code> <p>The message key.</p> required <code>raw_headers</code> <code>MessageHeaders</code> <p>The headers from the delay topic message (not the original ones).</p> required <p>Returns:</p> Type Description <code>ProcessingOutcome</code> <p>ProcessingOutcome.ACK: If the message was successfully republished                    or if it was malformed and should be discarded.</p> <p>Raises:     Exception: Propagates exceptions from the producer (e.g., PublishError)                to trigger the consumer's retry/DLQ mechanism for the                delay topic itself.</p>"},{"location":"athomic/integration/messaging/delay/#nala.athomic.integration.messaging.delay.republisher.DelayedMessageHandler.__init__","title":"<code>__init__(producer=None)</code>","text":"<p>Initializes the handler.</p> <p>Parameters:</p> Name Type Description Default <code>producer</code> <code>Optional[ProducerProtocol]</code> <p>An optional producer instance for publishing the unwrapped       message. If not provided, one will be created lazily via       the factory (Dependency Injection pattern).</p> <code>None</code>"},{"location":"athomic/integration/messaging/internals/","title":"Messaging Internals &amp; Architecture","text":""},{"location":"athomic/integration/messaging/internals/#overview","title":"Overview","text":"<p>This page provides a deeper look into the internal components that orchestrate the message consumption lifecycle in the Athomic Layer. Understanding these components is useful for advanced debugging, customization, and contributing to the framework.</p>"},{"location":"athomic/integration/messaging/internals/#1-messaginglifecyclemanager","title":"1. <code>MessagingLifecycleManager</code>","text":"<p>This is the entry point for the entire consumer system. At application startup, its primary responsibilities are:</p> <ul> <li>Discovery: It inspects the application's code to find all functions that have been decorated with <code>@subscribe_to</code> or <code>@ordered_consumer</code>.</li> <li>Assembly: For each discovered handler, it uses the <code>ConsumerFactory</code> to build a complete, self-contained consumer service. This involves assembling all necessary parts: the correct consumption strategy, message processor, DLQ handler, and the underlying Kafka client.</li> <li>Lifecycle Management: It registers each of these fully assembled consumer services with the main Lifecycle Manager, which then controls their <code>start</code> and <code>stop</code> sequences along with the rest of the application.</li> </ul>"},{"location":"athomic/integration/messaging/internals/#2-the-message-processing-flow","title":"2. The Message Processing Flow","text":"<p>When a raw message is fetched from Kafka, it goes through a well-defined internal pipeline before your handler function is ever called.</p>"},{"location":"athomic/integration/messaging/internals/#rawmessage","title":"<code>RawMessage</code>","text":"<p>This is a simple, framework-agnostic data class that represents a message as it comes from the broker. It contains the raw, unprocessed payload (<code>bytes</code>), key, and headers. This abstraction decouples the core processing logic from any specific client library (like <code>aiokafka</code>).</p>"},{"location":"athomic/integration/messaging/internals/#headersorchestrator","title":"<code>HeadersOrchestrator</code>","text":"<p>This is the central component for managing context propagation. Before any other processing, the headers from the <code>RawMessage</code> are passed to the <code>HeadersOrchestrator</code>. It works with a series of specialized \"adapters\" to extract and prepare the context:</p> <ul> <li><code>MessagingTelemetryAdapter</code>: Extracts W3C Trace Context headers (<code>traceparent</code>) and starts a new OpenTelemetry span, correctly linking it to the producer's trace.</li> <li><code>LineageAdapter</code>: Extracts data lineage information.</li> <li><code>ContextAdapter</code>: Extracts the main application context (<code>tenant_id</code>, <code>user_id</code>, etc.).</li> </ul> <p>The extracted context is then applied to the current execution scope using <code>restore_context()</code>, so it becomes available throughout the rest of the processing pipeline and in your handler.</p>"},{"location":"athomic/integration/messaging/internals/#tasks-message-processors","title":"<code>Tasks</code> (Message Processors)","text":"<p>The \"task\" of processing a message is handled by a Message Processor (e.g., <code>SingleMessageProcessor</code> or <code>BatchMessageProcessor</code>). This component is responsible for: 1.  Receiving the <code>RawMessage</code>. 2.  Passing the raw payload through the Payload Processing Pipeline in reverse (decode) order (e.g., decompress -&gt; decrypt -&gt; deserialize). 3.  Invoking your actual handler function with the final, deserialized Pydantic model. 4.  Capturing the result of your handler.</p>"},{"location":"athomic/integration/messaging/internals/#outcomes","title":"<code>Outcomes</code>","text":"<p>After your handler function is executed, the Message Processor wraps the result in a standardized <code>ConsumptionOutcome</code> object.</p> <ul> <li><code>ConsumptionOutcome</code>: If the handler completes successfully, this object holds the result.</li> <li><code>FailureContext</code>: If the handler raises an exception, this object is created instead. It captures the original raw message, the exception details, and other metadata. This context object is then passed to the DLQ Handler to orchestrate the retry/DLQ logic.</li> </ul> <p>This structured flow ensures that every message is handled consistently and that all necessary context for observability and resilience is available at every step of the process.</p>"},{"location":"athomic/integration/messaging/producer/","title":"Publishing Messages (Producer)","text":""},{"location":"athomic/integration/messaging/producer/#overview","title":"Overview","text":"<p>The Message Producer is the component responsible for sending messages to the configured message broker (e.g., Kafka). It provides a high-level, abstract interface that handles complex underlying tasks transparently, allowing you to focus on the data you want to send.</p> <p>When you publish a message, the producer automatically orchestrates: -   Payload Processing: The message is passed through the configured Payload Processing Pipeline for serialization, encryption, and compression. -   Context Propagation: W3C Trace Context and other execution context variables are automatically injected into the message headers. -   Publishing Strategy: The producer intelligently selects the correct strategy for dispatching the message, supporting both immediate and delayed delivery.</p>"},{"location":"athomic/integration/messaging/producer/#getting-a-producer-instance","title":"Getting a Producer Instance","text":"<p>The <code>Producer</code> is managed as a singleton to ensure efficient use of resources like connection pools. The correct way to obtain the producer instance is through the <code>ProducerFactory</code>.</p> <pre><code>from nala.athomic.integration.messaging import ProducerFactory\n\n# The factory will create and cache a singleton instance based on your configuration.\nproducer = ProducerFactory.create()\n</code></pre>"},{"location":"athomic/integration/messaging/producer/#publishing-a-message","title":"Publishing a Message","text":"<p>The <code>producer.publish()</code> method is the single entry point for sending all messages.</p>"},{"location":"athomic/integration/messaging/producer/#basic-immediate-publishing","title":"Basic (Immediate) Publishing","text":"<p>To send a message for immediate delivery, you provide the topic and the message payload. The payload can be a Pydantic model, a dictionary, or any other data type your configured serializer can handle.</p> <pre><code>from your_app.schemas import UserCreatedEvent\n\n# The message can be a Pydantic model\nevent_payload = UserCreatedEvent(user_id=\"user-123\", email=\"test@example.com\")\n\nawait producer.publish(\n    topic=\"user.events.v1\",\n    message=event_payload,\n    key=\"user-123\"  # The key is used for partitioning in Kafka\n)\n</code></pre>"},{"location":"athomic/integration/messaging/producer/#publishing-with-a-delay","title":"Publishing with a Delay","text":"<p>This is a key feature of the Athomic producer. To send a message that should only be processed after a delay, simply add the <code>delay_seconds</code> argument to the <code>publish</code> call.</p> <pre><code># This message will be sent to an intermediary delay topic and only\n# republished to its final destination after 300 seconds (5 minutes).\nawait producer.publish(\n    topic=\"notifications.v1\",\n    message={\"email\": \"test@example.com\", \"template\": \"follow_up\"},\n    key=\"user-123\",\n    delay_seconds=300\n)\n</code></pre> <p>Behind the scenes, the <code>DelayedPublishStrategy</code> wraps your message in an \"envelope\" and sends it to a pre-configured delay topic. A separate background service, the republisher, is responsible for consuming from these delay topics and sending the message to its final destination when the time is up. See the Delayed Messages documentation for more details.</p>"},{"location":"athomic/integration/messaging/producer/#api-reference","title":"API Reference","text":""},{"location":"athomic/integration/messaging/producer/#nala.athomic.integration.messaging.producers.protocol.ProducerProtocol","title":"<code>nala.athomic.integration.messaging.producers.protocol.ProducerProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the contract for sending messages to a messaging system.</p>"},{"location":"athomic/integration/messaging/producer/#nala.athomic.integration.messaging.producers.protocol.ProducerProtocol.publish","title":"<code>publish(message=None, topic='default_topic', key=None, headers=None, delay_seconds=None)</code>  <code>async</code>","text":"<p>Publishes a message, deciding the strategy based on parameters.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Optional[Any]</code> <p>The content of the message to be sent.</p> <code>None</code> <code>topic</code> <code>str</code> <p>The target topic (or exchange).</p> <code>'default_topic'</code> <code>key</code> <code>Optional[str]</code> <p>The routing or partitioning key for the message.</p> <code>None</code> <code>headers</code> <code>Optional[MessageHeaders]</code> <p>A MessageHeaders object with metadata.</p> <code>None</code> <code>delay_seconds</code> <code>Optional[int]</code> <p>If provided and &gt; 0, the message will be sent            using a delayed publishing strategy.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>PublishingOutcome</code> <code>PublishingOutcome</code> <p>An enum indicating the result of the operation,                e.g., SUCCESS_DIRECT or SUCCESS_DELAYED.</p> <p>Raises:</p> Type Description <code>PublishError</code> <p>If publishing fails after all internal retries.</p> <code>MessagingConnectionError</code> <p>If there are connection problems with the broker.</p> <code>NotImplementedError</code> <p>If a requested strategy (e.g., 'delayed') is not configured.</p>"},{"location":"athomic/integration/messaging/producer/#nala.athomic.integration.messaging.producers.factory.ProducerFactory","title":"<code>nala.athomic.integration.messaging.producers.factory.ProducerFactory</code>","text":"<p>A Factory class responsible for creating and caching concrete Producer instances.</p> <p>It implements the Singleton Pattern via an internal cache (<code>_cache</code>) to ensure that only one instance of the producer (per backend) exists across the application, optimizing connection resources. It also coordinates the assembly of all required strategies and processors.</p>"},{"location":"athomic/integration/messaging/producer/#nala.athomic.integration.messaging.producers.factory.ProducerFactory.clear","title":"<code>clear()</code>  <code>classmethod</code>","text":"<p>Clears the producer cache.</p> <p>This method should be used primarily for testing or when a full reinitialization of the messaging system is required.</p>"},{"location":"athomic/integration/messaging/producer/#nala.athomic.integration.messaging.producers.factory.ProducerFactory.create","title":"<code>create(settings=None, *, ignore_cache=False)</code>  <code>classmethod</code>","text":"<p>Creates a producer instance, fetching from the cache if available.</p> <p>The creation process involves: 1. Resolving the concrete Producer class from the registry. 2. Creating the PayloadProcessor based on pipeline configuration. 3. Resolving all available PublishingStrategies. 4. Injecting all dependencies into the producer instance.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[MessagingSettings]</code> <p>Optional explicit messaging settings. If None, global settings are used.</p> <code>None</code> <code>ignore_cache</code> <code>bool</code> <p>If True, forces the creation of a new producer instance.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>ProducerProtocol</code> <code>ProducerProtocol</code> <p>A fully assembled and configured producer instance.</p> <p>Raises:</p> Type Description <code>InvalidProducerConfigurationError</code> <p>If the backend setting is missing.</p> <code>MessagingConnectionError</code> <p>If any underlying component fails during creation.</p>"},{"location":"athomic/integration/messaging/producer/#nala.athomic.integration.messaging.producers.strategies.direct.DirectPublishStrategy","title":"<code>nala.athomic.integration.messaging.producers.strategies.direct.DirectPublishStrategy</code>","text":"<p>               Bases: <code>BasePublishingStrategy</code>, <code>PublishingStrategyProtocol</code></p> <p>Implements the strategy for publishing a message directly to its final destination topic without any delay or intermediate processing route.</p> <p>This is the default, high-performance path for message delivery, focusing on immediate serialization, telemetry capture, and dispatch.</p>"},{"location":"athomic/integration/messaging/producer/#nala.athomic.integration.messaging.producers.strategies.direct.DirectPublishStrategy.publish","title":"<code>publish(producer, message, topic, key, headers, delay_seconds)</code>  <code>async</code>","text":"<p>Processes and publishes the message immediately to the broker.</p> <p>It handles the injection of tracing context, lineage headers, payload processing (serialization, compression), and metrics recording.</p> <p>Parameters:</p> Name Type Description Default <code>producer</code> <code>BaseProducer</code> <p>The producer instance used for transport and processing.</p> required <code>message</code> <code>Optional[Any]</code> <p>The message payload.</p> required <code>topic</code> <code>str</code> <p>The destination topic name.</p> required <code>key</code> <code>Optional[Any]</code> <p>The message key.</p> required <code>headers</code> <code>Optional[MessageHeaders]</code> <p>The message headers.</p> required <code>delay_seconds</code> <code>Optional[int]</code> <p>Ignored by this strategy, as it's for immediate publishing.</p> required <p>Returns:</p> Name Type Description <code>PublishingOutcome</code> <code>PublishingOutcome</code> <p>SUCCESS_DIRECT if the message was successfully dispatched.</p>"},{"location":"athomic/integration/messaging/producer/#nala.athomic.integration.messaging.producers.strategies.delayed.DelayedPublishStrategy","title":"<code>nala.athomic.integration.messaging.producers.strategies.delayed.DelayedPublishStrategy</code>","text":"<p>               Bases: <code>BasePublishingStrategy</code>, <code>PublishingStrategyProtocol</code></p> <p>Implements the strategy for publishing a message with a delay.</p> <p>This strategy works by: 1. Finding the closest matching delay topic bucket based on <code>delay_seconds</code>. 2. Wrapping the original message and its metadata (target topic, key, headers)    in an envelope. 3. Publishing the envelope to the intermediary delay topic.</p> <p>A separate consumer service (the republisher) will later unwrap and send the original message to the final destination after the delay.</p>"},{"location":"athomic/integration/messaging/producer/#nala.athomic.integration.messaging.producers.strategies.delayed.DelayedPublishStrategy.publish","title":"<code>publish(producer, message, topic, key, headers, delay_seconds)</code>  <code>async</code>","text":"<p>Processes the message for delayed delivery and dispatches it to the appropriate delay topic.</p> <p>Parameters:</p> Name Type Description Default <code>producer</code> <code>BaseProducer</code> <p>The producer instance used for transport.</p> required <code>message</code> <code>Any</code> <p>The original message payload.</p> required <code>topic</code> <code>str</code> <p>The final destination topic.</p> required <code>key</code> <code>Optional[Any]</code> <p>The message key.</p> required <code>headers</code> <code>Optional[MessageHeaders]</code> <p>The message headers.</p> required <code>delay_seconds</code> <code>Optional[int]</code> <p>The duration of the delay.</p> required <p>Returns:</p> Name Type Description <code>PublishingOutcome</code> <code>PublishingOutcome</code> <p>SUCCESS_DELAYED if the message was successfully published to the delay topic.</p>"},{"location":"athomic/integration/messaging/retry-dlq/","title":"Retry &amp; Dead Letter Queue (DLQ)","text":""},{"location":"athomic/integration/messaging/retry-dlq/#overview","title":"Overview","text":"<p>The Retry and Dead Letter Queue (DLQ) mechanism is a critical resilience pattern that prevents message loss when a consumer fails to process a message. It provides an automated, configurable way to handle transient (temporary) and permanent processing failures gracefully.</p> <ul> <li>Retry: When a consumer handler fails with a transient error, the message is automatically republished to the original topic to be processed again. This is handled by the <code>DLQHandler</code>.</li> <li>Dead Letter Queue (DLQ): If a message fails to be processed after a configured number of retry attempts, it is considered a \"poison pill\" and is moved to a separate, dedicated topic called the Dead Letter Queue. This removes the failing message from the main processing queue, preventing it from blocking other messages.</li> </ul>"},{"location":"athomic/integration/messaging/retry-dlq/#the-failure-handling-flow","title":"The Failure Handling Flow","text":"<p>When a decorated consumer handler (<code>@subscribe_to</code>) raises an exception, the following automated flow is triggered:</p> <ol> <li>Failure Detection: The <code>BaseMessageProcessor</code> catches the exception.</li> <li>DLQ Handler Invocation: It packages all information about the failure (the original message, headers, exception details) into a <code>FailureContext</code> object and passes it to the <code>DLQHandler</code>.</li> <li>Retry Check: The <code>DLQHandler</code> inspects the message's headers to check the current retry attempt count (using the <code>x-retry-attempts</code> header).</li> <li>Decision:<ul> <li>If the attempt count is less than <code>max_attempts</code> (from your configuration), the handler republishes the message back to its original topic with an incremented retry header.</li> <li>If the attempt count has reached <code>max_attempts</code>, the handler constructs a detailed \"DLQ envelope\" (a JSON object with the original message and error details) and publishes it to the configured DLQ topic.</li> </ul> </li> </ol>"},{"location":"athomic/integration/messaging/retry-dlq/#the-dlq-processor","title":"The DLQ Processor","text":"<p>Once a message is in the DLQ topic, it needs to be handled. Athomic provides a dedicated background service, the <code>DeadLetterProcessorService</code>, which automatically subscribes to your DLQ topic.</p> <p>This service consumes the DLQ envelopes and passes them to a configured Dead Letter Strategy for final disposition.</p>"},{"location":"athomic/integration/messaging/retry-dlq/#available-strategies","title":"Available Strategies","text":"<ul> <li><code>logging_only</code> (Default): This strategy simply logs the full details of the failed message envelope as a structured <code>ERROR</code> log. This is the safest default, ensuring that failing messages are recorded for manual inspection without taking further automated action.</li> <li><code>republish_to_parking_lot</code>: This strategy republishes the failed message to yet another topic, a \"parking lot\", which can be used for long-term storage or offline analysis.</li> </ul>"},{"location":"athomic/integration/messaging/retry-dlq/#configuration","title":"Configuration","text":"<p>The retry and DLQ mechanism is configured under the <code>[integration.messaging.dlq]</code> section in your <code>settings.toml</code>.</p> <pre><code>[default.integration.messaging.dlq]\n# A master switch to enable or disable the entire retry/DLQ flow.\nenabled = true\n\n# The maximum number of times to retry processing a message before sending it to the DLQ.\nmax_attempts = 3\n\n# The name of the Dead Letter Queue topic where failed messages will be sent.\ntopic = \"platform.dead-letter-queue.v1\"\n\n# The strategy for handling messages that arrive in the DLQ.\n# Can be \"logging_only\" or \"republish_to_parking_lot\".\nstrategy = \"logging_only\"\n\n# (Optional) The topic to use if the 'republish_to_parking_lot' strategy is chosen.\n# parking_lot_topic = \"platform.parking-lot.v1\"\n</code></pre>"},{"location":"athomic/integration/messaging/retry-dlq/#api-reference","title":"API Reference","text":""},{"location":"athomic/integration/messaging/retry-dlq/#nala.athomic.integration.messaging.retry.handler.DLQHandler","title":"<code>nala.athomic.integration.messaging.retry.handler.DLQHandler</code>","text":"<p>Orchestrates the handling of consumption failures, implementing the retry mechanism (republishing) and the final dead-letter queue (DLQ) publishing.</p> <p>This class serves as the decision point: - If attempts &lt; max_attempts and it's a single message: Republish for retry. - If attempts &gt;= max_attempts or it's a batch failure: Send to DLQ.</p>"},{"location":"athomic/integration/messaging/retry-dlq/#nala.athomic.integration.messaging.retry.handler.DLQHandler.__init__","title":"<code>__init__(producer, retry_policy, serializer, topic, max_attempts)</code>","text":"<p>Initializes the DLQ handler with all necessary dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>producer</code> <code>ProducerProtocol</code> <p>The producer used to send messages (for retry or DLQ).</p> required <code>retry_policy</code> <code>MaxAttemptsPolicy</code> <p>The policy defining how retry headers and counts are managed.</p> required <code>serializer</code> <code>SerializerProtocol</code> <p>The serializer used for general data conversion.</p> required <code>topic</code> <code>str</code> <p>The destination topic name for the Dead Letter Queue.</p> required <code>max_attempts</code> <code>int</code> <p>The maximum number of times to retry a message before sending to DLQ.</p> required"},{"location":"athomic/integration/messaging/retry-dlq/#nala.athomic.integration.messaging.retry.handler.DLQHandler.handle_failure","title":"<code>handle_failure(context)</code>  <code>async</code>","text":"<p>Orchestrates failure handling using an explicit context object. - Single messages will be retried up to <code>max_attempts</code>. - Batch messages will be sent directly to the DLQ without retry attempts.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>FailureContext</code> <p>The <code>FailureContext</code> object containing details of the consumption failure.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the message was handled (retried or sent to DLQ), False otherwise.</p>"},{"location":"athomic/integration/messaging/retry-dlq/#nala.athomic.integration.messaging.retry.processor_service.DeadLetterProcessorService","title":"<code>nala.athomic.integration.messaging.retry.processor_service.DeadLetterProcessorService</code>","text":"<p>               Bases: <code>BaseService</code></p> <p>A dedicated background service that manages the lifecycle of a consumer subscribed to the Dead Letter Queue (DLQ).</p> <p>This service is backend-agnostic as it receives a pre-configured consumer instance via dependency injection (DIP). Its purpose is purely to ensure the DLQ consumer is started and stopped gracefully with the application.</p>"},{"location":"athomic/integration/messaging/retry-dlq/#nala.athomic.integration.messaging.retry.processor_service.DeadLetterProcessorService.__init__","title":"<code>__init__(consumer)</code>","text":"<p>Initializes the service with a pre-configured DLQ consumer.</p> <p>Parameters:</p> Name Type Description Default <code>consumer</code> <code>BaseConsumer</code> <p>A consumer instance that is already configured to subscribe       to the DLQ topic and process its messages. This consumer       is typically created by a Factory and injected here.</p> required"},{"location":"athomic/integration/messaging/retry-dlq/#nala.athomic.integration.messaging.retry.strategies.protocol.DeadLetterStrategyProtocol","title":"<code>nala.athomic.integration.messaging.retry.strategies.protocol.DeadLetterStrategyProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the contract for strategies that manage and handle messages that have permanently failed processing (Dead Letter Queue - DLQ).</p> <p>Any class implementing this protocol must provide a concrete implementation for the asynchronous 'handle' method.</p>"},{"location":"athomic/integration/messaging/retry-dlq/#nala.athomic.integration.messaging.retry.strategies.protocol.DeadLetterStrategyProtocol.handle","title":"<code>handle(failure_details)</code>  <code>async</code>","text":"<p>Processes a message that has permanently failed and requires a final disposition (e.g., logging, manual queue, or permanent storage).</p> <p>Parameters:</p> Name Type Description Default <code>failure_details</code> <code>Dict[str, Any]</code> <p>A dictionary containing all relevant information              about the failed message, including payload,              exception details, and retry history.</p> required <p>Returns:</p> Name Type Description <code>ProcessingOutcome</code> <code>ProcessingOutcome</code> <p>An enum indicating the result of the DLQ handling process.                - ACK: Indicates the handling strategy was successful                       and the message can be removed from the DLQ.                - REQUEUE: Indicates a temporary failure in the handling                           process, and the message should be requeued                           to the DLQ for another attempt.</p>"},{"location":"athomic/observability/health/","title":"Health &amp; Readiness Checks","text":""},{"location":"athomic/observability/health/#overview","title":"Overview","text":"<p>The Health &amp; Readiness module provides a standardized and extensible system for determining if the application is healthy and ready to handle traffic. This is a critical feature for running in orchestrated environments like Kubernetes, which rely on readiness probes to know when to add a service instance to the load balancer.</p> <p>The framework exposes an HTTP endpoint, typically <code>/readyz</code>, which runs a series of checks against all critical dependencies (databases, message brokers, external APIs) and reports their status.</p>"},{"location":"athomic/observability/health/#how-it-works","title":"How It Works","text":"<p>The system is built around a few core components that promote decoupling and extensibility:</p> <ol> <li> <p><code>ReadinessCheck</code> Protocol: A simple contract that any readiness check must follow. It requires a unique <code>name</code>, an <code>enabled()</code> method, and an asynchronous <code>check()</code> method that returns <code>True</code> for healthy or <code>False</code> for unhealthy.</p> </li> <li> <p><code>ReadinessRegistry</code>: A singleton registry where all readiness check instances are registered during application startup.</p> </li> <li> <p><code>ServiceReadinessCheck</code>: A generic and powerful implementation that can check the status of any Athomic <code>BaseService</code>. It automatically integrates with the service lifecycle, so a readiness check for the Kafka consumer, for example, simply queries <code>kafka_consumer.is_ready()</code>.</p> </li> <li> <p><code>/readyz</code> Endpoint: An internal API route that, when called, executes the <code>run_all()</code> method on the <code>ReadinessRegistry</code>. This runs all registered checks concurrently and aggregates their results into a single JSON response. The overall HTTP status will be <code>200 OK</code> only if all enabled checks pass.</p> </li> </ol>"},{"location":"athomic/observability/health/#how-to-add-a-custom-readiness-check","title":"How to Add a Custom Readiness Check","text":"<p>You can easily add your own application-specific readiness checks. For example, you might want to check the status of a critical third-party API that your service depends on.</p>"},{"location":"athomic/observability/health/#1-create-the-check-class","title":"1. Create the Check Class","text":"<p>Create a class that implements the <code>ReadinessCheck</code> protocol.</p> <pre><code># In your_app/health_checks.py\nfrom nala.athomic.http import HttpClientFactory\nfrom nala.athomic.observability.health import ReadinessCheck\n\nclass ExternalApiServiceCheck(ReadinessCheck):\n    name = \"external_api_status\"\n\n    def __init__(self):\n        # Get a pre-configured HTTP client from the factory\n        self.http_client = HttpClientFactory.create(\"my_external_api_client\")\n\n    def enabled(self) -&gt; bool:\n        # The check is enabled if the client itself is enabled in the config\n        return self.http_client.is_enabled()\n\n    async def check(self) -&gt; bool:\n        try:\n            # Perform a lightweight check, like a HEAD request or a health endpoint call\n            response = await self.http_client.get(\"/_health\")\n            return response.status_code == 200\n        except Exception:\n            return False\n</code></pre>"},{"location":"athomic/observability/health/#2-register-the-check","title":"2. Register the Check","text":"<p>In your application's startup sequence (e.g., <code>domain_initializers.py</code>), instantiate your check and register it.</p> <pre><code># In your_app/startup/domain_initializers.py\nfrom nala.athomic.observability.health import readiness_registry\nfrom your_app.health_checks import ExternalApiServiceCheck\n\ndef register_domain_services():\n    # ... other registrations ...\n\n    # Register your custom health check\n    readiness_registry.register(ExternalApiServiceCheck())\n</code></pre> <p>Your custom check will now be automatically executed and reported by the <code>/readyz</code> endpoint.</p>"},{"location":"athomic/observability/health/#example-response","title":"Example Response","text":"<p>A call to the <code>/readyz</code> endpoint will return a JSON response detailing the status of each check.</p> <pre><code>{\n  \"status\": \"unhealthy\",\n  \"checks\": {\n    \"consul_client\": \"ok\",\n    \"database_connection_manager\": \"ok\",\n    \"kafka_consumer_my_app.events.v1\": \"ok\",\n    \"external_api_status\": \"fail\"\n  }\n}\n</code></pre>"},{"location":"athomic/observability/health/#api-reference","title":"API Reference","text":""},{"location":"athomic/observability/health/#nala.athomic.observability.health.protocol.ReadinessCheck","title":"<code>nala.athomic.observability.health.protocol.ReadinessCheck</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the contract for an individual readiness check implementation.</p> <p>Any class that implements this protocol can be registered with the ReadinessRegistry to contribute to the overall application readiness state.</p>"},{"location":"athomic/observability/health/#nala.athomic.observability.health.protocol.ReadinessCheck.name","title":"<code>name</code>  <code>instance-attribute</code>","text":"<p>A unique, descriptive name for the check (e.g., 'database_connection').</p>"},{"location":"athomic/observability/health/#nala.athomic.observability.health.protocol.ReadinessCheck.check","title":"<code>check()</code>  <code>async</code>","text":"<p>Performs the asynchronous check of the dependency or resource.</p> <p>This method must be lightweight and fast to avoid delaying the readiness probe.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the resource is healthy (ready), False otherwise.</p>"},{"location":"athomic/observability/health/#nala.athomic.observability.health.protocol.ReadinessCheck.enabled","title":"<code>enabled()</code>","text":"<p>Determines if the check should be executed based on configuration or runtime environment.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the check should run, False otherwise.</p>"},{"location":"athomic/observability/health/#nala.athomic.observability.health.registry.ReadinessRegistry","title":"<code>nala.athomic.observability.health.registry.ReadinessRegistry</code>","text":"<p>A registry responsible for collecting and orchestrating all application readiness checks.</p> <p>This acts as a centralized source of truth for determining if the application and its core dependencies (databases, message brokers, external services) are fully initialized and ready to handle live traffic.</p>"},{"location":"athomic/observability/health/#nala.athomic.observability.health.registry.ReadinessRegistry.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the internal dictionary to store readiness checks, mapped by name.</p>"},{"location":"athomic/observability/health/#nala.athomic.observability.health.registry.ReadinessRegistry.register","title":"<code>register(check)</code>","text":"<p>Registers a new ReadinessCheck implementation with the registry.</p> <p>Parameters:</p> Name Type Description Default <code>check</code> <code>ReadinessCheck</code> <p>An instance of a ReadinessCheck protocol implementation.</p> required"},{"location":"athomic/observability/health/#nala.athomic.observability.health.registry.ReadinessRegistry.run_all","title":"<code>run_all()</code>  <code>async</code>","text":"<p>Executes all registered readiness checks asynchronously.</p> <p>It respects the <code>enabled()</code> status of each check and handles exceptions during execution by marking the check as failed.</p> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: A dictionary containing the name of each check             and its resulting status: 'ok', 'fail', or 'skipped'.</p>"},{"location":"athomic/observability/health/#nala.athomic.observability.health.checks.service_check.ServiceReadinessCheck","title":"<code>nala.athomic.observability.health.checks.service_check.ServiceReadinessCheck</code>","text":"<p>               Bases: <code>ReadinessCheck</code></p> <p>A generic readiness check implementation that verifies the health and readiness state of any core Athomic service implementing the BaseServiceProtocol.</p> <p>This check is a crucial part of the Dependency Inversion Principle, allowing the health system to query service status without knowing the service's internal implementation details.</p>"},{"location":"athomic/observability/health/#nala.athomic.observability.health.checks.service_check.ServiceReadinessCheck.__init__","title":"<code>__init__(service)</code>","text":"<p>Initializes the check by injecting the service instance to be monitored.</p> <p>Parameters:</p> Name Type Description Default <code>service</code> <code>BaseServiceProtocol</code> <p>The service instance (e.g., OutboxPublisher, HttpClient)      whose readiness state will be checked.</p> required"},{"location":"athomic/observability/health/#nala.athomic.observability.health.checks.service_check.ServiceReadinessCheck.check","title":"<code>check()</code>  <code>async</code>","text":"<p>Checks if the service is ready to operate (e.g., connected to its dependencies and initialized).</p> <p>Delegates the call directly to the service's <code>is_ready()</code> method.</p>"},{"location":"athomic/observability/health/#nala.athomic.observability.health.checks.service_check.ServiceReadinessCheck.enabled","title":"<code>enabled()</code>","text":"<p>Checks if the underlying service is enabled based on its configuration.</p> <p>Delegates the call directly to the service's <code>is_enabled()</code> method.</p>"},{"location":"athomic/observability/log/","title":"Structured Logging","text":""},{"location":"athomic/observability/log/#overview","title":"Overview","text":"<p>The logging module provides a powerful, structured, and secure logging system built upon the <code>Loguru</code> library. It is designed to be both highly performant and developer-friendly, with two primary goals:</p> <ol> <li>Rich, Structured Logs: Automatically enriches log records with contextual information from the <code>Context</code> module (e.g., <code>request_id</code>, <code>tenant_id</code>, <code>trace_id</code>). It can output logs as JSON, making them easy to parse, index, and query in modern log management systems.</li> <li>Security by Default: Includes a robust and extensible sensitive data masking engine that automatically redacts PII (Personally Identifiable Information) and secrets from log messages before they are written to any sink.</li> </ol>"},{"location":"athomic/observability/log/#how-to-use","title":"How to Use","text":"<p>Getting a logger instance is simple. The <code>get_logger()</code> function returns a pre-configured singleton instance of the Loguru logger.</p> <pre><code>from nala.athomic.observability import get_logger\n\n# It's recommended to get a logger scoped to your module\nlogger = get_logger(__name__)\n\ndef process_user_data(user_id: str):\n    # This log record will be automatically enriched with any\n    # context variables that are currently set.\n    logger.info(f\"Processing data for user {user_id}\")\n</code></pre>"},{"location":"athomic/observability/log/#sensitive-data-masking","title":"Sensitive Data Masking","text":"<p>This is a core security feature of the Athomic Layer. The <code>SensitiveDataFilter</code> automatically finds and redacts sensitive information from log messages before they are written.</p>"},{"location":"athomic/observability/log/#out-of-the-box-patterns","title":"Out-of-the-Box Patterns","text":"<p>The filter comes with a wide range of built-in patterns for common PII and secrets, including:</p> <ul> <li>JWTs: Replaces JSON Web Tokens with <code>***REDACTED_JWT***</code>.</li> <li>Credit Card Numbers: Performs PCI-compliant masking, showing only the last four digits (e.g., <code>****-****-****-1234</code>).</li> <li>Brazilian CPFs: Performs smart masking, showing only the verification digits (e.g., <code>***.***.***-56</code>).</li> <li>Phone Numbers: Performs smart masking, preserving the area code and last two digits (e.g., <code>(11) *****-**78</code>).</li> <li>Email Addresses: Replaces emails with <code>***@***.***</code>.</li> <li>Common Secret Keys: Redacts values for keys like <code>\"password\"</code>, <code>\"api_key\"</code>, and <code>\"token\"</code> in JSON-like strings.</li> <li>Authorization Headers: Redacts the token from <code>Authorization: Bearer ...</code> headers.</li> </ul>"},{"location":"athomic/observability/log/#pattern-scoring-system","title":"Pattern Scoring System","text":"<p>To ensure accuracy, the filter applies masking patterns in order of specificity. A highly specific pattern for a JWT will run before a more generic pattern for an email address, preventing incorrect redactions.</p>"},{"location":"athomic/observability/log/#adding-custom-masking-patterns","title":"Adding Custom Masking Patterns","text":"<p>You can easily extend the masker with your own regex patterns directly in your configuration file.</p> <pre><code># In settings.toml\n[default.logging]\n# ... other logging settings ...\n\n[[default.logging.sensitive_patterns]]\n# This will find any string matching \"secret-project-key-...\" and redact it.\nregex = \"secret-project-key-[a-zA-Z0-9]+\"\nreplacement = \"REDACTED_PROJECT_KEY\"\n\n[[default.logging.sensitive_patterns]]\n# You can also use regex capture groups.\nregex = \"user_phone_number=(d+)\"\nreplacement = \"user_phone_number=REDACTED\"\n</code></pre>"},{"location":"athomic/observability/log/#json-logging","title":"JSON Logging","text":"<p>For production environments, it is highly recommended to enable JSON logging. This formats every log entry as a structured JSON object, which can be easily ingested and indexed by platforms like Elasticsearch (ELK Stack), Datadog, Splunk, or Google Cloud Logging.</p> <p>To enable it, simply set <code>serialize = true</code> in your configuration.</p>"},{"location":"athomic/observability/log/#configuration","title":"Configuration","text":"<p>The logging system is configured under the <code>[logging]</code> section in your <code>settings.toml</code>.</p> <pre><code>[default.logging]\n# The minimum log level to process (e.g., \"DEBUG\", \"INFO\", \"WARNING\").\nlevel = \"INFO\"\n\n# If true, log records are formatted as JSON strings.\nserialize = true\n\n# If true, logs are written to a file instead of the console.\nlog_to_file = false\nlog_file_path = \".logs/app.log\"\nrotation = \"100 MB\"\nretention = \"7 days\"\n\n# A list of custom patterns for the sensitive data filter.\n[[default.logging.sensitive_patterns]]\nregex = \"my-custom-secret-([a-z0-9-]+)\"\nreplacement = \"my-custom-secret-REDACTED\"\n</code></pre>"},{"location":"athomic/observability/log/#api-reference","title":"API Reference","text":""},{"location":"athomic/observability/log/#nala.athomic.observability.get_logger","title":"<code>nala.athomic.observability.get_logger(name=None)</code>","text":"<p>Retrieves a logger instance, binding the component name if provided. This is the primary way for application modules to obtain a logger instance.</p>"},{"location":"athomic/observability/log/#nala.athomic.observability.log.filters.sensitive_data_filter.SensitiveDataFilter","title":"<code>nala.athomic.observability.log.filters.sensitive_data_filter.SensitiveDataFilter</code>","text":"<p>A callable filter designed to sanitize log messages by redacting or masking sensitive data (PII, secrets, tokens) before they are written to any sink.</p> <p>This filter is the core enforcement mechanism for log compliance and security.</p>"},{"location":"athomic/observability/log/#nala.athomic.observability.log.filters.sensitive_data_filter.SensitiveDataFilter.__call__","title":"<code>__call__(message)</code>","text":"<p>Makes the filter instance callable, which is the contract required by Loguru.</p>"},{"location":"athomic/observability/log/#nala.athomic.observability.log.filters.sensitive_data_filter.SensitiveDataFilter.__init__","title":"<code>__init__(patterns, repl='***REDACTED***')</code>","text":"<p>Initializes the filter by compiling all registered and configured patterns.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>List[tuple[Union[str, Pattern], Union[str, Callable[[Match], str]]]]</code> <p>A list of maskers/patterns, potentially unsorted.</p> required <code>repl</code> <code>str</code> <p>Default replacement string (not currently used in internal logic).</p> <code>'***REDACTED***'</code>"},{"location":"athomic/observability/log/#nala.athomic.observability.log.maskers.base_masker.BaseMasker","title":"<code>nala.athomic.observability.log.maskers.base_masker.BaseMasker</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract Base Class (ABC) for all sensitive data maskers within the Athomic framework.</p> <p>This class enforces the contract required by the SensitiveDataFilter, ensuring that every masker provides both a unique regex pattern and a specific masking implementation.</p>"},{"location":"athomic/observability/log/#nala.athomic.observability.log.maskers.base_masker.BaseMasker.mask","title":"<code>mask(match)</code>  <code>abstractmethod</code>","text":"<p>Receives the regex match object for the sensitive data and returns the masked (redacted) version of the data.</p> <p>This method should contain the core logic for intelligent masking.</p> <p>Parameters:</p> Name Type Description Default <code>match</code> <code>Match</code> <p>The regex match object containing the captured sensitive data.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The masked replacement string.</p>"},{"location":"athomic/observability/log/#nala.athomic.observability.log.maskers.base_masker.BaseMasker.pattern","title":"<code>pattern()</code>  <code>abstractmethod</code>","text":"<p>Returns the compiled regex pattern object that this masker is responsible for detecting.</p> <p>Returns:</p> Type Description <code>Pattern</code> <p>re.Pattern: The compiled regular expression.</p>"},{"location":"athomic/observability/metrics/","title":"Metrics","text":""},{"location":"athomic/observability/metrics/#overview","title":"Overview","text":"<p>The metrics module provides deep, out-of-the-box application monitoring using the Prometheus format, the de-facto standard for cloud-native observability. Nearly every component in the Athomic Layer is instrumented with detailed metrics, offering invaluable insights into performance, throughput, error rates, and overall system health.</p> <p>This allows you to create powerful dashboards and alerts to monitor your application in real-time.</p>"},{"location":"athomic/observability/metrics/#how-it-works","title":"How It Works","text":"<ol> <li>Metric Registration: Throughout the Athomic codebase, Prometheus metrics (<code>Counter</code>, <code>Histogram</code>, <code>Gauge</code>) are defined to track specific events and states.</li> <li>Instrumentation: Core components automatically update these metrics during their operation. For example, the <code>HttpClient</code> updates a histogram with request latency and a counter for successes or failures.</li> <li>Exposure: At startup, the application automatically starts a lightweight HTTP server that exposes a <code>/metrics</code> endpoint. This endpoint serves all registered metrics in the text-based Prometheus format.</li> <li>Scraping: A Prometheus server is then configured to periodically \"scrape\" (fetch) the data from this <code>/metrics</code> endpoint, storing it as a time series.</li> </ol>"},{"location":"athomic/observability/metrics/#the-metricscheduler","title":"The <code>MetricScheduler</code>","text":"<p>For metrics that cannot be updated on-event (like the current number of pending messages), Athomic includes a <code>MetricScheduler</code>. This is a background service that periodically runs \"Probes\" (<code>MetricProbe</code> implementations) to collect and update gauge-based metrics, such as Kafka consumer lag.</p>"},{"location":"athomic/observability/metrics/#built-in-metrics","title":"Built-in Metrics","text":"<p>Athomic provides a comprehensive set of built-in metrics. Below is a high-level summary of what is available out-of-the-box. For a full list, you can inspect the <code>/metrics</code> endpoint of a running application.</p> <ul> <li>API Layer:<ul> <li>Request rate, error rate, and latency (per route, method, and status code).</li> <li>Number of in-progress requests.</li> </ul> </li> <li>Database &amp; Cache:<ul> <li>Operation latency and total counts for each KV store and document database operation (<code>get</code>, <code>set</code>, <code>find_by_id</code>, etc.).</li> <li>Cache hit and miss ratios.</li> </ul> </li> <li>Messaging (Kafka):<ul> <li>Number of messages published and consumed.</li> <li>Message publishing latency.</li> <li>Consumer Lag per topic and partition.</li> <li>Total messages sent to the Dead Letter Queue (DLQ).</li> </ul> </li> <li>Resilience Patterns:<ul> <li>Circuit Breaker: State changes, failures recorded, and calls blocked.</li> <li>Retry: Total retry attempts and permanent failures.</li> <li>Rate Limiter: Total requests allowed and blocked per policy.</li> <li>Bulkhead: Concurrently executing requests and rejections.</li> </ul> </li> <li>Transactional Outbox:<ul> <li>Number of events processed (success/failure).</li> <li>Event processing lag (time between creation and publication).</li> <li>Pending messages for the \"hottest\" aggregate keys.</li> </ul> </li> </ul>"},{"location":"athomic/observability/metrics/#adding-custom-metrics","title":"Adding Custom Metrics","text":"<p>You can easily define and use your own custom metrics within your application's business logic using the <code>prometheus-client</code> library.</p> <pre><code>from prometheus_client import Counter\nfrom nala.athomic.observability import get_logger\n\nlogger = get_logger(__name__)\n\n# 1. Define your metric at the module level.\nORDERS_PROCESSED_TOTAL = Counter(\n    \"orders_processed_total\",\n    \"Total number of orders processed.\",\n    [\"order_type\", \"status\"]\n)\n\nclass OrderService:\n    async def process_order(self, order: Order):\n        try:\n            # ... your business logic ...\n\n            # 2. Increment the counter on success.\n            ORDERS_PROCESSED_TOTAL.labels(order_type=order.type, status=\"success\").inc()\n            logger.info(\"Order processed successfully.\")\n\n        except Exception:\n            # 3. Increment the counter with a different label on failure.\n            ORDERS_PROCESSED_TOTAL.labels(order_type=order.type, status=\"failure\").inc()\n            logger.error(\"Failed to process order.\")\n            raise\n</code></pre>"},{"location":"athomic/observability/metrics/#configuration","title":"Configuration","text":"<p>Metrics are configured under the <code>[observability]</code> and <code>[observability.metrics]</code> sections in your <code>settings.toml</code>.</p> <pre><code>[default.observability]\nenabled = true\n\n# If true, starts an HTTP server to expose Prometheus metrics.\nexporter_enabled = true\n\n# The port on which the Prometheus metrics server will listen.\nexporter_port = 9100\n\n  [default.observability.metrics]\n  enabled = true\n\n  # The interval in seconds at which the MetricScheduler runs its probes (e.g., for Kafka lag).\n  collection_interval_seconds = 60\n\n  # If true, access to the /metrics endpoint is restricted to the IPs below.\n  metrics_protection_enabled = false\n  allow_metrics_ips = [\"127.0.0.1\", \"10.0.0.5\"]\n</code></pre>"},{"location":"athomic/observability/metrics/#api-reference","title":"API Reference","text":""},{"location":"athomic/observability/metrics/#nala.athomic.observability.metrics.exporter.start_metrics_server","title":"<code>nala.athomic.observability.metrics.exporter.start_metrics_server()</code>","text":"<p>Initializes and starts the HTTP server to expose Prometheus metrics for scraping by external monitoring systems.</p> <p>This operation is conditional and runs only if observability is enabled in the application settings.</p>"},{"location":"athomic/observability/metrics/#nala.athomic.observability.metrics.metric_scheduler.MetricScheduler","title":"<code>nala.athomic.observability.metrics.metric_scheduler.MetricScheduler</code>","text":"<p>               Bases: <code>BaseService</code></p> <p>Manages a background task responsible for the periodic execution of all registered MetricProbes in the central ProbeRegistry.</p> <p>This service ensures continuous and asynchronous collection of application and infrastructure telemetry.</p>"},{"location":"athomic/observability/metrics/#nala.athomic.observability.metrics.metric_scheduler.MetricScheduler.__init__","title":"<code>__init__(collection_interval_seconds)</code>","text":"<p>Initializes the scheduler with the specified collection interval.</p> <p>Parameters:</p> Name Type Description Default <code>collection_interval_seconds</code> <code>int</code> <p>The frequency (in seconds) at which                          the probes should be executed.</p> required"},{"location":"athomic/observability/metrics/#nala.athomic.observability.metrics.protocol.MetricProbe","title":"<code>nala.athomic.observability.metrics.protocol.MetricProbe</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the contract for a component that periodically collects and updates specific application metrics (Probes).</p> <p>Any class implementing this protocol can be scheduled for asynchronous execution by a MetricScheduler service.</p>"},{"location":"athomic/observability/metrics/#nala.athomic.observability.metrics.protocol.MetricProbe.name","title":"<code>name</code>  <code>instance-attribute</code>","text":"<p>A unique, human-readable name identifying the metric probe (e.g., 'cpu_usage_probe').</p>"},{"location":"athomic/observability/metrics/#nala.athomic.observability.metrics.protocol.MetricProbe.update","title":"<code>update()</code>  <code>async</code>","text":"<p>Executes the metric collection logic (e.g., reads a value, calculates a delta) and updates the corresponding Prometheus collector.</p> <p>This method must be non-blocking (asynchronous).</p>"},{"location":"athomic/observability/tracing/","title":"Distributed Tracing","text":""},{"location":"athomic/observability/tracing/#overview","title":"Overview","text":"<p>The distributed tracing module provides end-to-end request tracing capabilities built on the OpenTelemetry standard, the industry benchmark for observability. This feature is invaluable for debugging performance bottlenecks and understanding complex, asynchronous workflows in a microservices architecture.</p> <p>It allows you to visualize the entire journey of a request as it travels through different services and components\u2014from the initial API call, through message brokers, to database queries and back.</p>"},{"location":"athomic/observability/tracing/#key-features","title":"Key Features","text":"<ul> <li>OpenTelemetry Native: Fully compliant with the OpenTelemetry standard, allowing integration with any OTLP-compatible backend (e.g., Jaeger, Datadog, Honeycomb).</li> <li>Auto-Instrumentation: Automatically creates trace spans for common I/O operations, including outgoing HTTP requests (<code>httpx</code>), database queries (<code>pymongo</code>), and cache interactions (<code>redis</code>).</li> <li>Effortless Manual Instrumentation: A simple <code>@with_observability</code> decorator allows you to add detailed tracing to your business logic with a single line of code.</li> <li>Automatic Context Propagation: The trace context (e.g., <code>trace_id</code>, <code>span_id</code>) is automatically propagated across service boundaries, including HTTP calls and messages sent via a broker like Kafka.</li> </ul>"},{"location":"athomic/observability/tracing/#how-it-works","title":"How It Works","text":""},{"location":"athomic/observability/tracing/#setup","title":"Setup","text":"<p>During application startup, the <code>setup_tracing()</code> function is automatically called. It configures the global OpenTelemetry SDK, initializes an OTLP exporter to send trace data to a collector, and applies auto-instrumentation patches to key libraries.</p>"},{"location":"athomic/observability/tracing/#instrumentation","title":"Instrumentation","text":"<ul> <li>Automatic: For libraries like <code>httpx</code> and <code>pymongo</code>, you get tracing for free. Every database query or external API call will appear as a span in your trace without you writing any extra code.</li> <li>Manual: For your own business logic, you should use the provided decorators to create spans that represent meaningful units of work.</li> </ul>"},{"location":"athomic/observability/tracing/#usage-the-with_observability-decorator","title":"Usage: The <code>@with_observability</code> Decorator","text":"<p>The easiest and recommended way to add tracing to your code is with the <code>@with_observability</code> decorator. It's a powerful tool that automatically handles the entire span lifecycle.</p> <p>When you decorate a function, it will: 1.  Start a new span when the function is called. 2.  Automatically add useful attributes to the span, such as the function's arguments. 3.  Measure the execution duration. 4.  Record any exceptions that occur. 5.  Set the span's final status (<code>OK</code> or <code>ERROR</code>).</p>"},{"location":"athomic/observability/tracing/#example","title":"Example","text":"<pre><code>from nala.athomic.observability.decorators import with_observability\n\nclass UserProfileService:\n    @with_observability(\n        name=\"service.get_user_profile\",\n        attributes_from_args={\"user_id\": \"user.id\"}\n    )\n    async def get_profile(self, user_id: str) -&gt; dict:\n        # ... your business logic ...\n        profile_data = await self.repository.find_by_id(user_id)\n        # ... more logic ...\n        return profile_data\n</code></pre> <p>In this example, every call to <code>get_profile</code> will generate a trace span named <code>\"service.get_user_profile\"</code>, and it will automatically include an attribute <code>user.id</code> with the value of the <code>user_id</code> argument.</p>"},{"location":"athomic/observability/tracing/#context-propagation","title":"Context Propagation","text":"<p>Athomic handles trace context propagation automatically across service boundaries:</p> <ul> <li>HTTP Requests: The <code>HttpClient</code> automatically injects W3C Trace Context headers into outgoing requests. The API middleware automatically extracts them from incoming requests, ensuring the trace continues seamlessly.</li> <li>Messaging: When a message is published, the <code>Producer</code> injects the trace context into the message headers. The <code>Consumer</code> on the other side extracts it, allowing a single trace to span from an API request to a background worker processing a resulting message.</li> </ul>"},{"location":"athomic/observability/tracing/#configuration","title":"Configuration","text":"<p>Tracing is configured under the <code>[observability]</code> section in your <code>settings.toml</code>.</p> <pre><code>[default.observability]\n# A master switch for all observability features (metrics and tracing).\nenabled = true\n\n# A master switch specifically for distributed tracing.\ntracing_enabled = true\n\n# The gRPC or HTTP endpoint of the OpenTelemetry Collector.\n# Spans will be sent here.\n# Example for a local Jaeger setup:\notlp_endpoint = \"http://localhost:4317\"\n\n# The sampling rate for traces (1.0 = 100%, 0.5 = 50%).\nsampling_rate = 1.0\n\n# Optional: Override the service name that appears in your tracing backend.\n# If not set, it defaults to the `app_name`.\nservice_name_override = \"my-awesome-service\"\n</code></pre>"},{"location":"athomic/observability/tracing/#api-reference","title":"API Reference","text":""},{"location":"athomic/observability/tracing/#nala.athomic.observability.tracing.setup_tracing","title":"<code>nala.athomic.observability.tracing.setup_tracing(settings=None)</code>","text":"<p>Initializes the OpenTelemetry Tracing SDK, configures the OTLP exporter, and instruments core libraries.</p> <p>This function is idempotent and serves as the central configuration point for distributed tracing.</p>"},{"location":"athomic/observability/tracing/#nala.athomic.observability.decorators.with_observability","title":"<code>nala.athomic.observability.decorators.with_observability</code>","text":""},{"location":"athomic/observability/tracing/#nala.athomic.observability.decorators.with_observability.build_span_attributes","title":"<code>build_span_attributes(bound_args, static_attributes, attributes_from_args)</code>","text":"<p>Constructs OpenTelemetry span attributes from context and function arguments.</p>"},{"location":"athomic/observability/tracing/#nala.athomic.observability.decorators.with_observability.log_call","title":"<code>log_call(logger, func_name, args, kwargs)</code>","text":"<p>Logs the function call, including arguments and execution context.</p>"},{"location":"athomic/observability/tracing/#nala.athomic.observability.decorators.with_observability.log_error","title":"<code>log_error(logger, func_name, error)</code>","text":"<p>Logs detailed exception information upon function failure.</p>"},{"location":"athomic/observability/tracing/#nala.athomic.observability.decorators.with_observability.log_result","title":"<code>log_result(logger, func_name, result)</code>","text":"<p>Logs the function return value upon successful completion.</p>"},{"location":"athomic/observability/tracing/#nala.athomic.observability.decorators.with_observability.with_observability","title":"<code>with_observability(name=None, kind=SpanKind.INTERNAL, attributes_from_args=None, static_attributes=None, should_log_result=True, log_args=True)</code>","text":"<p>A unified decorator for Observability that instruments a function with OpenTelemetry tracing and structured logging (call, result, error).</p> <p>This combines tracing and logging boilerplate into a single, declarative wrapper, adhering to Aspect-Oriented Programming (AOP).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>Optional name for the span. Defaults to the decorated function's name.</p> <code>None</code> <code>kind</code> <code>SpanKind</code> <p>The SpanKind (e.g., SERVER, CLIENT, CONSUMER, PRODUCER). Defaults to INTERNAL.</p> <code>INTERNAL</code> <code>attributes_from_args</code> <code>Optional[Dict[str, str]]</code> <p>A mapping of {function_arg_name: span_attribute_key}                   to dynamically extract values from the call signature.</p> <code>None</code> <code>static_attributes</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary of static key/value pairs to add to the span.</p> <code>None</code> <code>should_log_result</code> <code>bool</code> <p>If True, logs the function's return value. Defaults to True.</p> <code>True</code> <code>log_args</code> <code>bool</code> <p>If True, logs the function's arguments upon call. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable[..., Any]</code> <p>The decorator function that returns the wrapped function.</p>"},{"location":"athomic/observability/tracing/#nala.athomic.observability.get_tracer","title":"<code>nala.athomic.observability.get_tracer(name=None)</code>","text":"<p>Retrieves the global OpenTelemetry Tracer instance, optionally scoped by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>The name used to scope the tracer instance. Defaults to the   configured service name.</p> <code>None</code>"},{"location":"athomic/observability/tracing/#nala.athomic.observability.telemetry.adapters.messaging_adapter.MessagingTelemetryAdapter","title":"<code>nala.athomic.observability.telemetry.adapters.messaging_adapter.MessagingTelemetryAdapter</code>","text":"<p>               Bases: <code>TelemetryAdapterProtocol</code></p> <p>A concrete Telemetry Adapter for messaging systems.</p> <p>This adapter handles the injection and extraction of W3C Trace Context into and from the Athomic MessageHeaders object, enabling end-to-end tracing across message broker boundaries.</p>"},{"location":"athomic/observability/tracing/#nala.athomic.observability.telemetry.adapters.messaging_adapter.MessagingTelemetryAdapter.activate_from_headers","title":"<code>activate_from_headers(headers)</code>","text":"<p>A context manager that extracts the trace context from incoming message headers and activates it for the duration of the message processing.</p> <p>This is the primary method used by consumer implementations to continue the distributed trace from the producer.</p>"},{"location":"athomic/observability/tracing/#nala.athomic.observability.telemetry.adapters.messaging_adapter.MessagingTelemetryAdapter.extract_context","title":"<code>extract_context(headers)</code>","text":"<p>Extracts the trace context from an incoming MessageHeaders object and returns the active OpenTelemetry Context.</p>"},{"location":"athomic/observability/tracing/#nala.athomic.observability.telemetry.adapters.messaging_adapter.MessagingTelemetryAdapter.inject_context","title":"<code>inject_context(headers)</code>","text":"<p>Injects the current trace context and a unique 'message_id' into a MessageHeaders object for an outgoing message.</p>"},{"location":"athomic/performance/bootstrap/","title":"Performance Bootstrap (uvloop)","text":""},{"location":"athomic/performance/bootstrap/#overview","title":"Overview","text":"<p>The <code>athomic.performance.bootstrap</code> module contains utilities that should be run at the very beginning of the application's lifecycle to apply performance optimizations.</p>"},{"location":"athomic/performance/bootstrap/#install_uvloop_if_available","title":"<code>install_uvloop_if_available()</code>","text":"<p>The primary utility is <code>install_uvloop_if_available()</code>. uvloop is a high-performance, drop-in replacement for Python's default <code>asyncio</code> event loop. It is implemented in Cython and built on top of <code>libuv</code>, the same library that powers Node.js.</p> <p>By using uvloop, your application can achieve significant performance improvements for I/O-bound operations, often seeing a 2-4x increase in throughput.</p>"},{"location":"athomic/performance/bootstrap/#how-it-works","title":"How It Works","text":"<p>This function is called in your application's entrypoint (e.g., <code>main.py</code>) before any other code runs. It attempts to import <code>uvloop</code>. -   If the <code>uvloop</code> package is installed in the environment, it sets it as the global asyncio event loop policy. -   If it's not installed, it does nothing, and the application gracefully falls back to using the standard asyncio event loop.</p> <p>This allows <code>uvloop</code> to be an optional, production-only dependency.</p>"},{"location":"athomic/performance/cache/","title":"Caching","text":""},{"location":"athomic/performance/cache/#overview","title":"Overview","text":"<p>The Caching module provides a powerful, decorator-based system for caching the results of asynchronous functions. It is designed to be highly resilient and performant, implementing several advanced caching strategies out-of-the-box.</p>"},{"location":"athomic/performance/cache/#key-features","title":"Key Features","text":"<ul> <li><code>@cache</code> Decorator: The primary interface for caching function results.</li> <li><code>@invalidate_cache</code> Decorator: For declaratively invalidating cache keys.</li> <li>Resilient Fallback: Can be configured with a fallback cache (e.g., in-memory) that is used if the primary cache (e.g., Redis) is unavailable.</li> <li>Single-Flight Caching: Uses a distributed lock to prevent the \"thundering herd\" problem, where multiple concurrent requests for a missed key all trigger the expensive computation.</li> <li>Refresh-Ahead (Stale-While-Revalidate): Can serve stale data while a background task refreshes the cache, minimizing latency for users.</li> </ul>"},{"location":"athomic/performance/cache/#usage","title":"Usage","text":"<pre><code>from nala.athomic.performance import cache, invalidate_cache\n\nclass ProductService:\n    @cache(ttl=300, key_prefix=\"products\") # Cache results for 5 minutes\n    async def get_product_details(self, product_id: str) -&gt; dict:\n        # Expensive database call\n        return await db.fetch_product(product_id)\n\n    @invalidate_cache(key_prefix=\"products\", key_resolver=lambda result, **kwargs: f\"products:{kwargs['product_id']}\")\n    async def update_product_details(self, product_id: str, data: dict):\n        # Update the product in the database\n        # The cache for this product will be automatically invalidated.\n        return await db.update_product(product_id, data)\n</code></pre> <p>For more details on the resilient provider, see the Fallback documentation.</p>"},{"location":"athomic/performance/cache/#api-reference","title":"API Reference","text":""},{"location":"athomic/performance/cache/#nala.athomic.performance.cache.decorators.cache","title":"<code>nala.athomic.performance.cache.decorators.cache(ttl=60, key_prefix=None, key_resolver=None, use_jitter=False, use_lock=False, lock_timeout=30, refresh_ahead=False, refresh_threshold=None, provider=None, ttl_key=None)</code>","text":"<p>Decorator to cache the result of an asynchronous function using the Cache-Aside, Single-Flight, and Refresh-Ahead strategies.</p> <p>It collects all configuration parameters and delegates the complex execution logic to the CacheHandler.</p> <p>Parameters:</p> Name Type Description Default <code>ttl</code> <code>int</code> <p>Time to live (in seconds) for the cached item. Can be overridden by ttl_key.</p> <code>60</code> <code>key_prefix</code> <code>Optional[str]</code> <p>Static prefix for the cache key (e.g., 'user_service:').</p> <code>None</code> <code>key_resolver</code> <code>Optional[ContextualKeyResolverType]</code> <p>Custom function, string, or list to generate contextual keys.</p> <code>None</code> <code>use_jitter</code> <code>Optional[bool]</code> <p>If True, adds random variance to the TTL to prevent cache stampedes.</p> <code>False</code> <code>use_lock</code> <code>Optional[bool]</code> <p>If True, enables distributed locking (Single-Flight Caching).</p> <code>False</code> <code>lock_timeout</code> <code>Optional[int]</code> <p>Timeout in seconds for acquiring the distributed lock.</p> <code>30</code> <code>refresh_ahead</code> <code>Optional[bool]</code> <p>If True, enables the background refresh strategy (stale hit).</p> <code>False</code> <code>refresh_threshold</code> <code>Optional[float]</code> <p>Percentage of TTL (0.0 to 1.0) when the item is considered stale                and a background refresh should be triggered.</p> <code>None</code> <code>provider</code> <code>Optional[CacheProtocol]</code> <p>Optional explicit CacheProtocol instance (for testing). Defaults to       CacheFallbackFactory.create().</p> <code>None</code> <code>ttl_key</code> <code>Optional[str]</code> <p>Key name in Live Config to dynamically override the TTL.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable[..., Any]</code> <p>The decorator function.</p>"},{"location":"athomic/performance/cache/#nala.athomic.performance.cache.decorators.invalidate_cache","title":"<code>nala.athomic.performance.cache.decorators.invalidate_cache(key_prefix=None, key_resolver=None, provider=None)</code>","text":"<p>Decorator to invalidate cache after the decorated function is called. This decorator will call the <code>invalidate</code> method of the provided cache provider with the specified key prefix and context. Args:     key_prefix: An optional prefix to use for the cache keys.     key_resolver: An optional function to generate cache keys based on the function's context.     provider: An optional cache provider to use for invalidation.</p>"},{"location":"athomic/performance/compression_middleware/","title":"HTTP Compression Middleware","text":""},{"location":"athomic/performance/compression_middleware/#overview","title":"Overview","text":"<p>This module provides factories for creating ASGI compression middleware, which can automatically compress API responses to reduce payload size and improve network latency.</p>"},{"location":"athomic/performance/compression_middleware/#how-it-works","title":"How It Works","text":"<p>The <code>CompressionMiddlewareFactory</code> reads the application configuration and, if enabled, returns the configured middleware class (e.g., <code>GZipMiddleware</code> from Starlette) and its settings. This is then added to the main FastAPI/Starlette application's middleware stack.</p>"},{"location":"athomic/performance/compression_middleware/#configuration","title":"Configuration","text":"<pre><code>[default.performance.compression]\nenabled = true\n# Can be \"gzip\" or \"brotli\" (if brotli-asgi is installed)\nbackend = \"gzip\"\n# Don't compress small responses\nminimum_size = 500 # bytes\n</code></pre>"},{"location":"athomic/performance/compression_middleware/#api-reference","title":"API Reference","text":""},{"location":"athomic/performance/compression_middleware/#nala.athomic.performance.compression.factory.CompressionMiddlewareFactory","title":"<code>nala.athomic.performance.compression.factory.CompressionMiddlewareFactory</code>","text":"<p>Factory to create the configured compression middleware.</p>"},{"location":"athomic/resilience/adaptive_throttling/","title":"Adaptive Throttling","text":""},{"location":"athomic/resilience/adaptive_throttling/#overview","title":"Overview","text":"<p>Adaptive Throttling is a sophisticated, closed-loop resilience pattern that dynamically adjusts rate limits based on the real-time health of downstream services. While a standard rate limiter uses static, pre-configured limits, an adaptive throttler reacts to changing conditions to proactively prevent cascading failures.</p> <p>For example, if the P99 latency of a downstream service suddenly spikes, or its error rate increases, the adaptive throttling engine will automatically reduce the rate limit of calls to that service, giving it a chance to recover. Once the service's health metrics return to normal, the throttler will gradually relax the limit back to its configured maximum.</p> <p>This creates a self-regulating system that is far more resilient to partial outages and performance degradation than static rate limiting alone.</p>"},{"location":"athomic/resilience/adaptive_throttling/#how-it-works-the-feedback-loop","title":"How It Works: The Feedback Loop","text":"<p>The system is orchestrated by the <code>AdaptiveThrottlingService</code>, a background service that runs a continuous feedback loop:</p> <ol> <li> <p>Monitor: A <code>MetricsFetcher</code> periodically queries a monitoring system (like Prometheus) for key health indicators of downstream services. These are defined by you as PromQL queries in the configuration.</p> </li> <li> <p>Decide: The fetched metrics (e.g., <code>latency_p99</code>, <code>error_rate_percent</code>) are passed to a <code>DecisionAlgorithm</code>. The algorithm compares these real-time values against healthy thresholds defined in your configuration.</p> </li> <li> <p>Adjust:</p> <ul> <li>If a threshold is breached, the algorithm calculates a new, more restrictive rate limit (e.g., reducing the current limit by 20%).</li> <li>If the system is healthy, the algorithm gradually increases the rate limit back towards the statically configured maximum.</li> </ul> </li> <li> <p>Store: The newly calculated dynamic limit is stored in a distributed <code>AdaptiveStateStore</code> (e.g., Redis) with a Time-To-Live (TTL).</p> </li> <li> <p>Enforce: The <code>AdaptiveRateLimiterProvider</code> is configured to wrap the standard rate limiter. When a request is made, it first checks the <code>AdaptiveStateStore</code> for a dynamic limit. If one exists, it is enforced. Otherwise, the static limit from the configuration is used.</p> </li> </ol> <p>This cycle repeats continuously, allowing the system to autonomously adapt to the real-time health of its dependencies.</p>"},{"location":"athomic/resilience/adaptive_throttling/#configuration","title":"Configuration","text":"<p>Adaptive Throttling is a powerful feature that requires careful configuration of its two main parts: the rate limiter itself, and the adaptive engine that controls it.</p>"},{"location":"athomic/resilience/adaptive_throttling/#1-enable-the-adaptive-rate-limiter-provider","title":"1. Enable the Adaptive Rate Limiter Provider","text":"<p>First, in your <code>[resilience.rate_limiter]</code> section, you must set the <code>backend</code> to <code>\"adaptive\"</code>. This tells the <code>RateLimiterFactory</code> to create the <code>AdaptiveRateLimiterProvider</code>, which wraps your primary enforcement provider (like <code>limits</code>).</p> <pre><code>[default.resilience.rate_limiter]\n# Enable the adaptive provider as the main backend\nbackend = \"adaptive\"\n\n  # The adaptive provider wraps another provider. Configure the base provider here.\n  [default.resilience.rate_limiter.provider]\n  backend = \"limits\"\n  storage_backend = \"redis\"\n  redis_storage_uri = \"redis://localhost:6379/4\"\n  strategy = \"moving-window\"\n\n  # Your static policies still act as the MAXIMUM ceiling for the adaptive limits.\n  [default.resilience.rate_limiter.policies]\n  external_api = \"100/minute\"\n</code></pre>"},{"location":"athomic/resilience/adaptive_throttling/#2-configure-the-adaptive-throttling-engine","title":"2. Configure the Adaptive Throttling Engine","text":"<p>Next, configure the feedback loop engine in the <code>[resilience.adaptive_throttling]</code> section.</p> <pre><code>[default.resilience.adaptive_throttling]\nenabled = true\ncheck_interval_seconds = 15 # Run the feedback loop every 15 seconds.\n\n# Tell the engine which rate limit policies it should dynamically adapt.\npolicies_to_adapt = [\"external_api\"]\n\n  # --- State Store (where dynamic limits are stored) ---\n  state_store_backend = \"redis\"\n  state_store_uri = \"redis://localhost:6379/5\"\n  state_store_ttl_seconds = 300 # Dynamic limits expire after 5 minutes.\n\n  # --- Metrics Fetcher (where to get health data from) ---\n  metrics_fetcher_type = \"prometheus\"\n  metrics_fetcher_url = \"http://prometheus:9090\"\n\n    # Map internal metric names to your actual PromQL queries.\n    [default.resilience.adaptive_throttling.prometheus_queries]\n    latency_p99 = \"histogram_quantile(0.99, sum(rate(http_client_request_duration_seconds_bucket{service_name='http_external_api'}[1m])) by (le))\"\n    error_rate_percent = \"(sum(rate(http_client_requests_total{service_name='http_external_api', status='failure'}[1m])) / sum(rate(http_client_requests_total{service_name='http_external_api'}[1m]))) * 100\"\n\n  # --- Decision Algorithm Parameters ---\n  decision_algorithm = \"threshold\"\n    [default.resilience.adaptive_throttling.algorithm_params]\n    # Reduce limit if latency is over 2000ms\n    latency_threshold_ms = 2000.0\n    # Reduce limit if error rate is over 10%\n    error_threshold_percent = 10.0\n    # When breached, reduce the current limit by 25%\n    reduction_factor = 0.75\n    # When healthy, increase the current limit by 10%\n    increase_factor = 1.1\n</code></pre>"},{"location":"athomic/resilience/adaptive_throttling/#api-reference","title":"API Reference","text":""},{"location":"athomic/resilience/adaptive_throttling/#nala.athomic.resilience.adaptive_throttling.service.AdaptiveThrottlingService","title":"<code>nala.athomic.resilience.adaptive_throttling.service.AdaptiveThrottlingService</code>","text":"<p>               Bases: <code>BaseService</code></p> <p>Manages the lifecycle and core logic of the adaptive throttling engine.</p> <p>This background service periodically fetches system metrics, calculates optimal rate limits based on predefined algorithms and thresholds, and stores the dynamic limits for enforcement by the <code>AdaptiveRateLimiterProvider</code>. It inherits lifecycle management from <code>BaseService</code>.</p> <p>Attributes:</p> Name Type Description <code>adaptive_settings</code> <code>AdaptiveThrottlingSettings</code> <p>Specific configuration for the engine.</p> <code>rate_limit_settings</code> <code>RateLimiterSettings</code> <p>Global rate limiter settings (used as reference).</p> <code>state_store</code> <code>AdaptiveStateStore</code> <p>Component for storing and fetching dynamic limits.</p> <code>metrics_fetcher</code> <code>MetricsFetcher</code> <p>Component for gathering system metrics (e.g., from Prometheus).</p> <code>decision_algorithm</code> <code>DecisionAlgorithm</code> <p>Algorithm for calculating new limits.</p>"},{"location":"athomic/resilience/adaptive_throttling/#nala.athomic.resilience.adaptive_throttling.service.AdaptiveThrottlingService.__init__","title":"<code>__init__(settings=None)</code>","text":"<p>Initializes the AdaptiveThrottlingService.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[AdaptiveThrottlingSettings]</code> <p>Configuration settings. If None, loads from global settings.</p> <code>None</code>"},{"location":"athomic/resilience/adaptive_throttling/#nala.athomic.resilience.adaptive_throttling.service.AdaptiveThrottlingService.after_stop","title":"<code>after_stop()</code>  <code>async</code>","text":"<p>Hook called after the run loop is stopped to close background components.</p>"},{"location":"athomic/resilience/adaptive_throttling/#nala.athomic.resilience.adaptive_throttling.interfaces.DecisionAlgorithm","title":"<code>nala.athomic.resilience.adaptive_throttling.interfaces.DecisionAlgorithm</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Interface for algorithms that decide the new adaptive rate limit based on current conditions and metrics.</p>"},{"location":"athomic/resilience/adaptive_throttling/#nala.athomic.resilience.adaptive_throttling.interfaces.DecisionAlgorithm.calculate_new_limit","title":"<code>calculate_new_limit(policy_name, current_configured_limit, current_dynamic_limit, metrics)</code>","text":"<p>Calculates the new adaptive limit based on provided context and metrics.</p> <p>Parameters:</p> Name Type Description Default <code>policy_name</code> <code>str</code> <p>The name of the policy being adjusted (e.g., \"default\", \"premium\").</p> required <code>current_configured_limit</code> <code>str</code> <p>The rate limit string defined in the static                       configuration (RateLimiterSettings) for this policy                       (or the default). Acts as a ceiling/reference.</p> required <code>current_dynamic_limit</code> <code>Optional[str]</code> <p>The currently active dynamic limit string retrieved                    from the AdaptiveStateStore, if any.</p> required <code>metrics</code> <code>Dict[str, Any]</code> <p>The dictionary of metrics retrieved by the MetricsFetcher.</p> required <p>Returns:</p> Name Type Description <code>AdaptiveDecision</code> <code>AdaptiveDecision</code> <p>An object detailing the calculated decision (action and new limit).</p>"},{"location":"athomic/resilience/adaptive_throttling/#nala.athomic.resilience.adaptive_throttling.providers.adaptive_provider.AdaptiveRateLimiterProvider","title":"<code>nala.athomic.resilience.adaptive_throttling.providers.adaptive_provider.AdaptiveRateLimiterProvider</code>","text":"<p>               Bases: <code>RateLimiterProtocol</code></p> <p>A Rate Limiter Provider that dynamically adjusts limits based on system health.</p> <p>This provider acts as a decorator, wrapping a base rate limiter implementation. Before enforcing a limit, it queries an <code>AdaptiveStateStore</code> for a potentially more restrictive dynamic limit calculated by a separate Decision Engine. It then applies the effective limit using the base provider.</p> <p>Attributes:</p> Name Type Description <code>base_provider</code> <code>RateLimiterProtocol</code> <p>The underlying implementation for limit enforcement.</p> <code>config</code> <code>RateLimiterSettings</code> <p>The application's rate limiter configuration.</p> <code>state_store</code> <code>AdaptiveStateStore</code> <p>The store used to fetch current dynamic limits.</p>"},{"location":"athomic/resilience/adaptive_throttling/#nala.athomic.resilience.adaptive_throttling.providers.adaptive_provider.AdaptiveRateLimiterProvider.__init__","title":"<code>__init__(config, base_provider, state_store)</code>","text":"<p>Initializes the AdaptiveRateLimiterProvider.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>RateLimiterSettings</code> <p>The application's RateLimiterSettings object.</p> required <code>base_provider</code> <code>RateLimiterProtocol</code> <p>The underlying implementation                                  responsible for actual limit enforcement.</p> required <code>state_store</code> <code>AdaptiveStateStore</code> <p>The store used to fetch current dynamic limits.</p> required"},{"location":"athomic/resilience/adaptive_throttling/#nala.athomic.resilience.adaptive_throttling.providers.adaptive_provider.AdaptiveRateLimiterProvider.allow","title":"<code>allow(key, rate, policy=None)</code>  <code>async</code>","text":"<p>Checks if the request is allowed based on the dynamically adjusted limit.</p> <p>The method determines the effective limit (dynamic or configured) and delegates the final enforcement check to the base provider.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The identifier being rate limited.</p> required <code>rate</code> <code>str</code> <p>The configured rate limit string (default/maximum limit).</p> required <code>policy</code> <code>Optional[str]</code> <p>The policy name used to look up the dynamic limit.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if allowed, False otherwise.</p>"},{"location":"athomic/resilience/adaptive_throttling/#nala.athomic.resilience.adaptive_throttling.providers.adaptive_provider.AdaptiveRateLimiterProvider.clear","title":"<code>clear(key, rate)</code>  <code>async</code>","text":"<p>Clears rate limit counters in the base provider for the specific key and rate.</p> <p>Note: This operation only clears the counter state managed by the base provider's storage. Clearing the dynamic limit itself is the responsibility of the Decision Engine during the recovery cycle.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The identifier whose counters should be cleared.</p> required <code>rate</code> <code>str</code> <p>The rate limit rule associated with the key.</p> required"},{"location":"athomic/resilience/adaptive_throttling/#nala.athomic.resilience.adaptive_throttling.providers.adaptive_provider.AdaptiveRateLimiterProvider.get_current_usage","title":"<code>get_current_usage(key, rate)</code>  <code>async</code>","text":"<p>Gets the current usage count from the base provider based on the provided rate string.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The identifier being checked.</p> required <code>rate</code> <code>str</code> <p>The rate limit string rule to check usage against.</p> required <p>Returns:</p> Type Description <code>Optional[int]</code> <p>Optional[int]: The current usage count, or None if the operation fails.</p>"},{"location":"athomic/resilience/adaptive_throttling/#nala.athomic.resilience.adaptive_throttling.providers.adaptive_provider.AdaptiveRateLimiterProvider.reset","title":"<code>reset()</code>  <code>async</code>","text":"<p>Resets ALL rate limit counters in the base provider's storage.</p> <p>WARNING: This operation is potentially global and does NOT automatically clear dynamic limits in the <code>AdaptiveStateStore</code>.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Propagates any error from the base provider's reset attempt.</p>"},{"location":"athomic/resilience/backoff/","title":"Exponential Backoff","text":""},{"location":"athomic/resilience/backoff/#overview","title":"Overview","text":"<p>Exponential Backoff is a standard resilience strategy used to gradually increase the delay between consecutive actions. This is useful in two primary scenarios:</p> <ol> <li>Polling Idle Resources: When a background service is polling for work (like the <code>OutboxPublisher</code> checking for new events) and finds none, it's inefficient to poll again immediately. Exponential backoff increases the wait time between polls, reducing CPU and network usage during idle periods.</li> <li>Retrying Failed Operations: When retrying a failed call to a downstream service, applying an increasing delay between attempts gives the struggling service time to recover.</li> </ol> <p>The Athomic implementation provides a stateful <code>BackoffHandler</code> that manages this logic based on configurable, named policies.</p>"},{"location":"athomic/resilience/backoff/#key-features","title":"Key Features","text":"<ul> <li>Policy-Based: Define multiple named backoff policies with different timings (<code>min_delay</code>, <code>max_delay</code>, <code>factor</code>) for various use cases.</li> <li>Stateful Handler: The <code>BackoffHandler</code> automatically manages the current delay state, increasing it after each wait and resetting it when work is found.</li> <li>Live Configuration: Backoff policies can be tuned in real-time without an application restart.</li> </ul>"},{"location":"athomic/resilience/backoff/#how-it-works","title":"How It Works","text":"<p>The system is composed of three main components:</p> <ol> <li> <p><code>BackoffPolicy</code>: A simple data object that holds the rules for a backoff strategy:</p> <ul> <li><code>min_delay</code>: The initial and minimum wait time.</li> <li><code>max_delay</code>: The maximum time to wait, which caps the exponential growth.</li> <li><code>factor</code>: The multiplier used to increase the delay after each wait (e.g., a factor of <code>1.5</code> will increase the wait time by 50% on each step).</li> </ul> </li> <li> <p><code>BackoffHandler</code>: The stateful object that orchestrates the backoff logic. Its key methods are:</p> <ul> <li><code>wait()</code>: Asynchronously sleeps for the <code>current_delay</code> period and then calculates the next, longer delay.</li> <li><code>reset()</code>: Resets the <code>current_delay</code> back to the policy's <code>min_delay</code>.</li> </ul> </li> <li> <p><code>BackoffFactory</code>: A factory used to create configured <code>BackoffHandler</code> instances based on named policies from your <code>settings.toml</code>.</p> </li> </ol>"},{"location":"athomic/resilience/backoff/#use-case-the-outboxpublisher-polling-loop","title":"Use Case: The <code>OutboxPublisher</code> Polling Loop","text":"<p>The <code>OutboxPublisher</code> is a perfect example of this pattern in action:</p> <ul> <li>In its main loop, it polls the database for new events.</li> <li>If no events are found, it calls <code>backoff_handler.wait()</code>. The first time, it might wait 1 second. The next, 1.5 seconds, then 2.25, and so on, up to a configured maximum. This makes the service highly efficient when idle.</li> <li>As soon as it finds and processes an event, it immediately calls <code>backoff_handler.reset()</code>. This resets the delay to the minimum, ensuring the service becomes highly responsive as soon as there is work to do.</li> </ul>"},{"location":"athomic/resilience/backoff/#usage-example","title":"Usage Example","text":"<p>You can use the <code>BackoffHandler</code> in any custom polling loop.</p> <pre><code>import asyncio\nfrom nala.athomic.resilience.backoff import BackoffFactory\n\n# Get a handler configured with the \"my_worker_policy\" from settings.toml\nbackoff_factory = BackoffFactory()\nbackoff_handler = backoff_factory.create_handler(policy_name=\"my_worker_policy\")\n\nasync def my_polling_worker():\n    while True:\n        work_done = await poll_for_work()\n\n        if work_done:\n            # We found work, so reset the delay to be responsive\n            backoff_handler.reset()\n        else:\n            # No work found, wait with an increasing delay\n            print(\"No work found, backing off...\")\n            await backoff_handler.wait()\n</code></pre>"},{"location":"athomic/resilience/backoff/#configuration","title":"Configuration","text":"<p>You define backoff policies in your <code>settings.toml</code> under the <code>[resilience.backoff]</code> section.</p> <pre><code>[default.resilience.backoff]\nenabled = true\n\n  # A default policy if no specific one is requested.\n  [default.resilience.backoff.default_policy]\n  min_delay_seconds = 1.0\n  max_delay_seconds = 30.0\n  factor = 1.5\n\n  # A dictionary of named, reusable policies.\n  [default.resilience.backoff.policies]\n\n    # A policy for an aggressive, fast-polling worker.\n    [default.resilience.backoff.policies.outbox_publisher_polling]\n    min_delay_seconds = 0.1\n    max_delay_seconds = 5.0\n    factor = 1.2\n\n    # A policy for a slow, infrequent background job.\n    [default.resilience.backoff.policies.daily_cleanup_job]\n    min_delay_seconds = 60.0\n    max_delay_seconds = 3600.0 # 1 hour\n    factor = 2.0\n</code></pre>"},{"location":"athomic/resilience/backoff/#live-configuration","title":"Live Configuration","text":"<p>Because <code>BackoffSettings</code> is a <code>LiveConfigModel</code>, you can change any of these policy values in your live configuration source (e.g., Consul), and the changes will be reflected in the <code>BackoffHandler</code> instances without requiring a restart.</p>"},{"location":"athomic/resilience/backoff/#api-reference","title":"API Reference","text":""},{"location":"athomic/resilience/backoff/#nala.athomic.resilience.backoff.handler.BackoffHandler","title":"<code>nala.athomic.resilience.backoff.handler.BackoffHandler</code>","text":"<p>Manages the state and logic for an exponential backoff strategy.</p> <p>This handler keeps track of the current delay, dynamically increasing it based on the configured policy parameters (min_delay, max_delay, factor) and providing hooks for error handling.</p>"},{"location":"athomic/resilience/backoff/#nala.athomic.resilience.backoff.handler.BackoffHandler.__init__","title":"<code>__init__(policy, operation_name='unknown', on_error=None)</code>","text":"<p>Initializes the BackoffHandler.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>BackoffPolicy</code> <p>The immutable policy defining the backoff rules.</p> required <code>operation_name</code> <code>str</code> <p>A descriptive name for the operation being managed, used for dedicated logging. Defaults to \"unknown\".</p> <code>'unknown'</code> <code>on_error</code> <code>Optional[ErrorCallback]</code> <p>An asynchronous callback executed immediately after an error occurs but before sleeping.</p> <code>None</code>"},{"location":"athomic/resilience/backoff/#nala.athomic.resilience.backoff.handler.BackoffHandler.reset","title":"<code>reset()</code>","text":"<p>Resets the internal delay counter back to the minimum policy value.</p> <p>This should be called after a successful operation to immediately resume high-frequency polling or operation attempts.</p>"},{"location":"athomic/resilience/backoff/#nala.athomic.resilience.backoff.handler.BackoffHandler.wait","title":"<code>wait()</code>  <code>async</code>","text":"<p>Waits for the current delay period (typical usage for an idle/polling state) and then increases the delay for the next cycle.</p>"},{"location":"athomic/resilience/backoff/#nala.athomic.resilience.backoff.handler.BackoffHandler.wait_after_error","title":"<code>wait_after_error(exc)</code>  <code>async</code>","text":"<p>Executes the <code>on_error</code> callback (if configured) and then waits for the current delay period, increasing it for the next cycle.</p> <p>Parameters:</p> Name Type Description Default <code>exc</code> <code>Exception</code> <p>The exception that triggered the delay.</p> required"},{"location":"athomic/resilience/backoff/#nala.athomic.resilience.backoff.policy.BackoffPolicy","title":"<code>nala.athomic.resilience.backoff.policy.BackoffPolicy</code>  <code>dataclass</code>","text":"<p>A Value Object holding the configuration for an exponential backoff strategy.</p> <p>This policy defines the parameters used by the <code>BackoffHandler</code> to control the delay between consecutive polling cycles or retry attempts, ensuring the system waits increasingly longer after failures or during idle periods.</p> <p>Attributes:</p> Name Type Description <code>min_delay</code> <code>float</code> <p>The initial and minimum delay in seconds.</p> <code>max_delay</code> <code>float</code> <p>The maximum delay in seconds, capping the exponential increase.</p> <code>factor</code> <code>float</code> <p>The multiplier used to increase the delay after each step.</p>"},{"location":"athomic/resilience/backoff/#nala.athomic.resilience.backoff.factory.BackoffFactory","title":"<code>nala.athomic.resilience.backoff.factory.BackoffFactory</code>","text":"<p>Factory class responsible for creating configured instances of <code>BackoffHandler</code>.</p> <p>It resolves configuration policies (default or named override) and constructs a runnable handler ready for use in polling loops or retry mechanisms.</p>"},{"location":"athomic/resilience/backoff/#nala.athomic.resilience.backoff.factory.BackoffFactory.__init__","title":"<code>__init__(settings=None)</code>","text":"<p>Initializes the factory with resolved application settings for backoff.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[BackoffSettings]</code> <p>Explicit settings instance. If None, loads from global application settings.</p> <code>None</code>"},{"location":"athomic/resilience/backoff/#nala.athomic.resilience.backoff.factory.BackoffFactory.create_handler","title":"<code>create_handler(policy_name=None, operation_name='default', on_error=None)</code>","text":"<p>Creates a new <code>BackoffHandler</code> instance configured with the specified policy.</p> <p>The method resolves the correct configuration: prioritizing a named policy if provided and found, or falling back to the default policy otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>policy_name</code> <code>Optional[str]</code> <p>The name of a policy defined in settings (e.g., 'high_contention').</p> <code>None</code> <code>operation_name</code> <code>str</code> <p>A descriptive name for the operation using this handler, used for logging and metrics. Defaults to \"default\".</p> <code>'default'</code> <code>on_error</code> <code>Optional[ErrorCallback]</code> <p>An asynchronous callable hook executed before sleeping after a failure.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>BackoffHandler</code> <code>BackoffHandler</code> <p>A fully configured handler instance.</p>"},{"location":"athomic/resilience/backpressure/","title":"Backpressure","text":""},{"location":"athomic/resilience/backpressure/#overview","title":"Overview","text":"<p>Backpressure is a resilience pattern used to prevent a system from being overwhelmed by temporarily stopping the flow of work towards a component that is known to be failing or overloaded. It's a form of flow control that helps prevent cascading failures.</p> <p>In the Athomic Layer, the Backpressure mechanism is primarily used by background processing services, like the <code>OutboxPublisher</code>, to avoid repeatedly attempting to process a task or resource that is consistently causing errors.</p>"},{"location":"athomic/resilience/backpressure/#key-features","title":"Key Features","text":"<ul> <li>Temporary Throttling: Pauses work on a specific resource (e.g., a message <code>aggregate_key</code>) for a configurable duration after a failure.</li> <li>Distributed State: Uses a distributed Key-Value store (like Redis) to share the backpressure state across all instances of a service.</li> <li>Automatic Recovery: Since the backpressure flag is set with a Time-To-Live (TTL), the system automatically attempts to process the resource again after the embargo period expires.</li> </ul>"},{"location":"athomic/resilience/backpressure/#how-it-works","title":"How It Works","text":"<p>The system is orchestrated by the <code>BackpressureManager</code>.</p> <ol> <li> <p>Failure Detection: A service (like the <code>OutboxPublisher</code>) detects a failure associated with a specific resource. For example, it fails to acquire a lease for <code>order-123</code>, or it fails to publish a message for <code>order-123</code>.</p> </li> <li> <p>Applying Backpressure: The service calls <code>backpressure_manager.apply_backpressure(resource_id=\"order-123\", ...)</code>. This creates a key (e.g., <code>athomic:backpressure:order-123</code>) in the configured KV store with a specific TTL (e.g., 30 seconds). The existence of this key acts as a \"throttling\" flag.</p> </li> <li> <p>Checking Before Processing: In its next processing cycle, before attempting to work on any resources, the service first calls <code>backpressure_manager.filter_throttled([\"order-123\", \"order-456\", ...])</code>. This method checks the KV store for any backpressure flags.</p> </li> <li> <p>Skipping Work: The manager returns a filtered list containing only the resources that are not currently throttled. The service then proceeds to work only on this \"safe\" list, skipping <code>order-123</code> for this cycle.</p> </li> <li> <p>Automatic Expiration: After 30 seconds, the key <code>athomic:backpressure:order-123</code> automatically expires in Redis. In the next cycle, the <code>filter_throttled</code> method will no longer filter out <code>order-123</code>, and the service will attempt to process it again.</p> </li> </ol>"},{"location":"athomic/resilience/backpressure/#usage-example","title":"Usage Example","text":"<p>While primarily used internally by Athomic services, you could use the <code>BackpressureManager</code> in a custom background worker.</p> <pre><code>from nala.athomic.resilience.backpressure import BackpressureFactory\n\n# Get the singleton manager instance\nbackpressure_manager = BackpressureFactory.create()\n\nasync def process_batch_of_items(all_item_ids: list[str]):\n    # First, filter out any items that are currently throttled\n    safe_to_process_ids = await backpressure_manager.filter_throttled(all_item_ids)\n\n    for item_id in safe_to_process_ids:\n        try:\n            await do_work_on(item_id)\n        except Exception as e:\n            # If work fails, apply backpressure to this item for 60 seconds\n            print(f\"Work failed for {item_id}. Applying backpressure.\")\n            await backpressure_manager.apply_backpressure(\n                resource_id=item_id,\n                reason=\"processing_failed\",\n                ttl_seconds=60\n            )\n</code></pre>"},{"location":"athomic/resilience/backpressure/#configuration","title":"Configuration","text":"<p>The backpressure system is configured under the <code>[resilience.backpressure]</code> section in your <code>settings.toml</code>. It requires a KV store connection to manage its state.</p> <pre><code>[default.resilience.backpressure]\nenabled = true\n\n# The default duration in seconds to apply backpressure if not specified.\ndefault_ttl_seconds = 30\n\n# The prefix for all backpressure keys stored in the KV store.\nkey_prefix = \"athomic:backpressure\"\n\n# The name of the KVStore connection (from [database.kvstore]) to use.\nkv_store_connection_name = \"default_redis\"\n</code></pre>"},{"location":"athomic/resilience/backpressure/#api-reference","title":"API Reference","text":""},{"location":"athomic/resilience/backpressure/#nala.athomic.resilience.backpressure.manager.BackpressureManager","title":"<code>nala.athomic.resilience.backpressure.manager.BackpressureManager</code>","text":"<p>Manages the state of throttled resources in a distributed Key-Value (KV) store.</p> <p>This class implements the core logic of the Backpressure pattern by using time-to-live (TTL) keys in a shared store to mark resources that are temporarily overloaded or failing. This prevents downstream services from being overwhelmed.</p> <p>Attributes:</p> Name Type Description <code>storage</code> <code>KVStoreProtocol</code> <p>The KV store client used for state persistence.</p> <code>key_prefix</code> <code>str</code> <p>The static prefix for all keys in the store.</p> <code>default_ttl</code> <code>int</code> <p>The default expiration time in seconds for a backpressure flag.</p>"},{"location":"athomic/resilience/backpressure/#nala.athomic.resilience.backpressure.manager.BackpressureManager.__init__","title":"<code>__init__(storage, key_prefix, default_ttl)</code>","text":"<p>Initializes the BackpressureManager.</p> <p>Parameters:</p> Name Type Description Default <code>storage</code> <code>KVStoreProtocol</code> <p>The KV store implementation (e.g., Redis).</p> required <code>key_prefix</code> <code>str</code> <p>The key prefix used for all backpressure flags.</p> required <code>default_ttl</code> <code>int</code> <p>The default duration for a backpressure embargo in seconds.</p> required"},{"location":"athomic/resilience/backpressure/#nala.athomic.resilience.backpressure.manager.BackpressureManager.apply_backpressure","title":"<code>apply_backpressure(resource_id, reason, ttl_seconds=None)</code>  <code>async</code>","text":"<p>Marks a resource as throttled by setting a time-limited flag in the KV store.</p> <p>Parameters:</p> Name Type Description Default <code>resource_id</code> <code>str</code> <p>The unique ID of the resource (e.g., aggregate key).</p> required <code>reason</code> <code>str</code> <p>The cause of the backpressure (e.g., 'publish_error', 'lease_conflict').</p> required <code>ttl_seconds</code> <code>Optional[int]</code> <p>The duration of the embargo. If None, uses the default TTL.</p> <code>None</code>"},{"location":"athomic/resilience/backpressure/#nala.athomic.resilience.backpressure.manager.BackpressureManager.filter_throttled","title":"<code>filter_throttled(resource_ids)</code>  <code>async</code>","text":"<p>Filters a list of resource IDs concurrently, returning only those that are NOT throttled.</p> <p>Parameters:</p> Name Type Description Default <code>resource_ids</code> <code>List[str]</code> <p>The input list of resource identifiers.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A new list containing only the resource IDs that are currently operational.</p>"},{"location":"athomic/resilience/backpressure/#nala.athomic.resilience.backpressure.manager.BackpressureManager.is_throttled","title":"<code>is_throttled(resource_id)</code>  <code>async</code>","text":"<p>Checks if a resource is currently under a backpressure embargo.</p> <p>Parameters:</p> Name Type Description Default <code>resource_id</code> <code>str</code> <p>The unique ID of the resource.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the resource is throttled (key exists), False otherwise.</p>"},{"location":"athomic/resilience/backpressure/#nala.athomic.resilience.backpressure.factory.BackpressureFactory","title":"<code>nala.athomic.resilience.backpressure.factory.BackpressureFactory</code>","text":"<p>Factory responsible for creating the singleton instance of <code>BackpressureManager</code>.</p> <p>This factory resolves the necessary dependencies\u2014the configuration settings and the configured Key-Value (KV) store client\u2014to instantiate the manager. It performs a runtime check to ensure the backpressure mechanism is enabled.</p>"},{"location":"athomic/resilience/backpressure/#nala.athomic.resilience.backpressure.factory.BackpressureFactory.create","title":"<code>create()</code>  <code>classmethod</code>","text":"<p>Creates and returns a configured instance of <code>BackpressureManager</code>.</p> <p>The process involves: 1. Checking if the feature is enabled in application settings. 2. Resolving the designated KV store client via <code>ConnectionManagerFactory</code>. 3. Instantiating the manager with the KV store, key prefix, and default TTL.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the backpressure mechanism is disabled in the settings.</p> <p>Returns:</p> Name Type Description <code>BackpressureManager</code> <code>BackpressureManager</code> <p>A fully initialized instance ready for use.</p>"},{"location":"athomic/resilience/bulkhead/","title":"Bulkhead","text":""},{"location":"athomic/resilience/bulkhead/#overview","title":"Overview","text":"<p>The Bulkhead pattern is a resilience mechanism designed to isolate failures in one part of an application from affecting others. It works by limiting the number of concurrent executions for a specific operation.</p> <p>Imagine a ship's hull, which is divided into isolated compartments (bulkheads). If one compartment is breached and floods, the bulkheads prevent the water from sinking the entire ship. Similarly, in a software system, if a downstream service becomes slow, the bulkhead pattern prevents that slowness from consuming all available application resources (like worker threads or connections), which would otherwise cause a cascading failure across your entire service.</p> <p>The Athomic implementation uses <code>asyncio.Semaphore</code> to enforce these concurrency limits on any asynchronous function via the <code>@bulkhead</code> decorator.</p>"},{"location":"athomic/resilience/bulkhead/#key-features","title":"Key Features","text":"<ul> <li>Concurrency Limiting: Restrict how many instances of a function can run simultaneously.</li> <li>Policy-Based: Define named policies with different concurrency limits for different operations in your configuration.</li> <li>Fail-Fast Mechanism: When a bulkhead's limit is reached, new calls don't wait in a queue. They are rejected immediately, raising a <code>BulkheadRejectedError</code>. This \"fail-fast\" behavior is crucial for shedding load and protecting system stability.</li> <li>Full Observability: All bulkhead activity is instrumented with Prometheus metrics, tracking concurrent requests, accepted calls, and rejections for each policy.</li> </ul>"},{"location":"athomic/resilience/bulkhead/#how-it-works","title":"How It Works","text":"<ol> <li>Policies &amp; Semaphores: You define named policies in your configuration, each with a specific concurrency limit (e.g., <code>payment_api: 5</code>). The singleton <code>BulkheadService</code> manages an <code>asyncio.Semaphore</code> for each policy.</li> <li><code>@bulkhead</code> Decorator: You apply the <code>@bulkhead(policy=\"...\")</code> decorator to an <code>async</code> function.</li> <li>Acquisition Attempt: When the decorated function is called, it attempts to acquire a \"slot\" from the semaphore associated with its policy. This is a non-blocking check.</li> <li>Execution or Rejection:<ul> <li>If a slot is available, the function executes normally. When it completes (or fails), the slot is released.</li> <li>If no slots are available (the limit is reached), the acquisition fails immediately, and a <code>BulkheadRejectedError</code> is raised without executing the function.</li> </ul> </li> </ol>"},{"location":"athomic/resilience/bulkhead/#usage-example","title":"Usage Example","text":"<p>Let's say you have two functions: one is a highly resource-intensive video processing task, and the other is a fast metadata lookup. You can use separate bulkheads to ensure the slow video task can't block the fast metadata lookups.</p> <pre><code>from nala.athomic.resilience.bulkhead import bulkhead, BulkheadRejectedError\n\n# This policy is defined in settings.toml with a low limit (e.g., 2)\n@bulkhead(policy=\"video_processing\")\nasync def generate_video_thumbnail(video_id: str):\n    # Very slow and resource-intensive I/O operation\n    await process_video(video_id)\n\n# This policy has a higher limit (e.g., 50)\n@bulkhead(policy=\"metadata_lookup\")\nasync def fetch_user_metadata(user_id: str):\n    # Fast database call\n    await db.fetch_user(user_id)\n\nasync def handle_request(video_id: str):\n    try:\n        # If 2 video tasks are already running, this call will fail immediately\n        await generate_video_thumbnail(video_id)\n    except BulkheadRejectedError:\n        # You can now handle the rejection gracefully, e.g., by returning a 429 Too Many Requests\n        print(\"The system is currently busy processing other videos. Please try again later.\")\n</code></pre>"},{"location":"athomic/resilience/bulkhead/#configuration","title":"Configuration","text":"<p>You configure your bulkhead policies in <code>settings.toml</code> under the <code>[resilience.bulkhead]</code> section.</p> <pre><code>[default.resilience.bulkhead]\nenabled = true\n\n# The default concurrency limit to apply if a policy is not found.\ndefault_limit = 20\n\n  # A dictionary of named policies and their specific concurrency limits.\n  [default.resilience.bulkhead.policies]\n  video_processing = 2\n  metadata_lookup = 50\n  external_api_calls = 10\n</code></pre>"},{"location":"athomic/resilience/bulkhead/#api-reference","title":"API Reference","text":""},{"location":"athomic/resilience/bulkhead/#nala.athomic.resilience.bulkhead.decorator.bulkhead","title":"<code>nala.athomic.resilience.bulkhead.decorator.bulkhead(policy)</code>","text":"<p>Decorator to protect an asynchronous function with a concurrency limit (Bulkhead).</p> <p>This pattern isolates resource usage by limiting the number of concurrent executions for the decorated function, preventing cascading failures. It uses the <code>BulkheadService</code> to acquire a slot before execution.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>str</code> <p>The unique name of the bulkhead policy to apply,           as defined in the application configuration.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable[..., Any]</code> <p>The decorator function.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the decorated function is not asynchronous.</p>"},{"location":"athomic/resilience/bulkhead/#nala.athomic.resilience.bulkhead.service.BulkheadService","title":"<code>nala.athomic.resilience.bulkhead.service.BulkheadService</code>","text":"<p>Manages all bulkhead policies and their corresponding semaphores.</p> <p>This service enforces concurrency limits on asynchronous operations based on predefined policies, preventing cascading failures by isolating resource usage. It lazily creates an <code>asyncio.Semaphore</code> for each unique policy name.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <code>BulkheadSettings</code> <p>The bulkhead configuration settings.</p> <code>_semaphores</code> <code>Dict[str, Semaphore]</code> <p>Cache of active semaphores, keyed by policy name.</p>"},{"location":"athomic/resilience/bulkhead/#nala.athomic.resilience.bulkhead.service.BulkheadService.__init__","title":"<code>__init__(settings)</code>","text":"<p>Initializes the BulkheadService.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>BulkheadSettings</code> <p>The application's bulkhead configuration.</p> required"},{"location":"athomic/resilience/bulkhead/#nala.athomic.resilience.bulkhead.service.BulkheadService.acquire","title":"<code>acquire(policy)</code>  <code>async</code>","text":"<p>Acquires a slot from the bulkhead for a given policy, using an async context manager.</p> <p>If the bulkhead is full, it raises <code>BulkheadRejectedError</code> immediately (non-blocking acquisition attempt with a minimal timeout). Observability metrics are updated for both accepted and rejected calls.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>str</code> <p>The name of the bulkhead policy to enforce.</p> required <p>Raises:</p> Type Description <code>BulkheadRejectedError</code> <p>If the acquisition times out (meaning the bulkhead is full).</p>"},{"location":"athomic/resilience/bulkhead/#nala.athomic.resilience.bulkhead.exceptions.BulkheadRejectedError","title":"<code>nala.athomic.resilience.bulkhead.exceptions.BulkheadRejectedError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when a call is rejected because the bulkhead is full.</p>"},{"location":"athomic/resilience/circuit_breaker/","title":"Circuit Breaker","text":""},{"location":"athomic/resilience/circuit_breaker/#overview","title":"Overview","text":"<p>The Circuit Breaker is a stateful resilience pattern designed to prevent an application from repeatedly attempting to execute an operation that is likely to fail. When a downstream service is struggling, the circuit breaker \"opens\" to stop sending requests to it, allowing the failing service time to recover and preventing the upstream service from wasting resources.</p> <p>The Athomic implementation is built on the <code>aiobreaker</code> library and is managed by a central <code>CircuitBreakerService</code>.</p>"},{"location":"athomic/resilience/circuit_breaker/#the-three-states","title":"The Three States","text":"<p>A circuit breaker operates as a state machine with three states:</p> <ol> <li><code>CLOSED</code>: This is the normal, healthy state. All calls are allowed to pass through to the protected function. The breaker counts failures, and if the count exceeds a configured threshold (<code>fail_max</code>), it transitions to <code>OPEN</code>.</li> <li><code>OPEN</code>: In this state, all calls to the protected function are blocked immediately without execution, raising a <code>CircuitBreakerError</code>. The breaker remains <code>OPEN</code> for a configured duration (<code>reset_timeout</code>). After the timeout expires, it transitions to <code>HALF_OPEN</code>.</li> <li><code>HALF_OPEN</code>: In this state, the breaker allows a single \"probe\" call to pass through.<ul> <li>If this single call succeeds, the breaker transitions back to <code>CLOSED</code>.</li> <li>If it fails, the breaker transitions back to <code>OPEN</code>, restarting the reset timeout.</li> </ul> </li> </ol>"},{"location":"athomic/resilience/circuit_breaker/#how-it-works","title":"How It Works","text":""},{"location":"athomic/resilience/circuit_breaker/#circuit_breaker-decorator","title":"<code>@circuit_breaker</code> Decorator","text":"<p>The primary way to use the pattern is by applying the <code>@circuit_breaker</code> decorator to any asynchronous function that performs a potentially failing operation (like an external API call). You must give each circuit a unique <code>name</code>.</p> <pre><code>from nala.athomic.resilience import circuit_breaker\n\n@circuit_breaker(name=\"payment_service_api\")\nasync def call_payment_service(payment_data: dict):\n    # This call is now protected by the 'payment_service_api' circuit.\n    response = await http_client.post(\"/v1/payments\", json=payment_data)\n    return response\n</code></pre>"},{"location":"athomic/resilience/circuit_breaker/#circuitbreakerservice","title":"<code>CircuitBreakerService</code>","text":"<p>A singleton <code>CircuitBreakerService</code> manages all named circuit breakers in the application. It lazily creates and caches a breaker instance for each unique name, configured according to your settings.</p>"},{"location":"athomic/resilience/circuit_breaker/#distributed-state-storage","title":"Distributed State Storage","text":"<p>The state of each circuit breaker (its current state, failure count) must be stored somewhere. Athomic supports two backends:</p> <ul> <li><code>local</code>: In-memory storage. The state is not shared between service instances and is lost on restart. Ideal for local development and testing.</li> <li><code>redis</code>: (Recommended for production). Stores the state in Redis. This allows all instances of your service to share the same circuit state, so if one instance detects a failure, all other instances will also open the circuit for that service.</li> </ul>"},{"location":"athomic/resilience/circuit_breaker/#configuration","title":"Configuration","text":"<p>You configure the circuit breaker module under the <code>[resilience.circuit_breaker]</code> section in your <code>settings.toml</code>. You can define global defaults and then override them for specific, named circuits.</p> <pre><code>[default.resilience.circuit_breaker]\nenabled = true\nnamespace = \"cb\" # A prefix for all keys in the storage backend.\n\n# --- Default settings for all circuits ---\ndefault_fail_max = 5 # Open the circuit after 5 consecutive failures.\ndefault_reset_timeout_sec = 30.0 # Keep the circuit open for 30 seconds.\n\n  # --- Storage Provider ---\n  # Use Redis for distributed state in production.\n  [default.resilience.circuit_breaker.provider]\n  backend = \"redis\"\n    [default.resilience.circuit_breaker.provider.redis]\n    # Reuses a KVStore connection configuration.\n    uri = \"redis://localhost:6379/3\"\n\n  # --- Specific Overrides for Named Circuits ---\n  # This section is a dictionary of named circuit configurations.\n  [default.resilience.circuit_breaker.circuits]\n    # Override settings for the circuit named \"payment_service_api\".\n    [default.resilience.circuit_breaker.circuits.payment_service_api]\n    fail_max = 3 # More sensitive: open after only 3 failures.\n    reset_timeout_sec = 60.0 # Keep open for 60 seconds.\n</code></pre>"},{"location":"athomic/resilience/circuit_breaker/#live-configuration","title":"Live Configuration","text":"<p>The settings for individual circuits (under <code>[resilience.circuit_breaker.circuits]</code>) can be updated at runtime without an application restart if you are using a live configuration provider like Consul. This allows you to tune the sensitivity of your circuit breakers in response to production incidents.</p>"},{"location":"athomic/resilience/circuit_breaker/#api-reference","title":"API Reference","text":""},{"location":"athomic/resilience/circuit_breaker/#nala.athomic.resilience.circuit_breaker.decorator.circuit_breaker","title":"<code>nala.athomic.resilience.circuit_breaker.decorator.circuit_breaker(name=None)</code>","text":"<p>Decorator to protect an asynchronous function with a Circuit Breaker pattern.</p> <p>This decorator ensures that calls to the decorated function are monitored for failures. If the failure rate exceeds a threshold, the circuit opens, and subsequent calls are blocked, failing fast to allow the system to recover. All execution and state logic are delegated to the central <code>CircuitBreakerService</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>The unique name for this circuit. If None, the                   fully qualified name of the decorated function (<code>func.__qualname__</code>) is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable[..., Any]</code> <p>The decorator function.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the decorated function is not asynchronous.</p>"},{"location":"athomic/resilience/circuit_breaker/#nala.athomic.resilience.circuit_breaker.service.CircuitBreakerService","title":"<code>nala.athomic.resilience.circuit_breaker.service.CircuitBreakerService</code>","text":"<p>Manages and executes operations protected by the Circuit Breaker pattern.</p> <p>This service acts as the central factory and orchestrator for <code>aiobreaker.CircuitBreaker</code> instances. It lazily creates and caches a breaker for each unique circuit name, resolving its configuration (fail_max, reset_timeout) and distributed storage based on application settings.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <code>CircuitBreakerSettings</code> <p>The configuration for the circuit breaker module.</p> <code>_breakers</code> <code>Dict[str, CircuitBreaker]</code> <p>Cache of active circuit breaker instances.</p>"},{"location":"athomic/resilience/circuit_breaker/#nala.athomic.resilience.circuit_breaker.service.CircuitBreakerService.__init__","title":"<code>__init__(settings)</code>","text":"<p>Initializes the CircuitBreakerService.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>CircuitBreakerSettings</code> <p>Configuration for the circuit breaker module.</p> required"},{"location":"athomic/resilience/circuit_breaker/#nala.athomic.resilience.circuit_breaker.service.CircuitBreakerService.execute","title":"<code>execute(circuit_name, func, *args, **kwargs)</code>  <code>async</code>","text":"<p>Executes an asynchronous function protected by the specified circuit breaker.</p> <p>It automatically wraps the execution with an OpenTelemetry span and handles the <code>CircuitBreakerError</code> case for proper observability.</p> <p>Parameters:</p> Name Type Description Default <code>circuit_name</code> <code>str</code> <p>The name of the circuit to use.</p> required <code>func</code> <code>Callable</code> <p>The asynchronous function to execute.</p> required <code>*args</code> <code>Any</code> <p>Positional arguments for the function.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments for the function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the executed function.</p> <p>Raises:</p> Type Description <code>CircuitBreakerError</code> <p>If the circuit is open and the call is blocked.</p>"},{"location":"athomic/resilience/circuit_breaker/#nala.athomic.resilience.circuit_breaker.exceptions.CircuitBreakerError","title":"<code>nala.athomic.resilience.circuit_breaker.exceptions.CircuitBreakerError = aiobreaker.CircuitBreakerError</code>  <code>module-attribute</code>","text":""},{"location":"athomic/resilience/fallback/","title":"Fallback","text":""},{"location":"athomic/resilience/fallback/#overview","title":"Overview","text":"<p>The Fallback pattern provides an alternative execution path for an operation when it fails. Instead of allowing a failure to cascade through the system, a fallback can return a default value, data from an alternative source, or trigger a different logic path. This allows the application to degrade gracefully rather than failing completely.</p> <p>Athomic provides two distinct implementations of this pattern:</p> <ol> <li>A generic <code>@fallback_handler</code> decorator for protecting any function.</li> <li>A specialized <code>FallbackKVProvider</code> for creating highly resilient, multi-layer caching strategies.</li> </ol>"},{"location":"athomic/resilience/fallback/#1-generic-fallback-with-fallback_handler","title":"1. Generic Fallback with <code>@fallback_handler</code>","text":"<p>This decorator allows you to wrap any asynchronous function with one or more fallback functions. If the primary function raises an exception, the fallback functions are executed in order until one succeeds.</p>"},{"location":"athomic/resilience/fallback/#how-it-works","title":"How It Works","text":"<p>The <code>FallbackLogicHandler</code> executes your primary function. If an exception occurs, it catches it and begins iterating through the list of fallback functions you provided, calling them with the same arguments as the original function. The result of the first successful fallback is returned. If all fallbacks also fail, a single <code>FallbackError</code> containing all the underlying exceptions is raised.</p>"},{"location":"athomic/resilience/fallback/#usage-example","title":"Usage Example","text":"<pre><code>from nala.athomic.resilience.fallback import fallback_handler\n\n# A fallback function that returns a default static value\nasync def get_default_user_config(*args, **kwargs):\n    return {\"theme\": \"dark\", \"notifications\": \"enabled\"}\n\n# Another fallback that could try a different source\nasync def get_user_config_from_legacy_system(user_id: str):\n    # ... logic to call a legacy API ...\n    pass\n\n@fallback_handler(fallbacks=[get_user_config_from_legacy_system, get_default_user_config])\nasync def get_user_config_from_primary(user_id: str) -&gt; dict:\n    \"\"\"\n    Attempts to fetch user config from the primary Redis cache.\n    If Redis is down, it tries the legacy system.\n    If that also fails, it returns a hardcoded default.\n    \"\"\"\n    # This call might fail if Redis is unavailable\n    return await redis_cache.get(f\"user-config:{user_id}\")\n</code></pre>"},{"location":"athomic/resilience/fallback/#2-the-fallbackkvprovider-for-resilient-caching","title":"2. The <code>FallbackKVProvider</code> (for Resilient Caching)","text":"<p>This is a specialized, high-performance implementation of the fallback pattern designed specifically for caching. It's a <code>KVStoreProtocol</code> provider that wraps a chain of other KV store providers (e.g., a primary Redis cache and a secondary in-memory cache).</p>"},{"location":"athomic/resilience/fallback/#key-features","title":"Key Features","text":""},{"location":"athomic/resilience/fallback/#read-strategies","title":"Read Strategies","text":"<p>You can configure when the fallback is triggered: -   <code>on_error</code>: The fallback is used only if the primary cache is down (e.g., Redis connection fails). A normal cache miss on the primary will not trigger the fallback. -   <code>on_miss_or_error</code>: (Default) The fallback is used if the primary cache is down OR if the key is not found (a cache miss).</p>"},{"location":"athomic/resilience/fallback/#write-strategies","title":"Write Strategies","text":"<p>You can configure how writes are propagated through the cache layers: -   <code>write_around</code>: (Default) Writes are only sent to the primary cache. The fallback caches are not updated. -   <code>write_through</code>: Writes are sent to the primary cache and then propagated to all fallback caches concurrently.</p>"},{"location":"athomic/resilience/fallback/#self-healing","title":"Self-Healing","text":"<p>This is a powerful feature of the <code>FallbackKVProvider</code>. If a value is successfully retrieved from a fallback cache (e.g., the in-memory cache), a background task is automatically spawned to write that value back into the primary cache (and any other preceding caches that failed). This \"heals\" the primary cache, so the next request for the same key will be a hit on the fastest layer.</p>"},{"location":"athomic/resilience/fallback/#configuration","title":"Configuration","text":"<p>The <code>FallbackKVProvider</code> is typically configured as part of your main cache setup in <code>settings.toml</code>.</p> <pre><code>[default.performance.cache]\nenabled = true\n# The primary provider for the cache is a connection named \"main_redis\"\nkv_store_connection_name = \"main_redis\"\n\n  # --- Fallback Configuration ---\n  [default.performance.cache.fallback]\n  enabled = true\n  # Define the read strategy\n  read_strategy = \"on_miss_or_error\"\n\n  # An ordered list of KVStore connection names to use as fallbacks.\n  # Here, it will try \"local_memory_cache\" if \"main_redis\" misses or fails.\n  provider_connection_names = [\"local_memory_cache\"]\n\n# You must also define the KV store connections themselves:\n[default.database.kvstore]\n  [default.database.kvstore.main_redis]\n  # ... redis config ...\n\n  [default.database.kvstore.local_memory_cache]\n  provider.backend = \"local\"\n</code></pre>"},{"location":"athomic/resilience/fallback/#api-reference","title":"API Reference","text":""},{"location":"athomic/resilience/fallback/#nala.athomic.resilience.fallback.decorator.fallback_handler","title":"<code>nala.athomic.resilience.fallback.decorator.fallback_handler(fallbacks)</code>","text":"<p>Decorator that applies the Fallback resilience pattern to a function.</p> <p>It wraps the decorated primary function with an execution chain: if the primary function fails for any reason, the configured list of fallback functions is executed sequentially until one succeeds.</p> <p>Parameters:</p> Name Type Description Default <code>fallbacks</code> <code>Union[Callable, List[Callable]]</code> <p>A single callable or an ordered list of callables (sync or async) to be tried as alternatives if the decorated function raises an exception.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable[[Callable[..., T]], Callable[..., T]]</code> <p>The decorator function.</p>"},{"location":"athomic/resilience/fallback/#nala.athomic.resilience.fallback.providers.fallback_kv_provider.FallbackKVProvider","title":"<code>nala.athomic.resilience.fallback.providers.fallback_kv_provider.FallbackKVProvider</code>","text":"<p>               Bases: <code>BaseService</code>, <code>KVStoreProtocol</code></p> <p>A Key-Value (KV) provider that implements a resilient fallback and self-healing strategy.</p> <p>It orchestrates a chain of underlying KV providers: trying the primary first, and then sequentially trying fallbacks in case of failure or cache miss (depending on the configured read strategy).</p> <p>Attributes:</p> Name Type Description <code>primary</code> <code>KVStoreProtocol</code> <p>The main, preferred KV store provider.</p> <code>fallbacks</code> <code>List[KVStoreProtocol]</code> <p>Ordered list of secondary providers.</p> <code>write_strategy</code> <code>WriteStrategyProtocol</code> <p>Strategy defining how write operations are handled across providers.</p> <code>read_strategy</code> <code>ReadStrategyProtocol</code> <p>Strategy defining when the fallback chain is triggered.</p> <code>enable_self_healing</code> <code>bool</code> <p>If True, successful reads from a fallback provider                         will asynchronously write the value back to the failed/missing providers.</p>"},{"location":"athomic/resilience/fallback/#nala.athomic.resilience.fallback.providers.fallback_kv_provider.FallbackKVProvider.__init__","title":"<code>__init__(primary_provider, fallback_providers, read_strategy_type=FallbackReadStrategyType.ON_MISS_OR_ERROR, write_strategy_type=FallbackWriteStrategyType.WRITE_AROUND, enable_self_healing=True)</code>","text":"<p>Initializes the FallbackKVProvider.</p> <p>Parameters:</p> Name Type Description Default <code>primary_provider</code> <code>KVStoreProtocol</code> <p>The primary KV store instance.</p> required <code>fallback_providers</code> <code>List[KVStoreProtocol]</code> <p>A list of secondary providers,                                         ordered by preference.</p> required <code>read_strategy_type</code> <code>FallbackReadStrategyType</code> <p>Defines the trigger for the fallback read chain.</p> <code>ON_MISS_OR_ERROR</code> <code>write_strategy_type</code> <code>FallbackWriteStrategyType</code> <p>Defines how write operations propagate.</p> <code>WRITE_AROUND</code> <code>enable_self_healing</code> <code>bool</code> <p>Enables background healing of failed providers on successful reads.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>primary_provider</code> is None.</p>"},{"location":"athomic/resilience/fallback/#nala.athomic.resilience.fallback.providers.fallback_kv_provider.FallbackKVProvider.clear","title":"<code>clear()</code>  <code>async</code>","text":"<p>Clears all keys in the provider(s) based on the configured write strategy.</p>"},{"location":"athomic/resilience/fallback/#nala.athomic.resilience.fallback.providers.fallback_kv_provider.FallbackKVProvider.delete","title":"<code>delete(key)</code>  <code>async</code>","text":"<p>Deletes a key based on the configured write strategy.</p>"},{"location":"athomic/resilience/fallback/#nala.athomic.resilience.fallback.providers.fallback_kv_provider.FallbackKVProvider.exists","title":"<code>exists(key)</code>  <code>async</code>","text":"<p>Checks for the existence of a key by delegating the check/fallback logic to the configured read strategy.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the key exists in any available provider, False otherwise.</p>"},{"location":"athomic/resilience/fallback/#nala.athomic.resilience.fallback.providers.fallback_kv_provider.FallbackKVProvider.get","title":"<code>get(key)</code>  <code>async</code>","text":"<p>Retrieves a value by delegating the complex read/fallback logic to the configured read strategy.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to retrieve.</p> required <p>Returns:</p> Type Description <code>Optional[Any]</code> <p>Optional[Any]: The value from the first successful provider, or None.</p>"},{"location":"athomic/resilience/fallback/#nala.athomic.resilience.fallback.providers.fallback_kv_provider.FallbackKVProvider.get_final_client","title":"<code>get_final_client()</code>  <code>async</code>","text":"<p>Returns the raw client of the primary provider for direct access.</p>"},{"location":"athomic/resilience/fallback/#nala.athomic.resilience.fallback.providers.fallback_kv_provider.FallbackKVProvider.get_sync_client","title":"<code>get_sync_client()</code>","text":"<p>Returns the synchronous client of the primary provider for direct access.</p>"},{"location":"athomic/resilience/fallback/#nala.athomic.resilience.fallback.providers.fallback_kv_provider.FallbackKVProvider.heal","title":"<code>heal(key, value, providers_to_heal)</code>  <code>async</code>","text":"<p>Asynchronously attempts to heal failed or missed providers by writing the successfully retrieved value back to them.</p> <p>This method is non-blocking and is executed as a fire-and-forget background task.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key that was successfully retrieved.</p> required <code>value</code> <code>Any</code> <p>The value associated with the key.</p> required <code>providers_to_heal</code> <code>List[KVStoreProtocol]</code> <p>The chain of providers (including the primary)                                        that failed or missed the key.</p> required"},{"location":"athomic/resilience/fallback/#nala.athomic.resilience.fallback.providers.fallback_kv_provider.FallbackKVProvider.is_available","title":"<code>is_available()</code>","text":"<p>Returns True if AT LEAST ONE provider is connected and ready.</p>"},{"location":"athomic/resilience/fallback/#nala.athomic.resilience.fallback.providers.fallback_kv_provider.FallbackKVProvider.set","title":"<code>set(key, value, ttl=None, nx=False)</code>  <code>async</code>","text":"<p>Sets a key-value pair based on the configured write strategy.</p>"},{"location":"athomic/resilience/fallback/#nala.athomic.resilience.fallback.exceptions.FallbackError","title":"<code>nala.athomic.resilience.fallback.exceptions.FallbackError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when all fallback attempts fail.</p>"},{"location":"athomic/resilience/idempotency/","title":"Idempotency","text":""},{"location":"athomic/resilience/idempotency/#overview","title":"Overview","text":"<p>Idempotency is a critical property of distributed systems that ensures an operation can be performed multiple times with the same result as if it were performed only once. This is essential for building reliable APIs and message handlers, as it allows clients to safely retry requests (e.g., after a network failure) without causing duplicate side effects, such as creating two orders or charging a credit card twice.</p> <p>The Athomic Layer provides a powerful <code>@idempotent</code> decorator that makes any asynchronous function idempotent by storing its result and using a distributed lock to handle concurrent requests.</p>"},{"location":"athomic/resilience/idempotency/#key-features","title":"Key Features","text":"<ul> <li>Safe Retries: Protects <code>POST</code>, <code>PUT</code>, and other state-changing operations from creating duplicate resources.</li> <li>Single-Flight Execution: Uses a distributed lock to ensure that for a given idempotency key, the business logic is executed only once, even under high concurrency.</li> <li>Cache-Aside Result Storage: The result of the first successful operation is stored in a Key-Value store. Subsequent requests with the same key receive this stored result immediately.</li> <li>Declarative Usage: Enforce idempotency with a simple and clean decorator.</li> </ul>"},{"location":"athomic/resilience/idempotency/#how-it-works-single-flight-cache-aside","title":"How It Works: Single-Flight Cache-Aside","text":"<p>The <code>@idempotent</code> decorator orchestrates a sophisticated, race-condition-free workflow:</p> <ol> <li> <p>Key Resolution: When the decorated function is called, a unique idempotency key is generated for that specific operation. This is typically resolved from a request header (e.g., <code>Idempotency-Key</code>) or a field in a message payload.</p> </li> <li> <p>Cache Check (Attempt 1): The system first checks the configured KV store (e.g., Redis) to see if a result for this key has already been stored. If a result is found, it is returned immediately.</p> </li> <li> <p>Distributed Lock: If no result is found (a cache miss), the system attempts to acquire a distributed lock for that idempotency key. This is the crucial \"Single-Flight\" step:</p> <ul> <li>Only the first request to arrive will acquire the lock.</li> <li>Any other concurrent requests for the same key will wait for the lock to be released.</li> </ul> </li> <li> <p>Double-Checked Locking: After acquiring the lock, the system checks the cache a second time. This handles the case where another request finished computing the result while the current request was waiting for the lock. If a result is now found, it is returned, and the original function is not executed.</p> </li> <li> <p>Execution &amp; Storage: If the cache is still empty, the original decorated function (your business logic) is executed. Its result is then stored in the KV store with a configured TTL, and finally, the lock is released.</p> </li> <li> <p>Conflict: If a request cannot acquire the lock within its timeout, it means the operation is already in progress. In this case, an <code>IdempotencyConflictError</code> is raised, which can be translated to an <code>HTTP 409 Conflict</code> response.</p> </li> </ol>"},{"location":"athomic/resilience/idempotency/#usage-example","title":"Usage Example","text":"<p>Here is how you would protect a FastAPI endpoint that creates a new resource. The client is expected to provide a unique <code>Idempotency-Key</code> header.</p> <pre><code>import uuid\nfrom fastapi import Header, Request\nfrom nala.athomic.resilience.idempotency import idempotent, IdempotencyConflictError\n\n@router.post(\"/orders\")\n@idempotent(\n    # The key resolver is a lambda that extracts the key from the request kwargs.\n    key=lambda request, **kwargs: request.headers.get(\"Idempotency-Key\"),\n    lock_timeout=10 # Wait up to 10s for a concurrent request to finish.\n)\nasync def create_order(request: Request, order_data: dict):\n    \"\"\"\n    Creates a new order. This operation is idempotent.\n    \"\"\"\n    # This logic will only be executed ONCE for a given Idempotency-Key.\n    order_id = await order_service.create(order_data)\n    return {\"status\": \"created\", \"order_id\": order_id}\n\n# You would typically have an exception handler to catch the conflict error.\n@app.exception_handler(IdempotencyConflictError)\nasync def idempotency_conflict_handler(request: Request, exc: IdempotencyConflictError):\n    return JSONResponse(\n        status_code=409,\n        content={\"message\": \"Request conflict: An identical request is already being processed.\"},\n    )\n</code></pre>"},{"location":"athomic/resilience/idempotency/#configuration","title":"Configuration","text":"<p>The idempotency module is configured under the <code>[resilience.idempotency]</code> section in your <code>settings.toml</code>. It requires a KV store for storing results and managing locks.</p> <pre><code>[default.resilience.idempotency]\nenabled = true\n\n# The default Time-To-Live in seconds for stored idempotency results.\ndefault_ttl_seconds = 86400 # 24 hours\n\n# The default time in seconds to wait for a distributed lock.\ndefault_lock_timeout_seconds = 10\n\n  # The KVStore configuration for storing results and locks.\n  [default.resilience.idempotency.kvstore]\n  namespace = \"idempotency\"\n    [default.resilience.idempotency.kvstore.provider]\n    backend = \"redis\"\n    uri = \"redis://localhost:6379/6\"\n</code></pre>"},{"location":"athomic/resilience/idempotency/#api-reference","title":"API Reference","text":""},{"location":"athomic/resilience/idempotency/#nala.athomic.resilience.idempotency.decorator.idempotent","title":"<code>nala.athomic.resilience.idempotency.decorator.idempotent(key, ttl=None, lock_timeout=None, storage=None, locker=None)</code>","text":"<p>Decorator to make an asynchronous function idempotent.</p> <p>The primary goal is to ensure that a function is executed logically only once for a given idempotency key, returning the cached result on all subsequent calls within the key's TTL.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>ContextualKeyResolverType</code> <p>A resolver mechanism to generate the unique key from function arguments. Can be a callable (function/lambda) or an f-string template referencing kwargs (e.g., \"order:{order_id}\").</p> required <code>ttl</code> <code>Optional[int]</code> <p>Time-to-live (in seconds) for the stored result. Uses configured default if None.</p> <code>None</code> <code>lock_timeout</code> <code>Optional[int]</code> <p>Time (in seconds) to wait for the distributed lock                           during cache miss before raising a conflict error.                           Uses configured default if None.</p> <code>None</code> <code>storage</code> <code>Optional[KVStoreProtocol]</code> <p>Optional storage provider instance (for DI/testing).</p> <code>None</code> <code>locker</code> <code>Optional[LockingProtocol]</code> <p>Optional distributed locking provider instance (for DI/testing).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable[..., Any]</code> <p>The decorator function.</p>"},{"location":"athomic/resilience/idempotency/#nala.athomic.resilience.idempotency.exceptions.IdempotencyConflictError","title":"<code>nala.athomic.resilience.idempotency.exceptions.IdempotencyConflictError</code>","text":"<p>               Bases: <code>IdempotencyError</code></p> <p>Raised when an idempotent operation is attempted for a key that is already being processed by a concurrent request.</p>"},{"location":"athomic/resilience/idempotency/#nala.athomic.resilience.idempotency.handler.IdempotencyHandler","title":"<code>nala.athomic.resilience.idempotency.handler.IdempotencyHandler</code>","text":"<p>Orchestrates the entire idempotency check and execution logic, implementing the Cache-Aside pattern combined with Single-Flight locking.</p> <p>The flow ensures that a unique operation is computed only once, and concurrent attempts wait for the first successful result.</p>"},{"location":"athomic/resilience/idempotency/#nala.athomic.resilience.idempotency.handler.IdempotencyHandler.__init__","title":"<code>__init__(func, args, kwargs, key_resolver, ttl=None, lock_timeout=None, storage=None, locker=None, settings=None)</code>","text":"<p>Initializes the IdempotencyHandler.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>The original asynchronous function being decorated.</p> required <code>args</code> <code>Tuple</code> <p>Positional arguments passed to the function.</p> required <code>kwargs</code> <code>Dict</code> <p>Keyword arguments passed to the function.</p> required <code>key_resolver</code> <code>ContextualKeyResolverType</code> <p>The mechanism used to generate the unique idempotency key.</p> required <code>ttl</code> <code>Optional[int]</code> <p>Time-to-live for the stored result. Overrides configured default if provided.</p> <code>None</code> <code>lock_timeout</code> <code>Optional[int]</code> <p>Time to wait to acquire the distributed lock. Overrides configured default.</p> <code>None</code> <code>storage</code> <code>Optional[KVStoreProtocol]</code> <p>Optional storage provider instance (for DI/testing).</p> <code>None</code> <code>locker</code> <code>Optional[LockingProtocol]</code> <p>Optional distributed locking provider instance (for DI/testing).</p> <code>None</code> <code>settings</code> <code>Optional[IdempotencySettings]</code> <p>Optional settings instance.</p> <code>None</code>"},{"location":"athomic/resilience/idempotency/#nala.athomic.resilience.idempotency.handler.IdempotencyHandler.execute","title":"<code>execute()</code>  <code>async</code>","text":"<p>Executes the full idempotency flow: check cache, acquire lock, compute, and store result.</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the operation (either cached or newly computed).</p> <p>Raises:</p> Type Description <code>IdempotencyConflictError</code> <p>If the distributed lock cannot be acquired within the timeout.</p>"},{"location":"athomic/resilience/leasing/","title":"Distributed Leasing","text":""},{"location":"athomic/resilience/leasing/#overview","title":"Overview","text":"<p>Distributed Leasing is an advanced resilience pattern used for leader election or to ensure a single service instance has exclusive ownership of a long-running task or resource. It is a more sophisticated form of distributed locking.</p> <p>The key difference between a lease and a lock is that a lease is time-based and requires the holder to actively maintain ownership by sending periodic \"heartbeats\" to renew it. If the leaseholder crashes or becomes unresponsive, its lease automatically expires after a configured duration, allowing another healthy instance to acquire it and take over the task.</p> <p>This pattern is critical for building fault-tolerant, active-passive systems. A primary use case within the Athomic Layer is in the <code>OutboxPublisher</code>, where leasing ensures that only one publisher instance is processing events for a specific <code>aggregate_key</code> at any given time.</p>"},{"location":"athomic/resilience/leasing/#key-features","title":"Key Features","text":"<ul> <li>Exclusive Ownership: Guarantees that only one worker in a cluster can operate on a leased resource.</li> <li>Fault Tolerance: Leases automatically expire, allowing a new worker to take over if the current owner fails.</li> <li>Automatic Renewal: An active lease is automatically renewed in a background task (heartbeat) as long as the worker is healthy.</li> <li>Simple Interface: Implemented via a clean <code>async with</code> context manager.</li> </ul>"},{"location":"athomic/resilience/leasing/#how-it-works","title":"How It Works","text":"<ol> <li> <p>Acquisition: A worker requests a lease on a specific resource by calling <code>lease_manager.acquire(\"resource_id\")</code> inside an <code>async with</code> block. The manager attempts to atomically acquire the lease from the backend (Redis) using a <code>SET NX EX</code> command.</p> </li> <li> <p>Heartbeat: If the lease is acquired successfully, the <code>LeaseManager</code> yields a <code>LeaseHolder</code> object. This object immediately starts a background <code>asyncio.Task</code> which periodically sends a heartbeat to the backend, renewing the lease before it expires.</p> </li> <li> <p>Execution: The code inside the <code>async with</code> block executes. The worker can now safely perform its task, confident that it has exclusive ownership of the resource.</p> </li> <li> <p>Release: When the <code>async with</code> block is exited (either by completing successfully or by raising an exception), the heartbeat task is automatically stopped, and the lease is explicitly released from the backend.</p> </li> </ol>"},{"location":"athomic/resilience/leasing/#usage-example","title":"Usage Example","text":"<p>Here is a conceptual example of how a background worker could use leasing to ensure only one instance is processing a specific job at a time.</p> <pre><code>from nala.athomic.resilience.leasing import LeaseFactory, LeaseAcquisitionError\n\nlease_manager = LeaseFactory.create()\njob_id = \"process-daily-reports\"\n\nasync def run_daily_report_job():\n    try:\n        # Try to acquire an exclusive lease for this job\n        async with lease_manager.acquire(resource_id=job_id):\n            # This code will only be executed by the single worker\n            # that successfully acquired the lease.\n            print(\"Lease acquired! Running the daily report job...\")\n            await generate_reports()\n            print(\"Job finished, releasing lease.\")\n\n    except LeaseAcquisitionError:\n        # This will be raised if another worker already holds the lease.\n        print(\"Could not acquire lease. Another worker is already running the job.\")\n    except Exception as e:\n        print(f\"An error occurred during the job: {e}\")\n</code></pre>"},{"location":"athomic/resilience/leasing/#configuration","title":"Configuration","text":"<p>The leasing system is configured under the <code>[resilience.leasing]</code> section in your <code>settings.toml</code>. It requires a KV store connection (Redis) to manage the lease state.</p> <pre><code>[default.resilience.leasing]\nenabled = true\n\n# The name of the KVStore connection (from [database.kvstore]) to use.\nkv_store_connection_name = \"default_redis\"\n\n# The duration in seconds a lease is held before it expires if not renewed.\nduration_seconds = 60\n\n# How often the background heartbeat task should renew the lease.\n# This MUST be shorter than duration_seconds.\nrenewal_interval_seconds = 20\n\n# The default time in seconds a process will wait to acquire a lease.\ndefault_acquire_timeout_seconds = 5\n</code></pre>"},{"location":"athomic/resilience/leasing/#api-reference","title":"API Reference","text":""},{"location":"athomic/resilience/leasing/#nala.athomic.resilience.leasing.manager.LeaseManager","title":"<code>nala.athomic.resilience.leasing.manager.LeaseManager</code>","text":"<p>Orchestrates the acquisition, heartbeat renewal, and release of distributed leases.</p> <p>This class provides a high-level <code>async context manager</code> interface for the leasing mechanism, handling the complex distributed locking logic, continuous renewal in the background, and graceful cleanup.</p> <p>Attributes:</p> Name Type Description <code>provider</code> <code>LeaseProtocol</code> <p>The underlying, concrete distributed lease provider (e.g., Redis).</p> <code>settings</code> <code>LeasingSettings</code> <p>The configuration for lease duration and timeouts.</p> <code>owner_id</code> <code>str</code> <p>A unique identifier for this service instance, used to claim ownership of leases.</p>"},{"location":"athomic/resilience/leasing/#nala.athomic.resilience.leasing.manager.LeaseManager.__init__","title":"<code>__init__(provider, settings)</code>","text":"<p>Initializes the LeaseManager.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>LeaseProtocol</code> <p>The underlying distributed lease provider.</p> required <code>settings</code> <code>LeasingSettings</code> <p>The configuration settings.</p> required"},{"location":"athomic/resilience/leasing/#nala.athomic.resilience.leasing.manager.LeaseManager.acquire","title":"<code>acquire(resource_id)</code>  <code>async</code>","text":"<p>Attempts to acquire an exclusive lease on a specified resource.</p> <p>This is a non-blocking operation managed by a timeout. If successful, it yields a <code>LeaseHolder</code> that automatically renews the lease in a background task (heartbeat).</p> <p>Parameters:</p> Name Type Description Default <code>resource_id</code> <code>str</code> <p>The unique identifier of the resource to lock (e.g., a message aggregate key).</p> required <p>Yields:</p> Name Type Description <code>LeaseHolder</code> <code>AsyncGenerator[LeaseHolder, None]</code> <p>An object representing the active lease, providing renewal management.</p> <p>Raises:</p> Type Description <code>LeaseAcquisitionError</code> <p>If the lease cannot be acquired within the configured acquisition timeout.</p>"},{"location":"athomic/resilience/leasing/#nala.athomic.resilience.leasing.holder.LeaseHolder","title":"<code>nala.athomic.resilience.leasing.holder.LeaseHolder</code>","text":"<p>Represents an acquired lease and manages its renewal heartbeat.</p> <p>An instance of this class is yielded by the <code>LeaseManager</code>'s context manager, encapsulating the active lease and the background task responsible for periodically renewing its expiration time.</p>"},{"location":"athomic/resilience/leasing/#nala.athomic.resilience.leasing.holder.LeaseHolder.is_acquired","title":"<code>is_acquired</code>  <code>property</code>","text":"<p>Returns True if the lease was successfully acquired.</p>"},{"location":"athomic/resilience/leasing/#nala.athomic.resilience.leasing.holder.LeaseHolder.__init__","title":"<code>__init__(lease, provider, settings)</code>","text":"<p>Initializes the LeaseHolder.</p> <p>Parameters:</p> Name Type Description Default <code>lease</code> <code>Optional[Lease]</code> <p>The acquired lease object. None if acquisition failed.</p> required <code>provider</code> <code>LeaseProtocol</code> <p>The concrete lease provider used for renewal/release operations.</p> required <code>settings</code> <code>LeasingSettings</code> <p>The configuration defining renewal intervals and durations.</p> required"},{"location":"athomic/resilience/leasing/#nala.athomic.resilience.leasing.holder.LeaseHolder.start_heartbeat","title":"<code>start_heartbeat()</code>","text":"<p>Starts the background renewal task if a lease was acquired.</p>"},{"location":"athomic/resilience/leasing/#nala.athomic.resilience.leasing.holder.LeaseHolder.stop_heartbeat","title":"<code>stop_heartbeat()</code>  <code>async</code>","text":"<p>Stops the background renewal task gracefully and waits for it to finish.</p>"},{"location":"athomic/resilience/leasing/#nala.athomic.resilience.leasing.exceptions.LeaseAcquisitionError","title":"<code>nala.athomic.resilience.leasing.exceptions.LeaseAcquisitionError</code>","text":"<p>               Bases: <code>LeaseError</code></p> <p>Raised when a worker fails to acquire a lease on a resource.</p>"},{"location":"athomic/resilience/locking/","title":"Distributed Locking","text":""},{"location":"athomic/resilience/locking/#overview","title":"Overview","text":"<p>Distributed Locking is a resilience pattern that provides a mechanism for mutual exclusion across multiple processes or service instances. It ensures that only one process can execute a critical section of code at a time for a specific, shared resource.</p> <p>This is essential for preventing race conditions in distributed systems. For example, if two requests try to update a user's account balance at the same time, a distributed lock can ensure that these operations happen sequentially, not concurrently, thus preventing data corruption.</p> <p>The Athomic implementation provides a simple yet powerful <code>@distributed_lock</code> decorator to protect any asynchronous function.</p>"},{"location":"athomic/resilience/locking/#key-features","title":"Key Features","text":"<ul> <li>Declarative Use: Protect critical sections of code with a simple decorator.</li> <li>Dynamic Key Resolution: Lock keys can be dynamically generated from the arguments of the decorated function.</li> <li>Multiple Backends: Supports a distributed Redis backend for production and a local in-memory backend for testing.</li> <li>Deadlock Prevention: Locks are configured with a timeout, ensuring they are automatically released even if a process crashes.</li> </ul>"},{"location":"athomic/resilience/locking/#how-it-works","title":"How It Works","text":"<ol> <li>Decorator: You apply the <code>@distributed_lock(key=\"...\", timeout=...)</code> decorator to an <code>async</code> function.</li> <li>Key Resolution: When the decorated function is called, the <code>key</code> template string is formatted using the function's arguments. For example, a key of <code>\"user-balance:{user_id}\"</code> for a function call with <code>user_id=123</code> will resolve to <code>\"user-balance:123\"</code>.</li> <li>Acquisition Attempt: The system then attempts to acquire a lock for this resolved key from the configured provider (e.g., Redis). It will wait for up to the specified <code>timeout</code>.</li> <li>Execution: If the lock is acquired, the original function is executed. Once the function completes (either by returning or raising an exception), the lock is always released automatically.</li> <li>Failure: If the lock cannot be acquired within the timeout (because another process holds it), a <code>LockAcquisitionError</code> is raised immediately, and the function is not executed.</li> </ol>"},{"location":"athomic/resilience/locking/#available-providers","title":"Available Providers","text":"<ul> <li><code>RedisLockProvider</code>: The recommended provider for production. It uses Redis's atomic operations to implement a reliable, distributed lock. It reuses the application's main <code>KVStore</code> client for the connection.</li> <li><code>LocalLockProvider</code>: An in-memory lock provider that uses <code>asyncio.Lock</code>. It is suitable for single-process applications or for running tests without external dependencies. It is not distributed.</li> </ul>"},{"location":"athomic/resilience/locking/#usage-example","title":"Usage Example","text":"<p>Imagine a function that needs to safely deduct a value from a user's balance.</p> <pre><code>from nala.athomic.resilience.locking import distributed_lock, LockAcquisitionError\n\nclass BalanceService:\n    @distributed_lock(key=\"balance:{user_id}\", timeout=10)\n    async def deduct_from_balance(self, user_id: str, amount: float):\n        \"\"\"\n        Safely deducts from a user's balance. Only one process can\n        execute this for the same user_id at a time.\n        \"\"\"\n        current_balance = await db.get_balance(user_id)\n        if current_balance &lt; amount:\n            raise ValueError(\"Insufficient funds.\")\n\n        await db.set_balance(user_id, current_balance - amount)\n\nasync def handle_payment(user_id: str, amount: float):\n    try:\n        await balance_service.deduct_from_balance(user_id, amount)\n    except LockAcquisitionError:\n        # This occurs if another request for the same user is already processing.\n        # You can ask the client to retry the request.\n        print(\"Could not process payment at this time, please try again.\")\n</code></pre>"},{"location":"athomic/resilience/locking/#configuration","title":"Configuration","text":"<p>The locking provider is configured under the <code>[resilience.locking]</code> section in your <code>settings.toml</code>.</p> <pre><code>[default.resilience.locking]\nenabled = true\n\n# The default time in seconds a lock is held before it automatically expires.\nlock_timeout_sec = 30\n\n  # Configure the backend provider\n  [default.resilience.locking.provider]\n  backend = \"redis\"\n\n    # The redis provider reuses a KVStore connection configuration.\n    [default.resilience.locking.provider.kvstore]\n    # The namespace and other wrappers will apply to the lock keys\n    namespace = \"locks\"\n      [default.resilience.locking.provider.kvstore.provider]\n      backend = \"redis\"\n      uri = \"redis://localhost:6379/5\"\n</code></pre>"},{"location":"athomic/resilience/locking/#api-reference","title":"API Reference","text":""},{"location":"athomic/resilience/locking/#nala.athomic.resilience.locking.decorator.distributed_lock","title":"<code>nala.athomic.resilience.locking.decorator.distributed_lock(key, timeout=30)</code>","text":"<p>Decorator that enforces mutual exclusion for an asynchronous function using a distributed lock.</p> <p>The decorator ensures that only one call runs at a time for a given unique key, preventing race conditions in a distributed environment.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>A template for the lock key. Can use arguments from the        decorated function (e.g., \"payment:{payment_id}\").</p> required <code>timeout</code> <code>int</code> <p>Time (in seconds) to wait to acquire the lock before failing.            Defaults to 30 seconds.</p> <code>30</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>The decorator function.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the decorated function is not asynchronous.</p>"},{"location":"athomic/resilience/locking/#nala.athomic.resilience.locking.protocol.LockingProtocol","title":"<code>nala.athomic.resilience.locking.protocol.LockingProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the contract for a locking provider, abstracting the mechanism used to ensure mutual exclusion for shared resources.</p> <p>Implementations can be either local (in-memory) for single-process use or distributed (e.g., Redis) for microservices in a clustered environment.</p>"},{"location":"athomic/resilience/locking/#nala.athomic.resilience.locking.protocol.LockingProtocol.acquire","title":"<code>acquire(key, timeout=30)</code>  <code>async</code>","text":"<p>Acquires a lock for a specific key, intended for use with the <code>async with</code> statement.</p> <p>The operation blocks until the lock is acquired or the timeout is reached.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The unique resource identifier to be locked (e.g., \"user:123\").</p> required <code>timeout</code> <code>int</code> <p>The maximum time (in seconds) to wait to acquire the lock.            Defaults to 30 seconds.</p> <code>30</code> <p>Yields:</p> Name Type Description <code>None</code> <code>AsyncGenerator[None, None]</code> <p>Execution continues inside the block only if the lock is successfully acquired.</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If the lock cannot be acquired within the specified timeout.</p>"},{"location":"athomic/resilience/locking/#nala.athomic.resilience.locking.exceptions.LockAcquisitionError","title":"<code>nala.athomic.resilience.locking.exceptions.LockAcquisitionError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when a distributed lock cannot be acquired within the specified timeout.</p>"},{"location":"athomic/resilience/rate_limiter/","title":"Rate Limiter","text":""},{"location":"athomic/resilience/rate_limiter/#overview","title":"Overview","text":"<p>The Rate Limiter is a resilience pattern used to control the frequency of operations, such as API requests or function calls, over a period of time. It is essential for:</p> <ul> <li>Protecting downstream services from being overwhelmed.</li> <li>Preventing resource starvation and ensuring fair usage.</li> <li>Enforcing API usage quotas for different clients or user tiers.</li> </ul> <p>The Athomic rate limiter is a flexible, policy-based system that allows you to declaratively apply rate limits to any asynchronous function using the <code>@rate_limited</code> decorator.</p>"},{"location":"athomic/resilience/rate_limiter/#key-features","title":"Key Features","text":"<ul> <li>Policy-Based: Define named, reusable rate limit policies (e.g., <code>\"100/hour\"</code>, <code>\"10/second\"</code>) in your configuration.</li> <li>Multiple Backends &amp; Strategies: Supports multiple provider backends, including a highly flexible one based on the <code>limits</code> library that allows for different strategies (e.g., <code>fixed-window</code>, <code>moving-window</code>).</li> <li>Context-Aware: Automatically generates unique keys based on the current context, enabling per-user or per-tenant rate limiting.</li> <li>Live Configuration: Rate limit policies can be adjusted at runtime without restarting the application, allowing you to respond to traffic spikes dynamically.</li> <li>Adaptive Throttling: Can integrate with an advanced Adaptive Throttling engine to automatically adjust limits based on real-time system health metrics.</li> </ul>"},{"location":"athomic/resilience/rate_limiter/#how-it-works","title":"How It Works","text":"<ol> <li>Decorator: You apply the <code>@rate_limited(policy=\"...\")</code> decorator to an <code>async</code> function.</li> <li><code>RateLimiterService</code>: When the function is called, the decorator invokes the central <code>RateLimiterService</code>.</li> <li>Key Generation: The service uses the <code>ContextualKeyGenerator</code> to create a unique key for the operation based on the policy name, function name, and the current execution context (like <code>tenant_id</code> or <code>user_id</code>).</li> <li>Provider Check: The service then asks the configured provider (e.g., the <code>limits</code> provider) if a request for that key is allowed under the specified rate limit string. If the request is denied, a <code>RateLimitExceeded</code> exception is raised.</li> </ol>"},{"location":"athomic/resilience/rate_limiter/#available-providers","title":"Available Providers","text":"<ul> <li><code>limits</code> Provider: The default and most flexible provider. It is an adapter for the powerful <code>limits</code> library and can be configured to use different storage backends (in-memory for testing, Redis for distributed state) and different algorithms (fixed-window, moving-window).</li> <li><code>redis</code> Provider: A lightweight, custom implementation that uses native Redis commands to enforce a simple fixed-window algorithm. It reuses the application's main <code>KVStore</code> client.</li> </ul>"},{"location":"athomic/resilience/rate_limiter/#usage-example","title":"Usage Example","text":"<p>Applying a rate limit to a function is as simple as adding the decorator.</p> <pre><code>from nala.athomic.resilience.rate_limiter import rate_limited, RateLimitExceeded\n\n@rate_limited(policy=\"api_heavy_usage\")\nasync def generate_large_report(user_id: str):\n    # This function can now only be called according to the\n    # \"api_heavy_usage\" policy defined in the settings.\n    # ...\n    pass\n\nasync def handle_request(user_id: str):\n    try:\n        await generate_large_report(user_id)\n    except RateLimitExceeded:\n        # Handle the case where the user has exceeded their quota\n        print(\"Too many requests, please try again later.\")\n</code></pre>"},{"location":"athomic/resilience/rate_limiter/#configuration","title":"Configuration","text":"<p>You define your rate limiting backend and policies in <code>settings.toml</code> under the <code>[resilience.rate_limiter]</code> section.</p> <pre><code>[default.resilience.rate_limiter]\nenabled = true\nbackend = \"limits\" # Use the 'limits' library provider\n\n  # --- Default settings for the 'limits' provider ---\n  [default.resilience.rate_limiter.provider]\n  backend = \"limits\"\n  # Use Redis for distributed state\n  storage_backend = \"redis\"\n  redis_storage_uri = \"redis://localhost:6379/4\"\n  # Use a more accurate strategy\n  strategy = \"moving-window\"\n\n  # --- Policy Definitions ---\n  # A default limit to apply if no specific policy is requested\n  default_policy_limit = \"1000/hour\"\n\n  # A dictionary of named, reusable policies\n  [default.resilience.rate_limiter.policies]\n  api_light_usage = \"100/minute\"\n  api_heavy_usage = \"10/minute\"\n  login_attempts = \"5/hour\"\n</code></pre>"},{"location":"athomic/resilience/rate_limiter/#live-configuration","title":"Live Configuration","text":"<p>Because <code>RateLimiterSettings</code> is a <code>LiveConfigModel</code>, you can change the limit strings for any policy in your live configuration source (e.g., Consul) and the changes will take effect immediately without a restart. This is extremely useful for mitigating traffic spikes or abuse in real-time.</p>"},{"location":"athomic/resilience/rate_limiter/#api-reference","title":"API Reference","text":""},{"location":"athomic/resilience/rate_limiter/#nala.athomic.resilience.rate_limiter.decorators.rate_limited","title":"<code>nala.athomic.resilience.rate_limiter.decorators.rate_limited</code>","text":""},{"location":"athomic/resilience/rate_limiter/#nala.athomic.resilience.rate_limiter.decorators.rate_limited.rate_limited","title":"<code>rate_limited(policy)</code>","text":"<p>Decorator that applies the Rate Limiting pattern to an asynchronous function.</p> <p>It delegates the entire enforcement logic to the central <code>RateLimiterService</code>. If the limit is exceeded, it raises a specific exception instead of executing the decorated function.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>str</code> <p>The name of the rate limit policy to apply (e.g., 'default', 'heavy_users'),           as defined in the configuration.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable[..., Awaitable[Any]]</code> <p>The decorator function.</p> <p>Raises:</p> Type Description <code>RateLimitExceeded</code> <p>If the request is blocked by the rate limiter.</p>"},{"location":"athomic/resilience/rate_limiter/#nala.athomic.resilience.rate_limiter.service.RateLimiterService","title":"<code>nala.athomic.resilience.rate_limiter.service.RateLimiterService</code>","text":"<p>Orchestrates rate limiting logic, integrating provider, configuration, key generation, and detailed observability.</p>"},{"location":"athomic/resilience/rate_limiter/#nala.athomic.resilience.rate_limiter.service.RateLimiterService.__init__","title":"<code>__init__(settings=None, provider=None)</code>","text":"<p>Initializes the RateLimiterService.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>Optional[RateLimiterProtocol]</code> <p>Allows injecting a specific provider, primarily for testing.       If None, the default provider is created via the factory.</p> <code>None</code>"},{"location":"athomic/resilience/rate_limiter/#nala.athomic.resilience.rate_limiter.service.RateLimiterService.check","title":"<code>check(policy, *key_parts)</code>  <code>async</code>","text":"<p>Performs a rate limit check with integrated, detailed observability.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>str</code> <p>The name of the rate limit policy to apply.</p> required <code>*key_parts</code> <code>str</code> <p>Logical parts of the key (e.g., function name, resource ID).</p> <code>()</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the request is allowed, False if it is blocked.</p>"},{"location":"athomic/resilience/rate_limiter/#nala.athomic.resilience.rate_limiter.protocol.RateLimiterProtocol","title":"<code>nala.athomic.resilience.rate_limiter.protocol.RateLimiterProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the interface for a context-aware rate limiter. All implementations must respect the allow/clear/reset contract.</p>"},{"location":"athomic/resilience/rate_limiter/#nala.athomic.resilience.rate_limiter.protocol.RateLimiterProtocol.allow","title":"<code>allow(key, rate, policy=None)</code>  <code>async</code>","text":"<p>Checks if the given key is allowed under the specified rate limit and consumes a token if allowed.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The identifier for the entity being rate limited (e.g., user ID, IP).</p> required <code>rate</code> <code>str</code> <p>The rate limit string (e.g., \"5/second\", \"100/minute\").</p> required <code>policy</code> <code>Optional[str]</code> <p>An optional policy identifier to apply specific rate limits.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the request is allowed, False if the rate limit has been exceeded.</p>"},{"location":"athomic/resilience/rate_limiter/#nala.athomic.resilience.rate_limiter.protocol.RateLimiterProtocol.clear","title":"<code>clear(key, rate)</code>  <code>async</code>","text":"<p>Resets the rate limit counters specifically for the given key and rate limit rule.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The identifier whose rate limit counters to reset.</p> required <code>rate</code> <code>str</code> <p>The rate limit string rule associated with the key to clear         (needed to identify the correct counter(s)).</p> required"},{"location":"athomic/resilience/rate_limiter/#nala.athomic.resilience.rate_limiter.protocol.RateLimiterProtocol.get_current_usage","title":"<code>get_current_usage(key, rate)</code>  <code>async</code>","text":"<p>Optional: Returns the current usage count for the given key under a specific rate. May not be supported by all implementations or strategies.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The identifier to query.</p> required <code>rate</code> <code>str</code> <p>The rate limit string rule to check usage against.</p> required <p>Returns:</p> Type Description <code>Optional[int]</code> <p>Optional[int]: Number of requests used in the window, if supported.</p>"},{"location":"athomic/resilience/rate_limiter/#nala.athomic.resilience.rate_limiter.protocol.RateLimiterProtocol.reset","title":"<code>reset()</code>  <code>async</code>","text":"<p>Resets all rate limit counters managed by this storage instance. WARNING: Use with extreme caution, especially with shared storage like Redis,          as this will affect all keys managed by this limiter instance.</p>"},{"location":"athomic/resilience/rate_limiter/#nala.athomic.resilience.rate_limiter.exceptions.RateLimitExceeded","title":"<code>nala.athomic.resilience.rate_limiter.exceptions.RateLimitExceeded</code>","text":"<p>               Bases: <code>Exception</code></p>"},{"location":"athomic/resilience/retry/","title":"Retry","text":""},{"location":"athomic/resilience/retry/#overview","title":"Overview","text":"<p>The Retry pattern is a fundamental resilience mechanism that allows an application to automatically re-execute an operation that has failed due to a transient error, such as a temporary network glitch or a brief service unavailability. This prevents temporary issues from escalating into hard failures, significantly improving the stability and reliability of your service.</p> <p>The Athomic retry module is built on the robust and battle-tested <code>tenacity</code> library. It provides a highly configurable, policy-based approach to retries through a simple <code>@retry</code> decorator.</p>"},{"location":"athomic/resilience/retry/#key-features","title":"Key Features","text":"<ul> <li>Declarative Use: Apply complex retry logic with a single decorator.</li> <li>Policy-Based Configuration: Centrally define multiple named retry policies (e.g., \"fast_retry\", \"slow_retry\") in your configuration files.</li> <li>Exponential Backoff &amp; Jitter: Built-in support for exponential backoff with random jitter to prevent \"thundering herd\" problems.</li> <li>Live Configuration: Retry policies can be tuned and updated at runtime without restarting the application.</li> <li>Full Observability: Every retry attempt is logged and instrumented with traces and metrics.</li> </ul>"},{"location":"athomic/resilience/retry/#how-it-works","title":"How It Works","text":"<p>The retry mechanism is centered around Retry Policies. A policy is a set of rules that defines the retry behavior:</p> <ul> <li><code>attempts</code>: The maximum number of times to retry the operation.</li> <li><code>wait_min_seconds</code> / <code>wait_max_seconds</code>: The minimum and maximum delay between retries.</li> <li><code>backoff</code>: A multiplier for the delay, enabling exponential backoff.</li> <li><code>jitter</code>: A random factor added to the delay to de-synchronize retries from multiple clients.</li> <li><code>exceptions</code>: A list of specific exception types that should trigger a retry.</li> </ul> <p>The <code>@retry</code> decorator applies a policy to a function. When the decorated function is called, a <code>RetryHandler</code> manages the execution. If the function raises one of the configured exceptions, the handler waits for the calculated delay and then re-executes the function, up to the maximum number of attempts.</p>"},{"location":"athomic/resilience/retry/#usage-example","title":"Usage Example","text":"<p>You can apply a named retry policy directly to any asynchronous function.</p> <pre><code>from nala.athomic.resilience import retry, RetryFactory\n\n# In a real app, the factory would be a singleton.\nretry_factory = RetryFactory()\n# Get a pre-configured policy from your settings.toml\nfast_retry_policy = retry_factory.create_policy(name=\"fast_retry\")\n\n@retry(policy=fast_retry_policy)\nasync def call_flaky_service(request_data: dict) -&gt; dict:\n    \"\"\"\n    This function will be retried up to 3 times with a short,\n    exponential backoff if it fails with an HTTPException.\n    \"\"\"\n    # This call might fail temporarily\n    response = await http_client.post(\"/external-api\", json=request_data)\n    response.raise_for_status()\n    return response.json()\n</code></pre>"},{"location":"athomic/resilience/retry/#configuration","title":"Configuration","text":"<p>You define all your retry policies in <code>settings.toml</code> under the <code>[resilience.retry]</code> section. You can specify a <code>default_policy</code> and a dictionary of named <code>policies</code> for different use cases.</p> <pre><code>[default.resilience.retry]\nenabled = true\n\n  # This policy will be used if no specific policy is requested.\n  [default.resilience.retry.default_policy]\n  attempts = 3\n  wait_min_seconds = 1.0\n  wait_max_seconds = 10.0\n  backoff = 2.0 # Wait time will be ~1s, 2s, 4s...\n  exceptions = [\"HTTPRequestError\", \"HTTPTimeoutError\"]\n\n  # A dictionary of named, reusable policies.\n  [default.resilience.retry.policies]\n\n    # A policy for quick, internal retries.\n    [default.resilience.retry.policies.fast_retry]\n    attempts = 3\n    wait_min_seconds = 0.1\n    wait_max_seconds = 1.0\n    backoff = 1.5\n    exceptions = [\"HTTPException\"]\n\n    # A policy for long-running background tasks.\n    [default.resilience.retry.policies.long_retry]\n    attempts = 10\n    wait_min_seconds = 5.0\n    wait_max_seconds = 300.0 # 5 minutes\n    backoff = 2.0\n    exceptions = [\"ConnectionError\"]\n</code></pre>"},{"location":"athomic/resilience/retry/#live-configuration","title":"Live Configuration","text":"<p>Because the <code>RetrySettings</code> model is a <code>LiveConfigModel</code>, you can change any of these values (e.g., <code>attempts</code>, <code>wait_max_seconds</code>) in your live configuration source (like Consul), and the <code>RetryFactory</code> will use the new values for all subsequent operations without requiring an application restart.</p>"},{"location":"athomic/resilience/retry/#api-reference","title":"API Reference","text":""},{"location":"athomic/resilience/retry/#nala.athomic.resilience.retry.decorator.retry","title":"<code>nala.athomic.resilience.retry.decorator.retry(*, policy=None, operation_name=None, on_retry=None, on_fail=None, circuit_breaker_hook=None, logger=None, tracer=None)</code>","text":"<p>A decorator to add retry logic to synchronous or asynchronous functions, with optional hooks for retry, failure, circuit breaker, logging, and tracing. Args:     policy (Optional[RetryPolicy]): The retry policy to use. If None, a default policy may be applied.     operation_name (Optional[str]): An optional name for the operation, used for logging or tracing.     on_retry (Optional[Callable]): Optional callback invoked on each retry attempt.     on_fail (Optional[Callable]): Optional callback invoked when all retry attempts fail.     circuit_breaker_hook (Optional[Callable[[BaseException], None]]): Optional callback invoked when a circuit breaker event occurs.     logger (Optional[Callable]): Optional logger function for logging retry events.     tracer (Optional[Callable]): Optional tracer function for tracing retry events. Returns:     Callable: A decorator that wraps the target function with retry logic, supporting both sync and async functions. Usage:     @retry(policy=my_policy, on_retry=my_on_retry)     def my_function(...):         ...</p>"},{"location":"athomic/resilience/retry/#nala.athomic.resilience.retry.policy.RetryPolicy","title":"<code>nala.athomic.resilience.retry.policy.RetryPolicy</code>","text":"<p>Define the retry behavior: exceptions, attempts, delays, backoff, jitter, etc.</p>"},{"location":"athomic/resilience/retry/#nala.athomic.resilience.retry.factory.RetryFactory","title":"<code>nala.athomic.resilience.retry.factory.RetryFactory</code>","text":"<p>Factory class for creating retry policies and decorators for resilient operations. Attributes:     settings (RetrySettings): The retry settings used to configure retry policies. Methods:     init(settings: Optional[RetrySettings] = None):         Initializes the RetryFactory with the provided settings or defaults.     create_policy(**override_kwargs) -&gt; RetryPolicy:         Creates a RetryPolicy instance using the factory's settings, allowing for         optional overrides of policy attributes via keyword arguments.     create_retry_decorator(         Returns a retry decorator configured with the specified or generated policy,         and optional hooks for logging, tracing, retry, failure, and circuit breaker events.</p>"},{"location":"athomic/resilience/retry/#nala.athomic.resilience.retry.factory.RetryFactory.create_policy","title":"<code>create_policy(name=None)</code>","text":"<p>Creates a RetryPolicy instance based on a named policy from settings. Falls back to the default policy if the name is not found or not provided.</p>"},{"location":"athomic/resilience/retry/#nala.athomic.resilience.retry.factory.RetryFactory.create_retry_handler","title":"<code>create_retry_handler(policy_name=None, operation_name=None, **kwargs)</code>","text":"<p>Creates a RetryHandler instance configured with a specific named policy. This is the method to be used for dependency injection.</p>"},{"location":"athomic/resilience/retry/#nala.athomic.resilience.retry.handler.RetryHandler","title":"<code>nala.athomic.resilience.retry.handler.RetryHandler</code>","text":"<p>Manages the retry logic for synchronous and asynchronous functions based on a defined policy.</p> <p>This class encapsulates the retry behavior, using the <code>tenacity</code> library, and integrates framework features such as exponential backoff, observability (logging, tracing, metrics), and circuit breaker integration.</p>"},{"location":"athomic/resilience/retry/#nala.athomic.resilience.retry.handler.RetryHandler.__init__","title":"<code>__init__(policy, operation_name=None, on_retry=None, on_fail=None, circuit_breaker_hook=None, tracer=None, logger=None)</code>","text":"<p>Initializes the RetryHandler.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>RetryPolicy</code> <p>The retry policy defining behavior (attempts, delays, exceptions).</p> required <code>operation_name</code> <code>Optional[str]</code> <p>A descriptive name for the operation, used in logs and metrics.</p> <code>None</code> <code>on_retry</code> <code>Optional[RetryCallback]</code> <p>Hook executed before sleep on each failed retry attempt.</p> <code>None</code> <code>on_fail</code> <code>Optional[FailCallback]</code> <p>Hook executed when all retry attempts fail permanently.</p> <code>None</code> <code>circuit_breaker_hook</code> <code>Optional[Callable[[], bool]]</code> <p>Hook function that returns True if the circuit is open, aborting retries.</p> <code>None</code> <code>tracer</code> <code>Optional[Tracer]</code> <p>OpenTelemetry tracer instance.</p> <code>None</code> <code>logger</code> <code>Optional[Logger]</code> <p>Logger instance for instrumentation.</p> <code>None</code>"},{"location":"athomic/resilience/retry/#nala.athomic.resilience.retry.handler.RetryHandler.arun","title":"<code>arun(fn, *args, **kwargs)</code>  <code>async</code>","text":"<p>Executes an asynchronous function with the configured retry policy.</p>"},{"location":"athomic/resilience/retry/#nala.athomic.resilience.retry.handler.RetryHandler.run","title":"<code>run(fn, *args, **kwargs)</code>","text":"<p>Executes a synchronous function with the configured retry policy.</p>"},{"location":"athomic/resilience/sagas/","title":"Sagas for Distributed Transactions","text":""},{"location":"athomic/resilience/sagas/#overview","title":"Overview","text":"<p>The Saga pattern is an advanced resilience mechanism for managing data consistency across multiple services in a distributed transaction. Unlike traditional (ACID) transactions, which are not feasible in a microservices architecture, a saga is a sequence of local transactions where each step is coordinated with the next.</p> <p>If any step in the sequence fails, the saga executes a series of compensating actions in reverse order to undo the work that was already completed, thus maintaining overall data consistency.</p> <p>Athomic provides a complete framework for defining, executing, and recovering sagas, supporting two distinct execution models: Orchestration and Choreography.</p>"},{"location":"athomic/resilience/sagas/#core-concepts","title":"Core Concepts","text":"<ul> <li>Saga Definition: A static blueprint of the entire business transaction, created using the <code>SagaBuilder</code>.</li> <li>Step: A single, atomic operation within the saga. Each step consists of an Action and a Compensation.</li> <li>Action: A forward-moving operation that performs a task (e.g., <code>reserve_inventory</code>).</li> <li>Compensation: An operation that semantically undoes an Action (e.g., <code>release_inventory</code>).</li> <li>Saga State: The dynamic, persisted state of a single running saga instance. It tracks the current step, the business payload, and the history of executed steps.</li> </ul>"},{"location":"athomic/resilience/sagas/#execution-models","title":"Execution Models","text":""},{"location":"athomic/resilience/sagas/#1-orchestration-command-based","title":"1. Orchestration (Command-Based)","text":"<p>In this model, a central <code>OrchestrationSagaExecutor</code> manages the entire flow. It directly calls the action handlers for each step in sequence. If any action fails, the orchestrator is responsible for calling the corresponding compensation handlers in reverse order.</p> <ul> <li>Pros: Simple to understand, centralized logic, explicit control flow.</li> <li>Cons: Can lead to a \"god object\" orchestrator if the saga is very complex.</li> </ul>"},{"location":"athomic/resilience/sagas/#2-choreography-event-based","title":"2. Choreography (Event-Based)","text":"<p>In this model, there is no central orchestrator. The <code>ChoreographySagaExecutor</code> simply starts the saga by publishing the first action as a command/event on the message bus.</p> <p>Each step is then handled by an independent service (a \"participant\") that subscribes to its relevant event. After completing its local transaction, the participant uses the injected <code>SagaContext</code> to publish the command/event for the next step. Compensation also flows via events.</p> <ul> <li>Pros: Highly decoupled, services have no direct knowledge of each other.</li> <li>Cons: Can be harder to debug and visualize the end-to-end flow.</li> </ul>"},{"location":"athomic/resilience/sagas/#1-how-to-define-a-saga","title":"1. How to Define a Saga","text":"<p>Regardless of the execution model, you define a saga using the fluent <code>SagaBuilder</code>.</p> <pre><code># In your_app/sagas.py\nfrom nala.athomic.resilience.sagas import SagaBuilder, saga_definition_registry\n\n# Define the blueprint for an \"create_order\" saga\ncreate_order_saga = SagaBuilder(name=\"create_order_saga\") \n    .add_step(\n        name=\"reserve_inventory\",\n        action=\"tasks.inventory.reserve_stock\",\n        compensation=\"tasks.inventory.release_stock\"\n    ) \n    .add_step(\n        name=\"process_payment\",\n        action=\"tasks.payment.charge_card\",\n        compensation=\"tasks.payment.refund_charge\"\n    ) \n    .add_step(\n        name=\"update_order_status\",\n        action=\"tasks.orders.set_order_approved\",\n        compensation=\"tasks.orders.set_order_failed\"\n    ) \n    .build()\n\n# Register the definition so the SagaManager can find it\nsaga_definition_registry.register(create_order_saga)\n</code></pre> <p>The <code>action</code> and <code>compensation</code> strings are identifiers that the executor will resolve into callable functions or tasks.</p>"},{"location":"athomic/resilience/sagas/#2-how-to-execute-a-saga","title":"2. How to Execute a Saga","text":"<p>You start a new saga instance using the <code>SagaManager</code>.</p> <pre><code>from nala.athomic.resilience.sagas import SagaManager\n\nasync def handle_create_order_request(order_details: dict):\n    saga_manager = SagaManager()\n\n    initial_payload = {\n        \"order_id\": order_details[\"id\"],\n        \"customer_id\": order_details[\"customer_id\"],\n        \"amount\": order_details[\"total_price\"]\n    }\n\n    # This call will block until the saga completes or fails\n    final_state = await saga_manager.execute(\n        saga_name=\"create_order_saga\",\n        initial_payload=initial_payload\n    )\n\n    return final_state\n</code></pre>"},{"location":"athomic/resilience/sagas/#3-implementing-participants-for-choreography","title":"3. Implementing Participants (for Choreography)","text":"<p>If you are using the Choreography model, each action and compensation handler is a message consumer decorated with <code>@saga_participant</code>. The decorator injects a <code>SagaContext</code> object, which provides methods to safely advance or compensate the saga.</p> <pre><code># In your inventory service's consumers\nfrom nala.athomic.resilience.sagas import saga_participant, SagaContext\n\n@subscribe_to(\"tasks.inventory.reserve_stock\")\n@saga_participant()\nasync def reserve_stock_handler(context: SagaContext):\n    try:\n        inventory = await inventory_service.reserve(context.payload)\n\n        # Action succeeded: update the payload and complete the step.\n        # This will automatically publish the next event (\"tasks.payment.charge_card\").\n        await context.complete_step(updated_payload={\"inventory_info\": inventory})\n\n    except Exception as e:\n        # Action failed: initiate the compensation flow.\n        # This will publish the first compensation event (\"tasks.inventory.release_stock\").\n        await context.fail_and_compensate(error_details=str(e))\n</code></pre>"},{"location":"athomic/resilience/sagas/#configuration","title":"Configuration","text":"<p>The saga system is configured under the <code>[resilience.sagas]</code> section in your <code>settings.toml</code>.</p> <pre><code>[default.resilience.sagas]\nenabled = true\n\n# Choose the execution model: \"orchestration\" or \"choreography\"\nexecutor_type = \"orchestration\"\n\n# The name of the KVStore connection used for persisting saga state.\nkv_store_connection_name = \"default_redis\"\n\n  # Configuration for the \"Reaper\" service, which recovers stalled sagas.\n  [default.resilience.sagas.reaper]\n  enabled = true\n  poll_interval_seconds = 300 # Check every 5 minutes\n  stalled_threshold_seconds = 900 # A saga is stalled if not updated for 15 minutes\n</code></pre>"},{"location":"athomic/resilience/sagas/#api-reference","title":"API Reference","text":""},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.builder.SagaBuilder","title":"<code>nala.athomic.resilience.sagas.builder.SagaBuilder</code>","text":"<p>A fluent builder for creating immutable SagaDefinition objects.</p> <p>This class provides a declarative, chainable API for defining the sequence of steps, actions, and compensations that constitute a distributed transaction.</p>"},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.builder.SagaBuilder.__init__","title":"<code>__init__(name)</code>","text":"<p>Initializes the builder for a new saga definition.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The unique name for the saga, used for identification and logging.</p> required"},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.builder.SagaBuilder.add_step","title":"<code>add_step(name, action, compensation)</code>","text":"<p>Adds a new step to the saga sequence.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>A human-readable name for the step.</p> required <code>action</code> <code>str</code> <p>An identifier for the function/task that performs the action.</p> required <code>compensation</code> <code>str</code> <p>An identifier for the function/task that compensates the action.</p> required <p>Returns:</p> Type Description <code>SagaBuilder</code> <p>The builder instance to allow for method chaining.</p>"},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.builder.SagaBuilder.build","title":"<code>build()</code>","text":"<p>Constructs and returns the final, immutable SagaDefinition object.</p> <p>Once built, the definition should not be altered.</p> <p>Returns:</p> Type Description <code>SagaDefinition</code> <p>A configured and validated SagaDefinition instance.</p>"},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.manager.SagaManager","title":"<code>nala.athomic.resilience.sagas.manager.SagaManager</code>","text":"<p>The main entry point and public interface for the Saga distributed transaction framework.</p> <p>This manager is responsible for initiating a saga run by resolving the static <code>SagaDefinition</code> and delegating the entire execution lifecycle (forward actions and compensation) to the configured <code>SagaExecutorProtocol</code>.</p> <p>Attributes:</p> Name Type Description <code>executor</code> <code>SagaExecutorProtocol</code> <p>The saga executor responsible for running the process (e.g., Orchestration or Choreography).</p> <code>registry</code> <p>The repository holding registered saga definitions.</p>"},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.manager.SagaManager.__init__","title":"<code>__init__(executor=None)</code>","text":"<p>Initializes the SagaManager.</p> <p>It automatically resolves and injects the <code>SagaStateRepository</code> and the appropriate <code>SagaExecutor</code> implementation if one is not provided.</p> <p>Parameters:</p> Name Type Description Default <code>executor</code> <code>Optional[SagaExecutorProtocol]</code> <p>An optional, pre-configured                                       saga executor instance for dependency injection in tests.</p> <code>None</code>"},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.manager.SagaManager.execute","title":"<code>execute(saga_name, initial_payload)</code>  <code>async</code>","text":"<p>Initiates a new instance of a registered saga.</p> <p>Parameters:</p> Name Type Description Default <code>saga_name</code> <code>str</code> <p>The unique name of the saga definition to execute.</p> required <code>initial_payload</code> <code>Dict[str, Any]</code> <p>The business data required to start the transaction.</p> required <p>Returns:</p> Name Type Description <code>SagaState</code> <code>SagaState</code> <p>The final state object of the saga upon completion or failure.</p> <p>Raises:</p> Type Description <code>SagaError</code> <p>If the saga definition cannot be found in the registry.</p>"},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.participant.saga_participant","title":"<code>nala.athomic.resilience.sagas.participant.saga_participant()</code>","text":"<p>Decorator for a message consumer function to designate it as a participant in a choreographed saga.</p> <p>This decorator handles the necessary boilerplate for saga participation: 1. Extracts the <code>saga_id</code> from the incoming message. 2. Loads the current <code>SagaState</code> from the repository. 3. Validates that the saga is in a runnable state (RUNNING or COMPENSATING). 4. Injects a rich <code>SagaContext</code> object into the decorated function, which    allows the handler to safely advance or compensate the saga.</p>"},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.context.SagaContext","title":"<code>nala.athomic.resilience.sagas.context.SagaContext</code>","text":"<p>An active context object injected into saga participant handlers.</p> <p>It holds the current, mutable state of the saga and provides high-level methods to safely transition the saga's state (advance or fail/compensate), abstracting away the underlying repository and messaging infrastructure.</p>"},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.context.SagaContext.payload","title":"<code>payload</code>  <code>property</code>","text":"<p>Returns the mutable business payload associated with the saga.</p>"},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.context.SagaContext.saga_id","title":"<code>saga_id</code>  <code>property</code>","text":"<p>Returns the unique ID of the current saga instance.</p>"},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.context.SagaContext.__init__","title":"<code>__init__(state, definition, repository, producer)</code>","text":"<p>Initializes the SagaContext.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>SagaState</code> <p>The current, mutable state of the running saga instance.</p> required <code>definition</code> <code>SagaDefinition</code> <p>The static blueprint defining the steps of the saga.</p> required <code>repository</code> <code>SagaStateRepositoryProtocol</code> <p>The persistence layer for state updates.</p> required <code>producer</code> <code>ProducerProtocol</code> <p>The messaging component used to publish events (commands).</p> required"},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.context.SagaContext.complete_step","title":"<code>complete_step(updated_payload=None)</code>  <code>async</code>","text":"<p>Marks the current step as succeeded and triggers the next action in the saga chain.</p> <p>If this is the final step, the saga status is set to <code>COMPLETED</code>. Otherwise, the next action is published as an event via the producer.</p> <p>Parameters:</p> Name Type Description Default <code>updated_payload</code> <code>Optional[Dict[str, Any]]</code> <p>Optional dictionary to update                                         or merge into the saga's persistent payload.</p> <code>None</code>"},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.context.SagaContext.fail_and_compensate","title":"<code>fail_and_compensate(error_details='')</code>  <code>async</code>","text":"<p>Marks the current step as failed, initiates the compensation flow, and publishes the first compensation event.</p> <p>Parameters:</p> Name Type Description Default <code>error_details</code> <code>str</code> <p>A message detailing the reason for the failure.</p> <code>''</code>"},{"location":"athomic/resilience/sagas/#nala.athomic.resilience.sagas.models.SagaState","title":"<code>nala.athomic.resilience.sagas.models.SagaState</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the dynamic, persisted state of a single, running saga instance.</p> <p>This is the core model managed by the <code>SagaStateRepository</code>, tracking the saga's progress, status, and accumulated payload.</p> <p>Attributes:</p> Name Type Description <code>saga_id</code> <code>UUID</code> <p>The unique instance ID of the running saga.</p> <code>saga_name</code> <code>str</code> <p>The name of the saga definition used to start this instance.</p> <code>status</code> <code>SagaStatus</code> <p>The current state of the saga (RUNNING, COMPLETED, etc.).</p> <code>current_step</code> <code>int</code> <p>The index of the last or currently executing step in the definition.</p> <code>payload</code> <code>Dict[str, Any]</code> <p>The mutable business data for the transaction, updated by steps.</p> <code>context</code> <code>Dict[str, Any]</code> <p>The propagated <code>ExecutionContext</code> (e.g., <code>trace_id</code>, <code>tenant_id</code>) captured at initiation.</p> <code>step_history</code> <code>List[SagaStepHistory]</code> <p>An ordered log of execution attempts for each step.</p> <code>created_at</code> <code>datetime</code> <p>The UTC time when the saga instance was created.</p> <code>updated_at</code> <code>datetime</code> <p>The UTC time of the last state update (used by the reaper service).</p>"},{"location":"athomic/resilience/sharding/","title":"Distributed Workload Sharding","text":""},{"location":"athomic/resilience/sharding/#overview","title":"Overview","text":"<p>Distributed Workload Sharding (also known as partitioning) is a pattern for dynamically distributing a set of tasks or data items across a cluster of active worker instances. It is a fundamental technique for achieving horizontal scalability and high availability for background processing services.</p> <p>The Athomic implementation uses Consistent Hashing to ensure that the workload is distributed in a stable and predictable way.</p>"},{"location":"athomic/resilience/sharding/#core-use-case","title":"Core Use Case","text":"<p>The primary use case for sharding in the Athomic Layer is the <code>OutboxPublisher</code>. When you have multiple instances of your service running, sharding ensures that all instances can share the work of publishing outbox events without duplicating effort or creating conflicts. Each publisher instance will claim and process only a specific subset of the pending events.</p>"},{"location":"athomic/resilience/sharding/#core-technology-consistent-hashing","title":"Core Technology: Consistent Hashing","text":"<p>Traditional sharding (e.g., using a modulo operator) can be inefficient. If a worker is added or removed, the entire distribution of work changes, causing massive reshuffling.</p> <p>Athomic uses Consistent Hashing (via the <code>uhashring</code> library) to solve this. With consistent hashing, when a worker joins or leaves the cluster, only a small, necessary fraction of the workload is redistributed. This minimizes disruption and ensures the system remains stable during scaling events or deployments.</p>"},{"location":"athomic/resilience/sharding/#how-it-works-integration-with-service-discovery","title":"How It Works (Integration with Service Discovery)","text":"<p>The sharding mechanism is tightly integrated with a Service Discovery backend (like Consul).</p> <ol> <li> <p>Registration: At startup, each worker instance that participates in a sharding group (e.g., each <code>OutboxPublisher</code> service) registers itself with a specific <code>group_name</code> (e.g., <code>\"outbox-publishers\"</code>) in the service discovery backend. Each instance gets a unique ID.</p> </li> <li> <p>Discovery: When it's time to process work, the <code>ShardingService</code> queries the service discovery backend to get a list of all currently healthy workers registered for its group.</p> </li> <li> <p>Hashing &amp; Filtering: A consistent hash ring is built using the IDs of the active workers. The service then takes the total list of work items (e.g., all pending <code>aggregate_key</code>s from the outbox) and passes them through the hash ring. It claims and processes only the items that hash to its own unique instance ID.</p> </li> <li> <p>Deregistration: On graceful shutdown, the worker deregisters itself. On the next processing cycle, the hash ring will be rebuilt by the remaining workers without the departed instance, and its workload will be automatically and stably redistributed.</p> </li> </ol>"},{"location":"athomic/resilience/sharding/#usage-example","title":"Usage Example","text":"<p>While the <code>OutboxPublisher</code> uses this internally, you could use the <code>ShardingService</code> for any custom distributed worker.</p> <pre><code>from nala.athomic.resilience.sharding import ShardingFactory\n\n# Get the sharding service for a specific policy defined in settings.toml\nsharding_service = ShardingFactory.create(policy_name=\"my_custom_workers\")\n\nasync def process_all_tenants():\n    # 1. Get the complete list of work from a central source\n    all_tenant_ids = await db.get_all_tenant_ids() # e.g., [\"tenant-a\", \"tenant-b\", ...]\n\n    # 2. Filter the list to get only the items this instance is responsible for\n    my_tenant_ids = await sharding_service.filter_owned_items(all_tenant_ids)\n\n    print(f\"This worker owns {len(my_tenant_ids)} tenants.\")\n\n    # 3. Process only the assigned workload\n    for tenant_id in my_tenant_ids:\n        await process_data_for(tenant_id)\n</code></pre>"},{"location":"athomic/resilience/sharding/#configuration","title":"Configuration","text":"<p>You configure sharding policies in your <code>settings.toml</code> under the <code>[resilience.sharding]</code> section. A policy simply defines a <code>group_name</code>, which must correspond to the name used for service discovery.</p> <pre><code>[default.resilience.sharding]\nenabled = true\n\n  # A dictionary of named sharding policies\n  [default.resilience.sharding.policies]\n    # This policy is used by the OutboxPublisher\n    [default.resilience.sharding.policies.outbox_publishers]\n    # This name must match the service name registered in your discovery backend (e.g., Consul)\n    group_name = \"outbox-publisher-service\"\n\n    # You can define other policies for other worker types\n    [default.resilience.sharding.policies.my_custom_workers]\n    group_name = \"custom-worker-service\"\n</code></pre>"},{"location":"athomic/resilience/sharding/#api-reference","title":"API Reference","text":""},{"location":"athomic/resilience/sharding/#nala.athomic.resilience.sharding.protocol.ShardingProtocol","title":"<code>nala.athomic.resilience.sharding.protocol.ShardingProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the contract for a generic service that distributes items among a dynamic group of workers.</p> <p>This protocol abstracts the mechanism for load balancing workloads (like processing message aggregates or tasks) using consistent hashing across multiple service instances.</p>"},{"location":"athomic/resilience/sharding/#nala.athomic.resilience.sharding.protocol.ShardingProtocol.deregister_self","title":"<code>deregister_self()</code>  <code>async</code>","text":"<p>Deregisters the current service instance from the sharding group, allowing its workload to be redistributed to remaining active workers.</p> <p>This must be called during graceful service shutdown.</p>"},{"location":"athomic/resilience/sharding/#nala.athomic.resilience.sharding.protocol.ShardingProtocol.filter_owned_items","title":"<code>filter_owned_items(items)</code>  <code>async</code>","text":"<p>Given a list of all potential items (e.g., aggregate keys), returns the subset that belongs to the current worker instance based on a consistent hashing algorithm and the current set of active workers.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>List[str]</code> <p>A list of all item identifiers.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The subset of items assigned to this worker instance.</p>"},{"location":"athomic/resilience/sharding/#nala.athomic.resilience.sharding.protocol.ShardingProtocol.register_self","title":"<code>register_self()</code>  <code>async</code>","text":"<p>Registers the current service instance as an active worker in the sharding group via a service discovery mechanism (e.g., Consul).</p> <p>This must be called during service startup.</p>"},{"location":"athomic/resilience/sharding/#nala.athomic.resilience.sharding.service.ShardingService","title":"<code>nala.athomic.resilience.sharding.service.ShardingService</code>","text":"<p>               Bases: <code>ShardingProtocol</code></p> <p>Orchestrates distributed workload assignment for a group of workers using consistent hashing and a Service Discovery backend.</p> <p>This service ensures that a set of items (e.g., message aggregate keys or tasks) is distributed stably among currently active service instances (workers).</p> <p>Attributes:</p> Name Type Description <code>policy_settings</code> <code>ShardingPolicySettings</code> <p>Configuration defining the target worker group name.</p> <code>server_settings</code> <code>ServerSettings</code> <p>Configuration defining the host and port for registration.</p> <code>group_name</code> <code>str</code> <p>The name used for service discovery (e.g., 'outbox-publishers').</p> <code>instance_id</code> <code>str</code> <p>A unique identifier for this specific worker instance.</p> <code>discovery</code> <code>ServiceDiscoveryProtocol</code> <p>The client for service discovery operations.</p>"},{"location":"athomic/resilience/sharding/#nala.athomic.resilience.sharding.service.ShardingService.__init__","title":"<code>__init__(policy_settings, server_settings)</code>","text":"<p>Initializes the ShardingService with policy and network configuration.</p> <p>Parameters:</p> Name Type Description Default <code>policy_settings</code> <code>ShardingPolicySettings</code> <p>The sharding policy configuration.</p> required <code>server_settings</code> <code>ServerSettings</code> <p>The server host and port configuration.</p> required"},{"location":"athomic/resilience/sharding/#nala.athomic.resilience.sharding.service.ShardingService.deregister_self","title":"<code>deregister_self()</code>  <code>async</code>","text":"<p>Deregisters the current service instance from the sharding group during graceful shutdown.</p>"},{"location":"athomic/resilience/sharding/#nala.athomic.resilience.sharding.service.ShardingService.filter_owned_items","title":"<code>filter_owned_items(items)</code>  <code>async</code>","text":"<p>Filters a list of items, returning only those mathematically owned by this worker instance based on the consistent hash ring.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>List[str]</code> <p>The list of all item identifiers (e.g., aggregate keys).</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The subset of items assigned to this worker.</p>"},{"location":"athomic/resilience/sharding/#nala.athomic.resilience.sharding.service.ShardingService.register_self","title":"<code>register_self()</code>  <code>async</code>","text":"<p>Registers the current service instance as an active worker in the group via the Service Discovery provider.</p>"},{"location":"athomic/resilience/sharding/#nala.athomic.resilience.sharding.factory.ShardingFactory","title":"<code>nala.athomic.resilience.sharding.factory.ShardingFactory</code>","text":"<p>Manages singleton instances of ShardingService, one per configured policy.</p>"},{"location":"athomic/resilience/sharding/#nala.athomic.resilience.sharding.factory.ShardingFactory.clear","title":"<code>clear()</code>  <code>classmethod</code>","text":"<p>Clears all cached instances. Used for testing.</p>"},{"location":"athomic/resilience/sharding/#nala.athomic.resilience.sharding.factory.ShardingFactory.create","title":"<code>create(policy_name)</code>  <code>classmethod</code>","text":"<p>Creates and returns a singleton instance of the ShardingService for a specific, named policy defined in the configuration.</p>"},{"location":"athomic/resilience/timeout/","title":"Timeouts &amp; Cancellation","text":""},{"location":"athomic/resilience/timeout/#overview","title":"Overview","text":"<p>The Timeout module provides a robust and flexible way to enforce time limits on operations. This is a critical resilience pattern that prevents requests from hanging indefinitely, which could otherwise consume and exhaust system resources.</p> <p>The key feature of Athomic's implementation is Contextual Timeout Cancellation. This allows a single, top-level timeout to be respected across a deep call stack of multiple, nested asynchronous operations.</p>"},{"location":"athomic/resilience/timeout/#how-it-works-contextual-timeout-cancellation","title":"How It Works: Contextual Timeout Cancellation","text":"<p>The system works through the coordinated use of two decorators and the <code>Context</code> module.</p> <ol> <li> <p>Setting the Deadline with <code>@timeout</code>: You apply the main <code>@timeout(seconds=...)</code> decorator to a high-level function, like an API route handler. When this function is called, the decorator records an absolute <code>deadline</code> timestamp (current time + timeout seconds) in the current <code>ExecutionContext</code>.</p> </li> <li> <p>Enforcing the Deadline with <code>@cancellable_operation</code>: You apply the <code>@cancellable_operation</code> decorator to lower-level, internal functions that are called within the initial <code>@timeout</code> block (e.g., service methods, repository calls).</p> </li> <li> <p>Dynamic Cancellation: Each <code>@cancellable_operation</code> decorator reads the <code>deadline</code> from the context, calculates the remaining time, and applies that remaining time as its own timeout for the function it wraps.</p> </li> </ol> <p>This creates a powerful effect: if an API endpoint has a 30-second timeout, and the first 20 seconds are spent in one service call, a subsequent cancellable operation will automatically have a timeout of less than 10 seconds to ensure the total deadline is met. If the deadline has already passed, it will be cancelled immediately.</p>"},{"location":"athomic/resilience/timeout/#usage","title":"Usage","text":""},{"location":"athomic/resilience/timeout/#1-the-timeout-decorator","title":"1. The <code>@timeout</code> Decorator","text":"<p>Use this decorator on top-level functions that represent a complete unit of work, like an API endpoint handler. This sets the overall deadline for the entire operation.</p> <pre><code>from nala.athomic.resilience.timeout import timeout, TimeoutException\n\n@router.post(\"/process-data\")\n@timeout(seconds=15.0)\nasync def process_data_endpoint(data: dict):\n    \"\"\"\n    This entire request, including all downstream calls,\n    must complete within 15 seconds.\n    \"\"\"\n    try:\n        # This service call is decorated with @cancellable_operation\n        result = await data_service.process(data)\n        return {\"status\": \"success\", \"result\": result}\n    except TimeoutException:\n        return {\"status\": \"error\", \"message\": \"Processing timed out.\"}\n</code></pre>"},{"location":"athomic/resilience/timeout/#2-the-cancellable_operation-decorator","title":"2. The <code>@cancellable_operation</code> Decorator","text":"<p>Use this decorator on internal functions that are part of a larger workflow initiated by a function with a <code>@timeout</code>.</p> <pre><code>from nala.athomic.resilience.timeout import cancellable_operation\n\nclass DataService:\n    @cancellable_operation\n    async def process(self, data: dict):\n        # This operation will respect the deadline set by the\n        # @timeout decorator on the API endpoint.\n        await self.repository.save(data)\n        await self.external_api.notify(data)\n</code></pre>"},{"location":"athomic/resilience/timeout/#3-the-dynamic_timeout-decorator","title":"3. The <code>@dynamic_timeout</code> Decorator","text":"<p>As an alternative to <code>@timeout</code>, you can use <code>@dynamic_timeout</code> if you need the timeout duration to be specified at call time rather than at decoration time.</p> <pre><code>from nala.athomic.resilience.timeout import dynamic_timeout\n\n@dynamic_timeout(default=10.0)\nasync def fetch_data(url: str):\n    # ...\n    pass\n\n# This call will use a 5-second timeout\nawait fetch_data(\"[http://example.com](http://example.com)\", seconds=5.0)\n\n# This call will use the 10-second default\nawait fetch_data(\"[http://another-example.com](http://another-example.com)\")\n</code></pre>"},{"location":"athomic/resilience/timeout/#configuration","title":"Configuration","text":"<p>This module does not require any configuration in <code>settings.toml</code>. All timeout behavior is defined directly in the code via decorator arguments.</p>"},{"location":"athomic/resilience/timeout/#api-reference","title":"API Reference","text":""},{"location":"athomic/resilience/timeout/#nala.athomic.resilience.timeout.decorators.timeout","title":"<code>nala.athomic.resilience.timeout.decorators.timeout(seconds, fallback=None, *, preserve_context=False)</code>","text":"<p>Decorator to apply a maximum execution time limit (timeout) to synchronous or asynchronous functions.</p> <p>The timeout uses the application's internal mechanism (<code>run_with_timeout</code>) to handle both concurrency types and ensures consistent context propagation and cleanup.</p> <p>Parameters:</p> Name Type Description Default <code>seconds</code> <code>float</code> <p>The total timeout duration in seconds.</p> required <code>fallback</code> <code>Optional[Callable]</code> <p>An optional zero-argument callable (sync or async)                            to execute if the primary function times out.</p> <code>None</code> <code>preserve_context</code> <code>bool</code> <p>Internal flag. If True, skips cleaning the timeout deadline                      from the context upon exiting the wrapper (primarily for testing purposes).</p> <code>False</code> <p>Returns:</p> Type Description <code>Callable[[F], F]</code> <p>Callable[[F], F]: The wrapped function with timeout behavior.</p>"},{"location":"athomic/resilience/timeout/#nala.athomic.resilience.timeout.cancellation.cancellable_operation","title":"<code>nala.athomic.resilience.timeout.cancellation.cancellable_operation(func)</code>","text":"<p>Decorator that wraps an asynchronous operation to respect a global timeout deadline set earlier in the execution context by the <code>@timeout</code> decorator.</p> <p>This mechanism enforces dynamic timeout cancellation: if the remaining time until the context deadline is less than the function's execution time, <code>asyncio.wait_for</code> is used to enforce the remaining duration.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Awaitable[Any]]</code> <p>The asynchronous function to wrap.</p> required <p>Returns:</p> Type Description <code>Callable[..., Awaitable[Any]]</code> <p>Callable[..., Awaitable[Any]]: The wrapped function with cancellation logic.</p> <p>Raises:</p> Type Description <code>CancelledException</code> <p>If the deadline has already passed before the function starts.</p> <code>TimeoutException</code> <p>If the function exceeds the remaining time before the deadline.</p>"},{"location":"athomic/resilience/timeout/#nala.athomic.resilience.timeout.exceptions.TimeoutException","title":"<code>nala.athomic.resilience.timeout.exceptions.TimeoutException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the execution of a function exceeds the defined timeout.</p>"},{"location":"athomic/resilience/timeout/#nala.athomic.resilience.timeout.exceptions.CancelledException","title":"<code>nala.athomic.resilience.timeout.exceptions.CancelledException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when execution is cancelled gracefully before timeout.</p>"},{"location":"athomic/security/auth/","title":"Authentication &amp; Authorization","text":""},{"location":"athomic/security/auth/#overview","title":"Overview","text":"<p>The Authentication and Authorization module provides a flexible, policy-based system for securing API endpoints. It is designed to clearly separate the concepts of Authentication (verifying who a user or service is) and Authorization (determining what they are allowed to do).</p>"},{"location":"athomic/security/auth/#key-features","title":"Key Features","text":"<ul> <li>Policy-Based: Access control is defined by simple, named policies (e.g., \"public\", \"private\", \"internal\").</li> <li>Multiple Mechanisms: Out-of-the-box support for common authentication mechanisms like JWT Bearer tokens and API Keys (both shared and per-client).</li> <li>Decoupled Architecture: A clean separation between Policies (Executors), which define what access is required, and Authentication Providers, which handle how credentials are validated.</li> <li>Extensible: New authentication providers or authorization policies can be added by implementing simple protocols and registering them.</li> </ul>"},{"location":"athomic/security/auth/#core-concepts","title":"Core Concepts","text":""},{"location":"athomic/security/auth/#policies-executors","title":"Policies (Executors)","text":"<p>An Authorization Policy is a named rule that defines the security requirements for an endpoint. For example, a \"private\" policy might require a valid JWT, while an \"internal\" policy might require a specific IP address.</p> <p>In Athomic, policies are implemented by classes that follow the <code>AuthExecutorProtocol</code>. The <code>AuthService</code> uses a central registry to look up the correct executor for a given policy name.</p>"},{"location":"athomic/security/auth/#authentication-providers","title":"Authentication Providers","text":"<p>An Authentication Provider is a component that performs the actual credential validation. For example, the <code>JWTAuth</code> provider knows how to decode a JWT, verify its signature, and check its expiration. The <code>APIKeyDBProvider</code> knows how to look up an API key's hash in a database.</p> <p>All providers implement the <code>AuthProviderProtocol</code>, allowing the executors to be decoupled from the specific validation logic.</p>"},{"location":"athomic/security/auth/#authservice","title":"<code>AuthService</code>","text":"<p>The <code>AuthService</code> is the central orchestrator that ties everything together. When an endpoint needs to be secured, you ask the <code>AuthService</code> to authorize the request against a specific policy. The service finds the correct executor for that policy and runs it, which in turn uses the appropriate authentication provider.</p>"},{"location":"athomic/security/auth/#available-policies-executors","title":"Available Policies (Executors)","text":"<p>Athomic comes with a set of pre-configured policies:</p> <ul> <li><code>public</code>: Allows anonymous, unrestricted access.</li> <li><code>private</code>: Requires a valid JWT Bearer token in the <code>Authorization</code> header.</li> <li><code>shared_api_key</code>: Requires a valid, globally shared API key in the configured header.</li> <li><code>db_api_key</code>: Requires a valid, per-client API key that is looked up in the database.</li> <li><code>internal</code>: Requires the request's IP address to be on a configured allowlist.</li> </ul>"},{"location":"athomic/security/auth/#how-to-secure-an-endpoint","title":"How to Secure an Endpoint","text":"<p>Securing an endpoint is typically done at the web framework layer, for example, using a FastAPI dependency. The dependency would be responsible for extracting credentials from the request, building a <code>SecurityRequestContext</code>, and calling the <code>AuthService</code>.</p>"},{"location":"athomic/security/auth/#conceptual-fastapi-example","title":"Conceptual FastAPI Example","text":"<pre><code>from fastapi import Depends, HTTPException, Request\nfrom nala.athomic.security.auth import AuthService, SecurityRequestContext\nfrom nala.athomic.security.exceptions import AthomicAuthError, AthomicForbiddenError\n\nauth_service = AuthService() # Singleton instance\n\nasync def require_private_access(request: Request):\n    \"\"\"A FastAPI dependency that enforces the 'private' policy.\"\"\"\n\n    context = SecurityRequestContext(\n        client_host=request.client.host,\n        headers=dict(request.headers)\n    )\n\n    try:\n        # Ask the AuthService to authorize the request against the \"private\" policy\n        authenticated_user = await auth_service.authorize(context, \"private\")\n        return authenticated_user\n    except AthomicAuthError as e:\n        raise HTTPException(status_code=401, detail=str(e))\n    except AthomicForbiddenError as e:\n        raise HTTPException(status_code=403, detail=str(e))\n\n@router.get(\"/profile\")\nasync def get_user_profile(user: AuthenticatedUser = Depends(require_private_access)):\n    # This endpoint is now protected. The 'user' object contains the\n    # validated claims from the JWT.\n    return {\"user_id\": user.sub, \"scopes\": user.scopes}\n</code></pre>"},{"location":"athomic/security/auth/#configuration","title":"Configuration","text":"<p>Authentication is configured under the <code>[security.auth]</code> section in your <code>settings.toml</code>.</p> <pre><code>[default.security.auth]\n# A master switch to enable or disable authentication.\nenable = true\n\n# The default backend to use. Not always necessary as policies are explicit.\nbackend = \"jwt\"\n\n  # Configuration for JWT-based authentication.\n  [default.security.auth.jwt]\n  # The secret key for signing tokens. Best practice is to use a secret reference.\n  secret_key = { path = \"auth/jwt\", key = \"secret\" }\n  algorithm = \"HS256\"\n  expiration_minutes = 60\n\n  # Configuration for API Key-based authentication.\n  [default.security.auth.api_key]\n  enabled = true\n  header_name = \"X-API-KEY\"\n\n  # The secret for the \"shared_api_key\" policy.\n  secret_key = { path = \"auth/api-key\", key = \"shared-secret\" }\n\n  # The database connection for the \"db_api_key\" policy.\n  document_db_connection_name = \"default_mongo\"\n</code></pre>"},{"location":"athomic/security/auth/#api-reference","title":"API Reference","text":""},{"location":"athomic/security/auth/#nala.athomic.security.auth.service.AuthService","title":"<code>nala.athomic.security.auth.service.AuthService</code>","text":"<p>The central authorization service.</p> <p>This class acts as an Orchestrator, delegating the specific policy execution logic to the appropriate <code>AuthExecutorProtocol</code> instance, which is retrieved through a standardized registry. It integrates metrics for all policy check attempts.</p>"},{"location":"athomic/security/auth/#nala.athomic.security.auth.service.AuthService.__init__","title":"<code>__init__(registry=None)</code>","text":"<p>Initializes the service with a specific executor registry.</p> <p>Parameters:</p> Name Type Description Default <code>registry</code> <code>Optional[AuthExecutorRegistry]</code> <p>An optional registry instance                                       for dependency injection (DI).                                       Defaults to the global singleton.</p> <code>None</code>"},{"location":"athomic/security/auth/#nala.athomic.security.auth.service.AuthService.authorize","title":"<code>authorize(context, policy_name)</code>  <code>async</code>","text":"<p>Finds the correct executor in the registry and executes the authorization check.</p> <p>This method encapsulates the authorization flow: it records the attempt, retrieves the executor, executes the check, and logs the final success or failure.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>SecurityRequestContext</code> <p>The request context containing headers and client information.</p> required <code>policy_name</code> <code>str</code> <p>The name of the authorization policy to enforce (e.g., 'private', 'db_api_key').</p> required <p>Returns:</p> Name Type Description <code>AuthenticatedUser</code> <code>AuthenticatedUser</code> <p>A user object representing the authenticated identity.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no executor is registered for the specified <code>policy_name</code>.</p> <code>Exception</code> <p>Propagates the exception raised by the executor upon authorization failure.</p>"},{"location":"athomic/security/auth/#nala.athomic.security.auth.executors.protocols.AuthExecutorProtocol","title":"<code>nala.athomic.security.auth.executors.protocols.AuthExecutorProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the contract for an authorization execution strategy.</p> <p>Each executor is responsible for enforcing a single, specific form of validation (e.g., JWT token check, shared API key validation, IP allowlist check), adhering to the Single Responsibility Principle (SRP).</p>"},{"location":"athomic/security/auth/#nala.athomic.security.auth.executors.protocols.AuthExecutorProtocol.execute","title":"<code>execute(context)</code>  <code>async</code>","text":"<p>Executes the authorization logic against the provided request context.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>SecurityRequestContext</code> <p>The request context containing client host                             and headers necessary for validation.</p> required <p>Returns:</p> Name Type Description <code>AuthenticatedUser</code> <code>AuthenticatedUser</code> <p>An object representing the authenticated identity and its claims.</p> <p>Raises:</p> Type Description <code>AthomicAuthError</code> <p>If authentication fails (e.g., invalid token, expired, revoked).</p> <code>AthomicForbiddenError</code> <p>If access is denied (e.g., IP not allowed, insufficient scope).</p>"},{"location":"athomic/security/auth/#nala.athomic.security.auth.protocol.AuthProviderProtocol","title":"<code>nala.athomic.security.auth.protocol.AuthProviderProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p>"},{"location":"athomic/security/auth/#nala.athomic.security.auth.protocol.AuthProviderProtocol.check","title":"<code>check(token)</code>  <code>async</code>","text":"<p>Verify a token and return its decoded payload if valid.</p> <p>:param token: The token to verify. :return: A dictionary with the decoded payload. :raises: AuthenticationError or other relevant exception if invalid.</p>"},{"location":"athomic/security/auth/#nala.athomic.security.auth.protocol.AuthProviderProtocol.create_token","title":"<code>create_token(subject, scope=None)</code>","text":"<p>Create a signed token for the given subject.</p> <p>:param subject: Unique identifier for the token owner (e.g., user ID). :param scope: Optional scope string for authorization. :return: A JWT or compatible token as a string.</p>"},{"location":"athomic/security/auth/#nala.athomic.security.auth.models.auth_user.AuthenticatedUser","title":"<code>nala.athomic.security.auth.models.auth_user.AuthenticatedUser</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents an authenticated identity extracted from a validated token payload (e.g., JWT).</p> <p>This model provides a normalized view of the authenticated user's claims, used throughout the application for authorization decisions.</p> <p>Attributes:</p> Name Type Description <code>sub</code> <code>str</code> <p>The unique identifier for the user or entity (Subject) that the token represents.</p> <code>scopes</code> <code>List[str]</code> <p>A list of normalized (lowercase, split) user permission scopes.</p> <code>roles</code> <code>List[str]</code> <p>A list of roles assigned to the user.</p> <code>jti</code> <code>str</code> <p>The unique ID of the token (JWT ID), crucial for token denylist/revocation checks.</p> <code>exp</code> <code>int</code> <p>The timestamp (UNIX epoch) when the token expires.</p> <code>raw_scope</code> <code>str</code> <p>The original, unmodified scope string from the token payload.</p>"},{"location":"athomic/security/auth/#nala.athomic.security.auth.models.auth_user.AuthenticatedUser.from_payload","title":"<code>from_payload(payload)</code>  <code>classmethod</code>","text":"<p>Creates an <code>AuthenticatedUser</code> instance from a raw decoded token payload dictionary.</p> <p>This factory method handles normalization, such as splitting the raw scope string into a list of lowercase scopes.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>Dict[str, Any]</code> <p>The decoded token payload (dictionary).</p> required <p>Returns:</p> Name Type Description <code>AuthenticatedUser</code> <code>AuthenticatedUser</code> <p>A validated instance of the user model.</p>"},{"location":"athomic/security/crypto/","title":"Cryptography","text":""},{"location":"athomic/security/crypto/#overview","title":"Overview","text":"<p>The Cryptography module provides a simple, high-level abstraction for symmetric (secret key) encryption and decryption. It is built to be secure by default, leveraging the robust and widely trusted Fernet implementation from the <code>cryptography</code> library.</p> <p>Its primary use case is to provide an end-to-end encryption layer for message payloads as part of the <code>Payload Processing Pipeline</code>, but it can also be used for any general-purpose encryption needs within your application.</p>"},{"location":"athomic/security/crypto/#key-features","title":"Key Features","text":"<ul> <li>Strong Encryption: Uses Fernet (AES-128 in CBC mode with PKCS7 padding, signed with HMAC using SHA256).</li> <li>Secure Key Handling: The encryption key (<code>fernet_key</code>) is configured as a secret and resolved at runtime via the Secrets Management module.</li> <li>Lazy Initialization: The Fernet client is only initialized on its first use, preventing unnecessary key handling at startup.</li> <li>Pipeline Integration: Seamlessly integrates into the messaging payload pipeline to automatically encrypt and decrypt messages.</li> </ul>"},{"location":"athomic/security/crypto/#how-it-works","title":"How It Works","text":"<p>The module is centered around the <code>CryptoProviderProtocol</code>, which defines a simple <code>encrypt_bytes</code> and <code>decrypt_bytes</code> interface. The default implementation is the <code>FernetCryptoProvider</code>.</p> <p>The <code>CryptoProviderFactory</code> is a singleton factory used to obtain a configured instance of the provider. It ensures that the same key and client are used throughout the application.</p>"},{"location":"athomic/security/crypto/#integration-with-the-messaging-pipeline","title":"Integration with the Messaging Pipeline","text":"<p>The most common way to use the crypto module is to add it as a step in the messaging payload pipeline. This automatically encrypts the entire message payload after serialization and before it is sent to the broker. On the consumer side, it automatically decrypts the payload before deserialization.</p> <p>To enable this, simply add a <code>\"crypto\"</code> step to your pipeline configuration.</p> <pre><code># In settings.toml\n[default.integration.messaging.payload]\n\n# The pipeline is executed in order for encoding, and in reverse for decoding.\n# Flow: Pydantic Model -&gt; [Serializer] -&gt; Bytes -&gt; [Crypto] -&gt; Encrypted Bytes\n[[default.integration.messaging.payload.pipeline]]\nstep = \"serializer\"\n\n[[default.integration.messaging.payload.pipeline]]\nstep = \"crypto\"\n</code></pre> <p>With this configuration, all messages will be transparently encrypted and decrypted as they are produced and consumed.</p>"},{"location":"athomic/security/crypto/#direct-usage-example","title":"Direct Usage Example","text":"<p>You can also use the crypto provider directly for general-purpose encryption tasks.</p> <pre><code>from nala.athomic.security.crypto import CryptoProviderFactory\n\nasync def encrypt_sensitive_data(data: str) -&gt; bytes:\n    crypto_provider = CryptoProviderFactory.create()\n\n    data_bytes = data.encode('utf-8')\n    encrypted_bytes = await crypto_provider.encrypt_bytes(data_bytes)\n\n    return encrypted_bytes\n\nasync def decrypt_sensitive_data(encrypted_data: bytes) -&gt; str:\n    crypto_provider = CryptoProviderFactory.create()\n\n    decrypted_bytes = await crypto_provider.decrypt_bytes(encrypted_data)\n\n    return decrypted_bytes.decode('utf-8')\n</code></pre>"},{"location":"athomic/security/crypto/#configuration","title":"Configuration","text":"<p>The cryptography module requires a <code>fernet_key</code> to be configured under the <code>[security.crypto]</code> section in your <code>settings.toml</code>. For production, this must be a reference to a secret stored in a secure backend like Vault.</p> <pre><code>[default.security.crypto]\n# The Fernet key must be 32 URL-safe base64-encoded bytes.\n# In production, this should always be a secret reference.\nfernet_key = { path = \"global/crypto\", key = \"fernet-key\" }\n</code></pre> <p>You can generate a new Fernet key using the <code>cryptography</code> library:</p> <pre><code>from cryptography.fernet import Fernet\nkey = Fernet.generate_key()\nprint(key.decode())\n</code></pre>"},{"location":"athomic/security/crypto/#api-reference","title":"API Reference","text":""},{"location":"athomic/security/crypto/#nala.athomic.security.crypto.protocol.CryptoProviderProtocol","title":"<code>nala.athomic.security.crypto.protocol.CryptoProviderProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the contract for a cryptographic provider that operates on raw bytes.</p> <p>This protocol ensures that any concrete implementation (e.g., Fernet, AES) conforms to a standardized asynchronous interface for encryption and decryption, adhering to the Dependency Inversion Principle (DIP).</p>"},{"location":"athomic/security/crypto/#nala.athomic.security.crypto.protocol.CryptoProviderProtocol.decrypt_bytes","title":"<code>decrypt_bytes(encrypted_data)</code>  <code>async</code>","text":"<p>Decrypts an encrypted byte string.</p> <p>Parameters:</p> Name Type Description Default <code>encrypted_data</code> <code>bytes</code> <p>The encrypted byte string.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The original, decrypted byte string.</p> <p>Raises:</p> Type Description <code>CryptoOperationError</code> <p>If decryption fails (e.g., invalid key, invalid token).</p>"},{"location":"athomic/security/crypto/#nala.athomic.security.crypto.protocol.CryptoProviderProtocol.encrypt_bytes","title":"<code>encrypt_bytes(data)</code>  <code>async</code>","text":"<p>Encrypts a raw byte string.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>The raw, unencrypted byte string.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The encrypted byte string.</p>"},{"location":"athomic/security/crypto/#nala.athomic.security.crypto.factory.CryptoProviderFactory","title":"<code>nala.athomic.security.crypto.factory.CryptoProviderFactory</code>","text":"<p>Manages the singleton instance of the CryptoProvider.</p>"},{"location":"athomic/security/crypto/#nala.athomic.security.crypto.fernet_provider.FernetCryptoProvider","title":"<code>nala.athomic.security.crypto.fernet_provider.FernetCryptoProvider</code>","text":"<p>               Bases: <code>CryptoProviderProtocol</code>, <code>CredentialResolve</code></p> <p>A cryptographic provider implementation using the Fernet symmetric encryption standard.</p> <p>This class adheres to the CryptoProviderProtocol and manages the entire lifecycle and security of the Fernet key: 1. Lazy Initialization: The key is resolved and the Fernet instance is created    only on the first call to <code>encrypt_bytes</code> or <code>decrypt_bytes</code>. 2. Asynchronous Credential Resolution: It uses the <code>CredentialResolve</code> mixin    to securely fetch the encryption key from an external source (e.g., Vault). 3. Integrated Observability: All encryption and decryption attempts are    instrumented with distributed tracing and Prometheus metrics.</p> <p>Attributes:</p> Name Type Description <code>settings</code> <code>CryptoSettings</code> <p>The configuration for the crypto module.</p> <code>key_source</code> <p>The source object (e.g., <code>SecretStr</code>, <code>CredentialProxy</code>) for the Fernet key.</p> <code>fernet</code> <code>Optional[Fernet]</code> <p>The underlying Fernet encryption instance.</p> <code>_init_lock</code> <code>Lock</code> <p>A lock to ensure thread-safe, single-time initialization.</p> <code>tracer</code> <p>An OpenTelemetry tracer instance.</p> <code>provider_name</code> <code>str</code> <p>The name used for metric and span labeling.</p>"},{"location":"athomic/security/crypto/#nala.athomic.security.crypto.fernet_provider.FernetCryptoProvider.__init__","title":"<code>__init__(settings=None)</code>","text":"<p>Initializes the provider, but delays Fernet instance creation.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[CryptoSettings]</code> <p>The configuration settings for the provider.</p> <code>None</code>"},{"location":"athomic/security/crypto/#nala.athomic.security.crypto.fernet_provider.FernetCryptoProvider.decrypt_bytes","title":"<code>decrypt_bytes(encrypted_data)</code>  <code>async</code>","text":"<p>Decrypts an encrypted byte string, wrapped with tracing and metrics.</p> <p>Parameters:</p> Name Type Description Default <code>encrypted_data</code> <code>bytes</code> <p>The encrypted byte string.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The original, decrypted byte string.</p> <p>Raises:</p> Type Description <code>CryptoOperationError</code> <p>If decryption fails due to an invalid token                   or any other internal error.</p>"},{"location":"athomic/security/crypto/#nala.athomic.security.crypto.fernet_provider.FernetCryptoProvider.encrypt_bytes","title":"<code>encrypt_bytes(data)</code>  <code>async</code>","text":"<p>Encrypts a raw byte string, wrapped with tracing and metrics.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>The raw, unencrypted byte string.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The encrypted byte string.</p> <p>Raises:</p> Type Description <code>CryptoOperationError</code> <p>If encryption fails for any reason (e.g., internal error).</p>"},{"location":"athomic/security/crypto/#nala.athomic.security.crypto.adapters.CryptoStepAdapter","title":"<code>nala.athomic.security.crypto.adapters.CryptoStepAdapter</code>","text":"<p>               Bases: <code>ProcessingStepProtocol</code></p> <p>An adapter that integrates a CryptoProviderProtocol into the payload processing pipeline.</p> <p>This class acts as the Adapter for the encryption step (part of the Pipes and Filters pattern), mapping the generic <code>encode</code>/<code>decode</code> operations required by the <code>PayloadProcessor</code> to the concrete <code>encrypt_bytes</code>/<code>decrypt_bytes</code> methods of the underlying <code>CryptoProviderProtocol</code>. This adheres to the SRP by ensuring the <code>CryptoProvider</code> focuses only on the cryptographic operation, while the adapter focuses only on the interface transformation.</p> <p>Attributes:</p> Name Type Description <code>crypto</code> <code>CryptoProviderProtocol</code> <p>The underlying provider instance that performs                              the actual encryption and decryption.</p>"},{"location":"athomic/security/crypto/#nala.athomic.security.crypto.adapters.CryptoStepAdapter.__init__","title":"<code>__init__(crypto_provider)</code>","text":"<p>Initializes the adapter with the core cryptographic provider.</p> <p>Parameters:</p> Name Type Description Default <code>crypto_provider</code> <code>CryptoProviderProtocol</code> <p>An instance of the concrete                                      cryptographic provider (e.g., <code>FernetCryptoProvider</code>).</p> required"},{"location":"athomic/security/crypto/#nala.athomic.security.crypto.adapters.CryptoStepAdapter.decode","title":"<code>decode(data, **kwargs)</code>  <code>async</code>","text":"<p>Decrypts the data (inbound pipeline flow).</p> <p>Maps the pipeline's <code>decode</code> call to the provider's <code>decrypt_bytes</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The encrypted payload to be decrypted, expected as bytes.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments (ignored by this step).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The decrypted, original data as bytes.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input <code>data</code> is not of type bytes.</p> <code>CryptoOperationError</code> <p>Propagated from the underlying provider if decryption fails.</p>"},{"location":"athomic/security/crypto/#nala.athomic.security.crypto.adapters.CryptoStepAdapter.encode","title":"<code>encode(data, **kwargs)</code>  <code>async</code>","text":"<p>Encrypts the data (outbound pipeline flow).</p> <p>Maps the pipeline's <code>encode</code> call to the provider's <code>encrypt_bytes</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The payload to be encrypted, expected as bytes.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments (ignored by this step).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The encrypted data as bytes.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input <code>data</code> is not of type bytes.</p>"},{"location":"athomic/security/secrets/","title":"Secrets Management","text":""},{"location":"athomic/security/secrets/#overview","title":"Overview","text":"<p>The Secrets Management module provides a secure and centralized system for handling sensitive data like API keys, database passwords, and authentication tokens. Instead of hardcoding secrets in configuration files or environment variables, Athomic treats them as references to be resolved at runtime from a secure, external backend.</p> <p>This approach significantly improves the security posture of the application and is a best practice for modern cloud-native development.</p>"},{"location":"athomic/security/secrets/#key-features","title":"Key Features","text":"<ul> <li>Multiple Backends: Out-of-the-box support for fetching secrets from HashiCorp Vault, environment variables, or local files.</li> <li>Runtime Resolution: Secrets are fetched \"just-in-time\" only when they are first needed, not at application startup. This reduces the time they are held in memory and supports dynamic secret rotation.</li> <li>Extensible Provider Model: The system is built on a protocol, making it easy to add new secret providers in the future.</li> <li>Resilient and Observable: All interactions with the secrets backend are fully instrumented with tracing and metrics.</li> </ul>"},{"location":"athomic/security/secrets/#the-just-in-time-resolution-flow","title":"The Just-in-Time Resolution Flow","text":"<p>The core of the secrets module is its just-in-time (JIT) resolution mechanism. Here is how it works:</p> <ol> <li> <p>Configure a Reference: In your <code>settings.toml</code>, you define a reference to a secret instead of the secret itself. This is done using a <code>path</code> and <code>key</code> structure.</p> <p>```toml</p> </li> <li> <p>Startup &amp; Proxy Replacement: When the application starts, the <code>SecretsManager</code> service traverses the entire configuration object. When it finds a secret reference, it replaces it in-memory with a lightweight <code>CredentialProxy</code> object. This proxy knows how to fetch the secret but doesn't do it yet.</p> </li> <li> <p>First Access &amp; Resolution: Later, when a component (like a database connector) needs the password, it tries to access the value. This triggers the <code>CredentialProxy</code>, which then makes a live, asynchronous call to the configured secrets provider (e.g., Vault) to fetch the actual secret value.</p> </li> </ol> <p>This lazy-loading approach is highly secure and efficient.</p>"},{"location":"athomic/security/secrets/#instead-of-this","title":"Instead of this:","text":""},{"location":"athomic/security/secrets/#password-my-secret-password-pragma-allowlist-secret","title":"password = \"my-secret-password\" # pragma: allowlist secret","text":""},{"location":"athomic/security/secrets/#you-do-this","title":"You do this:","text":"<p>password = { path = \"database/mongo/prod\", key = \"password\" } ```</p>"},{"location":"athomic/security/secrets/#available-providers","title":"Available Providers","text":""},{"location":"athomic/security/secrets/#vaultsecretsprovider","title":"<code>VaultSecretsProvider</code>","text":"<p>The recommended provider for production environments. It connects to a HashiCorp Vault server and supports both Token and AppRole authentication methods. It also includes a background task for automatic, continuous renewal of the Vault token, ensuring long-running services don't lose access.</p>"},{"location":"athomic/security/secrets/#envsecretsprovider","title":"<code>EnvSecretsProvider</code>","text":"<p>This provider reads secrets from environment variables. It constructs the variable name to look for based on the reference's path and key. For example, a reference with <code>path=\"database/mongo\"</code> and <code>key=\"password\"</code> would resolve to the environment variable <code>NALA_SECRET_DATABASE_MONGO_PASSWORD</code>.</p>"},{"location":"athomic/security/secrets/#filesecretsprovider","title":"<code>FileSecretsProvider</code>","text":"<p>This provider reads secrets from files on the local filesystem. This is particularly useful for containerized environments like Docker and Kubernetes, where secrets are often mounted as files into the container (e.g., <code>/var/run/secrets/db-password</code>).</p>"},{"location":"athomic/security/secrets/#configuration","title":"Configuration","text":"<p>The secrets provider is configured under the <code>[security.secrets]</code> section in your <code>settings.toml</code>.</p>"},{"location":"athomic/security/secrets/#vault-example","title":"Vault Example","text":"<pre><code>[default.security.secrets]\nbackend = \"vault\"\n\n  [default.security.secrets.provider]\n  addr = \"http://localhost:8200\"\n  auth_method = \"token\"\n  token = \"my-root-token\" # Use a secret reference in production!\n</code></pre>"},{"location":"athomic/security/secrets/#environment-variable-example","title":"Environment Variable Example","text":"<pre><code>[default.security.secrets]\nbackend = \"env\"\n</code></pre>"},{"location":"athomic/security/secrets/#file-example","title":"File Example","text":"<pre><code>[default.security.secrets]\nbackend = \"file\"\n  [default.security.secrets.provider]\n  base_path = \"/var/run/secrets\" # Base directory for secret files\n</code></pre>"},{"location":"athomic/security/secrets/#api-reference","title":"API Reference","text":""},{"location":"athomic/security/secrets/#nala.athomic.security.secrets.manager.SecretsManager","title":"<code>nala.athomic.security.secrets.manager.SecretsManager</code>","text":"<p>Orchestrates the resolution of external secret references within application settings.</p> <p>This manager is responsible for traversing the entire <code>AppSettings</code> structure and replacing static <code>SecretValue</code> objects (references to external backends like Vault) with lazy, asynchronous <code>CredentialProxy</code> objects. This ensures that actual secret values are only fetched just-in-time when first accessed by a consuming component, supporting secure, runtime secret rotation.</p> <p>Attributes:</p> Name Type Description <code>app_settings</code> <code>AppSettings</code> <p>The validated application configuration object.</p> <code>secrets_provider</code> <code>SecretsProtocol</code> <p>The concrete provider (e.g., Vault, Env)                                 used to fetch the raw secret data.</p>"},{"location":"athomic/security/secrets/#nala.athomic.security.secrets.manager.SecretsManager.__init__","title":"<code>__init__(settings)</code>","text":"<p>Initializes the SecretsManager.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>AppSettings</code> <p>The application's root configuration object.</p> required"},{"location":"athomic/security/secrets/#nala.athomic.security.secrets.manager.SecretsManager.resolve_settings","title":"<code>resolve_settings()</code>  <code>async</code>","text":"<p>Traverses the entire AppSettings object and replaces all SecretValue instances with CredentialProxy instances that fetch the secret just-in-time.</p> <p>This method is typically called early in the application's startup sequence after configuration is loaded but before services connect.</p>"},{"location":"athomic/security/secrets/#nala.athomic.security.secrets.protocol.SecretsProtocol","title":"<code>nala.athomic.security.secrets.protocol.SecretsProtocol</code>","text":"<p>               Bases: <code>BaseServiceProtocol</code>, <code>Protocol</code></p> <p>Defines the core contract for a secrets provider implementation.</p> <p>This protocol adheres to the Dependency Inversion Principle (DIP), decoupling the <code>SecretsManager</code> and consumers of credentials from the concrete source (e.g., HashiCorp Vault, environment variables). By inheriting from <code>BaseServiceProtocol</code>, all providers gain automatic lifecycle management (<code>connect</code>/<code>close</code>) by the framework.</p>"},{"location":"athomic/security/secrets/#nala.athomic.security.secrets.protocol.SecretsProtocol.get_secret","title":"<code>get_secret(path, key)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Retrieves a specific secret value as raw bytes from the configured backend.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The logical path or mount point where the secret resides         (e.g., 'database/production').</p> required <code>key</code> <code>str</code> <p>The specific key within the path that holds the secret value        (e.g., 'password').</p> required <p>Returns:</p> Type Description <code>Optional[bytes]</code> <p>Optional[bytes]: The resolved secret value as raw bytes, or None if              the secret is not found.</p> <p>Raises:</p> Type Description <code>SecretNotFoundError</code> <p>If the path or key does not exist.</p> <code>SecretAccessError</code> <p>For permission or other access-related failures.</p>"},{"location":"athomic/security/secrets/#nala.athomic.credentials.proxy.CredentialProxy","title":"<code>nala.athomic.credentials.proxy.CredentialProxy</code>","text":"<p>A lazy-loading proxy for a secret value.</p> <p>It holds a reference to the secrets provider and the secret's location (path/key). It fetches the secret's actual value \"just-in-time\" only when its <code>get()</code> method is explicitly awaited. This ensures that the application always uses the most up-to-date credential, supporting secret rotation.</p>"},{"location":"athomic/security/secrets/#nala.athomic.credentials.proxy.CredentialProxy.get","title":"<code>get()</code>  <code>async</code>","text":"<p>Fetches the secret's value just-in-time from the secrets provider.</p> <p>Returns:</p> Type Description <code>Optional[Any]</code> <p>A Resolved secret value, or None if not found.</p>"},{"location":"athomic/security/secrets/#nala.athomic.security.secrets.providers.vault_secrets_provider.VaultSecretsProvider","title":"<code>nala.athomic.security.secrets.providers.vault_secrets_provider.VaultSecretsProvider</code>","text":"<p>               Bases: <code>SecretsBase</code></p> <p>A secrets provider that fetches secrets from HashiCorp Vault.</p> <p>This provider implements the <code>SecretsProtocol</code> using the synchronous <code>hvac</code> client, wrapping all blocking operations in <code>asyncio.to_thread</code>. It handles: 1. Authentication: Supports AppRole and Token methods. 2. Token Renewal: Runs a background loop (<code>_run_loop</code>) to automatically    renew the token before expiration. 3. Lifecycle: Manages authentication (<code>_connect</code>) and token revocation (<code>_close</code>).</p> <p>Attributes:</p> Name Type Description <code>settings</code> <p>The Vault-specific configuration settings.</p> <code>client</code> <p>The underlying synchronous <code>hvac.Client</code> instance.</p>"},{"location":"athomic/security/secrets/#nala.athomic.security.secrets.providers.vault_secrets_provider.VaultSecretsProvider.__init__","title":"<code>__init__(settings)</code>","text":"<p>Initializes the VaultSecretsProvider.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>SecretsSettings</code> <p>The configuration for the secrets module.</p> required"},{"location":"nala/ai/ai-agent-dev/","title":"\ud83e\udd16 Agente Dev \u2013 athomic-docs","text":"<p>Este documento descreve a proposta e arquitetura inicial do Agente de Desenvolvimento Aut\u00f4nomo para o projeto <code>athomic-docs</code>.</p> <p>O objetivo deste agente \u00e9 automatizar a implementa\u00e7\u00e3o de tarefas t\u00e9cnicas seguindo os padr\u00f5es de qualidade definidos pelo framework Nala-IQPY, com a\u00e7\u00f5es rastre\u00e1veis, seguras e audit\u00e1veis.</p>"},{"location":"nala/ai/ai-agent-dev/#objetivo","title":"\ud83c\udfaf Objetivo","text":"<p>Permitir que um agente leia tarefas de um backlog, gere c\u00f3digo, testes e documenta\u00e7\u00e3o automaticamente, valide a qualidade, fa\u00e7a commit e envie para o reposit\u00f3rio em uma branch dedicada.</p>"},{"location":"nala/ai/ai-agent-dev/#funcionalidades-planejadas","title":"\ud83e\udde9 Funcionalidades Planejadas","text":"<ul> <li>[x] Leitura de tarefas a partir de um arquivo <code>backlog.yaml</code></li> <li>[x] Cria\u00e7\u00e3o de branch com nome descritivo (ex: <code>feat/TASK-001-cache-ttl</code>)</li> <li>[x] Gera\u00e7\u00e3o de c\u00f3digo seguindo o padr\u00e3o do projeto (com decorators, registries, etc.)</li> <li>[x] Inclus\u00e3o de testes unit\u00e1rios e documenta\u00e7\u00e3o <code>.md</code></li> <li>[x] Execu\u00e7\u00e3o de <code>ruff</code>, <code>pytest</code>, <code>mypy</code> e <code>pre-commit</code></li> <li>[x] Commits sem\u00e2nticos com prefixo <code>bot:</code></li> <li>[x] Push da branch e sugest\u00e3o de PR</li> <li>[ ] Registro em changelog (futuro)</li> <li>[ ] Aprova\u00e7\u00e3o autom\u00e1tica em ambientes internos (futuro)</li> </ul>"},{"location":"nala/ai/ai-agent-dev/#comportamento-esperado","title":"\ud83e\udde0 Comportamento Esperado","text":"<ol> <li>O agente \u00e9 executado (via CLI ou job agendado)</li> <li>Ele l\u00ea a pr\u00f3xima tarefa <code>pendente</code> no <code>backlog.yaml</code></li> <li>Cria a branch: <code>feat/TASK-001-nome-normalizado</code></li> <li>Gera os arquivos necess\u00e1rios (c\u00f3digo, testes, doc)</li> <li>Roda as ferramentas de qualidade</li> <li>Faz commit com mensagens claras (ex: <code>bot(TASK-001): adicionar m\u00f3dulo de cache com fallback</code>)</li> <li>Faz push da branch para o reposit\u00f3rio remoto</li> </ol>"},{"location":"nala/ai/ai-agent-dev/#estrutura-sugerida","title":"\ud83d\udcc2 Estrutura Sugerida","text":"<pre><code>agent/\n\u251c\u2500\u2500 runner.py                  # Loop principal\n\u251c\u2500\u2500 task_parser.py             # Leitor e validador do backlog.yaml\n\u251c\u2500\u2500 task_executor.py           # Gera\u00e7\u00e3o de c\u00f3digo com LLM\n\u251c\u2500\u2500 quality_checker.py         # Valida lint, testes e typecheck\n\u251c\u2500\u2500 git_utils.py               # Cria\u00e7\u00e3o de branch, commits e push\n\u251c\u2500\u2500 prompts/                   # Templates de prompt baseados no IQPY\n\u251c\u2500\u2500 config.yaml                # Caminhos, tokens, op\u00e7\u00f5es\n</code></pre>"},{"location":"nala/ai/ai-agent-dev/#padroes-de-seguranca-e-conduta","title":"\ud83d\udee1\ufe0f Padr\u00f5es de Seguran\u00e7a e Conduta","text":"<ul> <li>O agente segue o C\u00f3digo de Conduta</li> <li>Toda altera\u00e7\u00e3o gerada \u00e9 rastre\u00e1vel</li> <li>Nunca edita arquivos fora do escopo da tarefa</li> <li>Commits seguem COMMIT_GUIDE.md</li> </ul>"},{"location":"nala/ai/ai-agent-dev/#como-executar","title":"\ud83d\udee0\ufe0f Como Executar","text":"<pre><code>poetry run python agent/runner.py\n</code></pre>"},{"location":"nala/ai/ai-agent-dev/#proximos-passos","title":"\ud83d\udd2e Pr\u00f3ximos Passos","text":"<ul> <li>Integra\u00e7\u00e3o com GitHub API para criar PRs automaticamente</li> <li>Suporte a gera\u00e7\u00e3o de ADRs</li> <li>Feedback via issues e coment\u00e1rios automatizados</li> </ul>"},{"location":"nala/ai/ai-agent-dev/#contribuicoes","title":"\ud83e\udd1d Contribui\u00e7\u00f5es","text":"<p>Sugest\u00f5es s\u00e3o bem-vindas! Abrir uma issue ou PR com novas ideias para o agente.</p>"},{"location":"nala/ai/ai-architecture/","title":"\ud83e\udde0 AI Architecture \u2014 athomic-docs","text":"<p>Este documento descreve a arquitetura proposta para integrar Intelig\u00eancia Artificial como Servi\u00e7o (AIaaS) no projeto <code>athomic-docs</code>.</p>"},{"location":"nala/ai/ai-architecture/#objetivo","title":"\ud83c\udfaf Objetivo","text":"<p>Criar uma estrutura modular, segura e extens\u00edvel que permita integrar agentes de IA como parte do ecossistema <code>athomic-docs</code>, viabilizando uso de modelos, gera\u00e7\u00e3o de conte\u00fado, an\u00e1lise de logs, recomenda\u00e7\u00f5es inteligentes e auditoria.</p>"},{"location":"nala/ai/ai-architecture/#visao-geral-por-camadas","title":"\ud83e\uddf1 Vis\u00e3o Geral por Camadas","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           API GATEWAY / BFF       \u2502 \u25c4\u2500\u2500 Interface REST/gRPC + Auth + Routing\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     ORCHESTRATOR / SERVICE HUB     \u2502 \u25c4\u2500\u2500 Encaminha requisi\u00e7\u00f5es para agentes IA\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502        IA ENGINE (Agentes IA)      \u2502 \u25c4\u2500\u2500 LLMs, modelos, heur\u00edsticas, prompts\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502         MODELOS &amp; RUNTIME          \u2502 \u25c4\u2500\u2500 OpenAI / Vertex / HuggingFace / Local\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502         DATA PIPELINE &amp; LOGS       \u2502 \u25c4\u2500\u2500 Entrada/sa\u00edda, analytics, observability\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502      CONFIG / SECRETS / PERMS      \u2502 \u25c4\u2500\u2500 Dynaconf, Vault, RBAC, IAM\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"nala/ai/ai-architecture/#modulos-sugeridos","title":"\ud83d\udce6 M\u00f3dulos Sugeridos","text":"M\u00f3dulo Fun\u00e7\u00e3o <code>ai_orchestrator.py</code> Roteia chamadas entre agentes IA, aplica fallback e valida inputs <code>agents/</code> Cada agente \u00e9 uma unidade de IA (ex: resumo, an\u00e1lise, recomenda\u00e7\u00e3o) <code>models/</code> Estrat\u00e9gias: OpenAI, VertexAI, HuggingFace, custom, local <code>prompts/</code> Templates de prompt, versionados e reutiliz\u00e1veis <code>usage_logger.py</code> Loga entradas/sa\u00eddas, tokens, tempo, erro <code>audit_trail.py</code> Cria trilha audit\u00e1vel de decis\u00f5es IA <code>security.py</code> Controles de permiss\u00e3o, autentica\u00e7\u00e3o, escopo <code>observability/</code> Logs estruturados, traces, m\u00e9tricas <code>llm_gateway.py</code> Abstra\u00e7\u00e3o para chamadas a LLMs (OpenAI, Vertex, Local, etc.)"},{"location":"nala/ai/ai-architecture/#fluxo-de-uma-requisicao-ia-tipica","title":"\ud83d\udd01 Fluxo de uma Requisi\u00e7\u00e3o IA T\u00edpica","text":"<pre><code>sequenceDiagram\nClient -&gt;&gt; API Gateway: POST /ia/analise\nAPI Gateway -&gt;&gt; Orchestrator: Valida escopo, envia requisi\u00e7\u00e3o\nOrchestrator -&gt;&gt; Agent: Chama agente \"analise_de_feedback\"\nAgent -&gt;&gt; LLM Gateway: Gera prompt e consulta modelo\nLLM Gateway -&gt;&gt; Model (ex: Vertex): Chamada com prompt + contexto\nModel --&gt;&gt; LLM Gateway: Resposta bruta\nLLM Gateway --&gt;&gt; Agent: Retorna resposta refinada\nAgent --&gt;&gt; Orchestrator: Retorna resultado\nOrchestrator -&gt;&gt; Logger + Audit Trail\nOrchestrator -&gt;&gt; API Gateway: Envia resultado\nAPI Gateway -&gt;&gt; Client: Retorna resposta\n</code></pre>"},{"location":"nala/ai/ai-architecture/#backlog-inicial-para-aiaas","title":"\ud83d\udcdd Backlog Inicial para AIaaS","text":"Tarefa Tag Prioridade Criar m\u00f3dulo <code>ai_orchestrator.py</code> com suporte a agentes [core] alta Definir contrato base para um <code>IAgent</code> com <code>run(payload)</code> [core] alta Implementar agente <code>ResumoDeTextoAgent</code> [feature] m\u00e9dia Criar wrapper <code>llm_gateway.py</code> com suporte a OpenAI [core] alta Suporte a cache de respostas IA com TTL [feature] m\u00e9dia Implementar <code>usage_logger.py</code> [observ] alta Implementar <code>audit_trail.py</code> [govern] m\u00e9dia Criar reposit\u00f3rio de <code>prompt_templates</code> [docs] m\u00e9dia Definir estrutura de fallback para agentes IA [resil] alta Suporte a m\u00faltiplos provedores LLM via <code>models/</code> [ext] alta"},{"location":"nala/ai/ai-architecture/#consideracoes-de-seguranca","title":"\ud83d\udd10 Considera\u00e7\u00f5es de Seguran\u00e7a","text":"<ul> <li>Os agentes devem respeitar escopos e permiss\u00f5es definidos no token JWT.</li> <li>Toda entrada de IA deve ser sanitizada e validada.</li> <li>Fallback de agentes deve ser audit\u00e1vel e rastre\u00e1vel.</li> </ul>"},{"location":"nala/ai/ai-architecture/#principios-de-design","title":"\ud83d\udccc Princ\u00edpios de Design","text":"<ul> <li>Modularidade total</li> <li>Fallback seguro</li> <li>Observabilidade integrada</li> <li>Registro din\u00e2mico de agentes</li> <li>Suporte a execu\u00e7\u00e3o local e remota</li> </ul>"},{"location":"nala/ai/ai-roadmap/","title":"\ud83e\udd16 AI Roadmap \u2014 athomic-docs","text":"<p>Este documento define o plano de arquitetura e aplica\u00e7\u00e3o de Intelig\u00eancia Artificial no projeto <code>athomic-docs</code>.</p>"},{"location":"nala/ai/ai-roadmap/#objetivo","title":"\ud83c\udfaf Objetivo","text":"<p>Expandir o <code>athomic-docs</code> com capacidades inteligentes que tornem a API:</p> <ul> <li>Mais segura e proativa</li> <li>Capaz de gerar insights a partir de dados de uso</li> <li>Assistiva para desenvolvedores (melhor DevX)</li> <li>Inteligente na tomada de decis\u00e3o de fallback, cache, etc.</li> <li>Transparente e audit\u00e1vel no uso de IA</li> </ul>"},{"location":"nala/ai/ai-roadmap/#casos-de-uso-de-ia","title":"\ud83e\udde0 Casos de Uso de IA","text":""},{"location":"nala/ai/ai-roadmap/#seguranca-inteligente","title":"\ud83d\udd10 Seguran\u00e7a Inteligente","text":"<ul> <li>Classifica\u00e7\u00e3o de risco de autentica\u00e7\u00f5es com base em contexto</li> <li>Detec\u00e7\u00e3o de anomalias no uso de tokens ou escopos</li> <li>Recomenda\u00e7\u00f5es de escopos m\u00ednimos necess\u00e1rios</li> </ul>"},{"location":"nala/ai/ai-roadmap/#observabilidade-inteligente","title":"\ud83d\udcca Observabilidade Inteligente","text":"<ul> <li>Agrupamento de falhas por similaridade sem\u00e2ntica</li> <li>Resumos autom\u00e1ticos de logs e traces</li> <li>An\u00e1lise de picos e regress\u00f5es com explica\u00e7\u00f5es em linguagem natural</li> </ul>"},{"location":"nala/ai/ai-roadmap/#modularidade-adaptativa","title":"\ud83e\udde9 Modularidade Adaptativa","text":"<ul> <li>Recomenda\u00e7\u00f5es de fallback chains otimizadas</li> <li>Sugest\u00f5es de TTLs de cache baseadas em padr\u00f5es reais</li> <li>Auto-configura\u00e7\u00e3o de rate limits sugeridos</li> </ul>"},{"location":"nala/ai/ai-roadmap/#devx-assistida","title":"\ud83e\uddfe DevX Assistida","text":"<ul> <li>Gera\u00e7\u00e3o de exemplos autom\u00e1ticos de uso</li> <li>Respostas inteligentes para erros da API (ex: 401, 500)</li> <li>Explica\u00e7\u00f5es claras e a\u00e7\u00f5es recomendadas para debugging</li> </ul>"},{"location":"nala/ai/ai-roadmap/#governanca-e-auditoria","title":"\ud83e\udde0 Governan\u00e7a e Auditoria","text":"<ul> <li>Resumos audit\u00e1veis dos acessos a segredos</li> <li>Rastreabilidade das decis\u00f5es automatizadas via fallback</li> </ul>"},{"location":"nala/ai/ai-roadmap/#arquitetura-sugerida","title":"\ud83e\uddf1 Arquitetura Sugerida","text":"<pre><code>athomic/\n  ai/\n    insights_engine.py        # Coleta e gera insights a partir de dados da API\n    anomaly_detector.py       # Detec\u00e7\u00e3o de padr\u00f5es fora do comum (usu\u00e1rios, tokens, etc.)\n    observability_agent.py    # An\u00e1lise de logs e traces\n    suggestion_engine.py      # Sugest\u00f5es inteligentes (cache, fallback, TTL, etc.)\n    dx_assistant.py           # Auxilia com exemplos, docs e debugging\n    models/                   # Modelos finos ou l\u00f3gica de integra\u00e7\u00e3o com LLMs\n    registries.py             # Registro din\u00e2mico de estrat\u00e9gias de IA\n    agent_registry.py         # Plug de agentes com habilidades espec\u00edficas\n</code></pre>"},{"location":"nala/ai/ai-roadmap/#integracao-com-modulos-existentes","title":"\ud83e\udde9 Integra\u00e7\u00e3o com M\u00f3dulos Existentes","text":"<ul> <li>Event Hooks: capturar eventos relevantes (ex: tempo de resposta, falhas, chamadas suspeitas)</li> <li>Registry + Settings: ativar/desativar componentes de IA conforme ambiente</li> <li>OpenTelemetry + Logs: para rastreamento de decis\u00f5es tomadas por IA</li> <li>Fallback seguro: IA como recomenda\u00e7\u00e3o, nunca como bloqueador</li> </ul>"},{"location":"nala/ai/ai-roadmap/#primeiros-modulos-prioritarios","title":"\ud83d\udd1c Primeiros M\u00f3dulos Priorit\u00e1rios","text":"<ol> <li><code>insights_engine.py</code> \u2014 Coleta e sumariza\u00e7\u00e3o de dados de uso</li> <li><code>anomaly_detector.py</code> \u2014 Alertas sobre comportamentos suspeitos</li> <li><code>suggestion_engine.py</code> \u2014 Recomenda\u00e7\u00e3o de fallback e TTL baseado em padr\u00f5es</li> </ol>"},{"location":"nala/ai/ai-roadmap/#principios-de-implementacao","title":"\ud83d\udccc Princ\u00edpios de Implementa\u00e7\u00e3o","text":"<ul> <li>Transpar\u00eancia e rastreabilidade</li> <li>Modularidade e desacoplamento</li> <li>Sem depend\u00eancia de IA para funcionamento b\u00e1sico</li> <li>Possibilidade de rodar local ou com servi\u00e7os externos (ex: LLM via API)</li> </ul>"},{"location":"nala/ai/ai-roadmap/#proximos-passos","title":"\u2705 Pr\u00f3ximos Passos","text":"<ul> <li>[ ] Implementar <code>athomic/ai/insights_engine.py</code></li> <li>[ ] Criar event hooks nos pontos cr\u00edticos da API</li> <li>[ ] Adicionar exemplos de insights gerados</li> <li>[ ] Definir estrutura m\u00ednima para <code>models/</code> (modelos de sugest\u00e3o, risco, etc)</li> </ul>"},{"location":"nala/ai/ai-roadmap/#dev-permitir-automacoes-desse-tipo-de-fluxo-para-mais-tarefas-ou-processos-customizados","title":"Dev &lt;------ Permitir automa\u00e7\u00f5es desse tipo  de fluxo para mais tarefas ou processos customizados","text":"<ul> <li>[ ] Permitir baixar pacotes com o poetry e verificar a forma de utiliza\u00e7\u00e3o das libs.</li> <li>[ ] Intera\u00e7\u00e3o com poetry</li> <li>[ ] Avaliar logs e propor melhorias</li> <li>[ ] Implementar melhorias e executar test</li> <li>[ ] Verificar se o problema foi resolvido</li> <li>[ ] Executar checklist de arquitetura e boas praticas.</li> <li>[ ] Executar testes e checks de qualidade</li> <li>[ ] Abrir PR para a branch validation</li> <li>[ ] Cadastra monitoring/acompanhamento da solu\u00e7\u00e3o para quando for para prod</li> <li>[ ] Apos alguns dias verificar se o problema foi resolvido</li> <li>[ ] Marcar tarefa como concluida e remover monitors </li> </ul>"},{"location":"nala/ai/ai-roadmap/#arquitetura-dos-agents","title":"Arquitetura dos agents","text":"<ul> <li>[ ] Skills</li> <li>[ ] Tools</li> <li>[ ] Behaivors </li> <li>[ ] Objectve </li> </ul>"},{"location":"nala/ai/genai-knowledge-map/","title":"\ud83e\udde0 GenAI Knowledge Map \u2014 athomic-docs","text":"<p>Este documento organiza os conceitos essenciais de Intelig\u00eancia Artificial Generativa (GenAI), suas classifica\u00e7\u00f5es, tipos de modelo, estrat\u00e9gias de uso e ferramentas pr\u00e1ticas \u2014 com foco na aplica\u00e7\u00e3o dentro de plataformas modernas como a <code>athomic-docs</code>.</p>"},{"location":"nala/ai/genai-knowledge-map/#o-que-e-genai","title":"\ud83c\udfaf O que \u00e9 GenAI?","text":"<p>GenAI (Intelig\u00eancia Artificial Generativa) \u00e9 a sub\u00e1rea da IA focada em criar novos conte\u00fados (texto, imagem, som, c\u00f3digo, etc.) em vez de apenas classificar ou prever. Ela gera novos dados coerentes, com base em aprendizado de grandes volumes de dados.</p>"},{"location":"nala/ai/genai-knowledge-map/#1-classificacao-por-tipo-de-aprendizado","title":"\ud83d\udcca 1. Classifica\u00e7\u00e3o por Tipo de Aprendizado","text":"Tipo de Treinamento Descri\u00e7\u00e3o Exemplos Supervised Fine-Tuning Usa dados rotulados para ensinar uma tarefa espec\u00edfica Fine-tune do GPT em dados de FAQ Unsupervised Pretraining Aprende padr\u00f5es apenas com dados brutos Treinamento do GPT com internet Reinforcement Learning Modelo aprende por tentativa e erro com base em recompensa RLHF (Reinforcement Learning with Human Feedback) Few-shot / Zero-shot Usa exemplos m\u00ednimos (ou nenhum) na hora da infer\u00eancia Prompt: \u201cTraduza para franc\u00eas\u201d Instruction Tuning Ajustado para seguir comandos textuais de forma geral ChatGPT, Claude, Gemini, Mistral"},{"location":"nala/ai/genai-knowledge-map/#2-classificacao-por-tipo-de-modelo-gerativo","title":"\ud83e\udde0 2. Classifica\u00e7\u00e3o por Tipo de Modelo Gerativo","text":"Categoria O que gera Exemplos LLMs (Language) Texto, c\u00f3digo, respostas GPT, Claude, Mistral, LLaMA Diffusion Models Imagens, v\u00eddeos, \u00e1udio Stable Diffusion, Midjourney Multimodal Compreende e gera texto + imagem GPT-4V, Gemini 1.5, Claude 3 Opus Audio Models Gera\u00e7\u00e3o de voz e som MusicLM, ElevenLabs Code Models C\u00f3digo execut\u00e1vel, testes Codex, Code Llama, DeepSeek Coder"},{"location":"nala/ai/genai-knowledge-map/#3-classificacao-por-aplicacao-use-cases","title":"\ud83e\udde9 3. Classifica\u00e7\u00e3o por Aplica\u00e7\u00e3o (Use Cases)","text":"\u00c1rea de Aplica\u00e7\u00e3o Exemplos Texto &amp; Escrita Copywriting, resumo, tradu\u00e7\u00e3o, storytelling Pesquisa &amp; RAG Busca + Gera\u00e7\u00e3o com fontes (Retrieval-Augmented Generation) Assist\u00eancia de C\u00f3digo Autocompletar, explicar, sugerir refatora\u00e7\u00e3o An\u00e1lise de Sentimentos Classifica\u00e7\u00e3o de texto com nuances emocionais Suporte a Cliente Chatbots, FAQ autom\u00e1ticos, gera\u00e7\u00e3o de respostas personalizadas Educa\u00e7\u00e3o &amp; Tutoria Explica\u00e7\u00e3o de conceitos, gera\u00e7\u00e3o de quizzes, tutoria personalizada Design &amp; Arte Gera\u00e7\u00e3o de imagem, logo, cenas, filtros Dados &amp; BI Perguntas em linguagem natural sobre dados (ex: DataCopilot) Agentes Inteligentes Fluxos aut\u00f4nomos que executam planos e metas"},{"location":"nala/ai/genai-knowledge-map/#4-estrategias-comuns-de-uso","title":"\ud83e\udde0 4. Estrat\u00e9gias Comuns de Uso","text":"Estrat\u00e9gia Exemplo pr\u00e1tico Prompt Engineering Engenharia de instru\u00e7\u00f5es com exemplos e formata\u00e7\u00f5es RAG (Retrieval-Augmented) Combina base de dados + LLM (ex: chatbot sobre sua base de conhecimento) Fine-tuning / LoRA Ajusta o modelo com dados pr\u00f3prios sem treinar tudo do zero Tools &amp; Plugins LLMs chamam ferramentas externas com base no input (ex: fun\u00e7\u00e3o Python, DB) Agentic Workflows LLMs com mem\u00f3ria, ferramentas, metas e coordena\u00e7\u00e3o"},{"location":"nala/ai/genai-knowledge-map/#ferramentas-e-frameworks-populares","title":"\ud83d\udee0\ufe0f Ferramentas e Frameworks Populares","text":"Categoria Ferramentas/Frameworks Prompting LangChain, Guidance, PromptLayer RAG LlamaIndex, Weaviate, Qdrant, Pinecone Fine-tuning LoRA, PEFT, HuggingFace Trainer LLM Orchestration LangGraph, AutoGen, CrewAI Hosting &amp; APIs OpenAI, Vertex AI, Azure OpenAI, Together AI Observabilidade PromptLayer, TruLens, LangFuse"},{"location":"nala/ai/genai-knowledge-map/#resumo-visual","title":"\u2705 Resumo Visual","text":"<pre><code>                        GenAI\n                        \u2502\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502                           \u2502\n            Tipo de Treino         Tipo de Modelo\n                \u2502                           \u2502\n            Supervised             LLM / Diffusion / Code / Multi\n            Unsupervised                    \u2502\n            RLHF / Few-shot                 \u25bc\n                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Aplica\u00e7\u00f5es \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                        \u2502 Texto \u2502 C\u00f3digo \u2502 BI \u2502 Imagem \u2502 Agente \u2502\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"nala/ai/genai-knowledge-map/#aplicacao-pratica-na-athomic-docs","title":"\ud83e\uddfe Aplica\u00e7\u00e3o pr\u00e1tica na <code>athomic-docs</code>","text":"<ul> <li>GenAI pode ser usado em agentes (<code>rag_agent</code>, <code>summarizer_agent</code>)</li> <li>Suporte a estrat\u00e9gias como RAG, Fine-Tune, Prompt Templates</li> <li>Integra\u00e7\u00e3o modular com ferramentas (retrievers, LLMs, caches, etc)</li> <li>Observabilidade e fallback aplic\u00e1veis a pipelines generativos</li> </ul>"},{"location":"nala/ai/genai-knowledge-map/#notas-pessoais","title":"\ud83d\udcdd Notas Pessoais","text":"<ul> <li>[ ] Incluir links para papers ou posts \u00fateis</li> <li>[ ] Adicionar anota\u00e7\u00f5es sobre quais j\u00e1 testei</li> <li>[ ] Marcar ferramentas que quero experimentar</li> </ul>"},{"location":"quality/benchmarks/","title":"\ud83d\udccc Frameworks, Ferramentas e Metodologias relacionadas ao Nala-IQ","text":"<p>Consolidado de tudo que j\u00e1 mapeamos: o que o Nala-IQ cobre, cobre parcialmente ou ainda n\u00e3o cobre.</p>"},{"location":"quality/benchmarks/#ja-cobertos-total","title":"\u2705 J\u00e1 cobertos (Total)","text":"<ul> <li>ISO/IEC 25010 (Qualidade de Software)</li> <li>Descri\u00e7\u00e3o: Modelo de qualidade (funcionalidade, confiabilidade, usabilidade, efici\u00eancia, manuten\u00e7\u00e3o, portabilidade).</li> <li>Dom\u00ednios Nala-IQ: Design &amp; Documenta\u00e7\u00e3o, Robustez, Performance.</li> <li>Cobertura: Total.</li> <li>Impacto: Muito alto (refer\u00eancia global).</li> <li> <p>Observa\u00e7\u00e3o: Base conceitual forte, j\u00e1 bem refletida nos crit\u00e9rios.</p> </li> <li> <p>OWASP API Security Top 10</p> </li> <li>Descri\u00e7\u00e3o: Top vulnerabilidades de seguran\u00e7a em APIs.</li> <li>Dom\u00ednios Nala-IQ: Seguran\u00e7a, Resili\u00eancia.</li> <li>Cobertura: Total.</li> <li>Impacto: Muito alto (refer\u00eancia obrigat\u00f3ria).</li> <li> <p>Observa\u00e7\u00e3o: Excelente para vincular crit\u00e9rios de seguran\u00e7a diretamente.</p> </li> <li> <p>DORA / DevOps Metrics</p> </li> <li>Descri\u00e7\u00e3o: M\u00e9tricas de entrega cont\u00ednua (lead time, MTTR, change failure rate, etc.).</li> <li>Dom\u00ednios Nala-IQ: DevOps, Observabilidade, Resili\u00eancia.</li> <li>Cobertura: Total.</li> <li>Impacto: Alto.</li> <li> <p>Observa\u00e7\u00e3o: J\u00e1 integrado nas dimens\u00f5es de DevOps/Qualidade.</p> </li> <li> <p>Microservices Patterns (Circuit Breaker, Saga, Outbox, etc.)</p> </li> <li>Descri\u00e7\u00e3o: Padr\u00f5es de resili\u00eancia e consist\u00eancia para sistemas distribu\u00eddos.</li> <li>Dom\u00ednios Nala-IQ: Robustez, Resili\u00eancia, Persist\u00eancia.</li> <li>Cobertura: Total.</li> <li>Impacto: Alto, especialmente em arquiteturas modernas.</li> <li> <p>Observa\u00e7\u00e3o: Reflete maturidade pr\u00e1tica de engenharia.</p> </li> <li> <p>API Governance Frameworks (Apigee, MuleSoft, WSO2)</p> </li> <li>Descri\u00e7\u00e3o: Checklists e pr\u00e1ticas de governan\u00e7a de APIs em plataformas de mercado.</li> <li>Dom\u00ednios Nala-IQ: Governan\u00e7a, Ciclo de Vida, DX.</li> <li>Cobertura: Total.</li> <li>Impacto: Alto.</li> <li> <p>Observa\u00e7\u00e3o: O Nala-IQ generaliza e vai al\u00e9m do vendor lock-in.</p> </li> <li> <p>Green Software Engineering Principles</p> </li> <li>Descri\u00e7\u00e3o: Princ\u00edpios de efici\u00eancia energ\u00e9tica e sustentabilidade em software.</li> <li>Dom\u00ednios Nala-IQ: Sustentabilidade.</li> <li>Cobertura: Total.</li> <li>Impacto: Alto (tend\u00eancia de ESG e efici\u00eancia).</li> <li> <p>Observa\u00e7\u00e3o: Diferencial pouco presente em frameworks de APIs.</p> </li> <li> <p>Developer Experience Research</p> </li> <li>Descri\u00e7\u00e3o: Estudos de boas pr\u00e1ticas para onboarding, documenta\u00e7\u00e3o, SDKs e portais de devs.</li> <li>Dom\u00ednios Nala-IQ: Experi\u00eancia do Desenvolvedor (DX).</li> <li>Cobertura: Total.</li> <li>Impacto: Muito alto.</li> <li> <p>Observa\u00e7\u00e3o: Um dos diferenciais mais fortes do Nala-IQ.</p> </li> <li> <p>LGPD / GDPR / HIPAA</p> </li> <li>Descri\u00e7\u00e3o: Regulamenta\u00e7\u00f5es de prote\u00e7\u00e3o de dados pessoais e sens\u00edveis.</li> <li>Dom\u00ednios Nala-IQ: Compliance &amp; Regulamenta\u00e7\u00f5es, Seguran\u00e7a.</li> <li>Cobertura: Total.</li> <li>Impacto: Muito alto.</li> <li> <p>Observa\u00e7\u00e3o: Essencial para setores regulados.</p> </li> <li> <p>ITIL v4</p> </li> <li>Descri\u00e7\u00e3o: Governan\u00e7a e ciclo de vida de servi\u00e7os de TI.</li> <li>Dom\u00ednios Nala-IQ: Governan\u00e7a, Ciclo de Vida.</li> <li>Cobertura: Total.</li> <li>Impacto: Alto.</li> <li>Observa\u00e7\u00e3o: J\u00e1 aparece na forma de SLAs, processos e governan\u00e7a de APIs.</li> </ul>"},{"location":"quality/benchmarks/#parcialmente-cobertos","title":"\ud83d\udfe1 Parcialmente cobertos","text":"<ul> <li>FinOps Framework</li> <li>Descri\u00e7\u00e3o: Gest\u00e3o de custos, TCO, ROI em nuvem e TI.</li> <li>Dom\u00ednios Nala-IQ: Sustentabilidade, Governan\u00e7a.</li> <li>Cobertura: Parcial.</li> <li>Impacto: Muito alto (custos s\u00e3o cr\u00edticos).</li> <li>O que falta: Sub-dimens\u00e3o de FinOps expl\u00edcita com crit\u00e9rios de TCO/ROI.</li> <li> <p>Por qu\u00ea: Tornaria mais tang\u00edvel o valor financeiro das APIs.</p> </li> <li> <p>Cultura &amp; Pessoas (Team Topologies, DevOps Culture, CMMI People)</p> </li> <li>Descri\u00e7\u00e3o: Estruturas de equipe, ownership, cultura organizacional.</li> <li>Dom\u00ednios Nala-IQ: Governan\u00e7a, Qualidade de C\u00f3digo.</li> <li>Cobertura: Parcial.</li> <li>Impacto: Muito alto (fator humano \u00e9 chave).</li> <li>O que falta: Nova dimens\u00e3o XVI dedicada a Cultura &amp; Pessoas.</li> <li> <p>Por qu\u00ea: Dar visibilidade estrat\u00e9gica ao capital humano.</p> </li> <li> <p>Gest\u00e3o de Riscos (ISO 31000 / NIST RMF)</p> </li> <li>Descri\u00e7\u00e3o: Frameworks de gest\u00e3o de riscos operacionais e estrat\u00e9gicos.</li> <li>Dom\u00ednios Nala-IQ: Seguran\u00e7a, Compliance, Governan\u00e7a.</li> <li>Cobertura: Parcial.</li> <li>Impacto: Alto.</li> <li>O que falta: Crit\u00e9rios expl\u00edcitos de riscos para APIs (financeiros, operacionais, regulat\u00f3rios).</li> <li> <p>Por qu\u00ea: Ampliar robustez e governan\u00e7a.</p> </li> <li> <p>WCAG (Acessibilidade \u2013 a11y)</p> </li> <li>Descri\u00e7\u00e3o: Padr\u00f5es de acessibilidade digital.</li> <li>Dom\u00ednios Nala-IQ: Experi\u00eancia do Desenvolvedor (DX).</li> <li>Cobertura: Parcial.</li> <li>Impacto: M\u00e9dio-alto (importante em contexto global).</li> <li>O que falta: Sub-dimens\u00e3o de acessibilidade em DX.</li> <li> <p>Por qu\u00ea: Inclus\u00e3o e conformidade internacional.</p> </li> <li> <p>ISO/IEC 25012 (Data Quality Model)</p> </li> <li>Descri\u00e7\u00e3o: Padr\u00e3o de qualidade de dados (integridade, acur\u00e1cia, consist\u00eancia).</li> <li>Dom\u00ednios Nala-IQ: Persist\u00eancia, Governan\u00e7a.</li> <li>Cobertura: Parcial.</li> <li>Impacto: Alto.</li> <li> <p>O que falta: Crit\u00e9rios mais claros sobre integridade e acur\u00e1cia de dados de APIs.</p> </li> <li> <p>SRE Practices (SLOs, Error Budgets, Reliability)</p> </li> <li>Descri\u00e7\u00e3o: Pr\u00e1ticas do Google SRE para confiabilidade de servi\u00e7os.</li> <li>Dom\u00ednios Nala-IQ: Observabilidade, Resili\u00eancia.</li> <li>Cobertura: Parcial.</li> <li>Impacto: Muito alto.</li> <li>O que falta: Crit\u00e9rios para gest\u00e3o ativa de SLOs/Error Budgets.</li> <li>Por qu\u00ea: Conectar SLAs com confiabilidade pr\u00e1tica.</li> </ul>"},{"location":"quality/benchmarks/#ainda-nao-cobertos-horizontes-futuros","title":"\u26aa Ainda n\u00e3o cobertos (Horizontes futuros)","text":""},{"location":"quality/benchmarks/#seguranca-privacidade","title":"\ud83d\udd10 Seguran\u00e7a &amp; Privacidade","text":"<ul> <li>Zero Trust Architecture (NIST 800-207)</li> <li>ISO/IEC 27001 / 27701 (seguran\u00e7a e privacidade organizacional)</li> <li>CSA API Security Guidance</li> <li>NIST AI Risk Management Framework (AI RMF)</li> </ul>"},{"location":"quality/benchmarks/#dados-interoperabilidade","title":"\ud83d\udcca Dados &amp; Interoperabilidade","text":"<ul> <li>Open Data Maturity Models (ODMM)</li> <li>FAIR Principles (Findable, Accessible, Interoperable, Reusable)</li> <li>ISO/IEC 11179 (Metadata Registries)</li> </ul>"},{"location":"quality/benchmarks/#arquitetura-engenharia","title":"\ud83d\udcbb Arquitetura &amp; Engenharia","text":"<ul> <li>TOGAF (Enterprise Architecture Framework)</li> <li>C4 Model / Arc42 (documenta\u00e7\u00e3o arquitetural)</li> <li>Twelve-Factor App (princ\u00edpios para apps cloud-native)</li> </ul>"},{"location":"quality/benchmarks/#sustentabilidade","title":"\ud83c\udf0d Sustentabilidade","text":"<ul> <li>GHG Protocol / ISO 14064 (contabiliza\u00e7\u00e3o de carbono digital)</li> <li>SBTi (Science-Based Targets initiative)</li> </ul>"},{"location":"quality/benchmarks/#cultura-gestao","title":"\ud83d\udc65 Cultura &amp; Gest\u00e3o","text":"<ul> <li>SAFe (Scaled Agile Framework) </li> <li>OKRs (Objectives &amp; Key Results)</li> </ul>"},{"location":"quality/benchmarks/#negocio","title":"\ud83d\udcb5 Neg\u00f3cio","text":"<ul> <li>Balanced Scorecard (BSC) </li> <li>Porter\u2019s Value Chain </li> <li>Platform Design Toolkit</li> </ul>"},{"location":"quality/benchmarks/#qualidade-testes","title":"\ud83e\uddea Qualidade &amp; Testes","text":"<ul> <li>ISO/IEC/IEEE 29119 (padr\u00e3o internacional de testes de software) </li> <li>Mutation Testing Frameworks (ex.: PIT, Stryker)</li> </ul>"},{"location":"quality/benchmarks/#insights-adicionais","title":"\ud83c\udfaf Insights adicionais","text":"<ul> <li>O Nala-IQ j\u00e1 \u00e9 l\u00edder em integra\u00e7\u00e3o t\u00e9cnica e regulat\u00f3ria \u2192 diferencia\u00e7\u00e3o imediata.  </li> <li>V2 deve priorizar: FinOps, Cultura &amp; Pessoas, Acessibilidade (alto impacto, esfor\u00e7o moderado).  </li> <li>V3 pode incluir Riscos, Data Quality e SRE.  </li> <li>V4 em diante pode expandir para Zero Trust, ESG avan\u00e7ado, Monetiza\u00e7\u00e3o e modelos de neg\u00f3cio.  </li> </ul>"},{"location":"quality/execution-plan/","title":"\ud83d\udccc Plano de Execu\u00e7\u00e3o \u2013 Site de Documenta\u00e7\u00e3o &amp; Refer\u00eancias Nala-IQ","text":"<p>Objetivo: criar um portal estilo refactoring.guru, com conte\u00fado aberto (educativo) e premium (consultoria/servi\u00e7os).</p>"},{"location":"quality/execution-plan/#objetivos-principais","title":"\ud83c\udfaf Objetivos Principais","text":"<ul> <li>Tornar o Nala-IQ refer\u00eancia global em maturidade de APIs.  </li> <li>Atrair comunidade e autoridade atrav\u00e9s de conte\u00fado aberto e educativo.  </li> <li>Monetizar via conte\u00fado premium, consultoria, treinamentos e SaaS.  </li> </ul>"},{"location":"quality/execution-plan/#estrutura-do-portal","title":"\ud83d\uddc2\ufe0f Estrutura do Portal","text":""},{"location":"quality/execution-plan/#secoes-abertas-gratuitas","title":"Se\u00e7\u00f5es abertas (gratuitas)","text":"<ol> <li>Home \u2192 vis\u00e3o geral, proposta do Nala-IQ.  </li> <li>Dimens\u00f5es &amp; Crit\u00e9rios \u2192 documenta\u00e7\u00e3o organizada, cada crit\u00e9rio explicado (benef\u00edcios, riscos, exemplos, refer\u00eancias).  </li> <li>Blog / Insights \u2192 artigos sobre APIs, boas pr\u00e1ticas, tend\u00eancias.  </li> <li>Exemplos pr\u00e1ticos \u2192 snippets de configs, diagramas, casos resumidos.  </li> <li>Gloss\u00e1rio \u2192 termos essenciais de APIs e maturidade.  </li> </ol>"},{"location":"quality/execution-plan/#secoes-premium-restritas","title":"Se\u00e7\u00f5es premium (restritas)","text":"<ol> <li>Playbooks \u2192 guias passo a passo para aplicar cada dimens\u00e3o.  </li> <li>Scorecards &amp; Question\u00e1rios \u2192 planilhas, formul\u00e1rios ou SaaS para autoavalia\u00e7\u00e3o.  </li> <li>Benchmarks \u2192 compara\u00e7\u00e3o com setor/mercado.  </li> <li>Casos completos \u2192 estudos de caso detalhados (ex.: API de Pagamentos).  </li> <li>Certifica\u00e7\u00e3o Nala-IQ \u2192 programa de certifica\u00e7\u00e3o de times/empresas.  </li> </ol>"},{"location":"quality/execution-plan/#fases-de-execucao","title":"\ud83d\ude80 Fases de Execu\u00e7\u00e3o","text":""},{"location":"quality/execution-plan/#fase-1-preparacao-0-1-mes","title":"Fase 1 \u2013 Prepara\u00e7\u00e3o (0-1 m\u00eas)","text":"<ul> <li>Definir escopo inicial (conte\u00fado aberto v1 = 14 dimens\u00f5es + crit\u00e9rios principais).  </li> <li>Escolher stack para site (ex.: Docusaurus, VuePress, Hugo, MkDocs).  </li> <li>Criar identidade visual m\u00ednima (logo, cores, tipografia).  </li> <li>Estruturar reposit\u00f3rio (GitHub/GitLab).  </li> </ul> <p>Entreg\u00e1veis: - Prot\u00f3tipo de navega\u00e7\u00e3o do site. - Design inicial + branding b\u00e1sico. - Primeiros 3-5 crit\u00e9rios documentados.  </p>"},{"location":"quality/execution-plan/#fase-2-lancamento-beta-2-3-meses","title":"Fase 2 \u2013 Lan\u00e7amento Beta (2-3 meses)","text":"<ul> <li>Publicar site com conte\u00fado aberto inicial (dimens\u00f5es + exemplos).  </li> <li>Adicionar blog com 3 artigos estrat\u00e9gicos.  </li> <li>Criar p\u00e1gina de call-to-action para consultoria/treinamentos.  </li> <li>Coletar feedback de comunidade inicial.  </li> </ul> <p>Entreg\u00e1veis: - Site p\u00fablico online. - Documenta\u00e7\u00e3o inicial (m\u00ednimo 7 dimens\u00f5es). - 3 artigos no blog.  </p>"},{"location":"quality/execution-plan/#fase-3-conteudo-premium-3-6-meses","title":"Fase 3 \u2013 Conte\u00fado Premium (3-6 meses)","text":"<ul> <li>Adicionar playbooks premium (apenas para clientes/assinantes).  </li> <li>Criar scorecard interativo (planilha ou mini-app).  </li> <li>Montar 1-2 casos de uso completos (API de cat\u00e1logo, API de pagamentos).  </li> <li>Lan\u00e7ar programa piloto de certifica\u00e7\u00e3o.  </li> </ul> <p>Entreg\u00e1veis: - \u00c1rea premium ativa. - Primeiro scorecard. - Primeiros clientes premium (consultoria ou certifica\u00e7\u00e3o).  </p>"},{"location":"quality/execution-plan/#fase-4-escala-e-produto-6-12-meses","title":"Fase 4 \u2013 Escala e Produto (6-12 meses)","text":"<ul> <li>Evoluir scorecard \u2192 SaaS (avalia\u00e7\u00e3o cont\u00ednua de maturidade).  </li> <li>Ampliar conte\u00fado premium (benchmarks setoriais, estudos avan\u00e7ados).  </li> <li>Expandir comunidade (newsletter, eventos, webinars).  </li> <li>Consolidar certifica\u00e7\u00e3o Nala-IQ.  </li> </ul> <p>Entreg\u00e1veis: - MVP SaaS (monitoramento cont\u00ednuo). - 10+ estudos de caso. - Comunidade ativa em torno do framework.  </p>"},{"location":"quality/execution-plan/#indicadores-de-sucesso","title":"\ud83d\udcca Indicadores de Sucesso","text":"<ul> <li>Curto prazo (3 meses): site no ar, primeiras visitas org\u00e2nicas, leads de consultoria.  </li> <li>M\u00e9dio prazo (6 meses): primeiros clientes premium, scorecard em uso.  </li> <li>Longo prazo (12 meses): Nala-IQ reconhecido como refer\u00eancia \u2192 in\u00edcio de certifica\u00e7\u00f5es.  </li> </ul>"},{"location":"quality/execution-plan/#requisitos-e-recursos","title":"\ud83d\udee0\ufe0f Requisitos e Recursos","text":"<ul> <li>Stack: Docusaurus (React) ou MkDocs (Python) para docs + CMS leve para blog.  </li> <li>Design: identidade visual clara (padr\u00e3o refactoring.guru / OWASP).  </li> <li>Conte\u00fado: equipe editorial m\u00ednima (1-2 pessoas para redigir e revisar).  </li> <li>Comercial: modelo premium (consultoria, certifica\u00e7\u00e3o, SaaS).  </li> </ul>"},{"location":"quality/execution-plan/#resumo","title":"\u2705 Resumo","text":"<ul> <li>Aberto: educa, gera tr\u00e1fego, autoridade.  </li> <li>Premium: monetiza via consultoria, certifica\u00e7\u00e3o e SaaS.  </li> <li>Evolu\u00e7\u00e3o: docs \u2192 scorecards \u2192 SaaS \u2192 certifica\u00e7\u00e3o global.  </li> </ul>"},{"location":"quality/nala-code-quality-checklist/","title":"\u2705 Checklist de Qualidade Moderna para Projetos Python","text":"<p>Este documento descreve as melhores pr\u00e1ticas e ferramentas modernas para garantir a qualidade, seguran\u00e7a, escalabilidade e manutenibilidade de projetos Python. Ele serve como um guia pr\u00e1tico para equipes t\u00e9cnicas e pode ser usado como base para reviews, onboarding e defini\u00e7\u00e3o de crit\u00e9rios de aceita\u00e7\u00e3o de c\u00f3digo.</p>"},{"location":"quality/nala-code-quality-checklist/#estrutura-recomendada-de-projeto","title":"\ud83d\udce6 Estrutura recomendada de projeto","text":"<p>Organize o projeto com separa\u00e7\u00e3o clara entre o c\u00f3digo-fonte, testes, configura\u00e7\u00f5es e documenta\u00e7\u00e3o:</p> <pre><code>.\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 seu_pacote/\n\u251c\u2500\u2500 tests/\n\u2502   \u2514\u2500\u2500 test_alguma_coisa.py\n\u251c\u2500\u2500 docs/\n\u251c\u2500\u2500 .github/workflows/\n\u2502   \u2514\u2500\u2500 lint.yml\n</code></pre>"},{"location":"quality/nala-code-quality-checklist/#1-linter-ruff","title":"\ud83e\uddfc 1. Linter: Ruff","text":"<p>Objetivo: Garantir boas pr\u00e1ticas de codifica\u00e7\u00e3o, removendo c\u00f3digo morto, imports n\u00e3o usados e problemas comuns de estilo.</p> <p>Import\u00e2ncia: R\u00e1pido e com grande cobertura, substitui ferramentas como Flake8, isort e pycodestyle com um s\u00f3 bin\u00e1rio.</p> <p>Comando:</p> <pre><code>poetry run ruff check .\npoetry run ruff check . --fix  # corrige automaticamente\n</code></pre>"},{"location":"quality/nala-code-quality-checklist/#2-formatter-black","title":"\ud83c\udfa8 2. Formatter: Black","text":"<p>Objetivo: Aplicar um estilo de formata\u00e7\u00e3o autom\u00e1tico e consistente.</p> <p>Import\u00e2ncia: Elimina discuss\u00f5es sobre estilo de c\u00f3digo, melhora a legibilidade e facilita reviews.</p> <p>Comando:</p> <pre><code>poetry run black .\n</code></pre>"},{"location":"quality/nala-code-quality-checklist/#3-type-checking-mypy","title":"\ud83e\udde0 3. Type Checking: mypy","text":"<p>Objetivo: Verificar erros de tipo est\u00e1ticos em tempo de desenvolvimento.</p> <p>Import\u00e2ncia: Reduz erros em tempo de execu\u00e7\u00e3o e facilita manuten\u00e7\u00e3o em projetos grandes.</p> <p>Comando:</p> <pre><code>poetry run mypy src/\n</code></pre> <p>Exemplo de configura\u00e7\u00e3o no <code>pyproject.toml</code>:</p> <pre><code>[tool.mypy]\npython_version = \"3.11\"\nstrict = true\ndisallow_untyped_defs = true\nignore_missing_imports = true\n</code></pre>"},{"location":"quality/nala-code-quality-checklist/#4-testes-pytest","title":"\ud83e\uddea 4. Testes: pytest","text":"<p>Objetivo: Criar e rodar testes automatizados para garantir que o c\u00f3digo funcione como esperado.</p> <p>Import\u00e2ncia: Evita regress\u00f5es, documenta comportamento esperado e aumenta a confian\u00e7a em mudan\u00e7as.</p> <p>Comando:</p> <pre><code>poetry run pytest\n</code></pre>"},{"location":"quality/nala-code-quality-checklist/#5-ci-github-actions","title":"\ud83e\udd16 5. CI: GitHub Actions","text":"<p>Objetivo: Automatizar valida\u00e7\u00f5es (lint, testes, type checking) a cada commit ou pull request.</p> <p>Import\u00e2ncia: Garante qualidade cont\u00ednua e impede que c\u00f3digo quebrado chegue na main.</p> <p>Exemplo b\u00e1sico em <code>.github/workflows/lint.yml</code>:</p> <pre><code>name: Lint &amp; Test\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\njobs:\n  quality:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - uses: actions/setup-python@v4\n        with:\n          python-version: '3.11'\n      - uses: abatilo/actions-poetry@v2\n        with:\n          poetry-version: '1.7.1'\n\n      - name: Install dependencies\n        run: poetry install --no-root\n\n      - name: Ruff\n        run: poetry run ruff check .\n\n      - name: Black\n        run: poetry run black . --check\n\n      - name: Mypy\n        run: poetry run mypy src/\n\n      - name: Pytest\n        run: poetry run pytest\n</code></pre>"},{"location":"quality/nala-code-quality-checklist/#6-seguranca-bandit-safety","title":"\ud83d\udd12 6. Seguran\u00e7a: Bandit / Safety","text":"<p>Objetivo: Detectar falhas de seguran\u00e7a em depend\u00eancias (<code>safety</code>) e no c\u00f3digo-fonte (<code>bandit</code>).</p> <p>Import\u00e2ncia: Evita vulnerabilidades comuns em produ\u00e7\u00e3o.</p> <p>Comando:</p> <pre><code>poetry run bandit -r src/\npoetry run safety check\n</code></pre>"},{"location":"quality/nala-code-quality-checklist/#7-imports-e-codigo-morto-ruff","title":"\ud83d\udeae 7. Imports e c\u00f3digo morto: <code>ruff</code>","text":"<p>Objetivo: Identificar imports n\u00e3o usados, c\u00f3digo sem uso ou vari\u00e1veis n\u00e3o utilizadas.</p> <p>Import\u00e2ncia: Mant\u00e9m o c\u00f3digo limpo, reduz d\u00edvidas t\u00e9cnicas.</p> <p>Dica: use <code>--select F401,F841</code> com o <code>ruff</code>.</p>"},{"location":"quality/nala-code-quality-checklist/#8-documentacao-pdoc-ou-mkdocs","title":"\ud83e\uddf1 8. Documenta\u00e7\u00e3o: pdoc ou MkDocs","text":"<p>Objetivo: Documentar a API do projeto e os conceitos principais de forma acess\u00edvel.</p> <p>Import\u00e2ncia: Facilita o uso do c\u00f3digo por outras pessoas, novos membros e contribui\u00e7\u00f5es open-source.</p> <p>Comando (pdoc):</p> <pre><code>poetry add --group dev pdoc\npoetry run pdoc src/seu_pacote --html --output-dir docs\n</code></pre>"},{"location":"quality/nala-code-quality-checklist/#9-dependabot-github","title":"\ud83d\udee1\ufe0f 9. Dependabot (GitHub)","text":"<p>Objetivo: Automatizar verifica\u00e7\u00e3o de atualiza\u00e7\u00f5es e vulnerabilidades em depend\u00eancias.</p> <p>Import\u00e2ncia: Mant\u00e9m o projeto atualizado e seguro.</p> <p>Exemplo de <code>.github/dependabot.yml</code>:</p> <pre><code>version: 2\nupdates:\n  - package-ecosystem: \"pip\"\n    directory: \"/\"\n    schedule:\n      interval: \"weekly\"\n</code></pre>"},{"location":"quality/nala-code-quality-checklist/#conclusao","title":"\u2705 Conclus\u00e3o","text":"<p>Esse checklist garante que o seu projeto:</p> <ul> <li>Seja limpo, padronizado e f\u00e1cil de manter</li> <li>Tenha seguran\u00e7a e confian\u00e7a para escalar</li> <li>Possa ser auditado, testado e evolu\u00eddo com rapidez</li> </ul> <p>Recomendado como padr\u00e3o m\u00ednimo de qualidade para projetos Python modernos \ud83d\udc0d</p> <p>\u00daltima atualiza\u00e7\u00e3o: 2025/04/25</p>"},{"location":"quality/nala-iq--template-prototype-examples/","title":"\ud83d\udcca Nala-IQ Core v1 - Template XLSX Estruturado (Com Exemplos)","text":"<p>Este documento descreve a estrutura do template em <code>.xlsx</code> do Nala-IQ Core v1, incluindo exemplos fict\u00edcios em cada aba para ilustrar o uso.</p>"},{"location":"quality/nala-iq--template-prototype-examples/#aba-1-instrucoes-glossario","title":"\ud83d\udcdd Aba 1: Instru\u00e7\u00f5es &amp; Gloss\u00e1rio","text":""},{"location":"quality/nala-iq--template-prototype-examples/#bem-vindo-ao-nala-iq-core-v1","title":"\u270d\ufe0f Bem-vindo ao Nala-IQ Core v1","text":"<p>Este template ajuda a avaliar APIs usando o framework Nala-IQ Core, com 14 dimens\u00f5es e +300 crit\u00e9rios agn\u00f3sticos.</p>"},{"location":"quality/nala-iq--template-prototype-examples/#como-usar-este-template","title":"\ud83d\udd0d Como Usar Este Template","text":"<ol> <li>Preencha a aba Question\u00e1rio com a autoavalia\u00e7\u00e3o da equipe.</li> <li>Marque as evid\u00eancias coletadas nas abas Checklist Documental e Checklist Automatizado.</li> <li>A aba Scorecard calcula automaticamente as pontua\u00e7\u00f5es.</li> </ol>"},{"location":"quality/nala-iq--template-prototype-examples/#escala-de-avaliacao","title":"\ud83d\udea6 Escala de Avalia\u00e7\u00e3o","text":"<ul> <li>[0] N\u00e3o Atende (NA)</li> <li>[1] Atende Parcialmente (AP)</li> <li>[2] Atende Plenamente (AT)</li> <li>[3] Supera Expectativas (SE)</li> <li>[X] N\u00e3o Aplic\u00e1vel (N/A)</li> </ul>"},{"location":"quality/nala-iq--template-prototype-examples/#glossario","title":"\ud83e\udde9 Gloss\u00e1rio","text":"Termo Defini\u00e7\u00e3o Dimens\u00e3o Um dos 14 grandes aspectos avaliados pelo Nala-IQ Sub-dimens\u00e3o Aspectos espec\u00edficos dentro da dimens\u00e3o Crit\u00e9rio Item verific\u00e1vel Evid\u00eancia Documento, log ou dado que comprova o atendimento ao crit\u00e9rio"},{"location":"quality/nala-iq--template-prototype-examples/#aba-2-questionario-nala-iq-core-v1","title":"\ud83e\uddee Aba 2: Question\u00e1rio Nala-IQ Core v1","text":"ID Dim Nome Dimens\u00e3o ID Sub Nome Sub ID Crit Texto Crit\u00e9rio Autoav Justificativa Pontos Fortes \u00c1reas de Melhoria A\u00e7\u00f5es Planejadas Aval Auditor Coment Auditor Criticidade I Design e Doc 1.1 Ades\u00e3o Padr\u00f5es 1.1.1 API segue REST n\u00edvel 2+? 2 Usamos OpenAPI, rotas REST... Documenta\u00e7\u00e3o completa Nem todos endpoints seguem padr\u00e3o Planejar refactor 2 Confirmado M\u00e9dia II Seguran\u00e7a 2.2 Valida\u00e7\u00e3o Entrada 2.2.3 Existe prote\u00e7\u00e3o contra SQLi? 3 ORM previne inje\u00e7\u00e3o Cobertura completa - - 3 Excelente Alta"},{"location":"quality/nala-iq--template-prototype-examples/#aba-3-scorecard","title":"\ud83d\udcca Aba 3: Scorecard","text":""},{"location":"quality/nala-iq--template-prototype-examples/#pontuacao-por-sub-dimensao","title":"\ud83d\udd22 Pontua\u00e7\u00e3o por Sub-dimens\u00e3o","text":"ID Dim Nome Dim ID Sub Nome Sub M\u00e9dia Sub I Design 1.1 Ades\u00e3o Padr\u00f5es 2.0 II Seguran\u00e7a 2.2 Valida\u00e7\u00e3o Entrada 3.0"},{"location":"quality/nala-iq--template-prototype-examples/#maturidade-por-dimensao","title":"\ud83c\udfc6 Maturidade por Dimens\u00e3o","text":"ID Dim Nome Dim M\u00e9dia Dim N\u00edvel Maturidade I Design 2.0 N\u00edvel 4: Gerenciado II Seguran\u00e7a 3.0 N\u00edvel 5: Otimizado"},{"location":"quality/nala-iq--template-prototype-examples/#aba-4-checklist-evidencias-documentais","title":"\ud83d\udcc1 Aba 4: Checklist Evid\u00eancias Documentais","text":"ID Dimens\u00e3o Tipo Documento Status Link / Obs 1 I OpenAPI Spec Coletado Link API Spec 2 II Pol\u00edtica Seguran\u00e7a Pendente Ser\u00e1 enviada segunda"},{"location":"quality/nala-iq--template-prototype-examples/#aba-5-checklist-dados-automatizados","title":"\u2699\ufe0f Aba 5: Checklist Dados Automatizados","text":"ID Dimens\u00e3o Tipo Relat\u00f3rio Status Link / Obs 1 II Relat\u00f3rio SAST Coletado Ver Resultados 2 V Dashboard Lat\u00eancia N\u00e3o Aplic\u00e1vel Sem tr\u00e1fego ainda <p>\ud83c\udf89 Fim do prot\u00f3tipo com exemplos fict\u00edcios. Este modelo demonstra como os dados podem ser organizados para suportar as an\u00e1lises do Nala-IQ.</p>"},{"location":"quality/nala-iq--template-prototype-structure/","title":"Guia T\u00e9cnico: Template de Avalia\u00e7\u00e3o Nala-IQ Core v1 (.xlsx)","text":""},{"location":"quality/nala-iq--template-prototype-structure/#1-introducao","title":"1. Introdu\u00e7\u00e3o","text":""},{"location":"quality/nala-iq--template-prototype-structure/#11-proposito-deste-documento","title":"1.1. Prop\u00f3sito deste Documento","text":"<p>Este documento \u00e9 o guia t\u00e9cnico oficial para o template da planilha de avalia\u00e7\u00e3o do Nala-IQ Core v1. Ele descreve a estrutura, o conte\u00fado, as f\u00f3rmulas e as melhores pr\u00e1ticas para a utiliza\u00e7\u00e3o desta ferramenta, que \u00e9 o cora\u00e7\u00e3o operacional do processo de diagn\u00f3stico Nala-IQ.</p>"},{"location":"quality/nala-iq--template-prototype-structure/#12-objetivo-do-template","title":"1.2. Objetivo do Template","text":"<p>O template <code>.xlsx</code> foi projetado para: * Estruturar a Coleta de Dados: Unificar a coleta de informa\u00e7\u00f5es qualitativas (autoavalia\u00e7\u00e3o, justificativas) e quantitativas (pontua\u00e7\u00f5es). * Facilitar o Processo de Avalia\u00e7\u00e3o: Servir como uma ferramenta central tanto para a equipe avaliada quanto para os avaliadores. * Automatizar o C\u00e1lculo de Resultados: Calcular automaticamente as pontua\u00e7\u00f5es por sub-dimens\u00e3o e os n\u00edveis de maturidade por dimens\u00e3o. * Gerar Insights Visuais: Fornecer uma base de dados pronta para a cria\u00e7\u00e3o de scorecards e gr\u00e1ficos (como o Radar Chart). * Garantir Consist\u00eancia e Rastreabilidade: Manter um registro detalhado e consistente de cada avalia\u00e7\u00e3o realizada.</p>"},{"location":"quality/nala-iq--template-prototype-structure/#2-estrutura-geral-da-planilha","title":"2. Estrutura Geral da Planilha","text":"<p>O template \u00e9 composto por 5 abas principais, cada uma com um prop\u00f3sito espec\u00edfico:</p> <ol> <li>Instru\u00e7\u00f5es &amp; Gloss\u00e1rio: Ponto de partida para qualquer usu\u00e1rio.</li> <li>Question\u00e1rio Nala-IQ Core v1: O formul\u00e1rio principal para coleta de dados da avalia\u00e7\u00e3o.</li> <li>Scorecard: Dashboard de resultados com c\u00e1lculos automatizados.</li> <li>Checklist de Evid\u00eancias Documentais: Ferramenta de apoio para rastrear documentos.</li> <li>Checklist de Dados Automatizados: Ferramenta de apoio para rastrear relat\u00f3rios de ferramentas.</li> </ol>"},{"location":"quality/nala-iq--template-prototype-structure/#3-detalhamento-das-abas","title":"3. Detalhamento das Abas","text":""},{"location":"quality/nala-iq--template-prototype-structure/#aba-1-instrucoes-glossario","title":"Aba 1: Instru\u00e7\u00f5es &amp; Gloss\u00e1rio","text":"<ul> <li>Prop\u00f3sito: Orientar o usu\u00e1rio sobre como preencher a planilha e fornecer defini\u00e7\u00f5es para termos chave do Nala-IQ, garantindo um entendimento comum.</li> <li>Estrutura do Conte\u00fado:<ul> <li>Se\u00e7\u00e3o 1: Bem-vindo ao Nala-IQ Core v1: Uma breve introdu\u00e7\u00e3o ao framework e ao objetivo da avalia\u00e7\u00e3o.</li> <li>Se\u00e7\u00e3o 2: Como Usar o Template: Instru\u00e7\u00f5es passo a passo sobre o preenchimento da Aba <code>Question\u00e1rio Nala-IQ Core v1</code> e das abas de <code>Checklist</code>.</li> <li>Se\u00e7\u00e3o 3: Escala de Avalia\u00e7\u00e3o: A defini\u00e7\u00e3o formal da escala de pontua\u00e7\u00e3o a ser usada:<ul> <li><code>[0] N\u00e3o Atende (NA):</code> O crit\u00e9rio n\u00e3o \u00e9 atendido ou n\u00e3o h\u00e1 evid\u00eancias de sua aplica\u00e7\u00e3o.</li> <li><code>[1] Atende Parcialmente (AP):</code> O crit\u00e9rio \u00e9 parcialmente atendido, implementado de forma inconsistente, ou existem lacunas significativas.</li> <li><code>[2] Atende Plenamente (AT):</code> O crit\u00e9rio \u00e9 consistentemente atendido conforme descrito e as pr\u00e1ticas est\u00e3o estabelecidas.</li> <li><code>[3] Supera Expectativas (SE):</code> O crit\u00e9rio \u00e9 plenamente atendido, e existem pr\u00e1ticas exemplares, inova\u00e7\u00e3o ou resultados que v\u00e3o al\u00e9m do esperado.</li> <li><code>[X] N\u00e3o Aplic\u00e1vel (N/A):</code> O crit\u00e9rio, ap\u00f3s an\u00e1lise, n\u00e3o se aplica ao contexto espec\u00edfico da API avaliada.</li> </ul> </li> <li>Se\u00e7\u00e3o 4: Gloss\u00e1rio Nala-IQ: Defini\u00e7\u00f5es para termos como <code>Dimens\u00e3o</code>, <code>Sub-dimens\u00e3o</code>, <code>Crit\u00e9rio</code>, <code>N\u00edvel de Maturidade</code>, <code>Evid\u00eancia</code>, e os 5 N\u00edveis de Maturidade (Inicial, Em Desenvolvimento, Definido, Gerenciado, Otimizado/L\u00edder).</li> </ul> </li> </ul>"},{"location":"quality/nala-iq--template-prototype-structure/#aba-2-questionario-nala-iq-core-v1","title":"Aba 2: Question\u00e1rio Nala-IQ Core v1","text":"<ul> <li>Prop\u00f3sito: Coletar os inputs da autoavalia\u00e7\u00e3o da equipe e, posteriormente, a avalia\u00e7\u00e3o consolidada do auditor para cada um dos mais de 300 crit\u00e9rios do framework.</li> <li>Descri\u00e7\u00e3o das Colunas:<ul> <li><code>A - ID Dimens\u00e3o:</code> Identificador textual da dimens\u00e3o (ex: \"I\", \"II\").</li> <li><code>B - Nome Dimens\u00e3o:</code> Nome completo da dimens\u00e3o (ex: \"Design e Documenta\u00e7\u00e3o da API\").</li> <li><code>C - ID Sub-dimens\u00e3o:</code> Identificador textual da sub-dimens\u00e3o (ex: \"1.1\", \"1.2\").</li> <li><code>D - Nome Sub-dimens\u00e3o:</code> Nome completo da sub-dimens\u00e3o (ex: \"Ades\u00e3o a Padr\u00f5es de Design\").</li> <li><code>E - ID Crit\u00e9rio:</code> Identificador textual do crit\u00e9rio (ex: \"1.1.1\").</li> <li><code>F - Texto do Crit\u00e9rio:</code> O texto completo do crit\u00e9rio, pr\u00e9-populado.</li> <li><code>G - Autoavalia\u00e7\u00e3o da Equipe (0-3, X):</code> Campo para a equipe da API selecionar sua avalia\u00e7\u00e3o.</li> <li><code>H - Justificativa / Evid\u00eancias (Equipe):</code> Campo de texto para a equipe justificar a autoavalia\u00e7\u00e3o e fornecer links/refer\u00eancias.</li> <li><code>I - Pontos Fortes (Equipe):</code> Campo de texto para a equipe destacar pr\u00e1ticas positivas relacionadas ao crit\u00e9rio.</li> <li><code>J - \u00c1reas de Melhoria (Equipe):</code> Campo de texto para a equipe identificar desafios e oportunidades.</li> <li><code>K - A\u00e7\u00f5es Planejadas (Equipe):</code> Campo de texto para a equipe descrever iniciativas em andamento ou planejadas.</li> <li><code>L - Avalia\u00e7\u00e3o do Auditor (0-3, X):</code> Campo para o avaliador consolidar a pontua\u00e7\u00e3o final ap\u00f3s an\u00e1lise das evid\u00eancias.</li> <li><code>M - Coment\u00e1rios / Recomenda\u00e7\u00f5es do Auditor:</code> Campo de texto para o avaliador adicionar suas observa\u00e7\u00f5es.</li> <li><code>N - Criticidade do Crit\u00e9rio (Auditor):</code> (Opcional para v1) Campo para o avaliador definir uma criticidade (Baixa, M\u00e9dia, Alta) para pondera\u00e7\u00f5es futuras.</li> </ul> </li> <li>Recomenda\u00e7\u00f5es T\u00e9cnicas:<ul> <li>As colunas <code>G</code> e <code>L</code> devem usar Valida\u00e7\u00e3o de Dados para permitir apenas as entradas \"0\", \"1\", \"2\", \"3\", \"X\".</li> <li>Utilizar Formata\u00e7\u00e3o Condicional na coluna <code>L</code> para colorir as c\u00e9lulas com base na pontua\u00e7\u00e3o, facilitando a identifica\u00e7\u00e3o visual de pontos cr\u00edticos.</li> </ul> </li> </ul>"},{"location":"quality/nala-iq--template-prototype-structure/#aba-3-scorecard","title":"Aba 3: Scorecard","text":"<ul> <li>Prop\u00f3sito: Calcular e apresentar automaticamente os resultados consolidados da avalia\u00e7\u00e3o, servindo como a fonte principal de dados para o relat\u00f3rio final. Esta aba deve ser protegida contra edi\u00e7\u00e3o manual, exceto por administradores do template.</li> <li>Estrutura e L\u00f3gica de C\u00e1lculo:<ul> <li>Se\u00e7\u00e3o 1: Pontua\u00e7\u00e3o por Sub-dimens\u00e3o<ul> <li>Colunas: <code>ID Dimens\u00e3o</code>, <code>Nome Dimens\u00e3o</code>, <code>ID Sub-dimens\u00e3o</code>, <code>Nome Sub-dimens\u00e3o</code>, <code>Pontua\u00e7\u00e3o M\u00e9dia (0-3)</code>.</li> <li>L\u00f3gica da F\u00f3rmula: A pontua\u00e7\u00e3o m\u00e9dia \u00e9 calculada usando uma f\u00f3rmula como <code>AVERAGEIFS</code> sobre a coluna <code>L</code> da Aba \"Question\u00e1rio\", filtrando pelo <code>ID Sub-dimens\u00e3o</code> correspondente e excluindo os crit\u00e9rios marcados como \"X\".</li> </ul> </li> <li>Se\u00e7\u00e3o 2: Maturidade por Dimens\u00e3o<ul> <li>Colunas: <code>ID Dimens\u00e3o</code>, <code>Nome Dimens\u00e3o</code>, <code>Pontua\u00e7\u00e3o M\u00e9dia (0-3)</code>, <code>N\u00edvel de Maturidade (1-5)</code>.</li> <li>L\u00f3gica da F\u00f3rmula (<code>Pontua\u00e7\u00e3o M\u00e9dia</code>): Calcula a m\u00e9dia das pontua\u00e7\u00f5es das sub-dimens\u00f5es que pertencem \u00e0 dimens\u00e3o correspondente.</li> <li>L\u00f3gica da F\u00f3rmula (<code>N\u00edvel de Maturidade</code>): Usa uma f\u00f3rmula <code>IFS</code> ou <code>VLOOKUP</code> para mapear a pontua\u00e7\u00e3o m\u00e9dia da dimens\u00e3o para um texto descritivo, com base na tabela de mapeamento definida (ex: se pontua\u00e7\u00e3o &gt;= 2.41, ent\u00e3o \"N\u00edvel 5: Otimizado/L\u00edder\", etc.).</li> </ul> </li> <li>Se\u00e7\u00e3o 3: Dados para Radar Chart<ul> <li>Colunas: <code>Nome Dimens\u00e3o</code>, <code>Pontua\u00e7\u00e3o M\u00e9dia da Dimens\u00e3o</code>.</li> <li>Prop\u00f3sito: Uma tabela simples e direta, pronta para ser selecionada como a fonte de dados para um gr\u00e1fico de radar no Excel/Google Sheets.</li> </ul> </li> </ul> </li> </ul>"},{"location":"quality/nala-iq--template-prototype-structure/#aba-4-checklist-de-evidencias-documentais","title":"Aba 4: Checklist de Evid\u00eancias Documentais","text":"<ul> <li>Prop\u00f3sito: Rastrear a coleta de documentos importantes que servem como evid\u00eancia para os crit\u00e9rios.</li> <li>Colunas:<ul> <li><code>ID Evid\u00eancia:</code> Identificador \u00fanico (ex: DOC-01).</li> <li><code>Dimens\u00e3o Nala-IQ Relevante:</code> Dimens\u00e3o principal relacionada \u00e0 evid\u00eancia.</li> <li><code>Tipo de Documento/Evid\u00eancia Sugerida:</code> Descri\u00e7\u00e3o do documento (ex: \"Pol\u00edtica de Versionamento de APIs\").</li> <li><code>Status:</code> Lista suspensa (Coletado, Pendente, N\u00e3o Aplic\u00e1vel, N\u00e3o Existente).</li> <li><code>Link para Evid\u00eancia / Localiza\u00e7\u00e3o / Observa\u00e7\u00f5es:</code> Campo de texto para o link ou notas.</li> </ul> </li> </ul>"},{"location":"quality/nala-iq--template-prototype-structure/#aba-5-checklist-de-dados-automatizados","title":"Aba 5: Checklist de Dados Automatizados","text":"<ul> <li>Prop\u00f3sito: Rastrear a coleta de relat\u00f3rios e sa\u00eddas de ferramentas automatizadas.</li> <li>Colunas:<ul> <li><code>ID Evid\u00eancia:</code> Identificador \u00fanico (ex: AUT-01).</li> <li><code>Dimens\u00e3o Nala-IQ Relevante:</code> Dimens\u00e3o principal relacionada \u00e0 evid\u00eancia.</li> <li><code>Tipo de Dado/Relat\u00f3rio Automatizado Sugerido:</code> Descri\u00e7\u00e3o do relat\u00f3rio (ex: \"Relat\u00f3rio de Scan de Vulnerabilidades de Depend\u00eancias\").</li> <li><code>Status:</code> Lista suspensa (Coletado, Pendente, N\u00e3o Aplic\u00e1vel, N\u00e3o Existente).</li> <li><code>Link para Evid\u00eancia / Localiza\u00e7\u00e3o / Observa\u00e7\u00f5es:</code> Campo de texto para o link ou notas.</li> </ul> </li> </ul>"},{"location":"quality/nala-iq--template-prototype-structure/#4-conclusao","title":"4. Conclus\u00e3o","text":"<p>Este template \u00e9 a ferramenta central para a execu\u00e7\u00e3o da metodologia Nala-IQ. Sua estrutura foi cuidadosamente projetada para garantir um processo de avalia\u00e7\u00e3o completo, consistente e que gera insights acion\u00e1veis. Ao seguir este guia, as equipes da Nalaminds estar\u00e3o aptas a construir e utilizar esta poderosa ferramenta para impulsionar a excel\u00eancia t\u00e9cnica em APIs.</p>"},{"location":"quality/nala-iq-complete/","title":"Nala-IQ Core v1: Framework de Avalia\u00e7\u00e3o de APIs","text":""},{"location":"quality/nala-iq-complete/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>O Nala-IQ Core \u00e9 um framework de avalia\u00e7\u00e3o agn\u00f3stico projetado para analisar a qualidade, maturidade e conformidade de APIs modernas. Ele \u00e9 estruturado em 14 dimens\u00f5es principais, cada uma contendo sub-dimens\u00f5es e crit\u00e9rios espec\u00edficos que cobrem aspectos t\u00e9cnicos, operacionais, estrat\u00e9gicos e de governan\u00e7a do ciclo de vida de uma API. Este documento detalha a vers\u00e3o 1.0 desses crit\u00e9rios agn\u00f3sticos.</p>"},{"location":"quality/nala-iq-complete/#dimensao-i-design-e-documentacao-da-api","title":"Dimens\u00e3o I: Design e Documenta\u00e7\u00e3o da API","text":"<p>(Foco: Clareza, usabilidade, padroniza\u00e7\u00e3o e documenta\u00e7\u00e3o da interface da API)</p>"},{"location":"quality/nala-iq-complete/#11-adesao-a-padroes-de-design","title":"1.1. Ades\u00e3o a Padr\u00f5es de Design","text":"<ul> <li>Crit\u00e9rio 1.1.1: A API segue um padr\u00e3o de design bem definido e documentado (ex: RESTful n\u00edvel 2+, GraphQL, gRPC)?</li> <li>Crit\u00e9rio 1.1.2: As conven\u00e7\u00f5es do padr\u00e3o escolhido s\u00e3o aplicadas de forma consistente em toda a API (endpoints, m\u00e9todos, tipos, etc.)?</li> </ul>"},{"location":"quality/nala-iq-complete/#12-modelagem-e-nomenclatura-de-recursos","title":"1.2. Modelagem e Nomenclatura de Recursos","text":"<ul> <li>Crit\u00e9rio 1.2.1: Os recursos expostos pela API s\u00e3o claramente identific\u00e1veis e modelados de forma intuitiva (ex: seguindo princ\u00edpios de orienta\u00e7\u00e3o a recursos)?</li> <li>Crit\u00e9rio 1.2.2: A nomenclatura utilizada para recursos (URIs), atributos (campos JSON/XML) e par\u00e2metros \u00e9 consistente, previs\u00edvel e segue um padr\u00e3o documentado (ex: <code>camelCase</code>, <code>snake_case</code>)?</li> <li>Crit\u00e9rio 1.2.3: Os URIs s\u00e3o bem estruturados, representando claramente a hierarquia e as rela\u00e7\u00f5es entre os recursos (quando aplic\u00e1vel)?</li> </ul>"},{"location":"quality/nala-iq-complete/#13-design-de-requisicoes-e-respostas","title":"1.3. Design de Requisi\u00e7\u00f5es e Respostas","text":"<ul> <li>Crit\u00e9rio 1.3.1: Os m\u00e9todos/verbos do protocolo (ex: HTTP GET/POST/PUT/PATCH/DELETE) s\u00e3o utilizados corretamente de acordo com sua sem\u00e2ntica (seguran\u00e7a, idempot\u00eancia)?</li> <li>Crit\u00e9rio 1.3.2: Os formatos de dados (media types) para requisi\u00e7\u00e3o e resposta s\u00e3o padronizados (ex: <code>application/json</code>, <code>application/xml</code>), consistentes e documentados?</li> <li>Crit\u00e9rio 1.3.3: A API oferece mecanismos para consulta eficiente de dados, como sele\u00e7\u00e3o de campos (proje\u00e7\u00e3o) e expans\u00e3o de recursos relacionados, onde apropriado?</li> <li>Crit\u00e9rio 1.3.4: Opera\u00e7\u00f5es que retornam listas de recursos implementam um mecanismo de pagina\u00e7\u00e3o padronizado e documentado?</li> <li>Crit\u00e9rio 1.3.5: A API fornece mecanismos consistentes e documentados para filtragem e ordena\u00e7\u00e3o de cole\u00e7\u00f5es de recursos?</li> </ul>"},{"location":"quality/nala-iq-complete/#14-estrategia-de-versionamento","title":"1.4. Estrat\u00e9gia de Versionamento","text":"<ul> <li>Crit\u00e9rio 1.4.1: A API utiliza uma estrat\u00e9gia de versionamento expl\u00edcita e claramente identific\u00e1vel pelo consumidor (ex: via Path URL <code>/v1/</code>, Header <code>Accept</code>, Query Param <code>?version=1</code>)?</li> <li>Crit\u00e9rio 1.4.2: A estrat\u00e9gia de versionamento adotada \u00e9 documentada?</li> <li>Crit\u00e9rio 1.4.3: Existe uma pol\u00edtica documentada para o ciclo de vida das vers\u00f5es da API (suporte, deprecia\u00e7\u00e3o, remo\u00e7\u00e3o)? (Link com Dimens\u00e3o XII. Governan\u00e7a)</li> </ul>"},{"location":"quality/nala-iq-complete/#15-padronizacao-de-tratamento-de-erros-interface","title":"1.5. Padroniza\u00e7\u00e3o de Tratamento de Erros (Interface)","text":"<ul> <li>Crit\u00e9rio 1.5.1: C\u00f3digos de status do protocolo (ex: HTTP Status Codes) s\u00e3o utilizados corretamente para sinalizar o resultado geral da opera\u00e7\u00e3o (sucesso, erro do cliente, erro do servidor)?</li> <li>Crit\u00e9rio 1.5.2: O corpo das respostas de erro segue um formato padronizado em toda a API, fornecendo informa\u00e7\u00f5es \u00fateis como um c\u00f3digo de erro espec\u00edfico da aplica\u00e7\u00e3o, uma mensagem descritiva e, opcionalmente, detalhes adicionais?</li> <li>Crit\u00e9rio 1.5.3: As informa\u00e7\u00f5es retornadas em mensagens de erro s\u00e3o cuidadosamente controladas para n\u00e3o expor detalhes sens\u00edveis de implementa\u00e7\u00e3o ou seguran\u00e7a?</li> </ul>"},{"location":"quality/nala-iq-complete/#16-qualidade-e-completude-da-documentacao","title":"1.6. Qualidade e Completude da Documenta\u00e7\u00e3o","text":"<ul> <li>Crit\u00e9rio 1.6.1: A API possui uma especifica\u00e7\u00e3o formal (ex: OpenAPI, AsyncAPI, GraphQL Schema, Protobuf) que descreve seu contrato?</li> <li>Crit\u00e9rio 1.6.2: A especifica\u00e7\u00e3o formal est\u00e1 atualizada em rela\u00e7\u00e3o \u00e0 implementa\u00e7\u00e3o da API?</li> <li>Crit\u00e9rio 1.6.3: A documenta\u00e7\u00e3o (seja gerada da especifica\u00e7\u00e3o ou escrita manualmente) descreve claramente o prop\u00f3sito da API, cada opera\u00e7\u00e3o/endpoint, seus par\u00e2metros, formatos de requisi\u00e7\u00e3o/resposta e c\u00f3digos de erro poss\u00edveis?</li> <li>Crit\u00e9rio 1.6.4: A documenta\u00e7\u00e3o \u00e9 facilmente acess\u00edvel e compreens\u00edvel para os desenvolvedores consumidores?</li> </ul>"},{"location":"quality/nala-iq-complete/#17-recursos-de-suporte-na-documentacao","title":"1.7. Recursos de Suporte na Documenta\u00e7\u00e3o","text":"<ul> <li>Crit\u00e9rio 1.7.1: A documenta\u00e7\u00e3o inclui exemplos pr\u00e1ticos de requisi\u00e7\u00f5es e respostas para as opera\u00e7\u00f5es mais comuns ou complexas?</li> <li>Crit\u00e9rio 1.7.2: Existem guias de \"primeiros passos\" (getting started), tutoriais ou receitas (recipes) para auxiliar os desenvolvedores na integra\u00e7\u00e3o inicial?</li> <li>Crit\u00e9rio 1.7.3: \u00c9 fornecido um ambiente interativo (ex: via Portal do Desenvolvedor com Swagger UI/Redoc) para explorar e testar a API facilmente? (Link com Dimens\u00e3o XIII. DX)</li> </ul>"},{"location":"quality/nala-iq-complete/#18-design-para-arquiteturas-orientadas-a-eventos-reforco-distribuido","title":"1.8. Design para Arquiteturas Orientadas a Eventos (Refor\u00e7o Distribu\u00eddo)","text":"<ul> <li>Crit\u00e9rio 1.8.1: Para APIs que publicam ou consomem eventos ass\u00edncronos, esses eventos (tipos, t\u00f3picos, payloads) s\u00e3o claramente definidos e documentados (ex: usando padr\u00f5es como AsyncAPI)?</li> <li>Crit\u00e9rio 1.8.2: O schema (contrato) dos payloads dos eventos \u00e9 bem definido, versionado e validado?</li> <li>Crit\u00e9rio 1.8.3: As garantias de entrega e o tratamento de erros para a comunica\u00e7\u00e3o baseada em eventos est\u00e3o definidos e documentados?</li> </ul>"},{"location":"quality/nala-iq-complete/#dimensao-ii-seguranca","title":"Dimens\u00e3o II: Seguran\u00e7a","text":"<p>(Foco: Prote\u00e7\u00e3o contra amea\u00e7as, autentica\u00e7\u00e3o, autoriza\u00e7\u00e3o, integridade dos dados)</p>"},{"location":"quality/nala-iq-complete/#21-autenticacao","title":"2.1. Autentica\u00e7\u00e3o","text":"<ul> <li>Crit\u00e9rio 2.1.1: A API implementa um mecanismo de autentica\u00e7\u00e3o para identificar corretamente os consumidores antes de permitir o acesso a recursos protegidos?</li> <li>Crit\u00e9rio 2.1.2: O mecanismo de autentica\u00e7\u00e3o utilizado \u00e9 um padr\u00e3o reconhecido e considerado seguro para o contexto (ex: OAuth 2.0, OpenID Connect, JWTs assinados com algoritmos fortes, mTLS, API Keys seguras e rotacion\u00e1veis)?</li> <li>Crit\u00e9rio 2.1.3: Credenciais, tokens ou chaves s\u00e3o transmitidos e armazenados de forma segura, seguindo as melhores pr\u00e1ticas?</li> </ul>"},{"location":"quality/nala-iq-complete/#22-autorizacao-e-controle-de-acesso","title":"2.2. Autoriza\u00e7\u00e3o e Controle de Acesso","text":"<ul> <li>Crit\u00e9rio 2.2.1: Um mecanismo de autoriza\u00e7\u00e3o \u00e9 aplicado ap\u00f3s a autentica\u00e7\u00e3o para verificar se o consumidor identificado tem permiss\u00e3o para realizar a opera\u00e7\u00e3o solicitada no recurso espec\u00edfico?</li> <li>Crit\u00e9rio 2.2.2: O princ\u00edpio do menor privil\u00e9gio \u00e9 consistentemente aplicado nas permiss\u00f5es concedidas?</li> <li>Crit\u00e9rio 2.2.3: As checagens de autoriza\u00e7\u00e3o s\u00e3o realizadas no backend, n\u00e3o confiando apenas em decis\u00f5es tomadas no frontend?</li> <li>Crit\u00e9rio 2.2.4: Modelos de controle de acesso (ex: RBAC, ABAC) s\u00e3o utilizados de forma apropriada \u00e0 complexidade das regras de permiss\u00e3o?</li> </ul>"},{"location":"quality/nala-iq-complete/#23-validacao-de-dados-de-entrada-input-validation","title":"2.3. Valida\u00e7\u00e3o de Dados de Entrada (Input Validation)","text":"<ul> <li>Crit\u00e9rio 2.3.1: Todos os dados provenientes de fontes n\u00e3o confi\u00e1veis (corpo da requisi\u00e7\u00e3o, par\u00e2metros de URL, headers) s\u00e3o validados quanto ao tipo, formato, comprimento e/ou valores permitidos antes de serem processados?</li> <li>Crit\u00e9rio 2.3.2: A valida\u00e7\u00e3o \u00e9 realizada na camada de entrada da API (fail-fast)?</li> <li>Crit\u00e9rio 2.3.3: As mensagens de erro de valida\u00e7\u00e3o s\u00e3o informativas para o consumidor, mas n\u00e3o exp\u00f5em detalhes internos da aplica\u00e7\u00e3o ou da valida\u00e7\u00e3o (evitando \"leaky validation\")?</li> </ul>"},{"location":"quality/nala-iq-complete/#24-seguranca-de-transporte","title":"2.4. Seguran\u00e7a de Transporte","text":"<ul> <li>Crit\u00e9rio 2.4.1: A comunica\u00e7\u00e3o com a API requer obrigatoriamente o uso de um protocolo de transporte seguro (TLS 1.2+, preferencialmente 1.3)?</li> <li>Crit\u00e9rio 2.4.2: Configura\u00e7\u00f5es de TLS (certificados, cifras, protocolos) seguem as melhores pr\u00e1ticas de seguran\u00e7a atuais?</li> </ul>"},{"location":"quality/nala-iq-complete/#25-gerenciamento-de-segredos-secrets-management","title":"2.5. Gerenciamento de Segredos (Secrets Management)","text":"<ul> <li>Crit\u00e9rio 2.5.1: \u00c9 proibido e verificado (ex: via pre-commit hooks, CI checks) o armazenamento de segredos (senhas, tokens, chaves) no reposit\u00f3rio de c\u00f3digo?</li> <li>Crit\u00e9rio 2.5.2: Uma solu\u00e7\u00e3o dedicada e segura \u00e9 utilizada para armazenar e fornecer segredos \u00e0 aplica\u00e7\u00e3o em tempo de execu\u00e7\u00e3o (ex: Vault, Secrets Manager da Cloud, Vari\u00e1veis de Ambiente injetadas de forma segura)?</li> <li>Crit\u00e9rio 2.5.3: Existem processos definidos e praticados para a rota\u00e7\u00e3o segura e peri\u00f3dica de segredos cr\u00edticos?</li> </ul>"},{"location":"quality/nala-iq-complete/#26-seguranca-de-dependencias","title":"2.6. Seguran\u00e7a de Depend\u00eancias","text":"<ul> <li>Crit\u00e9rio 2.6.1: \u00c9 utilizado um processo automatizado e regular (ex: integrado ao CI/CD) para escanear as depend\u00eancias diretas e transit\u00f3rias do projeto em busca de vulnerabilidades conhecidas (CVEs)?</li> <li>Crit\u00e9rio 2.6.2: Existe um processo para avaliar e remediar/mitigar as vulnerabilidades encontradas nas depend\u00eancias, priorizando as mais cr\u00edticas?</li> <li>Crit\u00e9rio 2.6.3: Apenas depend\u00eancias de fontes confi\u00e1veis e com boa reputa\u00e7\u00e3o de seguran\u00e7a s\u00e3o adicionadas ao projeto?</li> </ul>"},{"location":"quality/nala-iq-complete/#27-protecao-contra-abuso-rate-limiting-dos","title":"2.7. Prote\u00e7\u00e3o contra Abuso (Rate Limiting, DoS)","text":"<ul> <li>Crit\u00e9rio 2.7.1: Mecanismos de limita\u00e7\u00e3o de taxa (Rate Limiting) s\u00e3o implementados para restringir o n\u00famero de requisi\u00e7\u00f5es que um consumidor pode fazer em um determinado per\u00edodo, prevenindo abuso e sobrecarga?</li> <li>Crit\u00e9rio 2.7.2: A API possui prote\u00e7\u00f5es contra ataques b\u00e1sicos de nega\u00e7\u00e3o de servi\u00e7o, como limita\u00e7\u00e3o do tamanho m\u00e1ximo de payloads, timeouts adequados para requisi\u00e7\u00f5es e conex\u00f5es?</li> <li>Crit\u00e9rio 2.7.3: A infraestrutura da API inclui ou est\u00e1 protegida por solu\u00e7\u00f5es adicionais contra DoS/DDoS (ex: WAF, CDNs com prote\u00e7\u00e3o)?</li> </ul>"},{"location":"quality/nala-iq-complete/#28-configuracao-de-headers-de-seguranca-http","title":"2.8. Configura\u00e7\u00e3o de Headers de Seguran\u00e7a HTTP","text":"<ul> <li>Crit\u00e9rio 2.8.1: Headers HTTP que refor\u00e7am a seguran\u00e7a (ex: <code>Strict-Transport-Security</code>, <code>X-Content-Type-Options</code>, <code>X-Frame-Options</code>, <code>Content-Security-Policy</code> apropriado para APIs, <code>Referrer-Policy</code>) s\u00e3o configurados e enviados nas respostas?</li> <li>Crit\u00e9rio 2.8.2: Se a API suporta CORS (Cross-Origin Resource Sharing), a pol\u00edtica configurada \u00e9 o mais restritiva poss\u00edvel, permitindo apenas origens (origins) e m\u00e9todos necess\u00e1rios e confi\u00e1veis?</li> </ul>"},{"location":"quality/nala-iq-complete/#29-auditoria-e-logging-de-seguranca","title":"2.9. Auditoria e Logging de Seguran\u00e7a","text":"<ul> <li>Crit\u00e9rio 2.9.1: Eventos de seguran\u00e7a significativos (ex: autentica\u00e7\u00e3o bem-sucedida/falha, autoriza\u00e7\u00e3o negada, opera\u00e7\u00f5es administrativas, erros cr\u00edticos) s\u00e3o registrados em logs de auditoria?</li> <li>Crit\u00e9rio 2.9.2: Os logs de auditoria incluem informa\u00e7\u00f5es contextuais suficientes (timestamp, origem, usu\u00e1rio/ator, evento, resultado) para permitir an\u00e1lise forense e rastreamento, sem logar dados excessivamente sens\u00edveis (PII, segredos)?</li> <li>Crit\u00e9rio 2.9.3: Os logs s\u00e3o armazenados de forma segura, protegidos contra acesso n\u00e3o autorizado e adultera\u00e7\u00e3o, e retidos pelo per\u00edodo definido por pol\u00edticas ou regulamenta\u00e7\u00f5es?</li> </ul>"},{"location":"quality/nala-iq-complete/#dimensao-iii-desenvolvimento-e-qualidade-de-codigo","title":"Dimens\u00e3o III: Desenvolvimento e Qualidade de C\u00f3digo","text":"<p>(Foco: Pr\u00e1ticas de codifica\u00e7\u00e3o, manutenibilidade, legibilidade, gerenciamento de depend\u00eancias - na parte agn\u00f3stica, foca nos princ\u00edpios)</p>"},{"location":"quality/nala-iq-complete/#31-controle-de-versao-version-control","title":"3.1. Controle de Vers\u00e3o (Version Control)","text":"<ul> <li>Crit\u00e9rio 3.1.1: Todo o c\u00f3digo-fonte e artefatos de configura\u00e7\u00e3o relevantes da aplica\u00e7\u00e3o est\u00e3o armazenados em um sistema de controle de vers\u00e3o distribu\u00eddo (ex: Git)?</li> <li>Crit\u00e9rio 3.1.2: A equipe segue uma estrat\u00e9gia de branching documentada e consistente (ex: Gitflow, GitHub Flow, Trunk-Based Development) que facilita a colabora\u00e7\u00e3o e a integra\u00e7\u00e3o?</li> <li>Crit\u00e9rio 3.1.3: As mensagens de commit s\u00e3o claras, concisas e seguem um padr\u00e3o que descreve a mudan\u00e7a realizada?</li> <li>Crit\u00e9rio 3.1.4: Arquivos que n\u00e3o devem ser versionados (segredos, logs, depend\u00eancias baixadas, sa\u00eddas de build) s\u00e3o corretamente ignorados pelo sistema de controle de vers\u00e3o (ex: atrav\u00e9s de um arquivo <code>.gitignore</code>)?</li> </ul>"},{"location":"quality/nala-iq-complete/#32-gerenciamento-de-dependencias-dependency-management","title":"3.2. Gerenciamento de Depend\u00eancias (Dependency Management)","text":"<ul> <li>Crit\u00e9rio 3.2.1: As depend\u00eancias externas do projeto s\u00e3o explicitamente declaradas em um arquivo de manifesto padr\u00e3o para o ecossistema (ex: <code>requirements.txt</code>/<code>pyproject.toml</code>, <code>pom.xml</code>, <code>package.json</code>, <code>go.mod</code>)?</li> <li>Crit\u00e9rio 3.2.2: As vers\u00f5es exatas das depend\u00eancias (incluindo as transitivas) s\u00e3o \"travadas\" ou resolvidas de forma determin\u00edstica (ex: usando arquivos de lock como <code>poetry.lock</code>, <code>package-lock.json</code>, <code>go.sum</code>) para garantir builds reprodut\u00edveis em diferentes ambientes?</li> <li>Crit\u00e9rio 3.2.3: Existe um processo (preferencialmente automatizado) para revisar e atualizar as depend\u00eancias regularmente, com aten\u00e7\u00e3o especial a patches de seguran\u00e7a? (Link com II. Seguran\u00e7a)</li> <li>Crit\u00e9rio 3.2.4: Depend\u00eancias que n\u00e3o s\u00e3o mais utilizadas pelo projeto s\u00e3o removidas periodicamente?</li> </ul>"},{"location":"quality/nala-iq-complete/#33-legibilidade-e-manutenibilidade-do-codigo-code-readability-maintainability","title":"3.3. Legibilidade e Manutenibilidade do C\u00f3digo (Code Readability &amp; Maintainability)","text":"<ul> <li>Crit\u00e9rio 3.3.1: O c\u00f3digo adere a princ\u00edpios de design que promovem clareza e simplicidade (ex: DRY - Don't Repeat Yourself, KISS - Keep It Simple, Stupid, YAGNI - You Ain't Gonna Need It)?</li> <li>Crit\u00e9rio 3.3.2: A nomenclatura utilizada para vari\u00e1veis, constantes, fun\u00e7\u00f5es, classes, m\u00f3dulos, etc., \u00e9 significativa, consistente e segue as conven\u00e7\u00f5es de estilo da linguagem/plataforma?</li> <li>Crit\u00e9rio 3.3.3: Fun\u00e7\u00f5es, m\u00e9todos e classes t\u00eam responsabilidades bem definidas e s\u00e3o mantidos em um tamanho razo\u00e1vel para facilitar o entendimento e a modifica\u00e7\u00e3o?</li> <li>Crit\u00e9rio 3.3.4: Coment\u00e1rios s\u00e3o utilizados de forma eficaz para explicar a inten\u00e7\u00e3o (\"porqu\u00ea\") de trechos de c\u00f3digo complexos ou n\u00e3o \u00f3bvios, em vez de simplesmente parafrasear o c\u00f3digo (\"o qu\u00ea\")?</li> <li>Crit\u00e9rio 3.3.5: A complexidade ciclom\u00e1tica das fun\u00e7\u00f5es/m\u00e9todos \u00e9 mantida sob controle para evitar l\u00f3gica excessivamente aninhada ou dif\u00edcil de testar? (Pode ser verificado por ferramentas)</li> </ul>"},{"location":"quality/nala-iq-complete/#34-analise-estatica-e-qualidade-automatizada-static-analysis-automated-quality","title":"3.4. An\u00e1lise Est\u00e1tica e Qualidade Automatizada (Static Analysis &amp; Automated Quality)","text":"<ul> <li>Crit\u00e9rio 3.4.1: Ferramentas de an\u00e1lise est\u00e1tica (linters) s\u00e3o configuradas e utilizadas consistentemente para detectar potenciais bugs, code smells e desvios de padr\u00f5es de codifica\u00e7\u00e3o?</li> <li>Crit\u00e9rio 3.4.2: Ferramentas autom\u00e1ticas de formata\u00e7\u00e3o de c\u00f3digo s\u00e3o utilizadas para garantir um estilo de c\u00f3digo uniforme em todo o projeto?</li> <li>Crit\u00e9rio 3.4.3: Se aplic\u00e1vel \u00e0 linguagem, ferramentas de verifica\u00e7\u00e3o est\u00e1tica de tipos s\u00e3o utilizadas para aumentar a robustez e detectar erros de tipo em tempo de desenvolvimento/compila\u00e7\u00e3o?</li> <li>Crit\u00e9rio 3.4.4: As verifica\u00e7\u00f5es de an\u00e1lise est\u00e1tica e formata\u00e7\u00e3o s\u00e3o integradas ao fluxo de trabalho do desenvolvedor (ex: hooks de pre-commit) e/ou ao pipeline de CI para garantir a conformidade?</li> </ul>"},{"location":"quality/nala-iq-complete/#35-revisao-de-codigo-code-review","title":"3.5. Revis\u00e3o de C\u00f3digo (Code Review)","text":"<ul> <li>Crit\u00e9rio 3.5.1: Um processo de revis\u00e3o de c\u00f3digo por outros membros da equipe \u00e9 consistentemente aplicado antes que as altera\u00e7\u00f5es sejam integradas \u00e0 branch principal?</li> <li>Crit\u00e9rio 3.5.2: As revis\u00f5es de c\u00f3digo avaliam aspectos como corre\u00e7\u00e3o funcional, clareza, manutenibilidade, performance, seguran\u00e7a, ader\u00eancia \u00e0 arquitetura e aos padr\u00f5es estabelecidos?</li> <li>Crit\u00e9rio 3.5.3: O feedback fornecido nas revis\u00f5es \u00e9 construtivo e focado no c\u00f3digo, e existe um acompanhamento para garantir que os pontos levantados sejam tratados?</li> <li>Crit\u00e9rio 3.5.4: O processo de revis\u00e3o \u00e9 eficiente e n\u00e3o se torna um gargalo excessivo para o fluxo de desenvolvimento?</li> </ul>"},{"location":"quality/nala-iq-complete/#36-design-modular-e-acoplamento-modular-design-coupling","title":"3.6. Design Modular e Acoplamento (Modular Design &amp; Coupling)","text":"<ul> <li>Crit\u00e9rio 3.6.1: A aplica\u00e7\u00e3o \u00e9 estruturada em m\u00f3dulos, componentes ou pacotes com alta coes\u00e3o interna (responsabilidades relacionadas agrupadas) e baixo acoplamento externo (m\u00ednima depend\u00eancia direta de outros m\u00f3dulos)?</li> <li>Crit\u00e9rio 3.6.2: As interfaces entre os m\u00f3dulos s\u00e3o bem definidas e est\u00e1veis?</li> <li>Crit\u00e9rio 3.6.3: Princ\u00edpios de design como Separa\u00e7\u00e3o de Preocupa\u00e7\u00f5es (SoC), Encapsulamento e Invers\u00e3o de Depend\u00eancia s\u00e3o aplicados para promover a modularidade, flexibilidade e testabilidade?</li> <li>Crit\u00e9rio 3.6.4: Depend\u00eancias c\u00edclicas entre m\u00f3dulos s\u00e3o evitadas ou gerenciadas?</li> </ul>"},{"location":"quality/nala-iq-complete/#37-gerenciamento-de-debito-tecnico-technical-debt-management","title":"3.7. Gerenciamento de D\u00e9bito T\u00e9cnico (Technical Debt Management)","text":"<ul> <li>Crit\u00e9rio 3.7.1: A equipe tem um entendimento compartilhado do que constitui d\u00e9bito t\u00e9cnico no contexto do projeto?</li> <li>Crit\u00e9rio 3.7.2: D\u00e9bitos t\u00e9cnicos significativos (ex: c\u00f3digo que precisa de refatora\u00e7\u00e3o, falta de testes, solu\u00e7\u00f5es tempor\u00e1rias \"hacks\", tecnologias obsoletas) s\u00e3o identificados, documentados e rastreados (ex: como itens espec\u00edficos no backlog)?</li> <li>Crit\u00e9rio 3.7.3: Existe um processo ou aloca\u00e7\u00e3o de tempo dedicada para endere\u00e7ar e reduzir o d\u00e9bito t\u00e9cnico acumulado de forma proativa, em vez de apenas reativamente?</li> </ul>"},{"location":"quality/nala-iq-complete/#dimensao-iv-testes-e-garantia-de-qualidade","title":"Dimens\u00e3o IV: Testes e Garantia de Qualidade","text":"<p>(Foco: Estrat\u00e9gia de testes, tipos de testes, cobertura, automa\u00e7\u00e3o)</p>"},{"location":"quality/nala-iq-complete/#41-estrategia-e-planejamento-de-testes-test-strategy-planning","title":"4.1. Estrat\u00e9gia e Planejamento de Testes (Test Strategy &amp; Planning)","text":"<ul> <li>Crit\u00e9rio 4.1.1: Existe uma estrat\u00e9gia de testes definida (documentada ou claramente compreendida pela equipe) que descreve os diferentes n\u00edveis de teste a serem empregados (unit\u00e1rio, integra\u00e7\u00e3o, API, E2E, etc.), seus objetivos, escopo e quando s\u00e3o executados?</li> <li>Crit\u00e9rio 4.1.2: A abordagem de testes (ex: pir\u00e2mide de testes, diamante de testes) est\u00e1 alinhada com a arquitetura da aplica\u00e7\u00e3o (monolito, microservi\u00e7os) e os riscos associados?</li> <li>Crit\u00e9rio 4.1.3: As responsabilidades pela escrita, execu\u00e7\u00e3o e manuten\u00e7\u00e3o dos diferentes tipos de testes est\u00e3o claras dentro da equipe?</li> </ul>"},{"location":"quality/nala-iq-complete/#42-testes-unitarios-unit-testing","title":"4.2. Testes Unit\u00e1rios (Unit Testing)","text":"<ul> <li>Crit\u00e9rio 4.2.1: Testes unit\u00e1rios s\u00e3o consistentemente escritos para verificar a l\u00f3gica de neg\u00f3cio e o comportamento de unidades de c\u00f3digo isoladas (fun\u00e7\u00f5es, m\u00e9todos, classes, m\u00f3dulos)?</li> <li>Crit\u00e9rio 4.2.2: Depend\u00eancias externas (opera\u00e7\u00f5es de I/O de rede/disco, banco de dados, outros componentes) s\u00e3o efetivamente substitu\u00eddas por dubl\u00eas de teste (mocks, stubs, fakes) para garantir o isolamento e a velocidade dos testes unit\u00e1rios?</li> <li>Crit\u00e9rio 4.2.3: Os testes unit\u00e1rios cobrem n\u00e3o apenas o \"caminho feliz\", mas tamb\u00e9m casos de borda relevantes, valida\u00e7\u00f5es de entrada e tratamento de erros esperados dentro da unidade?</li> <li>Crit\u00e9rio 4.2.4: A su\u00edte de testes unit\u00e1rios \u00e9 r\u00e1pida o suficiente para ser executada frequentemente durante o desenvolvimento?</li> </ul>"},{"location":"quality/nala-iq-complete/#43-testes-de-integracao-integration-testing","title":"4.3. Testes de Integra\u00e7\u00e3o (Integration Testing)","text":"<ul> <li>Crit\u00e9rio 4.3.1: Testes de integra\u00e7\u00e3o s\u00e3o implementados para verificar a intera\u00e7\u00e3o e a comunica\u00e7\u00e3o correta entre dois ou mais componentes/m\u00f3dulos que colaboram (ex: entre a camada de servi\u00e7o e a camada de acesso a dados, entre a API e seu banco de dados)?</li> <li>Crit\u00e9rio 4.3.2: Esses testes validam os contratos (dados e comportamento) nas fronteiras dos componentes integrados?</li> <li>Crit\u00e9rio 4.3.3: O escopo dos testes de integra\u00e7\u00e3o \u00e9 gerenciado para focar na intera\u00e7\u00e3o espec\u00edfica, sem tentar testar o sistema inteiro?</li> </ul>"},{"location":"quality/nala-iq-complete/#44-testes-de-api-contrato-api-contract-testing","title":"4.4. Testes de API / Contrato (API / Contract Testing)","text":"<ul> <li>Crit\u00e9rio 4.4.1: Testes automatizados s\u00e3o executados diretamente contra a interface externa da API (ex: enviando requisi\u00e7\u00f5es HTTP e validando respostas) para verificar sua funcionalidade, o contrato (schema de requisi\u00e7\u00e3o/resposta, c\u00f3digos de status) e o tratamento de erros?</li> <li>Crit\u00e9rio 4.4.2: Os testes de API validam a conformidade com a especifica\u00e7\u00e3o formal da API (ex: OpenAPI)?</li> <li>Crit\u00e9rio 4.4.3: Em arquiteturas distribu\u00eddas (microservi\u00e7os), s\u00e3o utilizados testes de contrato orientado ao consumidor (ex: Pact) para garantir a compatibilidade entre APIs provedoras e consumidoras de forma independente e ass\u00edncrona? (Refor\u00e7o Distribu\u00eddo)</li> </ul>"},{"location":"quality/nala-iq-complete/#45-testes-de-ponta-a-ponta-end-to-end-testing","title":"4.5. Testes de Ponta a Ponta (End-to-End Testing)","text":"<ul> <li>Crit\u00e9rio 4.5.1: Testes E2E automatizados s\u00e3o implementados para validar fluxos de usu\u00e1rio ou cen\u00e1rios de neg\u00f3cio cr\u00edticos que perpassam m\u00faltiplas camadas da aplica\u00e7\u00e3o ou m\u00faltiplos servi\u00e7os?</li> <li>Crit\u00e9rio 4.5.2: A su\u00edte de testes E2E \u00e9 mantida enxuta e focada nos fluxos de maior valor/risco, devido ao seu maior custo de manuten\u00e7\u00e3o e tempo de execu\u00e7\u00e3o?</li> <li>Crit\u00e9rio 4.5.3: Os testes E2E s\u00e3o executados em um ambiente dedicado e est\u00e1vel que se assemelha ao m\u00e1ximo poss\u00edvel ao ambiente de produ\u00e7\u00e3o?</li> </ul>"},{"location":"quality/nala-iq-complete/#46-testes-nao-funcionais-non-functional-testing","title":"4.6. Testes N\u00e3o-Funcionais (Non-Functional Testing)","text":"<ul> <li>Crit\u00e9rio 4.6.1: Testes de performance, carga e/ou estresse s\u00e3o realizados periodicamente ou antes de releases importantes para avaliar a capacidade de resposta, vaz\u00e3o e utiliza\u00e7\u00e3o de recursos da API sob carga esperada e de pico? (Link com V. Escalabilidade)</li> <li>Crit\u00e9rio 4.6.2: Pr\u00e1ticas ou ferramentas de teste de seguran\u00e7a (ex: DAST automatizado no pipeline, scans de vulnerabilidade na infraestrutura, testes de penetra\u00e7\u00e3o peri\u00f3dicos) s\u00e3o empregadas para identificar falhas de seguran\u00e7a na API? (Link com II. Seguran\u00e7a)</li> <li>Crit\u00e9rio 4.6.3: Testes de resili\u00eancia ou pr\u00e1ticas de chaos engineering s\u00e3o considerados ou aplicados para verificar proativamente como a API e seus componentes reagem a falhas (ex: indisponibilidade de depend\u00eancias)? (Link com VI. Robustez)</li> </ul>"},{"location":"quality/nala-iq-complete/#47-automacao-de-testes-e-execucao-em-cicd-test-automation-cicd-execution","title":"4.7. Automa\u00e7\u00e3o de Testes e Execu\u00e7\u00e3o em CI/CD (Test Automation &amp; CI/CD Execution)","text":"<ul> <li>Crit\u00e9rio 4.7.1: A vasta maioria dos testes (unit\u00e1rios, integra\u00e7\u00e3o, API) \u00e9 automatizada e pode ser executada sem interven\u00e7\u00e3o manual?</li> <li>Crit\u00e9rio 4.7.2: A execu\u00e7\u00e3o dos testes automatizados relevantes (pelo menos unit\u00e1rios e integra\u00e7\u00e3o/API) \u00e9 uma etapa obrigat\u00f3ria no pipeline de Integra\u00e7\u00e3o Cont\u00ednua (CI) para cada commit ou pull request?</li> <li>Crit\u00e9rio 4.7.3: Uma falha em qualquer etapa cr\u00edtica de teste no pipeline impede a progress\u00e3o do c\u00f3digo (ex: bloqueia o merge ou o deploy para o pr\u00f3ximo ambiente)? (Link com VIII. DevOps)</li> <li>Crit\u00e9rio 4.7.4: Os resultados da execu\u00e7\u00e3o dos testes (sucesso, falha, logs, relat\u00f3rios) s\u00e3o facilmente acess\u00edveis e vis\u00edveis para a equipe de desenvolvimento?</li> </ul>"},{"location":"quality/nala-iq-complete/#48-gerenciamento-de-dados-de-teste-test-data-management","title":"4.8. Gerenciamento de Dados de Teste (Test Data Management)","text":"<ul> <li>Crit\u00e9rio 4.8.1: Existe uma estrat\u00e9gia clara e sustent\u00e1vel para gerar, provisionar e limpar os dados necess\u00e1rios para os diferentes n\u00edveis de teste (especialmente integra\u00e7\u00e3o, API e E2E)?</li> <li>Crit\u00e9rio 4.8.2: Dados de produ\u00e7\u00e3o reais, especialmente se contiverem informa\u00e7\u00f5es pessoais ou sens\u00edveis, NUNCA s\u00e3o utilizados diretamente em ambientes de teste n\u00e3o-produtivos; s\u00e3o aplicadas t\u00e9cnicas de mascaramento, anonimiza\u00e7\u00e3o ou gera\u00e7\u00e3o de dados sint\u00e9ticos? (Link com II. Seguran\u00e7a &amp; XIV. Compliance)</li> <li>Crit\u00e9rio 4.8.3: Os casos de teste s\u00e3o projetados, sempre que poss\u00edvel, para serem independentes uns dos outros em rela\u00e7\u00e3o ao estado dos dados, permitindo execu\u00e7\u00f5es mais confi\u00e1veis e paralelas?</li> </ul>"},{"location":"quality/nala-iq-complete/#49-analise-de-cobertura-e-metricas-de-teste-test-coverage-metrics-analysis","title":"4.9. An\u00e1lise de Cobertura e M\u00e9tricas de Teste (Test Coverage &amp; Metrics Analysis)","text":"<ul> <li>Crit\u00e9rio 4.9.1: Ferramentas de an\u00e1lise de cobertura de c\u00f3digo s\u00e3o utilizadas para gerar m\u00e9tricas sobre a porcentagem de linhas/branches/instru\u00e7\u00f5es exercitadas pelos testes automatizados (especialmente unit\u00e1rios)?</li> <li>Crit\u00e9rio 4.9.2: A m\u00e9trica de cobertura de c\u00f3digo \u00e9 utilizada como um indicador para identificar \u00e1reas potencialmente n\u00e3o testadas, mas n\u00e3o como o \u00fanico objetivo ou garantia de qualidade dos testes (foco na qualidade e relev\u00e2ncia dos testes)?</li> <li>Crit\u00e9rio 4.9.3: Outras m\u00e9tricas de qualidade e processo (ex: taxa de sucesso dos testes no CI, tempo de execu\u00e7\u00e3o da su\u00edte, n\u00famero de defeitos escapados para produ\u00e7\u00e3o, densidade de testes) s\u00e3o acompanhadas e utilizadas para melhoria cont\u00ednua?</li> </ul>"},{"location":"quality/nala-iq-complete/#dimensao-v-escalabilidade-e-performance","title":"Dimens\u00e3o V: Escalabilidade e Performance","text":"<p>(Foco: Capacidade de lidar com carga, tempo de resposta, uso eficiente de recursos)</p>"},{"location":"quality/nala-iq-complete/#51-arquitetura-para-escalabilidade-scalable-architecture-design","title":"5.1. Arquitetura para Escalabilidade (Scalable Architecture Design)","text":"<ul> <li>Crit\u00e9rio 5.1.1: A arquitetura da API foi projetada com escalabilidade horizontal em mente (ex: minimizando ou gerenciando estado local por inst\u00e2ncia para permitir adi\u00e7\u00e3o/remo\u00e7\u00e3o f\u00e1cil de r\u00e9plicas)?</li> <li>Crit\u00e9rio 5.1.2: A arquitetura evita pontos \u00fanicos de falha ou gargalos conhecidos que limitariam o escalonamento global da aplica\u00e7\u00e3o?</li> <li>Crit\u00e9rio 5.1.3: O design permite o escalonamento independente de diferentes componentes ou funcionalidades da API, se necess\u00e1rio (ex: separando cargas de trabalho intensivas)?</li> </ul>"},{"location":"quality/nala-iq-complete/#52-metricas-e-monitoramento-de-performance-performance-metrics-monitoring","title":"5.2. M\u00e9tricas e Monitoramento de Performance (Performance Metrics &amp; Monitoring)","text":"<ul> <li>Crit\u00e9rio 5.2.1: M\u00e9tricas chave de performance (ex: lat\u00eancia de requisi\u00e7\u00e3o em percentis p50/p90/p99, taxa de transfer\u00eancia/throughput [RPS/RPM], taxa de erros) s\u00e3o coletadas, agregadas e visualizadas em dashboards? (Link com IX. Observabilidade)</li> <li>Crit\u00e9rio 5.2.2: Alertas s\u00e3o configurados para notificar sobre degrada\u00e7\u00f5es significativas de performance ou viola\u00e7\u00f5es de SLAs de performance?</li> <li>Crit\u00e9rio 5.2.3: Ferramentas de An\u00e1lise de Performance de Aplica\u00e7\u00e3o (APM) ou profiling s\u00e3o utilizadas (em ambientes apropriados) para identificar gargalos espec\u00edficos no c\u00f3digo, queries ou chamadas a depend\u00eancias?</li> </ul>"},{"location":"quality/nala-iq-complete/#53-estrategias-de-cache-caching-strategies","title":"5.3. Estrat\u00e9gias de Cache (Caching Strategies)","text":"<ul> <li>Crit\u00e9rio 5.3.1: Mecanismos de cache (ex: cache em mem\u00f3ria local, cache distribu\u00eddo [Redis, Memcached], cache de borda [CDN], cache HTTP via headers) s\u00e3o empregados de forma eficaz para reduzir a lat\u00eancia percebida pelo cliente e a carga nos sistemas de backend?</li> <li>Crit\u00e9rio 5.3.2: As estrat\u00e9gias de invalida\u00e7\u00e3o, expira\u00e7\u00e3o (TTL) e/ou atualiza\u00e7\u00e3o do cache est\u00e3o bem definidas e adequadas aos requisitos de atualiza\u00e7\u00e3o (freshness) dos dados cacheados?</li> <li>Crit\u00e9rio 5.3.3: A efetividade do cache (ex: taxa de acerto - cache hit rate) \u00e9 monitorada para validar e otimizar a estrat\u00e9gia?</li> </ul>"},{"location":"quality/nala-iq-complete/#54-performance-de-acesso-a-dados-data-access-performance","title":"5.4. Performance de Acesso a Dados (Data Access Performance)","text":"<ul> <li>Crit\u00e9rio 5.4.1: O acesso a bancos de dados e outros armazenamentos persistentes \u00e9 otimizado atrav\u00e9s de modelagem de dados adequada, uso eficiente de \u00edndices e otimiza\u00e7\u00e3o de consultas/queries lentas? (Link com VII. Persist\u00eancia)</li> <li>Crit\u00e9rio 5.4.2: Pools de conex\u00f5es s\u00e3o utilizados e configurados corretamente para gerenciar conex\u00f5es com bancos de dados de forma eficiente?</li> <li>Crit\u00e9rio 5.4.3: O padr\u00e3o de acesso a dados evita problemas comuns de performance como N+1 queries ou busca excessiva de dados (over-fetching)?</li> </ul>"},{"location":"quality/nala-iq-complete/#55-gerenciamento-de-concorrencia-e-processamento-assincrono-concurrency-asynchronous-processing","title":"5.5. Gerenciamento de Concorr\u00eancia e Processamento Ass\u00edncrono (Concurrency &amp; Asynchronous Processing)","text":"<ul> <li>Crit\u00e9rio 5.5.1: A API utiliza um modelo de concorr\u00eancia (ex: baseado em threads, processos, I/O ass\u00edncrono/event-loop) que \u00e9 apropriado para a tecnologia e eficiente para o tipo de carga (I/O-bound vs CPU-bound)?</li> <li>Crit\u00e9rio 5.5.2: Opera\u00e7\u00f5es que envolvem espera por I/O (rede, disco) s\u00e3o tratadas de forma n\u00e3o-bloqueante ou ass\u00edncrona para liberar recursos e permitir o atendimento de outras requisi\u00e7\u00f5es?</li> <li>Crit\u00e9rio 5.5.3: Tarefas de longa dura\u00e7\u00e3o ou que n\u00e3o precisam ser conclu\u00eddas imediatamente s\u00e3o delegadas para processamento em background (ex: usando filas de mensagens)?</li> </ul>"},{"location":"quality/nala-iq-complete/#56-otimizacao-do-uso-de-recursos-resource-usage-optimization","title":"5.6. Otimiza\u00e7\u00e3o do Uso de Recursos (Resource Usage Optimization)","text":"<ul> <li>Crit\u00e9rio 5.6.1: O consumo de recursos computacionais (CPU, mem\u00f3ria RAM) pela aplica\u00e7\u00e3o \u00e9 monitorado e otimizado para evitar desperd\u00edcios ou esgotamento sob carga? (Link com IX. Observabilidade &amp; X. Sustentabilidade)</li> <li>Crit\u00e9rio 5.6.2: O c\u00f3digo \u00e9 escrito para ser eficiente em termos de uso de mem\u00f3ria (ex: evitando vazamentos, gerenciando grandes volumes de dados de forma eficiente)?</li> <li>Crit\u00e9rio 5.6.3: A serializa\u00e7\u00e3o e desserializa\u00e7\u00e3o de dados (ex: JSON parsing/generation) \u00e9 realizada por bibliotecas eficientes e n\u00e3o constitui um gargalo significativo?</li> <li>Crit\u00e9rio 5.6.4: O uso de recursos de rede (ex: n\u00famero de conex\u00f5es abertas, reutiliza\u00e7\u00e3o de conex\u00f5es [keep-alive]) \u00e9 gerenciado de forma eficiente?</li> </ul>"},{"location":"quality/nala-iq-complete/#57-balanceamento-de-carga-e-rede-load-balancing-network","title":"5.7. Balanceamento de Carga e Rede (Load Balancing &amp; Network)","text":"<ul> <li>Crit\u00e9rio 5.7.1: Um mecanismo de balanceamento de carga \u00e9 utilizado na frente das inst\u00e2ncias da API para distribuir o tr\u00e1fego de forma inteligente e garantir alta disponibilidade? (Link com VIII. DevOps &amp; VI. Robustez)</li> <li>Crit\u00e9rio 5.7.2: A topologia de rede e a localiza\u00e7\u00e3o geogr\u00e1fica dos servidores da API e de suas depend\u00eancias s\u00e3o consideradas para minimizar a lat\u00eancia de rede?</li> <li>Crit\u00e9rio 5.7.3: A compress\u00e3o de dados (ex: Gzip, Brotli) \u00e9 habilitada nas respostas da API para reduzir o tamanho dos dados transferidos pela rede?</li> </ul>"},{"location":"quality/nala-iq-complete/#58-performance-de-comunicacao-inter-servicos-reforco-distribuido","title":"5.8. Performance de Comunica\u00e7\u00e3o Inter-Servi\u00e7os (Refor\u00e7o Distribu\u00eddo)","text":"<ul> <li>Crit\u00e9rio 5.8.1: Em arquiteturas com m\u00faltiplas APIs ou microservi\u00e7os, a lat\u00eancia introduzida pela comunica\u00e7\u00e3o entre eles \u00e9 monitorada (ex: via tracing distribu\u00eddo) e considerada nos SLAs de performance?</li> <li>Crit\u00e9rio 5.8.2: Protocolos de comunica\u00e7\u00e3o eficientes (ex: gRPC com Protobuf, reutiliza\u00e7\u00e3o de conex\u00f5es HTTP) e/ou padr\u00f5es como agrega\u00e7\u00e3o de chamadas (API Gateway) ou batching s\u00e3o utilizados para otimizar a comunica\u00e7\u00e3o inter-servi\u00e7os?</li> <li>Crit\u00e9rio 5.8.3: O n\u00famero de saltos (hops) ou chamadas em cadeia entre servi\u00e7os para atender uma requisi\u00e7\u00e3o externa \u00e9 minimizado ou gerenciado para evitar lat\u00eancias excessivas?</li> </ul>"},{"location":"quality/nala-iq-complete/#dimensao-vi-robustez-e-confiabilidade","title":"Dimens\u00e3o VI: Robustez e Confiabilidade","text":"<p>(Foco: Capacidade da API de resistir a falhas, lidar com erros inesperados e operar de forma consistente ao longo do tempo)</p>"},{"location":"quality/nala-iq-complete/#61-tratamento-de-erros-e-excecoes-error-exception-handling","title":"6.1. Tratamento de Erros e Exce\u00e7\u00f5es (Error &amp; Exception Handling)","text":"<ul> <li>Crit\u00e9rio 6.1.1: Existe um mecanismo global ou padronizado na aplica\u00e7\u00e3o para capturar exce\u00e7\u00f5es n\u00e3o tratadas, prevenindo que elas causem o t\u00e9rmino abrupto do processo/inst\u00e2ncia da API?</li> <li>Crit\u00e9rio 6.1.2: Erros internos inesperados s\u00e3o mapeados para respostas de erro gen\u00e9ricas e seguras na interface da API (ex: HTTP 500), sem expor detalhes internos, stack traces ou informa\u00e7\u00f5es sens\u00edveis? (Link com I. Design &amp; II. Seguran\u00e7a)</li> <li>Crit\u00e9rio 6.1.3: A aplica\u00e7\u00e3o lida de forma previs\u00edvel e controlada com erros esperados (ex: valida\u00e7\u00e3o de entrada falhou, recurso n\u00e3o encontrado, conflito de estado), comunicando-os adequadamente ao cliente via c\u00f3digos de status e mensagens de erro padronizadas?</li> <li>Crit\u00e9rio 6.1.4: Erros e exce\u00e7\u00f5es significativas s\u00e3o registrados (logados) com contexto suficiente (ex: ID de requisi\u00e7\u00e3o, stack trace parcial seguro) para facilitar a an\u00e1lise e depura\u00e7\u00e3o pela equipe de desenvolvimento? (Link com IX. Observabilidade)</li> </ul>"},{"location":"quality/nala-iq-complete/#62-padroes-de-tolerancia-a-falhas-inter-servicos-inter-service-fault-tolerance-patterns-reforco-distribuido","title":"6.2. Padr\u00f5es de Toler\u00e2ncia a Falhas Inter-Servi\u00e7os (Inter-Service Fault Tolerance Patterns) (Refor\u00e7o Distribu\u00eddo)","text":"<ul> <li>Crit\u00e9rio 6.2.1: Chamadas a depend\u00eancias externas (outras APIs, servi\u00e7os de rede, bancos de dados) possuem timeouts configurados para evitar que a requisi\u00e7\u00e3o fique presa indefinidamente em caso de lentid\u00e3o ou falha da depend\u00eancia?</li> <li>Crit\u00e9rio 6.2.2: Mecanismos de retentativa autom\u00e1tica (retry), com estrat\u00e9gias de espera entre tentativas (backoff, ex: exponencial com jitter), s\u00e3o utilizados para lidar com falhas intermitentes ou transientes em chamadas a depend\u00eancias externas?</li> <li>Crit\u00e9rio 6.2.3: O padr\u00e3o Circuit Breaker (ou similar) \u00e9 aplicado em chamadas a depend\u00eancias cr\u00edticas para detectar falhas persistentes, interromper rapidamente as chamadas subsequentes (fail-fast) e evitar sobrecarga na depend\u00eancia ou na pr\u00f3pria API?</li> <li>Crit\u00e9rio 6.2.4: A configura\u00e7\u00e3o desses padr\u00f5es de resili\u00eancia (n\u00famero de retries, timeouts, limiares do circuit breaker) \u00e9 gerenci\u00e1vel e idealmente ajustada com base em dados de observabilidade ou testes espec\u00edficos?</li> </ul>"},{"location":"quality/nala-iq-complete/#63-idempotencia-idempotency","title":"6.3. Idempot\u00eancia (Idempotency)","text":"<ul> <li>Crit\u00e9rio 6.3.1: As opera\u00e7\u00f5es da API que modificam o estado do sistema (ex: POST para criar, PUT/PATCH para atualizar, DELETE para remover) s\u00e3o projetadas para serem idempotentes sempre que a sem\u00e2ntica da opera\u00e7\u00e3o permitir (ex: m\u00faltiplas chamadas DELETE para o mesmo recurso t\u00eam o mesmo resultado final)?</li> <li>Crit\u00e9rio 6.3.2: Para opera\u00e7\u00f5es cr\u00edticas que n\u00e3o s\u00e3o naturalmente idempotentes (ex: processar um pagamento, enviar uma notifica\u00e7\u00e3o), mecanismos como o uso de chaves de idempot\u00eancia (fornecidas pelo cliente ou geradas) s\u00e3o implementados para detectar e tratar requisi\u00e7\u00f5es duplicadas de forma segura?</li> <li>Crit\u00e9rio 6.3.3: O comportamento de idempot\u00eancia esperado (ou a necessidade de fornecer chaves de idempot\u00eancia) \u00e9 claramente documentado para os consumidores da API? (Link com I. Documenta\u00e7\u00e3o)</li> </ul>"},{"location":"quality/nala-iq-complete/#64-health-checks-e-reporte-de-status-health-checks-status-reporting","title":"6.4. Health Checks e Reporte de Status (Health Checks &amp; Status Reporting)","text":"<ul> <li>Crit\u00e9rio 6.4.1: A API exp\u00f5e um ou mais endpoints HTTP espec\u00edficos para verifica\u00e7\u00e3o de sa\u00fade (health checks - ex: <code>/health</code>, <code>/live</code>, <code>/ready</code>) que podem ser consultados por sistemas de monitoramento, load balancers ou orquestradores?</li> <li>Crit\u00e9rio 6.4.2: Existe uma distin\u00e7\u00e3o clara entre verifica\u00e7\u00f5es de \"liveness\" (inst\u00e2ncia est\u00e1 rodando) e \"readiness\" (inst\u00e2ncia est\u00e1 pronta para receber tr\u00e1fego, incluindo a sa\u00fade de depend\u00eancias cr\u00edticas)?</li> <li>Crit\u00e9rio 6.4.3: As verifica\u00e7\u00f5es realizadas pelos health checks s\u00e3o representativas da capacidade da inst\u00e2ncia de servir requisi\u00e7\u00f5es, mas s\u00e3o executadas de forma eficiente para n\u00e3o impactar a performance da aplica\u00e7\u00e3o?</li> </ul>"},{"location":"quality/nala-iq-complete/#65-gerenciamento-de-transacoes-transaction-management","title":"6.5. Gerenciamento de Transa\u00e7\u00f5es (Transaction Management)","text":"<ul> <li>Crit\u00e9rio 6.5.1: Para opera\u00e7\u00f5es que requerem m\u00faltiplas altera\u00e7\u00f5es em um ou mais armazenamentos de dados transacionais (ex: bancos de dados SQL), elas s\u00e3o agrupadas dentro de transa\u00e7\u00f5es para garantir atomicidade (ou todas as altera\u00e7\u00f5es s\u00e3o aplicadas com sucesso, ou nenhuma \u00e9)? (Link com VII. Persist\u00eancia)</li> <li>Crit\u00e9rio 6.5.2: O escopo e a dura\u00e7\u00e3o das transa\u00e7\u00f5es s\u00e3o gerenciados adequadamente para evitar bloqueios (locks) excessivos ou outros problemas de concorr\u00eancia?</li> <li>Crit\u00e9rio 6.5.3: Em cen\u00e1rios distribu\u00eddos que exigem consist\u00eancia at\u00f4mica ou coordenada entre m\u00faltiplos servi\u00e7os/recursos, padr\u00f5es apropriados (ex: Sagas para consist\u00eancia eventual, ou Two-Phase Commit com cautela) s\u00e3o considerados e implementados? (Refor\u00e7o Distribu\u00eddo)</li> </ul>"},{"location":"quality/nala-iq-complete/#66-desligamento-gracioso-graceful-shutdown","title":"6.6. Desligamento Gracioso (Graceful Shutdown)","text":"<ul> <li>Crit\u00e9rio 6.6.1: A aplica\u00e7\u00e3o \u00e9 capaz de interceptar sinais de sistema para desligamento (ex: SIGTERM enviado por Kubernetes ou systemd) e iniciar um processo de finaliza\u00e7\u00e3o controlada?</li> <li>Crit\u00e9rio 6.6.2: Durante o processo de desligamento gracioso, a aplica\u00e7\u00e3o tenta finalizar o processamento das requisi\u00e7\u00f5es que j\u00e1 estavam em andamento (dentro de um tempo limite configur\u00e1vel) antes de encerrar?</li> <li>Crit\u00e9rio 6.6.3: Recursos cr\u00edticos (ex: conex\u00f5es, arquivos, locks) s\u00e3o liberados de forma limpa durante o desligamento?</li> </ul>"},{"location":"quality/nala-iq-complete/#67-estrategias-de-fallback-e-degradacao-graciosa-fallback-graceful-degradation-strategies","title":"6.7. Estrat\u00e9gias de Fallback e Degrada\u00e7\u00e3o Graciosa (Fallback &amp; Graceful Degradation Strategies)","text":"<ul> <li>Crit\u00e9rio 6.7.1: Quando uma depend\u00eancia externa n\u00e3o cr\u00edtica falha, a API tenta fornecer uma resposta alternativa \u00fatil (ex: dados de um cache, um valor padr\u00e3o, uma resposta parcial) em vez de simplesmente retornar um erro 5xx?</li> <li>Crit\u00e9rio 6.7.2: A API foi projetada para operar, se poss\u00edvel, em um modo de funcionalidade reduzida (degrada\u00e7\u00e3o graciosa) caso depend\u00eancias essenciais estejam temporariamente indispon\u00edveis, priorizando as funcionalidades mais cr\u00edticas?</li> <li>Crit\u00e9rio 6.7.3: O acionamento de mecanismos de fallback ou a opera\u00e7\u00e3o em modo degradado s\u00e3o adequadamente registrados (logados) ou sinalizados para monitoramento? (Link com IX. Observabilidade)</li> </ul>"},{"location":"quality/nala-iq-complete/#dimensao-vii-persistencia-e-gerenciamento-de-dados","title":"Dimens\u00e3o VII: Persist\u00eancia e Gerenciamento de Dados","text":"<p>(Foco: Armazenamento, recupera\u00e7\u00e3o, seguran\u00e7a e ciclo de vida dos dados com os quais opera)</p>"},{"location":"quality/nala-iq-complete/#71-modelagem-de-dados-data-modeling","title":"7.1. Modelagem de Dados (Data Modeling)","text":"<ul> <li>Crit\u00e9rio 7.1.1: O modelo de dados (seja relacional, documental, grafo, etc.) representa de forma precisa, clara e eficiente as entidades de neg\u00f3cio e seus relacionamentos relevantes para a aplica\u00e7\u00e3o?</li> <li>Crit\u00e9rio 7.1.2: A modelagem considera os padr\u00f5es de consulta (queries) mais comuns e cr\u00edticos para otimizar a performance de leitura e escrita? (Link com V. Performance)</li> <li>Crit\u00e9rio 7.1.3: O modelo utiliza tipos de dados apropriados e aplica restri\u00e7\u00f5es (constraints) para garantir a integridade e a validade dos dados no n\u00edvel do armazenamento?</li> <li>Crit\u00e9rio 7.1.4: O modelo de dados est\u00e1 documentado (ex: atrav\u00e9s de diagramas ER, documenta\u00e7\u00e3o de schema NoSQL) e \u00e9 compreendido pela equipe?</li> </ul>"},{"location":"quality/nala-iq-complete/#72-escolha-e-uso-do-armazenamento-de-dados-data-store-selection-usage","title":"7.2. Escolha e Uso do Armazenamento de Dados (Data Store Selection &amp; Usage)","text":"<ul> <li>Crit\u00e9rio 7.2.1: A escolha da tecnologia de armazenamento de dados (ex: PostgreSQL, MongoDB, Redis, Cassandra, S3) est\u00e1 alinhada com os requisitos da aplica\u00e7\u00e3o em termos de estrutura dos dados, volume, performance, consist\u00eancia (CAP theorem trade-offs) e escalabilidade?</li> <li>Crit\u00e9rio 7.2.2: As funcionalidades espec\u00edficas da tecnologia de armazenamento escolhida (ex: \u00edndices especializados, transa\u00e7\u00f5es ACID, agrega\u00e7\u00f5es, TTLs) s\u00e3o utilizadas de forma adequada e eficiente?</li> <li>Crit\u00e9rio 7.2.3: Caso a aplica\u00e7\u00e3o utilize m\u00faltiplos armazenamentos de dados (persist\u00eancia poliglota), essa decis\u00e3o \u00e9 justificada e a complexidade adicional \u00e9 gerenciada?</li> </ul>"},{"location":"quality/nala-iq-complete/#73-camada-de-acesso-a-dados-data-access-layer","title":"7.3. Camada de Acesso a Dados (Data Access Layer)","text":"<ul> <li>Crit\u00e9rio 7.3.1: A l\u00f3gica de intera\u00e7\u00e3o com o(s) armazenamento(s) de dados est\u00e1 encapsulada em uma camada de abstra\u00e7\u00e3o dedicada (ex: usando padr\u00f5es como Repository, Data Access Object - DAO, ou atrav\u00e9s de um ORM/ODM bem configurado), isolando a l\u00f3gica de neg\u00f3cio dos detalhes de persist\u00eancia? (Link com III. Design Modular)</li> <li>Crit\u00e9rio 7.3.2: A interface dessa camada de acesso \u00e9 clara e exp\u00f5e as opera\u00e7\u00f5es necess\u00e1rias de forma consistente?</li> <li>Crit\u00e9rio 7.3.3: O uso de ORMs/ODMs (se aplic\u00e1vel) \u00e9 feito de forma consciente, monitorando e otimizando as queries geradas para evitar problemas de performance (ex: N+1, lazy loading ineficiente)? (Link com V. Performance)</li> </ul>"},{"location":"quality/nala-iq-complete/#74-evolucao-de-schema-e-migracoes-schema-evolution-migrations","title":"7.4. Evolu\u00e7\u00e3o de Schema e Migra\u00e7\u00f5es (Schema Evolution &amp; Migrations)","text":"<ul> <li>Crit\u00e9rio 7.4.1: Existe um processo automatizado, controlado por vers\u00e3o e repet\u00edvel para gerenciar e aplicar altera\u00e7\u00f5es no esquema do banco de dados (schema) conforme a aplica\u00e7\u00e3o evolui (ex: usando ferramentas de migra\u00e7\u00e3o dedicadas)?</li> <li>Crit\u00e9rio 7.4.2: As migra\u00e7\u00f5es s\u00e3o projetadas para serem seguras (ex: n\u00e3o destrutivas, revers\u00edveis sempre que poss\u00edvel) e para minimizar o impacto (ex: tempo de bloqueio) durante a aplica\u00e7\u00e3o em ambientes produtivos?</li> <li>Crit\u00e9rio 7.4.3: As migra\u00e7\u00f5es de esquema s\u00e3o testadas em ambientes de pr\u00e9-produ\u00e7\u00e3o que espelham o ambiente de produ\u00e7\u00e3o antes do deploy final? (Link com IV. Testes &amp; VIII. DevOps)</li> </ul>"},{"location":"quality/nala-iq-complete/#75-backup-recuperacao-e-retencao-de-dados-data-backup-recovery-retention","title":"7.5. Backup, Recupera\u00e7\u00e3o e Reten\u00e7\u00e3o de Dados (Data Backup, Recovery &amp; Retention)","text":"<ul> <li>Crit\u00e9rio 7.5.1: Procedimentos regulares e automatizados de backup s\u00e3o realizados para todos os dados persistentes cr\u00edticos?</li> <li>Crit\u00e9rio 7.5.2: Existem planos de recupera\u00e7\u00e3o de desastres documentados, e os procedimentos de restaura\u00e7\u00e3o a partir dos backups s\u00e3o testados periodicamente para validar sua efic\u00e1cia e garantir o cumprimento dos objetivos de tempo e ponto de recupera\u00e7\u00e3o (RTO/RPO)?</li> <li>Crit\u00e9rio 7.5.3: Pol\u00edticas de reten\u00e7\u00e3o de dados e backups est\u00e3o definidas e implementadas, considerando requisitos de neg\u00f3cio, legais e regulat\u00f3rios? (Link com XIV. Compliance)</li> </ul>"},{"location":"quality/nala-iq-complete/#76-seguranca-e-privacidade-dos-dados-armazenados-stored-data-security-privacy","title":"7.6. Seguran\u00e7a e Privacidade dos Dados Armazenados (Stored Data Security &amp; Privacy)","text":"<ul> <li>Crit\u00e9rio 7.6.1: Dados sens\u00edveis armazenados em repouso (at rest) s\u00e3o protegidos usando criptografia? (Link com II. Seguran\u00e7a)</li> <li>Crit\u00e9rio 7.6.2: O acesso da aplica\u00e7\u00e3o ao banco de dados utiliza credenciais espec\u00edficas com o m\u00ednimo de privil\u00e9gios necess\u00e1rios para sua opera\u00e7\u00e3o? (Link com II. Seguran\u00e7a)</li> <li>Crit\u00e9rio 7.6.3: A camada de acesso a dados utiliza t\u00e9cnicas seguras para prevenir vulnerabilidades como SQL Injection (ex: queries parametrizadas, ORMs seguros)? (Link com II. Seguran\u00e7a)</li> <li>Crit\u00e9rio 7.6.4: O tratamento de dados pessoais ou sujeitos a regulamenta\u00e7\u00f5es espec\u00edficas segue as diretrizes de privacidade e seguran\u00e7a exigidas (ex: LGPD, GDPR, HIPAA), incluindo mecanismos para atender aos direitos dos titulares (acesso, retifica\u00e7\u00e3o, exclus\u00e3o)? (Link com XIV. Compliance)</li> </ul>"},{"location":"quality/nala-iq-complete/#77-estrategias-de-consistencia-de-dados-data-consistency-strategies-reforco-distribuido","title":"7.7. Estrat\u00e9gias de Consist\u00eancia de Dados (Data Consistency Strategies) (Refor\u00e7o Distribu\u00eddo)","text":"<ul> <li>Crit\u00e9rio 7.7.1: O modelo de consist\u00eancia de dados (ex: forte, forte eventual, causal, leitura das pr\u00f3prias escritas) requerido pelas diferentes funcionalidades da aplica\u00e7\u00e3o \u00e9 compreendido e suportado pela(s) tecnologia(s) de armazenamento utilizada(s)?</li> <li>Crit\u00e9rio 7.7.2: Para opera\u00e7\u00f5es que modificam dados em m\u00faltiplos locais (diferentes tabelas, bancos de dados, ou servi\u00e7os distribu\u00eddos), s\u00e3o implementadas estrat\u00e9gias apropriadas para garantir a consist\u00eancia desejada (ex: transa\u00e7\u00f5es ACID locais, padr\u00e3o Saga para consist\u00eancia eventual, detec\u00e7\u00e3o e reconcilia\u00e7\u00e3o de conflitos)? (Link com VI. Robustez)</li> <li>Crit\u00e9rio 7.7.3: Os desenvolvedores est\u00e3o cientes das implica\u00e7\u00f5es da consist\u00eancia eventual (se utilizada) e projetam a l\u00f3gica de neg\u00f3cio e a interface do usu\u00e1rio para lidar com poss\u00edveis estados transit\u00f3rios ou dados temporariamente inconsistentes?</li> </ul>"},{"location":"quality/nala-iq-complete/#dimensao-viii-processos-de-build-deploy-e-devops","title":"Dimens\u00e3o VIII: Processos de Build, Deploy e DevOps","text":"<p>(Foco: Automa\u00e7\u00e3o, efici\u00eancia e seguran\u00e7a da entrega do software em produ\u00e7\u00e3o)</p>"},{"location":"quality/nala-iq-complete/#81-automacao-de-build-e-empacotamento-build-packaging-automation","title":"8.1. Automa\u00e7\u00e3o de Build e Empacotamento (Build &amp; Packaging Automation)","text":"<ul> <li>Crit\u00e9rio 8.1.1: O processo completo para transformar o c\u00f3digo-fonte em um artefato pronto para deploy (compila\u00e7\u00e3o, testes unit\u00e1rios, an\u00e1lise est\u00e1tica, empacotamento) \u00e9 totalmente automatizado e executado por uma ferramenta de build?</li> <li>Crit\u00e9rio 8.1.2: O build automatizado \u00e9 determin\u00edstico e reprodut\u00edvel (produz o mesmo resultado ao ser executado nas mesmas condi\u00e7\u00f5es/c\u00f3digo)?</li> <li>Crit\u00e9rio 8.1.3: O tempo de execu\u00e7\u00e3o do build \u00e9 otimizado para fornecer feedback r\u00e1pido aos desenvolvedores?</li> </ul>"},{"location":"quality/nala-iq-complete/#82-integracao-continua-continuous-integration-ci","title":"8.2. Integra\u00e7\u00e3o Cont\u00ednua (Continuous Integration - CI)","text":"<ul> <li>Crit\u00e9rio 8.2.1: Um sistema de CI \u00e9 utilizado para automaticamente executar o processo de build e testes (unit\u00e1rios, integra\u00e7\u00e3o) a cada altera\u00e7\u00e3o enviada ao controle de vers\u00e3o (em branches de feature ou principal)?</li> <li>Crit\u00e9rio 8.2.2: O pipeline de CI fornece feedback claro, r\u00e1pido e acion\u00e1vel sobre o sucesso ou falha da integra\u00e7\u00e3o, incluindo relat\u00f3rios de testes e an\u00e1lises de qualidade? (Link com IV. Testes &amp; III. Qualidade de C\u00f3digo)</li> <li>Crit\u00e9rio 8.2.3: A pr\u00e1tica de integrar o c\u00f3digo frequentemente (ex: v\u00e1rias vezes ao dia) \u00e0 branch principal \u00e9 adotada pela equipe para evitar integra\u00e7\u00f5es complexas e tardias?</li> </ul>"},{"location":"quality/nala-iq-complete/#83-gerenciamento-de-artefatos-artifact-management","title":"8.3. Gerenciamento de Artefatos (Artifact Management)","text":"<ul> <li>Crit\u00e9rio 8.3.1: Os artefatos gerados pelo processo de build (ex: imagens de container, bibliotecas, pacotes execut\u00e1veis) s\u00e3o versionados de forma inequ\u00edvoca e armazenados em um reposit\u00f3rio de artefatos centralizado e seguro (ex: Nexus, Artifactory, Docker Registry, ECR/GCR/ACR)?</li> <li>Crit\u00e9rio 8.3.2: Existe rastreabilidade clara entre a vers\u00e3o do artefato, a vers\u00e3o do c\u00f3digo-fonte que o gerou e os resultados do pipeline de CI/CD associado?</li> <li>Crit\u00e9rio 8.3.3: O acesso ao reposit\u00f3rio de artefatos \u00e9 controlado e pol\u00edticas de limpeza ou reten\u00e7\u00e3o s\u00e3o aplicadas para gerenciar o armazenamento?</li> </ul>"},{"location":"quality/nala-iq-complete/#84-estrategias-de-deploy-deployment-strategies","title":"8.4. Estrat\u00e9gias de Deploy (Deployment Strategies)","text":"<ul> <li>Crit\u00e9rio 8.4.1: Estrat\u00e9gias de deploy avan\u00e7adas (ex: Rolling Update, Blue/Green Deployment, Canary Release, A/B Testing) s\u00e3o utilizadas para realizar atualiza\u00e7\u00f5es em produ\u00e7\u00e3o com m\u00ednimo ou zero downtime e risco reduzido?</li> <li>Crit\u00e9rio 8.4.2: A escolha da estrat\u00e9gia de deploy \u00e9 consciente e adequada ao tipo de aplica\u00e7\u00e3o, arquitetura e requisitos de neg\u00f3cio (disponibilidade, toler\u00e2ncia a risco)?</li> <li>Crit\u00e9rio 8.4.3: O processo de execu\u00e7\u00e3o da estrat\u00e9gia de deploy escolhida \u00e9 amplamente automatizado?</li> </ul>"},{"location":"quality/nala-iq-complete/#85-entregadeploy-continuo-continuous-deliverydeployment-cd","title":"8.5. Entrega/Deploy Cont\u00ednuo (Continuous Delivery/Deployment - CD)","text":"<ul> <li>Crit\u00e9rio 8.5.1: O pipeline de CI/CD automatiza o deploy dos artefatos validados para ambientes de pr\u00e9-produ\u00e7\u00e3o (ex: homologa\u00e7\u00e3o, staging) - caracterizando Entrega Cont\u00ednua?</li> <li>Crit\u00e9rio 8.5.2: O processo de deploy para o ambiente de produ\u00e7\u00e3o \u00e9 automatizado e pode ser disparado com um clique ou automaticamente ap\u00f3s a aprova\u00e7\u00e3o de todas as etapas anteriores (caracterizando Deploy Cont\u00ednuo ou Entrega Cont\u00ednua com aprova\u00e7\u00e3o manual)?</li> <li>Crit\u00e9rio 8.5.3: O pipeline inclui valida\u00e7\u00f5es automatizadas p\u00f3s-deploy (ex: smoke tests, verifica\u00e7\u00e3o de health checks cr\u00edticos) para confirmar a sa\u00fade da nova vers\u00e3o antes de direcionar todo o tr\u00e1fego para ela?</li> </ul>"},{"location":"quality/nala-iq-complete/#86-infraestrutura-como-codigo-infrastructure-as-code-iac","title":"8.6. Infraestrutura como C\u00f3digo (Infrastructure as Code - IaC)","text":"<ul> <li>Crit\u00e9rio 8.6.1: A defini\u00e7\u00e3o, provisionamento e gerenciamento da infraestrutura da aplica\u00e7\u00e3o (redes, VMs, bancos de dados, balanceadores de carga, clusters de orquestra\u00e7\u00e3o) s\u00e3o feitos atrav\u00e9s de c\u00f3digo usando ferramentas declarativas ou imperativas de IaC (ex: Terraform, CloudFormation, Pulumi, Ansible, Bicep)?</li> <li>Crit\u00e9rio 8.6.2: O c\u00f3digo da infraestrutura \u00e9 versionado em um sistema de controle de vers\u00e3o, permitindo revis\u00e3o por pares, hist\u00f3rico de altera\u00e7\u00f5es e colabora\u00e7\u00e3o? (Link com III. Controle de Vers\u00e3o)</li> <li>Crit\u00e9rio 8.6.3: A aplica\u00e7\u00e3o das altera\u00e7\u00f5es de infraestrutura \u00e9 feita de forma automatizada, idempotente e integrada ao pipeline de CI/CD quando apropriado?</li> </ul>"},{"location":"quality/nala-iq-complete/#87-containerizacao-containerization","title":"8.7. Containeriza\u00e7\u00e3o (Containerization)","text":"<ul> <li>Crit\u00e9rio 8.7.1: A aplica\u00e7\u00e3o e suas depend\u00eancias de sistema operacional e bibliotecas s\u00e3o empacotadas em imagens de container (ex: usando Docker) para garantir a imutabilidade e a consist\u00eancia entre todos os ambientes?</li> <li>Crit\u00e9rio 8.7.2: As defini\u00e7\u00f5es das imagens (ex: Dockerfile) seguem boas pr\u00e1ticas de otimiza\u00e7\u00e3o de tamanho, seguran\u00e7a (usu\u00e1rio n\u00e3o-root, minimiza\u00e7\u00e3o de superf\u00edcie de ataque) e efici\u00eancia de build (cache de camadas)? (Link com II. Seguran\u00e7a &amp; X. Sustentabilidade)</li> <li>Crit\u00e9rio 8.7.3: As imagens de container s\u00e3o regularmente escaneadas em busca de vulnerabilidades conhecidas em suas camadas base e depend\u00eancias? (Link com II. Seguran\u00e7a)</li> </ul>"},{"location":"quality/nala-iq-complete/#88-orquestracao-orchestration","title":"8.8. Orquestra\u00e7\u00e3o (Orchestration)","text":"<ul> <li>Crit\u00e9rio 8.8.1: Para aplica\u00e7\u00f5es containerizadas, um orquestrador de containers (ex: Kubernetes, OpenShift, ECS, Nomad) \u00e9 utilizado para gerenciar o ciclo de vida (deploy, scaling, health monitoring, self-healing) das inst\u00e2ncias em ambientes de larga escala ou produ\u00e7\u00e3o?</li> <li>Crit\u00e9rio 8.8.2: Os requisitos de recursos (CPU, mem\u00f3ria) para os containers s\u00e3o definidos no orquestrador e seu consumo real \u00e9 monitorado? (Link com V. Performance &amp; IX. Observabilidade)</li> <li>Crit\u00e9rio 8.8.3: Mecanismos de descoberta de servi\u00e7o (service discovery) e balanceamento de carga interno entre inst\u00e2ncias/servi\u00e7os s\u00e3o providos e gerenciados pelo orquestrador?</li> </ul>"},{"location":"quality/nala-iq-complete/#89-gerenciamento-de-configuracao-configuration-management","title":"8.9. Gerenciamento de Configura\u00e7\u00e3o (Configuration Management)","text":"<ul> <li>Crit\u00e9rio 8.9.1: Configura\u00e7\u00f5es que variam entre ambientes (ex: connection strings, URLs de servi\u00e7os externos, chaves de API, feature flags) s\u00e3o externalizadas e n\u00e3o fazem parte do artefato de build (imagem de container, pacote)?</li> <li>Crit\u00e9rio 8.9.2: A inje\u00e7\u00e3o de configura\u00e7\u00e3o espec\u00edfica do ambiente \u00e9 feita de forma segura durante o deploy ou inicializa\u00e7\u00e3o da aplica\u00e7\u00e3o (ex: via vari\u00e1veis de ambiente, segredos do orquestrador, ConfigMaps, servi\u00e7os de configura\u00e7\u00e3o centralizados)? (Link com II. Seguran\u00e7a)</li> <li>Crit\u00e9rio 8.9.3: Existe um processo controlado, versionado e audit\u00e1vel para gerenciar e aplicar altera\u00e7\u00f5es de configura\u00e7\u00e3o nos diferentes ambientes?</li> </ul>"},{"location":"quality/nala-iq-complete/#810-gerenciamento-de-release-e-rollback-release-rollback-management","title":"8.10. Gerenciamento de Release e Rollback (Release &amp; Rollback Management)","text":"<ul> <li>Crit\u00e9rio 8.10.1: O processo de release de novas vers\u00f5es da aplica\u00e7\u00e3o (incluindo comunica\u00e7\u00e3o, agendamento, coordena\u00e7\u00e3o entre equipes se necess\u00e1rio) \u00e9 bem definido? (Link com XII. Governan\u00e7a)</li> <li>Crit\u00e9rio 8.10.2: Existem procedimentos claros e testados (preferencialmente automatizados) para reverter (rollback) para uma vers\u00e3o anterior est\u00e1vel da aplica\u00e7\u00e3o rapidamente em caso de falha cr\u00edtica detectada ap\u00f3s um deploy?</li> <li>Crit\u00e9rio 8.10.3: A capacidade de realizar rollback \u00e9 validada periodicamente (ex: atrav\u00e9s de testes ou simula\u00e7\u00f5es)?</li> </ul>"},{"location":"quality/nala-iq-complete/#dimensao-ix-monitoramento-e-observabilidade","title":"Dimens\u00e3o IX: Monitoramento e Observabilidade","text":"<p>(Foco: Entender como a API se comporta em produ\u00e7\u00e3o, permitindo a detec\u00e7\u00e3o proativa de problemas, a depura\u00e7\u00e3o eficiente e a tomada de decis\u00f5es informadas para melhorias)</p>"},{"location":"quality/nala-iq-complete/#91-coleta-e-gerenciamento-de-logs-log-collection-management","title":"9.1. Coleta e Gerenciamento de Logs (Log Collection &amp; Management)","text":"<ul> <li>Crit\u00e9rio 9.1.1: Os logs gerados pela aplica\u00e7\u00e3o e sua infraestrutura s\u00e3o emitidos em um formato estruturado (ex: JSON) para facilitar a an\u00e1lise e processamento automatizado?</li> <li>Crit\u00e9rio 9.1.2: Os logs de todas as inst\u00e2ncias da API e componentes relevantes s\u00e3o centralizados em um sistema de gerenciamento de logs dedicado (ex: ELK Stack, Splunk, Grafana Loki, solu\u00e7\u00f5es de nuvem como CloudWatch Logs ou Azure Monitor Logs)?</li> <li>Crit\u00e9rio 9.1.3: Os logs cont\u00eam informa\u00e7\u00f5es contextuais suficientes (ex: timestamp preciso, n\u00edvel de severidade, identificador de servi\u00e7o/inst\u00e2ncia, ID de correla\u00e7\u00e3o/trace, mensagem informativa) para depura\u00e7\u00e3o e an\u00e1lise, sem incluir dados sens\u00edveis desprotegidos? (Link com II. Seguran\u00e7a)</li> <li>Crit\u00e9rio 9.1.4: A plataforma de logs oferece capacidades eficientes de busca, filtragem, agrega\u00e7\u00e3o e an\u00e1lise dos dados coletados?</li> <li>Crit\u00e9rio 9.1.5: Existem pol\u00edticas definidas para a reten\u00e7\u00e3o dos logs, considerando necessidades operacionais, de auditoria e conformidade? (Link com XIV. Compliance)</li> </ul>"},{"location":"quality/nala-iq-complete/#92-coleta-e-analise-de-metricas-metrics-collection-analysis","title":"9.2. Coleta e An\u00e1lise de M\u00e9tricas (Metrics Collection &amp; Analysis)","text":"<ul> <li>Crit\u00e9rio 9.2.1: M\u00e9tricas chave de performance do sistema (ex: uso de CPU, mem\u00f3ria, I/O de disco, tr\u00e1fego de rede) e da aplica\u00e7\u00e3o (ex: lat\u00eancia de requisi\u00e7\u00f5es [p50, p90, p99], taxa de transfer\u00eancia [RPS], taxa de erros \u2013 \"Golden Signals\"/RED) s\u00e3o coletadas continuamente? (Link com V. Performance)</li> <li>Crit\u00e9rio 9.2.2: M\u00e9tricas de neg\u00f3cio relevantes (ex: n\u00famero de transa\u00e7\u00f5es conclu\u00eddas, valor processado, usu\u00e1rios ativos) s\u00e3o coletadas e correlacionadas com as m\u00e9tricas t\u00e9cnicas, quando aplic\u00e1vel, para entender o impacto no neg\u00f3cio?</li> <li>Crit\u00e9rio 9.2.3: As m\u00e9tricas s\u00e3o armazenadas em um sistema de s\u00e9ries temporais (TSDB) apropriado (ex: Prometheus, InfluxDB, solu\u00e7\u00f5es de APM/Cloud) que permite consultas, agrega\u00e7\u00f5es e visualiza\u00e7\u00e3o de tend\u00eancias hist\u00f3ricas?</li> <li>Crit\u00e9rio 9.2.4: A granularidade da coleta e o per\u00edodo de reten\u00e7\u00e3o das m\u00e9tricas s\u00e3o adequados para as necessidades de diagn\u00f3stico, planejamento de capacidade e an\u00e1lise de longo prazo?</li> </ul>"},{"location":"quality/nala-iq-complete/#93-tracing-distribuido-distributed-tracing-reforco-distribuido","title":"9.3. Tracing Distribu\u00eddo (Distributed Tracing) (Refor\u00e7o Distribu\u00eddo)","text":"<ul> <li>Crit\u00e9rio 9.3.1: Em arquiteturas onde uma requisi\u00e7\u00e3o pode atravessar m\u00faltiplos servi\u00e7os ou componentes (microservi\u00e7os, SOA), o tracing distribu\u00eddo \u00e9 implementado (ex: usando padr\u00f5es como OpenTelemetry e ferramentas como Jaeger, Zipkin, Datadog APM, Lightstep) para rastrear o ciclo de vida completo da requisi\u00e7\u00e3o?</li> <li>Crit\u00e9rio 9.3.2: Identificadores de correla\u00e7\u00e3o (trace IDs, span IDs) s\u00e3o gerados na entrada da requisi\u00e7\u00e3o e propagados consistentemente atrav\u00e9s de todas as chamadas inter-servi\u00e7os (s\u00edncronas e ass\u00edncronas)?</li> <li>Crit\u00e9rio 9.3.3: Os dados de tracing permitem visualizar o caminho da requisi\u00e7\u00e3o, identificar gargalos de lat\u00eancia em servi\u00e7os espec\u00edficos ou depend\u00eancias, e entender as intera\u00e7\u00f5es entre os componentes?</li> </ul>"},{"location":"quality/nala-iq-complete/#94-alertas-e-notificacoes-alerting-notifications","title":"9.4. Alertas e Notifica\u00e7\u00f5es (Alerting &amp; Notifications)","text":"<ul> <li>Crit\u00e9rio 9.4.1: Alertas automatizados s\u00e3o configurados com base em limiares (thresholds) significativos e estatisticamente relevantes para m\u00e9tricas chave (ex: aumento s\u00fabito na taxa de erros, lat\u00eancia acima do SLO, satura\u00e7\u00e3o de recursos) e para eventos de log cr\u00edticos?</li> <li>Crit\u00e9rio 9.4.2: Os alertas s\u00e3o direcionados para os canais e equipes corretas (ex: via e-mail, Slack, Microsoft Teams, PagerDuty, OpsGenie) e cont\u00eam informa\u00e7\u00f5es contextuais suficientes (o qu\u00ea, quando, onde, severidade, links para dashboards) para facilitar a triagem e investiga\u00e7\u00e3o inicial?</li> <li>Crit\u00e9rio 9.4.3: Existe um processo para revisar e refinar os alertas regularmente (tuning) para minimizar falsos positivos (ru\u00eddo) e falsos negativos (alertas perdidos)?</li> <li>Crit\u00e9rio 9.4.4: Para alertas cr\u00edticos, existem playbooks ou runbooks documentados que orientam a equipe sobre os passos de investiga\u00e7\u00e3o e mitiga\u00e7\u00e3o? (Link com VI. Robustez)</li> </ul>"},{"location":"quality/nala-iq-complete/#95-dashboards-e-visualizacao-dashboards-visualization","title":"9.5. Dashboards e Visualiza\u00e7\u00e3o (Dashboards &amp; Visualization)","text":"<ul> <li>Crit\u00e9rio 9.5.1: Dashboards s\u00e3o criados e mantidos para fornecer uma vis\u00e3o consolidada e em tempo real (ou near real-time) da sa\u00fade e performance da API, combinando m\u00e9tricas, logs e status de componentes (ex: usando Grafana, Kibana, dashboards de ferramentas de APM ou cloud)?</li> <li>Crit\u00e9rio 9.5.2: Os dashboards s\u00e3o projetados para serem facilmente interpret\u00e1veis e atender \u00e0s necessidades de diferentes p\u00fablicos (ex: dashboards operacionais para SREs/DevOps, dashboards de vis\u00e3o geral para lideran\u00e7a, dashboards de performance de neg\u00f3cio)?</li> <li>Crit\u00e9rio 9.5.3: Os dashboards permitem a correla\u00e7\u00e3o visual entre diferentes tipos de dados (ex: logs e m\u00e9tricas, m\u00e9tricas de aplica\u00e7\u00e3o e infraestrutura) para facilitar a identifica\u00e7\u00e3o de causas raiz de problemas?</li> </ul>"},{"location":"quality/nala-iq-complete/#96-monitoramento-de-disponibilidade-e-erros-availability-error-monitoring","title":"9.6. Monitoramento de Disponibilidade e Erros (Availability &amp; Error Monitoring)","text":"<ul> <li>Crit\u00e9rio 9.6.1: A disponibilidade (uptime) e a performance b\u00e1sica da API s\u00e3o monitoradas continuamente a partir de pontos de observa\u00e7\u00e3o externos (synthetic monitoring ou black-box probing) para simular a experi\u00eancia do cliente?</li> <li>Crit\u00e9rio 9.6.2: Erros e exce\u00e7\u00f5es n\u00e3o tratadas na aplica\u00e7\u00e3o s\u00e3o capturados, agregados, agrupados e rastreados por ferramentas especializadas (ex: Sentry, Rollbar, Bugsnag, Application Insights) para facilitar a identifica\u00e7\u00e3o, prioriza\u00e7\u00e3o e resolu\u00e7\u00e3o de bugs?</li> <li>Crit\u00e9rio 9.6.3: Objetivos de N\u00edvel de Servi\u00e7o (SLOs) para disponibilidade e performance est\u00e3o definidos, e existe um processo para medir e reportar o cumprimento desses SLOs? (Link com XII. Governan\u00e7a)</li> </ul>"},{"location":"quality/nala-iq-complete/#dimensao-x-sustentabilidade-e-eficiencia","title":"Dimens\u00e3o X: Sustentabilidade e Efici\u00eancia","text":"<p>(Foco: Impacto ambiental e uso consciente dos recursos computacionais)</p>"},{"location":"quality/nala-iq-complete/#101-otimizacao-do-consumo-de-recursos-computacionais-computational-resource-consumption-optimization","title":"10.1. Otimiza\u00e7\u00e3o do Consumo de Recursos Computacionais (Computational Resource Consumption Optimization)","text":"<ul> <li>Crit\u00e9rio 10.1.1: A aplica\u00e7\u00e3o \u00e9 otimizada para minimizar o uso de CPU e mem\u00f3ria, evitando processos ociosos que consomem ciclos desnecessariamente ou aloca\u00e7\u00e3o excessiva de mem\u00f3ria? (Link com V. Performance)</li> <li>Crit\u00e9rio 10.1.2: O dimensionamento dos recursos de infraestrutura (ex: inst\u00e2ncias de VM, containers, bancos de dados) \u00e9 ajustado (ex: via auto-scaling ou revis\u00f5es peri\u00f3dicas) para alinhar-se \u00e0 demanda real, evitando subutiliza\u00e7\u00e3o cr\u00f4nica? (Link com VIII. DevOps &amp; V. Performance)</li> <li>Crit\u00e9rio 10.1.3: Recursos que n\u00e3o est\u00e3o em uso ativo (ex: ambientes de teste fora do hor\u00e1rio de trabalho, inst\u00e2ncias ociosas) s\u00e3o desligados, desalocados ou escalonados para zero automaticamente ou por processo definido?</li> <li>Crit\u00e9rio 10.1.4: O uso de opera\u00e7\u00f5es de I/O (disco e rede) \u00e9 otimizado para reduzir a quantidade de dados lidos/escritos e transferidos, minimizando o consumo energ\u00e9tico associado?</li> </ul>"},{"location":"quality/nala-iq-complete/#102-eficiencia-no-design-de-software-e-algoritmos-efficient-software-algorithmic-design","title":"10.2. Efici\u00eancia no Design de Software e Algoritmos (Efficient Software &amp; Algorithmic Design)","text":"<ul> <li>Crit\u00e9rio 10.2.1: Na escolha de algoritmos e estruturas de dados, especialmente para opera\u00e7\u00f5es frequentes ou de larga escala, a efici\u00eancia em termos de complexidade de tempo e espa\u00e7o \u00e9 considerada para reduzir o consumo de recursos?</li> <li>Crit\u00e9rio 10.2.2: O design da aplica\u00e7\u00e3o favorece padr\u00f5es que otimizam o uso de recursos (ex: processamento em lote para tarefas agendadas, lazy loading de dados, minimiza\u00e7\u00e3o de loops ou computa\u00e7\u00f5es redundantes)?</li> <li>Crit\u00e9rio 10.2.3: A arquitetura da solu\u00e7\u00e3o (ex: monol\u00edtica, microservi\u00e7os, serverless) \u00e9 reavaliada periodicamente quanto \u00e0 sua adequa\u00e7\u00e3o e efici\u00eancia de recursos frente ao perfil de uso e evolu\u00e7\u00e3o da aplica\u00e7\u00e3o?</li> </ul>"},{"location":"quality/nala-iq-complete/#103-eficiencia-no-uso-e-transferencia-de-dados-data-usage-transfer-efficiency","title":"10.3. Efici\u00eancia no Uso e Transfer\u00eancia de Dados (Data Usage &amp; Transfer Efficiency)","text":"<ul> <li>Crit\u00e9rio 10.3.1: A API \u00e9 projetada para transferir apenas os dados estritamente necess\u00e1rios para cada opera\u00e7\u00e3o, utilizando t\u00e9cnicas como proje\u00e7\u00e3o de campos (field selection) ou consultas espec\u00edficas (ex: GraphQL)? (Link com I. Design)</li> <li>Crit\u00e9rio 10.3.2: Mecanismos de compress\u00e3o de dados (ex: Gzip, Brotli) s\u00e3o aplicados nas respostas da API para reduzir o volume de dados trafegados pela rede? (Link com V. Performance)</li> <li>Crit\u00e9rio 10.3.3: Estrat\u00e9gias de cache em m\u00faltiplos n\u00edveis (cliente, CDN, API gateway, aplica\u00e7\u00e3o, banco de dados) s\u00e3o utilizadas para minimizar a recomputa\u00e7\u00e3o e a retransmiss\u00e3o de dados? (Link com V. Performance)</li> <li>Crit\u00e9rio 10.3.4: Pol\u00edticas de ciclo de vida dos dados (reten\u00e7\u00e3o, arquivamento, exclus\u00e3o) s\u00e3o implementadas para otimizar o volume de dados armazenados ativamente e reduzir o consumo de armazenamento? (Link com VII. Persist\u00eancia &amp; XIV. Compliance)</li> </ul>"},{"location":"quality/nala-iq-complete/#104-consideracoes-sobre-a-infraestrutura-fisica-e-energetica-physical-energy-infrastructure-considerations","title":"10.4. Considera\u00e7\u00f5es sobre a Infraestrutura F\u00edsica e Energ\u00e9tica (Physical &amp; Energy Infrastructure Considerations)","text":"<ul> <li>Crit\u00e9rio 10.4.1: Na escolha de provedores de servi\u00e7os de nuvem ou data centers (quando aplic\u00e1vel e h\u00e1 poder de decis\u00e3o), s\u00e3o considerados os compromissos do provedor com o uso de energia renov\u00e1vel e a efici\u00eancia energ\u00e9tica de suas instala\u00e7\u00f5es (ex: PUE - Power Usage Effectiveness)? (Este crit\u00e9rio pode ter aplicabilidade vari\u00e1vel)</li> <li>Crit\u00e9rio 10.4.2: A sele\u00e7\u00e3o de regi\u00f5es geogr\u00e1ficas para a hospedagem da API e dos dados leva em conta a proximidade com os usu\u00e1rios principais para reduzir a energia gasta na transmiss\u00e3o de dados, sempre que poss\u00edvel e alinhado com outros requisitos (ex: soberania de dados)?</li> <li>Crit\u00e9rio 10.4.3: Para cargas de trabalho que exigem hardware espec\u00edfico, s\u00e3o consideradas op\u00e7\u00f5es de hardware com maior efici\u00eancia energ\u00e9tica (performance por watt)?</li> </ul>"},{"location":"quality/nala-iq-complete/#105-praticas-de-desenvolvimento-e-operacoes-com-foco-em-sustentabilidade-sustainable-development-operations-practices","title":"10.5. Pr\u00e1ticas de Desenvolvimento e Opera\u00e7\u00f5es com Foco em Sustentabilidade (Sustainable Development &amp; Operations Practices)","text":"<ul> <li>Crit\u00e9rio 10.5.1: Os pipelines de CI/CD s\u00e3o otimizados para efici\u00eancia, por exemplo, usando builds e testes incrementais, caches de depend\u00eancias e liberando recursos de build/teste rapidamente ap\u00f3s o uso? (Link com VIII. DevOps)</li> <li>Crit\u00e9rio 10.5.2: Existe uma pol\u00edtica ou pr\u00e1tica para desligar ou reduzir drasticamente os recursos de ambientes de n\u00e3o-produ\u00e7\u00e3o (desenvolvimento, teste, homologa\u00e7\u00e3o) durante per\u00edodos de inatividade prolongada?</li> <li>Crit\u00e9rio 10.5.3: A escolha de bibliotecas, frameworks e ferramentas de terceiros leva em considera\u00e7\u00e3o, entre outros fatores, seu potencial impacto na efici\u00eancia de recursos e performance geral da aplica\u00e7\u00e3o?</li> </ul>"},{"location":"quality/nala-iq-complete/#106-conscientizacao-medicao-e-cultura-de-sustentabilidade-sustainability-awareness-measurement-culture","title":"10.6. Conscientiza\u00e7\u00e3o, Medi\u00e7\u00e3o e Cultura de Sustentabilidade (Sustainability Awareness, Measurement &amp; Culture)","text":"<ul> <li>Crit\u00e9rio 10.6.1: A equipe possui conscientiza\u00e7\u00e3o sobre os princ\u00edpios da Engenharia de Software Sustent\u00e1vel (Green Software Engineering) e o impacto ambiental associado \u00e0s decis\u00f5es de design, desenvolvimento e opera\u00e7\u00e3o de software?</li> <li>Crit\u00e9rio 10.6.2: S\u00e3o feitas tentativas de medir ou estimar o consumo de recursos (ex: CPU-horas, energia) ou a pegada de carbono da aplica\u00e7\u00e3o, utilizando ferramentas de provedores de nuvem (ex: AWS Carbon Footprint Tool, Azure Emissions Dashboard) ou outras metodologias, quando dispon\u00edveis?</li> <li>Crit\u00e9rio 10.6.3: A sustentabilidade e a efici\u00eancia de recursos s\u00e3o consideradas como fatores nas discuss\u00f5es de arquitetura, na escolha de tecnologias e no planejamento de novas funcionalidades?</li> <li>Crit\u00e9rio 10.6.4: S\u00e3o incentivadas e compartilhadas pr\u00e1ticas de codifica\u00e7\u00e3o e design que resultem em menor consumo de recursos e maior efici\u00eancia energ\u00e9tica?</li> </ul>"},{"location":"quality/nala-iq-complete/#dimensao-xi-integracao-com-sistemas-externos-incluindo-iamldados","title":"Dimens\u00e3o XI: Integra\u00e7\u00e3o com Sistemas Externos (incluindo IA/ML/Dados)","text":"<p>(Foco: Intera\u00e7\u00e3o da API com outros servi\u00e7os, incluindo plataformas de dados e modelos de IA/ML)</p>"},{"location":"quality/nala-iq-complete/#111-design-e-contratos-de-integracao-integration-design-contracts","title":"11.1. Design e Contratos de Integra\u00e7\u00e3o (Integration Design &amp; Contracts)","text":"<ul> <li>Crit\u00e9rio 11.1.1: As integra\u00e7\u00f5es com sistemas externos (outras APIs, servi\u00e7os de terceiros) s\u00e3o baseadas em contratos bem definidos (ex: especifica\u00e7\u00f5es OpenAPI, schemas de eventos, WSDLs)?</li> <li>Crit\u00e9rio 11.1.2: A API lida de forma clara com as diferentes vers\u00f5es dos sistemas externos com os quais se integra, quando aplic\u00e1vel?</li> <li>Crit\u00e9rio 11.1.3: O design da integra\u00e7\u00e3o minimiza o acoplamento desnecess\u00e1rio com os detalhes de implementa\u00e7\u00e3o dos sistemas externos?</li> </ul>"},{"location":"quality/nala-iq-complete/#112-resiliencia-e-performance-em-integracoes-resilience-performance-in-integrations","title":"11.2. Resili\u00eancia e Performance em Integra\u00e7\u00f5es (Resilience &amp; Performance in Integrations)","text":"<ul> <li>Crit\u00e9rio 11.2.1: Padr\u00f5es de toler\u00e2ncia a falhas (timeouts, retries, circuit breakers) s\u00e3o aplicados especificamente para chamadas a sistemas externos, considerando suas caracter\u00edsticas de lat\u00eancia e confiabilidade? (Link com VI. Robustez)</li> <li>Crit\u00e9rio 11.2.2: O impacto na performance da API devido \u00e0 lat\u00eancia de sistemas externos \u00e9 monitorado e gerenciado (ex: atrav\u00e9s de chamadas ass\u00edncronas, otimiza\u00e7\u00e3o de payloads)? (Link com V. Performance)</li> <li>Crit\u00e9rio 11.2.3: Estrat\u00e9gias de fallback ou degrada\u00e7\u00e3o graciosa s\u00e3o implementadas caso um sistema externo cr\u00edtico esteja indispon\u00edvel? (Link com VI. Robustez)</li> </ul>"},{"location":"quality/nala-iq-complete/#113-seguranca-em-integracoes-security-in-integrations","title":"11.3. Seguran\u00e7a em Integra\u00e7\u00f5es (Security in Integrations)","text":"<ul> <li>Crit\u00e9rio 11.3.1: A API utiliza mecanismos seguros de autentica\u00e7\u00e3o e autoriza\u00e7\u00e3o ao se comunicar com sistemas externos (ex: OAuth2 client credentials, mTLS, API Keys seguras)? (Link com II. Seguran\u00e7a)</li> <li>Crit\u00e9rio 11.3.2: Os dados trocados com sistemas externos, especialmente se sens\u00edveis, s\u00e3o protegidos em tr\u00e2nsito (ex: via TLS)? (Link com II. Seguran\u00e7a)</li> <li>Crit\u00e9rio 11.3.3: O escopo de acesso concedido a sistemas externos (ou pela API a eles) segue o princ\u00edpio do menor privil\u00e9gio?</li> </ul>"},{"location":"quality/nala-iq-complete/#114-integracao-com-modelos-de-iaml-via-api-api-for-aiml-model-integration","title":"11.4. Integra\u00e7\u00e3o com Modelos de IA/ML via API (API for AI/ML Model Integration)","text":"<ul> <li>Crit\u00e9rio 11.4.1: Se a API serve como um endpoint para modelos de IA/ML, ela fornece uma interface clara e bem definida para entrada (features) e sa\u00edda (predi\u00e7\u00f5es/scores) do modelo?</li> <li>Crit\u00e9rio 11.4.2: A API realiza valida\u00e7\u00e3o dos dados de entrada antes de pass\u00e1-los ao modelo de IA/ML e, se necess\u00e1rio, transforma ou valida a sa\u00edda do modelo antes de retorn\u00e1-la ao cliente?</li> <li>Crit\u00e9rio 11.4.3: A API possui mecanismos para lidar com diferentes vers\u00f5es de modelos de IA/ML (ex: roteamento para vers\u00f5es espec\u00edficas, A/B testing de modelos via API)? (Link com XII. Governan\u00e7a &amp; VIII. DevOps)</li> <li>Crit\u00e9rio 11.4.4: A lat\u00eancia de infer\u00eancia do modelo \u00e9 considerada no design da API e nos SLAs de performance? (Link com V. Performance)</li> </ul>"},{"location":"quality/nala-iq-complete/#115-interacao-com-plataformas-de-dados-e-pipelines-de-ml-interaction-with-data-platforms-ml-pipelines","title":"11.5. Intera\u00e7\u00e3o com Plataformas de Dados e Pipelines de ML (Interaction with Data Platforms &amp; ML Pipelines)","text":"<ul> <li>Crit\u00e9rio 11.5.1: Se a API fornece dados para pipelines de treinamento de ML ou plataformas de dados (ex: Data Lakes, Feature Stores), os contratos de dados e os mecanismos de acesso s\u00e3o bem definidos, seguros e eficientes?</li> <li>Crit\u00e9rio 11.5.2: Se a API consome dados ou insights de plataformas de dados (ex: para personaliza\u00e7\u00e3o), a integra\u00e7\u00e3o \u00e9 resiliente e perform\u00e1tica?</li> <li>Crit\u00e9rio 11.5.3: O versionamento dos dados ou datasets consumidos/providos pela API para fins de ML/Analytics \u00e9 considerado para garantir reprodutibilidade?</li> </ul>"},{"location":"quality/nala-iq-complete/#116-gerenciamento-e-observabilidade-de-endpoints-de-iaml-aiml-endpoint-management-observability","title":"11.6. Gerenciamento e Observabilidade de Endpoints de IA/ML (AI/ML Endpoint Management &amp; Observability)","text":"<ul> <li>Crit\u00e9rio 11.6.1: Existem m\u00e9tricas espec\u00edficas para monitorar a performance e a sa\u00fade dos endpoints da API que servem modelos de IA/ML (ex: lat\u00eancia de infer\u00eancia, taxa de erro do modelo, utiliza\u00e7\u00e3o de recursos dos servidores de infer\u00eancia)? (Link com IX. Observabilidade)</li> <li>Crit\u00e9rio 11.6.2: S\u00e3o coletados dados (logs/m\u00e9tricas) sobre as predi\u00e7\u00f5es do modelo (ex: distribui\u00e7\u00e3o dos scores, input features \u2013 de forma anonimizada se necess\u00e1rio) para permitir o monitoramento de desvio de modelo (model drift) ou de dados (data drift) pela equipe de ML? (Link com IX. Observabilidade &amp; II. Seguran\u00e7a)</li> <li>Crit\u00e9rio 11.6.3: Existe um processo para atualizar ou reverter os modelos de IA/ML expostos pela API de forma controlada e com m\u00ednimo impacto? (Link com VIII. DevOps &amp; XII. Governan\u00e7a)</li> </ul>"},{"location":"quality/nala-iq-complete/#dimensao-xii-governanca-e-ciclo-de-vida-da-api","title":"Dimens\u00e3o XII: Governan\u00e7a e Ciclo de Vida da API","text":"<p>(Foco: Gerenciamento estrat\u00e9gico, consistente e alinhado com os objetivos da organiza\u00e7\u00e3o ao longo do tempo de vida da API)</p>"},{"location":"quality/nala-iq-complete/#121-estrategia-e-propriedade-da-api-api-strategy-ownership","title":"12.1. Estrat\u00e9gia e Propriedade da API (API Strategy &amp; Ownership)","text":"<ul> <li>Crit\u00e9rio 12.1.1: Existe uma estrat\u00e9gia de APIs documentada e comunicada que alinha o desenvolvimento, a exposi\u00e7\u00e3o e a evolu\u00e7\u00e3o das APIs com os objetivos de neg\u00f3cio e a arquitetura tecnol\u00f3gica da organiza\u00e7\u00e3o?</li> <li>Crit\u00e9rio 12.1.2: Cada API possui um propriet\u00e1rio (Product Owner ou papel similar) claramente definido, respons\u00e1vel por sua vis\u00e3o estrat\u00e9gica, roadmap, prioriza\u00e7\u00e3o de funcionalidades e sucesso geral?</li> <li>Crit\u00e9rio 12.1.3: O valor de neg\u00f3cio (ex: gera\u00e7\u00e3o de receita, efici\u00eancia operacional, habilita\u00e7\u00e3o de novos produtos) ou o prop\u00f3sito estrat\u00e9gico de cada API \u00e9 claramente articulado e compreendido pelas partes interessadas?</li> </ul>"},{"location":"quality/nala-iq-complete/#122-politicas-de-versionamento-versioning-policies","title":"12.2. Pol\u00edticas de Versionamento (Versioning Policies)","text":"<ul> <li>Crit\u00e9rio 12.2.1: A organiza\u00e7\u00e3o possui uma pol\u00edtica de versionamento de APIs padronizada e documentada (ex: definindo o uso de versionamento sem\u00e2ntico [SemVer], como lidar com altera\u00e7\u00f5es compat\u00edveis [minor/patch] vs. incompat\u00edveis [major/breaking change])? (Link com I. Design)</li> <li>Crit\u00e9rio 12.2.2: A pol\u00edtica especifica claramente quais tipos de altera\u00e7\u00f5es s\u00e3o consideradas \"breaking changes\" e, portanto, exigem um incremento na vers\u00e3o principal da API?</li> <li>Crit\u00e9rio 12.2.3: Existe um processo definido para comunicar proativamente aos consumidores sobre o lan\u00e7amento de novas vers\u00f5es, as altera\u00e7\u00f5es introduzidas e o impacto esperado?</li> </ul>"},{"location":"quality/nala-iq-complete/#123-politicas-de-depreciacao-e-descontinuacao-deprecation-sunset-policies","title":"12.3. Pol\u00edticas de Deprecia\u00e7\u00e3o e Descontinua\u00e7\u00e3o (Deprecation &amp; Sunset Policies)","text":"<ul> <li>Crit\u00e9rio 12.3.1: Existe uma pol\u00edtica formal e documentada para a deprecia\u00e7\u00e3o (marking as obsolete) e eventual descontinua\u00e7\u00e3o (sunset/retirement) de vers\u00f5es de API que n\u00e3o s\u00e3o mais estrat\u00e9gicas ou suportadas?</li> <li>Crit\u00e9rio 12.3.2: A pol\u00edtica de deprecia\u00e7\u00e3o define um per\u00edodo m\u00ednimo de aviso pr\u00e9vio e suporte continuado para uma vers\u00e3o depreciada antes de sua descontinua\u00e7\u00e3o efetiva, permitindo tempo para os consumidores migrarem?</li> <li>Crit\u00e9rio 12.3.3: Os consumidores da API s\u00e3o notificados formalmente e com anteced\u00eancia sobre os planos de deprecia\u00e7\u00e3o/descontinua\u00e7\u00e3o, recebendo orienta\u00e7\u00f5es claras sobre como migrar para vers\u00f5es mais recentes ou alternativas?</li> </ul>"},{"location":"quality/nala-iq-complete/#124-acordos-de-nivel-de-servico-slasslos-e-contratos-de-uso-usage-contracts","title":"12.4. Acordos de N\u00edvel de Servi\u00e7o (SLAs/SLOs) e Contratos de Uso (Usage Contracts)","text":"<ul> <li>Crit\u00e9rio 12.4.1: Objetivos de N\u00edvel de Servi\u00e7o (SLOs) internos para disponibilidade, performance e taxa de erro est\u00e3o definidos, monitorados e revisados para as APIs cr\u00edticas? (Link com IX. Observabilidade, V. Performance, VI. Robustez)</li> <li>Crit\u00e9rio 12.4.2: Para APIs consumidas por parceiros externos ou clientes pagantes, Acordos de N\u00edvel de Servi\u00e7o (SLAs) formais s\u00e3o estabelecidos, especificando as garantias de servi\u00e7o, responsabilidades e poss\u00edveis penalidades?</li> <li>Crit\u00e9rio 12.4.3: Existem termos de uso ou contratos que definem as condi\u00e7\u00f5es de uso da API, incluindo limites de taxa (quotas), pol\u00edticas de uso aceit\u00e1vel e, se aplic\u00e1vel, modelos de precifica\u00e7\u00e3o ou custos?</li> </ul>"},{"location":"quality/nala-iq-complete/#125-gerenciamento-de-acesso-e-conformidade-de-uso-access-management-usage-compliance","title":"12.5. Gerenciamento de Acesso e Conformidade de Uso (Access Management &amp; Usage Compliance)","text":"<ul> <li>Crit\u00e9rio 12.5.1: Existe um processo definido e audit\u00e1vel para gerenciar o ciclo de vida do acesso dos consumidores \u00e0s APIs (solicita\u00e7\u00e3o, aprova\u00e7\u00e3o, provisionamento, revoga\u00e7\u00e3o)? (Link com II. Seguran\u00e7a)</li> <li>Crit\u00e9rio 12.5.2: As permiss\u00f5es de acesso concedidas s\u00e3o periodicamente revisadas para garantir que continuam adequadas (princ\u00edpio do menor privil\u00e9gio) e para remover acessos desnecess\u00e1rios ou obsoletos?</li> <li>Crit\u00e9rio 12.5.3: O uso das APIs \u00e9 monitorado para verificar a conformidade com os termos de servi\u00e7o, limites de taxa e identificar padr\u00f5es de uso que possam indicar abuso ou viola\u00e7\u00e3o de pol\u00edticas? (Link com II. Seguran\u00e7a &amp; IX. Observabilidade)</li> </ul>"},{"location":"quality/nala-iq-complete/#126-catalogoregistro-de-apis-e-padroes-de-design-api-catalogregistry-design-standards","title":"12.6. Cat\u00e1logo/Registro de APIs e Padr\u00f5es de Design (API Catalog/Registry &amp; Design Standards)","text":"<ul> <li>Crit\u00e9rio 12.6.1: Um cat\u00e1logo ou portal de APIs centralizado \u00e9 mantido, permitindo que desenvolvedores (internos e/ou externos) descubram facilmente as APIs dispon\u00edveis, suas funcionalidades, documenta\u00e7\u00e3o, status do ciclo de vida e como obter acesso? (Link com I. Documenta\u00e7\u00e3o &amp; XIII. DX)</li> <li>Crit\u00e9rio 12.6.2: A organiza\u00e7\u00e3o possui e promove guias de estilo de design de API (API design guidelines) que estabelecem padr\u00f5es para nomenclatura, estrutura de dados, tratamento de erros, versionamento, etc., visando consist\u00eancia e qualidade?</li> <li>Crit\u00e9rio 12.6.3: Existe um processo de revis\u00e3o de design de API (API design review) para garantir que novas APIs ou altera\u00e7\u00f5es significativas em APIs existentes estejam alinhadas com os padr\u00f5es e a estrat\u00e9gia da organiza\u00e7\u00e3o?</li> </ul>"},{"location":"quality/nala-iq-complete/#127-monitoramento-de-uso-e-metricas-de-negocio-da-api-api-usage-monitoring-business-metrics","title":"12.7. Monitoramento de Uso e M\u00e9tricas de Neg\u00f3cio da API (API Usage Monitoring &amp; Business Metrics)","text":"<ul> <li>Crit\u00e9rio 12.7.1: O padr\u00e3o de uso das APIs (ex: volume de chamadas por endpoint, consumidores mais ativos, picos de tr\u00e1fego, crescimento ao longo do tempo) \u00e9 sistematicamente monitorado e analisado? (Link com IX. Observabilidade)</li> <li>Crit\u00e9rio 12.7.2: S\u00e3o coletadas e analisadas m\u00e9tricas que correlacionam o uso da API com indicadores de neg\u00f3cio (KPIs), como o n\u00famero de transa\u00e7\u00f5es de neg\u00f3cio facilitadas, receita atribu\u00edvel, redu\u00e7\u00e3o de custos ou tempo de ciclo?</li> <li>Crit\u00e9rio 12.7.3: Os insights gerados pelo monitoramento de uso e pelas m\u00e9tricas de neg\u00f3cio s\u00e3o utilizados para direcionar o roadmap da API, otimizar funcionalidades existentes e identificar novas oportunidades?</li> </ul>"},{"location":"quality/nala-iq-complete/#128-comunicacao-com-stakeholders-e-coleta-de-feedback-stakeholder-communication-feedback-collection","title":"12.8. Comunica\u00e7\u00e3o com Stakeholders e Coleta de Feedback (Stakeholder Communication &amp; Feedback Collection)","text":"<ul> <li>Crit\u00e9rio 12.8.1: Canais de comunica\u00e7\u00e3o eficazes (ex: portais, f\u00f3runs, newsletters, reuni\u00f5es) s\u00e3o estabelecidos para manter os consumidores da API e outros stakeholders (internos e externos) informados sobre o status, roadmap, novas releases e mudan\u00e7as importantes?</li> <li>Crit\u00e9rio 12.8.2: Existem mecanismos formais ou informais para coletar feedback dos desenvolvedores consumidores sobre a usabilidade, funcionalidade, documenta\u00e7\u00e3o e performance da API?</li> <li>Crit\u00e9rio 12.8.3: O feedback recebido \u00e9 sistematicamente analisado, priorizado e incorporado no processo de planejamento e evolu\u00e7\u00e3o da API? (Link com XIII. DX)</li> </ul>"},{"location":"quality/nala-iq-complete/#dimensao-xiii-experiencia-do-desenvolvedor-dx","title":"Dimens\u00e3o XIII: Experi\u00eancia do Desenvolvedor (DX)","text":"<p>(Foco: Facilidade com que os desenvolvedores conseguem entender, integrar e obter valor da API)</p>"},{"location":"quality/nala-iq-complete/#131-descoberta-e-compreensao-da-api-api-discovery-understanding","title":"13.1. Descoberta e Compreens\u00e3o da API (API Discovery &amp; Understanding)","text":"<ul> <li>Crit\u00e9rio 13.1.1: As APIs da organiza\u00e7\u00e3o s\u00e3o facilmente localiz\u00e1veis por desenvolvedores interessados, por exemplo, atrav\u00e9s de um portal de desenvolvedores centralizado, cat\u00e1logo de APIs ou plataforma de API management? (Link com XII. Governan\u00e7a)</li> <li>Crit\u00e9rio 13.1.2: A proposta de valor, os casos de uso prim\u00e1rios e as capacidades fundamentais de cada API s\u00e3o comunicados de forma clara e concisa, permitindo que um desenvolvedor avalie rapidamente sua relev\u00e2ncia? (Link com I. Documenta\u00e7\u00e3o)</li> <li>Crit\u00e9rio 13.1.3: Informa\u00e7\u00f5es sobre o ciclo de vida da API (ex: status est\u00e1vel, beta, depreciada), pol\u00edticas de uso e requisitos de acesso s\u00e3o facilmente encontradas? (Link com XII. Governan\u00e7a)</li> </ul>"},{"location":"quality/nala-iq-complete/#132-facilidade-de-onboarding-e-primeiros-passos-ease-of-onboarding-getting-started","title":"13.2. Facilidade de Onboarding e Primeiros Passos (Ease of Onboarding &amp; Getting Started)","text":"<ul> <li>Crit\u00e9rio 13.2.1: O processo para um novo desenvolvedor obter as credenciais necess\u00e1rias (ex: API keys, tokens OAuth) e realizar sua primeira chamada bem-sucedida \u00e0 API \u00e9 simples, r\u00e1pido e claramente documentado?</li> <li>Crit\u00e9rio 13.2.2: Existem guias de \"in\u00edcio r\u00e1pido\" (getting started) ou tutoriais passo a passo que orientam o desenvolvedor atrav\u00e9s da configura\u00e7\u00e3o inicial e dos primeiros casos de uso?</li> <li>Crit\u00e9rio 13.2.3: \u00c9 oferecido um ambiente de sandbox ou de teste que permite aos desenvolvedores experimentar a API de forma segura e isolada, sem afetar dados ou sistemas de produ\u00e7\u00e3o? (Link com VIII. DevOps)</li> </ul>"},{"location":"quality/nala-iq-complete/#133-qualidade-da-documentacao-e-recursos-de-aprendizagem-documentation-learning-resource-quality","title":"13.3. Qualidade da Documenta\u00e7\u00e3o e Recursos de Aprendizagem (Documentation &amp; Learning Resource Quality)","text":"<ul> <li>Crit\u00e9rio 13.3.1: A documenta\u00e7\u00e3o de refer\u00eancia da API (detalhando endpoints, m\u00e9todos, par\u00e2metros, estruturas de requisi\u00e7\u00e3o/resposta, c\u00f3digos de erro, modelos de autentica\u00e7\u00e3o) \u00e9 completa, precisa, atualizada e f\u00e1cil de consultar? (Link com I. Documenta\u00e7\u00e3o)</li> <li>Crit\u00e9rio 13.3.2: S\u00e3o fornecidos exemplos de c\u00f3digo pr\u00e1ticos, funcionais e copi\u00e1veis (snippets) nas linguagens de programa\u00e7\u00e3o mais relevantes para o p\u00fablico-alvo da API?</li> <li>Crit\u00e9rio 13.3.3: A documenta\u00e7\u00e3o \u00e9 interativa, permitindo que os desenvolvedores testem chamadas \u00e0 API diretamente da documenta\u00e7\u00e3o (ex: via interfaces como Swagger UI, Redoc, Postman Collections)?</li> <li>Crit\u00e9rio 13.3.4: Existem recursos de aprendizagem adicionais como tutoriais detalhados, receitas (cookbooks) para cen\u00e1rios comuns, estudos de caso, FAQs ou gloss\u00e1rios para facilitar o entendimento aprofundado?</li> </ul>"},{"location":"quality/nala-iq-complete/#134-ferramentas-de-suporte-ao-desenvolvedor-sdks-e-ferramentas-developer-support-tools-sdks-tooling","title":"13.4. Ferramentas de Suporte ao Desenvolvedor (SDKs e Ferramentas) (Developer Support Tools - SDKs &amp; Tooling)","text":"<ul> <li>Crit\u00e9rio 13.4.1: S\u00e3o fornecidas bibliotecas cliente ou SDKs (Software Development Kits) bem mantidos, versionados, idiom\u00e1ticos e de f\u00e1cil instala\u00e7\u00e3o/uso para as linguagens e plataformas mais importantes para os desenvolvedores consumidores, abstraindo a complexidade da comunica\u00e7\u00e3o HTTP e autentica\u00e7\u00e3o?</li> <li>Crit\u00e9rio 13.4.2: A documenta\u00e7\u00e3o dos SDKs \u00e9 clara, completa e inclui exemplos de uso?</li> <li>Crit\u00e9rio 13.4.3: Existem outras ferramentas que auxiliam o desenvolvedor (ex: CLIs para interagir com a API, plugins para IDEs, ferramentas de mock/simula\u00e7\u00e3o para testes, cole\u00e7\u00f5es Postman/Insomnia pr\u00e9-configuradas)?</li> </ul>"},{"location":"quality/nala-iq-complete/#135-design-da-api-orientado-a-usabilidade-api-design-for-usability","title":"13.5. Design da API Orientado \u00e0 Usabilidade (API Design for Usability)","text":"<ul> <li>Crit\u00e9rio 13.5.1: O design da API (nomenclatura, estrutura de URIs, formatos de dados, fluxos de intera\u00e7\u00e3o) \u00e9 intuitivo, consistente e previs\u00edvel, seguindo o princ\u00edpio da menor surpresa e minimizando a carga cognitiva para o desenvolvedor? (Link com I. Design)</li> <li>Crit\u00e9rio 13.5.2: As mensagens de erro retornadas pela API s\u00e3o claras, informativas para o desenvolvedor (indicando o que deu errado e, idealmente, como corrigir) e n\u00e3o exp\u00f5em informa\u00e7\u00f5es sens\u00edveis? (Link com I. Design &amp; II. Seguran\u00e7a)</li> <li>Crit\u00e9rio 13.5.3: A API \u00e9 projetada para ser o mais autoexplicativa poss\u00edvel, reduzindo a depend\u00eancia de consultas extensivas \u00e0 documenta\u00e7\u00e3o para tarefas comuns?</li> </ul>"},{"location":"quality/nala-iq-complete/#136-canais-de-suporte-e-comunidade-support-channels-community","title":"13.6. Canais de Suporte e Comunidade (Support Channels &amp; Community)","text":"<ul> <li>Crit\u00e9rio 13.6.1: Existem canais de suporte t\u00e9cnico dedicados e claramente comunicados para os desenvolvedores que consomem a API (ex: f\u00f3rum de discuss\u00e3o, sistema de help desk/tickets, e-mail de suporte, chat)?</li> <li>Crit\u00e9rio 13.6.2: O tempo de resposta e a qualidade do suporte oferecido atendem \u00e0s expectativas dos desenvolvedores e aos SLAs de suporte (se existentes)?</li> <li>Crit\u00e9rio 13.6.3: \u00c9 fomentada a cria\u00e7\u00e3o de uma comunidade em torno da API (interna ou externa), onde os desenvolvedores podem compartilhar conhecimento, fazer perguntas e colaborar?</li> </ul>"},{"location":"quality/nala-iq-complete/#137-coleta-e-gestao-de-feedback-do-desenvolvedor-developer-feedback-collection-management","title":"13.7. Coleta e Gest\u00e3o de Feedback do Desenvolvedor (Developer Feedback Collection &amp; Management)","text":"<ul> <li>Crit\u00e9rio 13.7.1: S\u00e3o disponibilizados mecanismos f\u00e1ceis e acess\u00edveis para que os desenvolvedores consumidores possam submeter feedback, reportar bugs, ou sugerir melhorias para a API, documenta\u00e7\u00e3o ou SDKs? (Link com XII. Governan\u00e7a)</li> <li>Crit\u00e9rio 13.7.2: Existe um processo estabelecido para coletar, analisar, priorizar e responder ao feedback dos desenvolvedores?</li> <li>Crit\u00e9rio 13.7.3: Os desenvolvedores s\u00e3o informados sobre o status de seus feedbacks (ex: bug corrigido, sugest\u00e3o incorporada ao roadmap) e sentem que suas contribui\u00e7\u00f5es s\u00e3o valorizadas?</li> </ul>"},{"location":"quality/nala-iq-complete/#dimensao-xiv-compliance-e-regulamentacoes","title":"Dimens\u00e3o XIV: Compliance e Regulamenta\u00e7\u00f5es","text":"<p>(Foco: Garantir que as obriga\u00e7\u00f5es legais e os padr\u00f5es da ind\u00fastria sejam atendidos)</p>"},{"location":"quality/nala-iq-complete/#141-identificacao-e-gerenciamento-de-requisitos-regulatorios-regulatory-requirement-identification-management","title":"14.1. Identifica\u00e7\u00e3o e Gerenciamento de Requisitos Regulat\u00f3rios (Regulatory Requirement Identification &amp; Management)","text":"<ul> <li>Crit\u00e9rio 14.1.1: Existe um processo formal para identificar, documentar e manter atualizado o conjunto de todas as leis, regulamentos, normas e padr\u00f5es contratuais de conformidade aplic\u00e1veis \u00e0 API, aos dados que ela processa e aos mercados em que opera?</li> <li>Crit\u00e9rio 14.1.2: As responsabilidades pela supervis\u00e3o e garantia da conformidade regulat\u00f3ria da API est\u00e3o claramente definidas e atribu\u00eddas dentro da organiza\u00e7\u00e3o (ex: a um Data Protection Officer - DPO, equipe de compliance, jur\u00eddico)?</li> <li>Crit\u00e9rio 14.1.3: S\u00e3o realizadas avalia\u00e7\u00f5es peri\u00f3dicas de impacto regulat\u00f3rio e de conformidade, especialmente antes de grandes altera\u00e7\u00f5es na API, entrada em novos mercados ou manipula\u00e7\u00e3o de novos tipos de dados sens\u00edveis?</li> </ul>"},{"location":"quality/nala-iq-complete/#142-privacidade-de-dados-e-conformidade-data-privacy-compliance-ex-lgpd-gdpr","title":"14.2. Privacidade de Dados e Conformidade (Data Privacy &amp; Compliance) (Ex: LGPD, GDPR)","text":"<ul> <li>Crit\u00e9rio 14.2.1: A API e os sistemas associados s\u00e3o projetados e operados seguindo os princ\u00edpios de privacidade desde a concep\u00e7\u00e3o e por padr\u00e3o (Privacy by Design &amp; by Default), como minimiza\u00e7\u00e3o da coleta de dados, limita\u00e7\u00e3o da finalidade, transpar\u00eancia, e seguran\u00e7a dos dados pessoais?</li> <li>Crit\u00e9rio 14.2.2: Existem mecanismos t\u00e9cnicos e processuais implementados para gerenciar o consentimento dos titulares dos dados (quando o consentimento \u00e9 a base legal) e para atender efetivamente aos seus direitos (ex: acesso, corre\u00e7\u00e3o, portabilidade, esquecimento/elimina\u00e7\u00e3o de dados pessoais)?</li> <li>Crit\u00e9rio 14.2.3: As pol\u00edticas de privacidade que descrevem como os dados pessoais s\u00e3o tratados pela API s\u00e3o claras, completas, facilmente acess\u00edveis aos titulares e mantidas atualizadas?</li> <li>Crit\u00e9rio 14.2.4: S\u00e3o conduzidas Avalia\u00e7\u00f5es de Impacto \u00e0 Prote\u00e7\u00e3o de Dados (AIPD/DPIA) para atividades de tratamento de dados pessoais que possam envolver alto risco aos direitos dos titulares?</li> </ul>"},{"location":"quality/nala-iq-complete/#143-controles-de-seguranca-de-dados-para-conformidade-data-security-controls-for-compliance","title":"14.3. Controles de Seguran\u00e7a de Dados para Conformidade (Data Security Controls for Compliance)","text":"<ul> <li>Crit\u00e9rio 14.3.1: S\u00e3o implementados e mantidos controles de seguran\u00e7a t\u00e9cnica e organizacional espec\u00edficos exigidos por regulamentos ou padr\u00f5es aplic\u00e1veis (ex: criptografia para dados em tr\u00e2nsito e em repouso, autentica\u00e7\u00e3o multifator, controle de acesso rigoroso, mascaramento/pseudonimiza\u00e7\u00e3o de dados)? (Link com II. Seguran\u00e7a &amp; VII. Persist\u00eancia)</li> <li>Crit\u00e9rio 14.3.2: Existem planos de resposta a incidentes documentados e testados para lidar com viola\u00e7\u00f5es de dados pessoais (data breaches), incluindo procedimentos para investiga\u00e7\u00e3o, conten\u00e7\u00e3o, erradica\u00e7\u00e3o, recupera\u00e7\u00e3o e notifica\u00e7\u00e3o \u00e0s autoridades e aos titulares afetados, conforme exigido por lei?</li> <li>Crit\u00e9rio 14.3.3: A efic\u00e1cia dos controles de seguran\u00e7a de dados \u00e9 regularmente validada atrav\u00e9s de auditorias de seguran\u00e7a, testes de penetra\u00e7\u00e3o ou certifica\u00e7\u00f5es de conformidade (quando aplic\u00e1vel ou exigido)?</li> </ul>"},{"location":"quality/nala-iq-complete/#144-residencia-soberania-e-localizacao-de-dados-data-residency-sovereignty-localization","title":"14.4. Resid\u00eancia, Soberania e Localiza\u00e7\u00e3o de Dados (Data Residency, Sovereignty &amp; Localization)","text":"<ul> <li>Crit\u00e9rio 14.4.1: Os requisitos legais e contratuais relativos \u00e0 resid\u00eancia (onde os dados s\u00e3o armazenados), soberania (jurisdi\u00e7\u00e3o legal sobre os dados) e localiza\u00e7\u00e3o de dados s\u00e3o compreendidos e explicitamente gerenciados pela arquitetura da API e sua infraestrutura de hospedagem?</li> <li>Crit\u00e9rio 14.4.2: Existem mecanismos t\u00e9cnicos e/ou organizacionais para garantir que os dados sejam armazenados e/ou processados em regi\u00f5es geogr\u00e1ficas espec\u00edficas, conforme as exig\u00eancias?</li> <li>Crit\u00e9rio 14.4.3: As implica\u00e7\u00f5es e os requisitos legais para transfer\u00eancias internacionais de dados s\u00e3o compreendidos e gerenciados de acordo com as regulamenta\u00e7\u00f5es aplic\u00e1veis (ex: cl\u00e1usulas contratuais padr\u00e3o, adequa\u00e7\u00e3o da legisla\u00e7\u00e3o do pa\u00eds de destino)?</li> </ul>"},{"location":"quality/nala-iq-complete/#145-trilhas-de-auditoria-e-capacidade-de-reporte-audit-trails-reporting-capabilities","title":"14.5. Trilhas de Auditoria e Capacidade de Reporte (Audit Trails &amp; Reporting Capabilities)","text":"<ul> <li>Crit\u00e9rio 14.5.1: Trilhas de auditoria detalhadas, seguras (protegidas contra adultera\u00e7\u00e3o) e com timestamps precisos s\u00e3o mantidas para registrar eventos significativos, como acesso a dados sens\u00edveis, modifica\u00e7\u00f5es em configura\u00e7\u00f5es de seguran\u00e7a, opera\u00e7\u00f5es administrativas e transa\u00e7\u00f5es cr\u00edticas? (Link com IX. Observabilidade)</li> <li>Crit\u00e9rio 14.5.2: A API e os sistemas de suporte possuem a capacidade de gerar relat\u00f3rios ou extrair dados de auditoria de forma eficiente para demonstrar conformidade com requisitos regulat\u00f3rios ou para investiga\u00e7\u00f5es internas/externas?</li> <li>Crit\u00e9rio 14.5.3: As trilhas de auditoria s\u00e3o retidas pelo per\u00edodo estipulado por pol\u00edticas internas ou exig\u00eancias legais/regulat\u00f3rias, e o acesso a elas \u00e9 restrito e controlado?</li> </ul>"},{"location":"quality/nala-iq-complete/#146-conformidade-com-padroes-especificos-da-industria-industry-specific-standards-compliance-ex-pci-dss-hipaa-open-finance","title":"14.6. Conformidade com Padr\u00f5es Espec\u00edficos da Ind\u00fastria (Industry-Specific Standards Compliance) (Ex: PCI-DSS, HIPAA, Open Finance)","text":"<ul> <li>Crit\u00e9rio 14.6.1: Caso a API opere em um setor sujeito a padr\u00f5es de conformidade espec\u00edficos da ind\u00fastria (ex: PCI-DSS para o setor de pagamentos, HIPAA para sa\u00fade nos EUA, BACEN para Open Finance no Brasil), esses padr\u00f5es s\u00e3o identificados e todos os controles t\u00e9cnicos e processuais necess\u00e1rios s\u00e3o implementados e mantidos?</li> <li>Crit\u00e9rio 14.6.2: A conformidade com esses padr\u00f5es de ind\u00fastria \u00e9 regularmente validada e atestada (ex: por autoavalia\u00e7\u00e3o, auditorias internas, ou certifica\u00e7\u00f5es por auditores externos qualificados), conforme o padr\u00e3o exige?</li> <li>Crit\u00e9rio 14.6.3: As equipes envolvidas no desenvolvimento, opera\u00e7\u00e3o e suporte da API recebem treinamento cont\u00ednuo sobre os requisitos dos padr\u00f5es de ind\u00fastria aplic\u00e1veis ao seu trabalho?</li> </ul>"},{"location":"quality/nala-iq-complete/#147-gerenciamento-de-riscos-e-conformidade-de-terceiros-third-party-risk-compliance-management","title":"14.7. Gerenciamento de Riscos e Conformidade de Terceiros (Third-Party Risk &amp; Compliance Management)","text":"<ul> <li>Crit\u00e9rio 14.7.1: Se a funcionalidade da API ou o tratamento de dados regulados depende de servi\u00e7os, componentes ou fornecedores terceirizados, existe um processo de due diligence para avaliar os riscos de seguran\u00e7a e a postura de conformidade desses terceiros antes da contrata\u00e7\u00e3o e periodicamente? (Link com XI. Integra\u00e7\u00e3o)</li> <li>Crit\u00e9rio 14.7.2: Os contratos com fornecedores terceirizados incluem cl\u00e1usulas espec\u00edficas sobre prote\u00e7\u00e3o de dados, seguran\u00e7a da informa\u00e7\u00e3o, confidencialidade e conformidade com as regulamenta\u00e7\u00f5es aplic\u00e1veis?</li> <li>Crit\u00e9rio 14.7.3: O acesso de terceiros a dados atrav\u00e9s da API (ou o acesso da API a dados de terceiros) \u00e9 monitorado e gerenciado para garantir que esteja alinhado com as pol\u00edticas, consentimentos e requisitos contratuais?</li> </ul>"},{"location":"quality/nala-iq-complete/#conclusao-do-nala-iq-core-v1","title":"Conclus\u00e3o do Nala-IQ Core v1","text":"<p>Com a defini\u00e7\u00e3o dos crit\u00e9rios para estas 14 dimens\u00f5es, conclu\u00edmos a primeira vers\u00e3o do Nala-IQ Core. Este framework representa um corpo de conhecimento abrangente para guiar o desenvolvimento, a avalia\u00e7\u00e3o e a evolu\u00e7\u00e3o de APIs modernas, seguras, resilientes, eficientes e alinhadas com as melhores pr\u00e1ticas da ind\u00fastria e as necessidades de neg\u00f3cio.</p>"},{"location":"quality/nala-iq-criteria/","title":"Nala iq criteria","text":"<p>Crit\u00e9rio: Comunica\u00e7\u00e3o via TLS 1.3 Dom\u00ednio: Seguran\u00e7a Sub-dom\u00ednio: Transporte seguro N\u00edvel de Impacto: Alto Peso: 5 Benef\u00edcios (atendido):   - Curto prazo: protege contra ataques MITM   - Longo prazo: garante compliance regulat\u00f3rio Riscos (n\u00e3o atendido):   - Curto prazo: exposi\u00e7\u00e3o a ataques   - Longo prazo: n\u00e3o conformidade (GDPR/PCI) Relacionados: Logging seguro, Gest\u00e3o de segredos Depend\u00eancias: Certificados v\u00e1lidos (Governan\u00e7a PKI) M\u00e9tricas sugeridas: % de endpoints com TLS &gt;= 1.3 Categoria: Obrigat\u00f3rio Normas de refer\u00eancia: OWASP, ISO 27001 Aplicabilidade: Todas as APIs p\u00fablicas e privadas A\u00e7\u00f5es recomendadas:   - Configurar load balancer para TLS 1.3   - Automatizar renova\u00e7\u00e3o de certificados (ex.: Let's Encrypt) Exemplo pr\u00e1tico: Configura\u00e7\u00e3o Nginx com ssl_protocols TLSv1.3 Links \u00fateis:   - https://owasp.org/API-Security/   - https://nginx.org/en/docs/http/configuring_https_servers.html Observa\u00e7\u00f5es: Usar monitoramento para validar expira\u00e7\u00e3o de certificados</p>"},{"location":"quality/nala-iq-diagnostico-v1/","title":"\ud83e\udde0 Nala-IQ | Relat\u00f3rio Diagn\u00f3stico de Maturidade da API","text":"<p>Vers\u00e3o da Avalia\u00e7\u00e3o: v1 Data da Avalia\u00e7\u00e3o: [Preencher] API Avaliada: [Nome da API] Organiza\u00e7\u00e3o: [Nome da Organiza\u00e7\u00e3o] Equipe Respons\u00e1vel: [Time/Nomes dos Participantes] Avaliador(es): [Nomes] </p>"},{"location":"quality/nala-iq-diagnostico-v1/#sumario-executivo","title":"\ud83d\udcca Sum\u00e1rio Executivo","text":"<p>Este relat\u00f3rio apresenta o diagn\u00f3stico de maturidade da API com base no framework Nala-IQ Core v1, composto por 14 dimens\u00f5es estrat\u00e9gicas. A avalia\u00e7\u00e3o foi conduzida utilizando uma abordagem h\u00edbrida, combinando autoavalia\u00e7\u00e3o, an\u00e1lise de dados automatizados, coleta documental e sess\u00f5es de valida\u00e7\u00e3o com avaliadores.</p> <p>Cada dimens\u00e3o foi pontuada em uma escala de 0 a 3:</p> <ul> <li>0 \u2013 N\u00e3o Atende</li> <li>1 \u2013 Atende Parcialmente</li> <li>2 \u2013 Atende Plenamente</li> <li>3 \u2013 Supera Expectativas</li> </ul>"},{"location":"quality/nala-iq-diagnostico-v1/#niveis-de-maturidade","title":"\ud83d\udd39 N\u00edveis de Maturidade","text":"Faixa de Pontua\u00e7\u00e3o N\u00edvel de Maturidade Descri\u00e7\u00e3o 0.00 \u2013 0.60 N\u00edvel 1: Inicial Pr\u00e1ticas ad-hoc ou ausentes 0.61 \u2013 1.20 N\u00edvel 2: Em Desenvolvimento Implementa\u00e7\u00f5es b\u00e1sicas e inconsistentes 1.21 \u2013 1.80 N\u00edvel 3: Definido Pr\u00e1ticas definidas e aplicadas regularmente 1.81 \u2013 2.40 N\u00edvel 4: Gerenciado Monitoramento, melhoria cont\u00ednua 2.41 \u2013 3.00 N\u00edvel 5: Otimizado Refer\u00eancia no setor, pr\u00e1ticas exemplares"},{"location":"quality/nala-iq-diagnostico-v1/#radar-de-maturidade-por-dimensao","title":"\ud83d\udee0\ufe0f Radar de Maturidade por Dimens\u00e3o","text":"<p>(Inserir gr\u00e1fico radar aqui, se aplic\u00e1vel. No caso de uso em ferramentas como Notion, usar emojis ou barras visuais para representar.)</p>"},{"location":"quality/nala-iq-diagnostico-v1/#maturidade-por-dimensao","title":"\ud83d\udcd8 Maturidade por Dimens\u00e3o","text":""},{"location":"quality/nala-iq-diagnostico-v1/#i-design-e-documentacao-da-api","title":"I. Design e Documenta\u00e7\u00e3o da API","text":"<p>M\u00e9dia da Dimens\u00e3o: [pontua\u00e7\u00e3o] N\u00edvel: [n\u00edvel de maturidade] - Pontos fortes: [...] - Oportunidades de melhoria: [...]</p>"},{"location":"quality/nala-iq-diagnostico-v1/#ii-seguranca","title":"II. Seguran\u00e7a","text":"<p>M\u00e9dia da Dimens\u00e3o: [pontua\u00e7\u00e3o] N\u00edvel: [n\u00edvel de maturidade] [...]</p> <p>(Repetir para as 14 dimens\u00f5es)</p>"},{"location":"quality/nala-iq-diagnostico-v1/#conclusoes-e-recomendacoes","title":"\ud83d\udd0e Conclus\u00f5es e Recomenda\u00e7\u00f5es","text":"<p>Pontos Fortes Globais: - [...]</p> <p>\u00c1reas Priorit\u00e1rias para Evolu\u00e7\u00e3o: - [...]</p> <p>Recomenda\u00e7\u00f5es de Curto Prazo: - [...]</p> <p>Recomenda\u00e7\u00f5es de Longo Prazo: - [...]</p>"},{"location":"quality/nala-iq-diagnostico-v1/#anexos","title":"\ud83d\udcce Anexos","text":"<ul> <li>Planilha de Autoavalia\u00e7\u00e3o (Question\u00e1rio Nala-IQ)</li> <li>Evid\u00eancias Documentais</li> <li>Relat\u00f3rios Automatizados</li> </ul>"},{"location":"quality/nala-iq-evaluation/","title":"\ud83d\udcca Avalia\u00e7\u00e3o T\u00e9cnica \u2014 Projeto <code>athomic-docs</code> via Nala-IQ","text":"<p>Esta avalia\u00e7\u00e3o foi realizada com base nos documentos <code>Nala-IQPY.pdf</code>, <code>nala-core-agent-instructions.md</code> e <code>snapshot.txt</code>.</p>"},{"location":"quality/nala-iq-evaluation/#visao-geral","title":"\ud83e\udde0 Vis\u00e3o Geral","text":"<p>O framework Nala-IQ define 14 dimens\u00f5es para avalia\u00e7\u00e3o de APIs robustas, escal\u00e1veis e seguras. A seguir, uma an\u00e1lise de cobertura e maturidade do projeto <code>athomic-docs</code> com base nestas dimens\u00f5es.</p>"},{"location":"quality/nala-iq-evaluation/#mapa-de-cobertura-por-dimensao","title":"\ud83e\udde9 Mapa de Cobertura por Dimens\u00e3o","text":"Dimens\u00e3o Nala-IQ Status Evid\u00eancias / Observa\u00e7\u00f5es 1. Design e Documenta\u00e7\u00e3o da API \ud83d\udfe2 Parcial Uso de ADRs, organiza\u00e7\u00e3o RESTful, documenta\u00e7\u00e3o inicial no <code>docs/</code>. Falta padroniza\u00e7\u00e3o completa de erros e versionamento via path. 2. Seguran\u00e7a \ud83d\udfe2 Parcial Tokens mascarados, secrets abstra\u00eddos, headers seguros. Faltam pol\u00edticas de CORS e gest\u00e3o de sess\u00e3o. 3. Qualidade de C\u00f3digo \ud83d\udfe2 Avan\u00e7ado Tipagem com <code>mypy</code>, lint com <code>ruff</code>, organiza\u00e7\u00e3o modular, testes. 4. Testes e Garantia de Qualidade \ud83d\udfe2 Parcial Testes unit\u00e1rios fortes, integra\u00e7\u00e3o em andamento. Cobertura com Redis, Consul e fallback. 5. Escalabilidade e Performance \ud83d\udfe2 Em Progresso Uso de Redis, TTL configur\u00e1vel, estrutura leve e desacoplada. 6. Robustez e Confiabilidade \ud83d\udfe2 Em Progresso <code>@retry_handler</code>, <code>@fallback_handler</code>, suporte a timeout. Planejado: circuit breaker mais robusto. 7. Persist\u00eancia e Dados \u26aa\ufe0f N\u00e3o Avaliado Pouco evidenciado. Uso de <code>beanie</code> com MongoDB, sem camada de reposit\u00f3rio clara. 8. Build/Deploy &amp; DevOps \ud83d\udfe1 Inicial <code>Makefile</code>, scripts de setup, CI/CD parcial. 9. Observabilidade \ud83d\udfe2 Avan\u00e7ado Logs estruturados, m\u00e9tricas Prometheus, tracing OTEL, health checks. 10. Sustentabilidade e Efici\u00eancia \ud83d\udfe2 Em Progresso Reuso de c\u00f3digo, configura\u00e7\u00e3o modular, baixo acoplamento. 11. Integra\u00e7\u00e3o com Sistemas Externos \ud83d\udfe2 Avan\u00e7ado Abstra\u00e7\u00f5es e providers para Redis, Consul, Kafka, Vault. 12. Governan\u00e7a e Ciclo de Vida \ud83d\udfe1 Inicial Versionamento parcial, changelog presente. Sem ciclo completo de deprecia\u00e7\u00e3o. 13. Experi\u00eancia do Dev (DX) \ud83d\udfe2 Avan\u00e7ado Estrutura clara, onboarding, templates e CLI de suporte. 14. Compliance e Regulamenta\u00e7\u00f5es \ud83d\udfe1 Inicial C\u00f3digo de conduta presente. Falta gest\u00e3o de dados sens\u00edveis e reten\u00e7\u00e3o."},{"location":"quality/nala-iq-evaluation/#pontos-fortes","title":"\u2705 Pontos Fortes","text":"<ul> <li>Modularidade robusta (<code>athomic/</code> com <code>config</code>, <code>secrets</code>, <code>observability</code>, <code>integration</code>);</li> <li>Agente definido com escopo claro e orientado por Nala-IQ;</li> <li>Documenta\u00e7\u00e3o clara no estilo de projetos maduros;</li> <li>Preparado para escalar com m\u00faltiplos providers e fallback.</li> </ul>"},{"location":"quality/nala-iq-evaluation/#oportunidades-de-melhoria","title":"\ud83d\udee0\ufe0f Oportunidades de Melhoria","text":"\u00c1rea A\u00e7\u00e3o recomendada Testes de integra\u00e7\u00e3o com FastAPI Adicionar coverage de APIs p\u00fablicas e falhas Persist\u00eancia Definir camada de reposit\u00f3rios e abstra\u00e7\u00f5es Governance de APIs Adotar versionamento via <code>/v1/</code>, <code>/v2/</code> LGPD/GDPR Criar pol\u00edticas de reten\u00e7\u00e3o, anonimiza\u00e7\u00e3o e consentimento Automa\u00e7\u00e3o de verifica\u00e7\u00e3o Integrar SonarQube, <code>radon</code> ou similares Fallback inteligente Evoluir para <code>FallbackChain</code> por m\u00f3dulo Chaos Testing Considerar <code>chaostoolkit</code> para valida\u00e7\u00e3o de resili\u00eancia"},{"location":"quality/nala-iq-evaluation/#proximas-etapas","title":"\ud83d\ude80 Pr\u00f3ximas Etapas","text":"<ul> <li>Gerar backlog t\u00e9cnico baseado nessa avalia\u00e7\u00e3o;</li> <li>Incluir crit\u00e9rios Nala-IQ no PR template;</li> <li>Iniciar revis\u00e3o de m\u00f3dulos ausentes (ex: persist\u00eancia, circuit breaker);</li> <li>Atualizar documenta\u00e7\u00e3o com escopo e roadmap por dimens\u00e3o.</li> </ul> <p>Documento gerado automaticamente com base no snapshot do projeto e framework Nala-IQ.</p>"},{"location":"quality/nala-iq-extension/","title":"Nala iq extension","text":""},{"location":"quality/nala-iq-extension/#avaliacao-geral-do-documento","title":"Avalia\u00e7\u00e3o Geral do Documento","text":"<p>O documento Nala-IQ Core v1 \u00e9 uma ferramenta excepcionalmente detalhada, abrangente e bem-estruturada. Trata-se de um framework de alt\u00edssimo calibre para a avalia\u00e7\u00e3o da maturidade e qualidade de APIs, cobrindo praticamente todo o ciclo de vida de uma API, desde a concep\u00e7\u00e3o estrat\u00e9gica at\u00e9 a opera\u00e7\u00e3o e descontinua\u00e7\u00e3o.</p> <p>\u00c9 evidente que foi criado com um profundo conhecimento de arquitetura de software, engenharia, seguran\u00e7a e governan\u00e7a. A sua natureza agn\u00f3stica a tecnologias espec\u00edficas o torna uma ferramenta universal e duradoura.</p>"},{"location":"quality/nala-iq-extension/#pontos-fortes","title":"Pontos Fortes","text":"<ul> <li><code>Completude e Abrang\u00eancia</code>: As 14 dimens\u00f5es cobrem aspectos t\u00e9cnicos, operacionais, estrat\u00e9gicos e de governan\u00e7a de forma hol\u00edstica. T\u00f3picos modernos e cruciais como Sustentabilidade (X), Experi\u00eancia do Desenvolvedor (XIII) e Compliance (XIV) s\u00e3o tratados como dimens\u00f5es de primeira classe, o que eleva o framework para al\u00e9m de um simples checklist t\u00e9cnico.</li> <li><code>Estrutura L\u00f3gica e Granularidade</code>: A hierarquia Dimens\u00e3o &gt; Sub-dimens\u00e3o &gt; Crit\u00e9rio \u00e9 clara e facilita a aplica\u00e7\u00e3o do framework. Os crit\u00e9rios s\u00e3o, em sua maioria, perguntas fechadas ou que levam a uma an\u00e1lise objetiva, tornando a avalia\u00e7\u00e3o mais pr\u00e1tica e menos subjetiva.</li> <li><code>Crit\u00e9rios Acion\u00e1veis</code>: Os crit\u00e9rios n\u00e3o s\u00e3o vagos. Por exemplo, em vez de dizer \"A API deve ser segura\", o framework detalha crit\u00e9rios espec\u00edficos como \"A comunica\u00e7\u00e3o com a API requer obrigatoriamente o uso de um protocolo de transporte seguro (TLS 1.2+, preferencialmente 1.3)?\" (Crit\u00e9rio 2.4.1). Isso torna os gaps identificados diretamente acion\u00e1veis.</li> <li><code>Conectividade entre Dimens\u00f5es</code>: O uso de anota\u00e7\u00f5es como (Link com...) e (Refor\u00e7o Distribu\u00eddo) \u00e9 um diferencial not\u00e1vel. Isso demonstra uma compreens\u00e3o madura de que essas dimens\u00f5es n\u00e3o s\u00e3o silos isolados; por exemplo, a seguran\u00e7a de dados (Dimens\u00e3o II) est\u00e1 intrinsecamente ligada \u00e0 persist\u00eancia (Dimens\u00e3o VII) e ao compliance (Dimens\u00e3o XIV).</li> <li><code>Foco em Arquiteturas Modernas</code>: A inclus\u00e3o de crit\u00e9rios espec\u00edficos para arquiteturas orientadas a eventos e sistemas distribu\u00eddos (microservi\u00e7os) mostra que o framework est\u00e1 alinhado com as pr\u00e1ticas e desafios atuais da engenharia de software.</li> </ul>"},{"location":"quality/nala-iq-extension/#pontos-de-melhoria-potencial-refinamentos","title":"Pontos de Melhoria Potencial (Refinamentos)","text":"<p>O framework j\u00e1 \u00e9 excelente, mas as seguintes sugest\u00f5es poderiam torn\u00e1-lo ainda mais pr\u00e1tico:</p> <ul> <li><code>Sistema de Pontua\u00e7\u00e3o ou N\u00edvel de Maturidade</code>: O framework apresenta os crit\u00e9rios, mas n\u00e3o sugere como pontuar ou classificar uma API. A adi\u00e7\u00e3o de um modelo de maturidade (ex: N\u00edvel 1 a 5 para cada dimens\u00e3o) ou um sistema de pontua\u00e7\u00e3o (ex: atribuindo pesos diferentes aos crit\u00e9rios com base em sua criticidade) poderia ajudar as equipes a quantificar a evolu\u00e7\u00e3o da qualidade de suas APIs ao longo do tempo e a comparar diferentes APIs de forma mais objetiva.</li> <li><code>Mecanismo de Prioriza\u00e7\u00e3o/Contextualiza\u00e7\u00e3o</code>: Nem todos os 147+ crit\u00e9rios ter\u00e3o a mesma import\u00e2ncia para todas as APIs. Uma API interna e n\u00e3o cr\u00edtica pode ter requisitos de governan\u00e7a e DX diferentes de uma API p\u00fablica e monetizada. O framework poderia sugerir um m\u00e9todo para que as equipes ponderem as dimens\u00f5es e os crit\u00e9rios com base no contexto da API (p\u00fablico-alvo, criticidade para o neg\u00f3cio, tipo de dados manipulados, etc.).</li> <li><code>Guia de Aplica\u00e7\u00e3o com Exemplos</code>: Incluir um ap\u00eandice com um ou dois exemplos de avalia\u00e7\u00e3o de uma API hipot\u00e9tica (ex: \"API de Cat\u00e1logo de Produtos\", \"API de Pagamentos\") poderia ilustrar como aplicar os crit\u00e9rios na pr\u00e1tica, como documentar os achados e como derivar um plano de a\u00e7\u00e3o.</li> </ul>"},{"location":"quality/nala-iq-extension/#quais-areas-ainda-podemos-adicionar","title":"Quais \u00e1reas ainda podemos adicionar?","text":"<p>Apesar de sua vastid\u00e3o, existem algumas \u00e1reas estrat\u00e9gicas e operacionais que poderiam ser formalizadas como novas dimens\u00f5es ou sub-dimens\u00f5es para tornar o Nala-IQ ainda mais completo.</p> <p>Aqui est\u00e3o duas sugest\u00f5es principais para novas dimens\u00f5es:</p>"},{"location":"quality/nala-iq-extension/#dimensao-xv-custo-e-viabilidade-financeira-finops","title":"Dimens\u00e3o XV: Custo e Viabilidade Financeira (FinOps)","text":"<p>(Foco: An\u00e1lise do custo total de propriedade (TCO), retorno sobre o investimento (ROI) e efici\u00eancia financeira da API.)</p> <p>Atualmente, a dimens\u00e3o de Sustentabilidade foca na efici\u00eancia de recursos, o que tem um impacto financeiro indireto. Uma dimens\u00e3o dedicada ao FinOps traria uma perspectiva de neg\u00f3cio crucial.</p> <ul> <li><code>Crit\u00e9rio 15.1. Rastreamento de Custos (Cost Tracking)</code>: Existem mecanismos para medir e atribuir os custos de infraestrutura (computa\u00e7\u00e3o, armazenamento, rede, servi\u00e7os de nuvem) diretamente \u00e0 opera\u00e7\u00e3o da API?</li> <li><code>Crit\u00e9rio 15.2. An\u00e1lise de Custo Total de Propriedade (TCO Analysis)</code>: O TCO da API (incluindo custos de desenvolvimento, manuten\u00e7\u00e3o, infraestrutura e suporte) \u00e9 calculado e revisado periodicamente?</li> <li><code>Crit\u00e9rio 15.3. Estrat\u00e9gia de Monetiza\u00e7\u00e3o (se aplic\u00e1vel)</code>: Para APIs externas, existe um modelo de precifica\u00e7\u00e3o claro e sustent\u00e1vel (ex: por chamada, subscri\u00e7\u00e3o, por volume de dados)? Como o uso \u00e9 medido para faturamento?</li> <li><code>Crit\u00e9rio 15.4. Otimiza\u00e7\u00e3o de Custos</code>: S\u00e3o realizadas an\u00e1lises proativas para otimizar os custos da API (ex: escolha de inst\u00e2ncias mais baratas, uso de inst\u00e2ncias spot/preemptible, negocia\u00e7\u00e3o com provedores) sem violar os SLOs de performance e disponibilidade?</li> <li><code>Crit\u00e9rio 15.5. An\u00e1lise de Retorno sobre o Investimento (ROI)</code>: O valor de neg\u00f3cio gerado pela API (seja receita direta, efici\u00eancia operacional, ou habilita\u00e7\u00e3o de novos produtos) \u00e9 medido e comparado com seu TCO para avaliar seu ROI?</li> </ul>"},{"location":"quality/nala-iq-extension/#dimensao-xvi-pessoas-e-cultura-organizacional","title":"Dimens\u00e3o XVI: Pessoas e Cultura Organizacional","text":"<p>(Foco: Avaliar as capacidades, processos e cultura da equipe respons\u00e1vel pela API, que s\u00e3o a base para a qualidade e evolu\u00e7\u00e3o do produto.)</p> <p>O framework avalia \"o que\" foi constru\u00eddo, mas n\u00e3o \"quem\" construiu e \"como\" trabalham juntos. Uma cultura forte \u00e9 um pr\u00e9-requisito para a excel\u00eancia cont\u00ednua.</p> <ul> <li><code>Crit\u00e9rio 16.1. Capacita\u00e7\u00e3o e Habilidades da Equipe (Team Skills &amp; Enablement)</code>: A equipe possui as habilidades t\u00e9cnicas e de neg\u00f3cio necess\u00e1rias para o ciclo de vida completo da API? Existe um plano para treinamento e desenvolvimento cont\u00ednuo?</li> <li><code>Crit\u00e9rio 16.2. Colabora\u00e7\u00e3o e Comunica\u00e7\u00e3o</code>: Os processos de colabora\u00e7\u00e3o entre diferentes pap\u00e9is (Dev, QA, Ops, PO, Seguran\u00e7a) s\u00e3o eficientes? A comunica\u00e7\u00e3o sobre decis\u00f5es t\u00e9cnicas e de produto \u00e9 transparente?</li> <li><code>Crit\u00e9rio 16.3. Ownership e Responsabilidade</code>: A equipe demonstra um forte senso de propriedade (ownership) sobre a API, sentindo-se respons\u00e1vel por sua qualidade, performance e sucesso em produ\u00e7\u00e3o?</li> <li><code>Crit\u00e9rio 16.4. Cultura de Melhoria Cont\u00ednua</code>: A equipe realiza retrospectivas ou post-mortems de forma regular e utiliza os aprendizados para melhorar processos, ferramentas e o pr\u00f3prio produto? Existe seguran\u00e7a psicol\u00f3gica para apontar falhas e sugerir melhorias?</li> <li><code>Crit\u00e9rio 16.5. Ado\u00e7\u00e3o e Suporte a Padr\u00f5es</code>: A equipe compreende e adere ativamente aos padr\u00f5es de design, seguran\u00e7a e governan\u00e7a definidos pela organiza\u00e7\u00e3o (mencionados no framework Nala-IQ)?</li> </ul>"},{"location":"quality/nala-iq-extension/#outras-sugestoes-sub-dimensoes","title":"Outras Sugest\u00f5es (Sub-dimens\u00f5es)","text":"<ul> <li><code>Acessibilidade (Accessibility - a11y)</code>: Poderia ser uma nova sub-dimens\u00e3o dentro da Dimens\u00e3oentas associadas sejam acess\u00edveis a  XIII: Experi\u00eancia do Desenvolvedor. Foco em garantir que o Portal do Desenvolvedor, a documenta\u00e7\u00e3o e outras ferrampessoas com defici\u00eancia, seguindo padr\u00f5es como o WCAG.</li> </ul> <p>Em resumo, o Nala-IQ Core v1 j\u00e1 \u00e9 um ativo estrat\u00e9gico de valor imensur\u00e1vel. A adi\u00e7\u00e3o de perspectivas sobre Custo (FinOps) e Pessoas/Cultura o transformaria em uma ferramenta de avalia\u00e7\u00e3o 360\u00ba, cobrindo n\u00e3o apenas o produto digital em si, mas tamb\u00e9m sua viabilidade financeira e o alicerce humano que o sustenta.</p>"},{"location":"quality/nala-iq-metas/","title":"Plano de Submiss\u00e3o (90 dias) \u2014 Nala-IQ: Score e Framework \u201cal\u00e9m do estado da arte\u201d","text":"<p>Objetivo: transformar o Nala-IQ em refer\u00eancia de mercado e acad\u00eamica, publicando (i) artigo de experi\u00eancia e (ii) artigo t\u00e9cnico com valida\u00e7\u00e3o emp\u00edrica, com pacote de replica\u00e7\u00e3o aberto (open-science) e estrat\u00e9gia de IP definida.</p>"},{"location":"quality/nala-iq-metas/#1-metas-e-produtos-finais","title":"1) Metas e produtos finais","text":"<p>Em 90 dias, entregar: - \u2705 Artigo \u201cExperience/Practice\u201d (execu\u00e7\u00e3o industrial e impacto) \u2014 pronto para um peri\u00f3dico/venue applied (ex.: IEEE Software / experi\u00eancia industrial) OU talk em evento (apidays/OWASP/SREcon). - \u2705 Artigo t\u00e9cnico (m\u00e9todo + dados) com avalia\u00e7\u00e3o emp\u00edrica, an\u00e1lises ablation e pacote de replica\u00e7\u00e3o (c\u00f3digo + dados anonimizados + Docker). - \u2705 Pacote de replica\u00e7\u00e3o (open): reposit\u00f3rio p\u00fablico com DOI (ex.: Zenodo) contendo c\u00f3digo, dados anonimizados, instru\u00e7\u00f5es e scripts de an\u00e1lise. - \u2705 Kit de comunica\u00e7\u00e3o: p\u00e1gina no site, infogr\u00e1ficos, video-short (2\u20133 min). - \u2705 IP: pedido de marca \u201cNala-IQ\u201d no INPI, registro de software no INPI (c\u00f3digo do scoring), pol\u00edtica de licen\u00e7as (open-core).</p>"},{"location":"quality/nala-iq-metas/#2-publicos-e-venues-alvo-sem-travar-em-datas","title":"2) P\u00fablicos e venues alvo (sem travar em datas)","text":"<ul> <li>Peri\u00f3dicos / applied: IEEE Software (experience/insights), JSS (Journal of Systems and Software), EMSE (Empirical Software Engineering).  </li> <li>Confer\u00eancias: ICSA (arquitetura de software), IEEE SERVICES/ICWS (servi\u00e7os e APIs), SREcon (SLO/error budget), OWASP Global AppSec (seguran\u00e7a de APIs), apidays (pr\u00e1tica de APIs).   <p>Datas de CFP variam; checar janelas correntes na Semana 1 e escolher 2 alvos principais (1 peri\u00f3dico + 1 confer\u00eancia) de acordo com o cronograma.</p> </li> </ul>"},{"location":"quality/nala-iq-metas/#3-contribuicao-cientificatecnica-mensagem-central","title":"3) Contribui\u00e7\u00e3o cient\u00edfica/t\u00e9cnica (mensagem central)","text":"<ul> <li>Score em camadas + penalidade assim\u00e9trica + gating, com Trust Delta (self vs. evidence), Percent-to-Target, Anti-Goodhart, propaga\u00e7\u00e3o por depend\u00eancias e intervalo de confian\u00e7a.  </li> <li>Valida\u00e7\u00e3o emp\u00edrica em m\u00faltiplas empresas/setores; correla\u00e7\u00e3o com SLO/OTel, MTTR, incidentes e m\u00e9tricas DORA.  </li> <li>Reprodutibilidade: pacote de replica\u00e7\u00e3o completo e crit\u00e9rios verific\u00e1veis (linters, scanners, queries OTel).</li> </ul>"},{"location":"quality/nala-iq-metas/#4-trilhas-de-trabalho-workstreams","title":"4) Trilhas de trabalho (workstreams)","text":""},{"location":"quality/nala-iq-metas/#ws1-metodo-scoring","title":"WS1 \u2014 M\u00e9todo &amp; Scoring","text":"<ul> <li>Formalizar as f\u00f3rmulas (Base/Temporal/Environmental + FMEA + Gating + PT-Target + Goodhart Guard).  </li> <li>Definir parametriza\u00e7\u00e3o padr\u00e3o (\u03b1, thresholds, \u03bb, perfis).  </li> <li>Implementar <code>/v1/score</code> com explainability (decomposi\u00e7\u00e3o da pontua\u00e7\u00e3o, penalidades, flags, IC).</li> </ul>"},{"location":"quality/nala-iq-metas/#ws2-evidencias-coleta","title":"WS2 \u2014 Evid\u00eancias &amp; Coleta","text":"<ul> <li>Conectores de evid\u00eancia (TLS scanner, OpenAPI lint, SLO via OTel, supply-chain/SBOM).  </li> <li>Pipeline de anonymiza\u00e7\u00e3o (IDs, hashing, filtros).  </li> <li>Question\u00e1rio de Self-Report mapeado aos crit\u00e9rios.</li> </ul>"},{"location":"quality/nala-iq-metas/#ws3-dados-benchmark","title":"WS3 \u2014 Dados &amp; Benchmark","text":"<ul> <li>Selecionar 3\u20135 empresas (opt-in, NDA) em setores distintos.  </li> <li>Medir score baseline e retest (30\u201360 dias) ap\u00f3s corre\u00e7\u00f5es.  </li> <li>Calcular quartis/medianas por dimens\u00e3o (baseline Nala-IQ) e correlacionar com SLO/DORA.</li> </ul>"},{"location":"quality/nala-iq-metas/#ws4-experimentos-analises","title":"WS4 \u2014 Experimentos &amp; An\u00e1lises","text":"<ul> <li>Ablation (retirar penalidade/gating/anti-Goodhart e observar piora na prioriza\u00e7\u00e3o).  </li> <li>Inter-rater reliability (consist\u00eancia entre avaliadores).  </li> <li>An\u00e1lises estat\u00edsticas (efeito, IC, gr\u00e1ficos).</li> </ul>"},{"location":"quality/nala-iq-metas/#ws5-artigos-artefatos","title":"WS5 \u2014 Artigos &amp; Artefatos","text":"<ul> <li>Paper t\u00e9cnico (m\u00e9todo + dados) e experience paper (hist\u00f3rias, impacto).  </li> <li>Figuras e tabelas (ver se\u00e7\u00e3o 7).  </li> <li>Pacote de replica\u00e7\u00e3o com DOI + guia \u201cReproduza os resultados\u201d.</li> </ul>"},{"location":"quality/nala-iq-metas/#ws6-ip-licencas","title":"WS6 \u2014 IP &amp; Licen\u00e7as","text":"<ul> <li>Marca Nala-IQ (INPI), registro de software (INPI), licen\u00e7as:  </li> <li>C\u00f3digo: Apache-2.0 (ou BSL se preferir open-core).  </li> <li>Conte\u00fado/figuras: CC-BY-NC.  </li> <li>Modelos/benchmarks premium: segredo de neg\u00f3cio.</li> </ul>"},{"location":"quality/nala-iq-metas/#ws7-comms-site","title":"WS7 \u2014 Comms &amp; Site","text":"<ul> <li>P\u00e1gina \u201cResearch\u201d com resumo, infogr\u00e1ficos, e link para replica\u00e7\u00e3o.  </li> <li>Blog post \u201cpor tr\u00e1s do score\u201d + \u201cresultados preliminares\u201d.  </li> <li>Slides deck e v\u00eddeo 2\u20133 min.</li> </ul>"},{"location":"quality/nala-iq-metas/#5-linha-do-tempo-90-dias","title":"5) Linha do tempo (90 dias)","text":""},{"location":"quality/nala-iq-metas/#semanas-12","title":"Semanas 1\u20132","text":"<ul> <li>Travar venues alvo e requisitos; montar protocolo de estudo (m\u00e9todo/\u00e9tica/anonimiza\u00e7\u00e3o).  </li> <li>Fechar <code>scoring_config.yml</code> e <code>/v1/score</code> (MVP).  </li> <li>Selecionar 3\u20135 empresas piloto e assinar NDAs.  </li> <li>Instrumentar conectores (TLS/OpenAPI/OTel).  </li> <li>Sa\u00eddas: Spec v1, protocolo, NDA, lista de pilotos, endpoint <code>/v1/score</code> MVP.</li> </ul>"},{"location":"quality/nala-iq-metas/#semanas-34","title":"Semanas 3\u20134","text":"<ul> <li>Coletar baseline (Self + Evidence) nas empresas piloto.  </li> <li>Rodar primeiro batch de an\u00e1lises explorat\u00f3rias.  </li> <li>Esqueleto do paper t\u00e9cnico e do experience paper.  </li> <li>Sa\u00eddas: dataset v0 (anonimizado), outline de artigos, primeiros gr\u00e1ficos.</li> </ul>"},{"location":"quality/nala-iq-metas/#semanas-58","title":"Semanas 5\u20138","text":"<ul> <li>Execu\u00e7\u00e3o dos experimentos (ablation, inter-rater, correla\u00e7\u00e3o SLO/DORA).  </li> <li>Implementar IC (bootstrap) e Goodhart Guard no score.  </li> <li>Preparar figuras finais e tabelas.  </li> <li>Sa\u00eddas: resultados consolidados, figuras/tabelas, pacote de replica\u00e7\u00e3o v0.</li> </ul>"},{"location":"quality/nala-iq-metas/#semanas-910","title":"Semanas 9\u201310","text":"<ul> <li>Reda\u00e7\u00e3o completa (R1) dos dois artigos.  </li> <li>Revis\u00e3o interna + revis\u00e3o por pares externos (2\u20133 reviewers).  </li> <li>Polir replication package (Docker + scripts reprodut\u00edveis + DOI).</li> </ul>"},{"location":"quality/nala-iq-metas/#semanas-1112","title":"Semanas 11\u201312","text":"<ul> <li>Vers\u00e3o final (R2), checagem de checklists dos venues.  </li> <li>Submiss\u00e3o 1 (peri\u00f3dico) + Submiss\u00e3o 2 (confer\u00eancia/talk).  </li> <li>Publicar landing research no site + blog post + infogr\u00e1ficos.</li> </ul>"},{"location":"quality/nala-iq-metas/#6-entregaveis-checklist","title":"6) Entreg\u00e1veis (checklist)","text":"<ul> <li>[ ] <code>scoring_config.yml</code> final  </li> <li>[ ] API <code>/v1/score</code> (com explainability + IC)  </li> <li>[ ] Conectores de evid\u00eancia (TLS, OpenAPI, OTel)  </li> <li>[ ] Dataset anonimizados (v1) + dicion\u00e1rio de dados  </li> <li>[ ] Relat\u00f3rios estat\u00edsticos (notebooks/scripts)  </li> <li>[ ] Paper t\u00e9cnico (PDF) + experience paper (PDF)  </li> <li>[ ] Replication package (c\u00f3digo + dados + Docker + DOI)  </li> <li>[ ] Infogr\u00e1ficos e slides deck  </li> <li>[ ] P\u00e1gina \u201cResearch\u201d no site  </li> <li>[ ] Pedido de marca (INPI) + registro de software</li> </ul>"},{"location":"quality/nala-iq-metas/#7-figuras-e-tabelas-pre-planejadas","title":"7) Figuras e Tabelas (pr\u00e9-planejadas)","text":"<p>Figuras 1. Arquitetura do score (camadas) 2. Pipeline de evid\u00eancias (Self vs Evidence) + Trust Delta 3. Mecanismo de penalidade FMEA + Gating (fluxo) 4. Anti-Goodhart (exemplo de dampening) 5. Radar por dimens\u00e3o (antes/depois) 6. Gr\u00e1ficos de correla\u00e7\u00e3o (score \u00d7 SLO/DORA/MTTR)</p> <p>Tabelas 1. Par\u00e2metros do score (\u03b1, thresholds, \u03bb, perfis) 2. Matriz S/O/D por crit\u00e9rio-chave 3. Resultados por empresa (anon) e por dimens\u00e3o 4. Ablation study (efeito de remover componentes) 5. Inter-rater reliability (\u03ba/ICC) 6. Percent-to-Target por dimens\u00e3o e quartis (benchmark)</p>"},{"location":"quality/nala-iq-metas/#8-pacote-de-replicacao-open-science","title":"8) Pacote de replica\u00e7\u00e3o (open-science)","text":"<p>Conte\u00fado m\u00ednimo: - <code>/code</code>: implementa\u00e7\u00e3o do score + scripts de an\u00e1lise - <code>/data</code>: datasets anonimizados (CSV/Parquet) + README + data-dict - <code>/docker</code>: Dockerfile + <code>make reproduce</code> - <code>/notebooks</code>: explora\u00e7\u00e3o, ablation, estat\u00edstica, gera\u00e7\u00e3o de figuras - <code>/artifact-metadata</code>: guia de avalia\u00e7\u00e3o, vers\u00e3o, hash, instru\u00e7\u00f5es - DOI (ex.: Zenodo) e licen\u00e7a: code Apache-2.0 / data CC-BY-NC</p>"},{"location":"quality/nala-iq-metas/#9-ip-etica-e-compliance","title":"9) IP, \u00e9tica e compliance","text":"<ul> <li>Marca \u201cNala-IQ\u201d (INPI) \u2014 pedido imediato.  </li> <li>Registro de software (INPI) \u2014 c\u00f3digo do score/pipelines.  </li> <li>Licen\u00e7as: c\u00f3digo Apache-2.0 (ou BSL), conte\u00fado CC-BY-NC, benchmarks premium como segredo.  </li> <li>\u00c9tica/privacidade: NDA com pilotos, anonimiza\u00e7\u00e3o, remo\u00e7\u00e3o de PII, checklist LGPD.  </li> <li>Defensive publication: subir preprint (ex.: arXiv/Zenodo) ap\u00f3s submiss\u00e3o formal.</li> </ul>"},{"location":"quality/nala-iq-metas/#10-estrutura-dos-artigos-resumo","title":"10) Estrutura dos artigos (resumo)","text":""},{"location":"quality/nala-iq-metas/#paper-tecnico-metodo-dados","title":"Paper t\u00e9cnico (m\u00e9todo + dados)","text":"<ol> <li>Introdu\u00e7\u00e3o (problema, lacunas, contribui\u00e7\u00f5es)  </li> <li>Trabalhos relacionados (SAMM/BSIMM, CVSS, DORA, SRE)  </li> <li>M\u00e9todo Nala-IQ (f\u00f3rmulas, componentes, IC, anti-Goodhart)  </li> <li>Dataset e protocolo (empresas, coleta, anonimiza\u00e7\u00e3o)  </li> <li>Resultados (correla\u00e7\u00f5es, ablation, \u03ba/ICC, PT-Target)  </li> <li>Amea\u00e7as \u00e0 validade e limita\u00e7\u00f5es  </li> <li>Conclus\u00f5es e disponibilidade (replication package, DOI)</li> </ol>"},{"location":"quality/nala-iq-metas/#experience-paper-talk","title":"Experience paper / Talk","text":"<ul> <li>Contexto, dores, desenho do score, \u201ccomo implementamos\u201d, resultados pr\u00e1ticos (antes/depois), ROI/tempo, li\u00e7\u00f5es aprendidas.</li> </ul>"},{"location":"quality/nala-iq-metas/#11-riscos-e-mitigacao","title":"11) Riscos e mitiga\u00e7\u00e3o","text":"<ul> <li>Baixa ades\u00e3o de pilotos \u2192 ampliar funil e incentivos (diagn\u00f3stico gratuito; insights exclusivos).  </li> <li>Dados ruidosos ou parciais \u2192 refor\u00e7ar evid\u00eancia autom\u00e1tica e normaliza\u00e7\u00e3o; IC para incerteza.  </li> <li>Prazos/CFP \u2192 planejar vers\u00e3o \u201cjournal-first\u201d (rolling) + talk (rolling CFPs em eventos de API).  </li> <li>Revisores pedirem mais evid\u00eancia \u2192 manter backlog de empresas e preparar 2\u00ba batch.</li> </ul>"},{"location":"quality/nala-iq-metas/#12-proximos-passos-imediatos-esta-semana","title":"12) Pr\u00f3ximos passos imediatos (esta semana)","text":"<ul> <li>[ ] Escolher 1 peri\u00f3dico + 1 confer\u00eancia/talk alvo e baixar os templates.  </li> <li>[ ] Finalizar <code>scoring_config.yml</code> e publicar <code>/v1/score</code> (MVP com explainability).  </li> <li>[ ] Fechar 3\u20135 pilotos (NDA) e agenda de coleta.  </li> <li>[ ] Criar repo \u201cnala-iq-replication\u201d e esqueleto de pastas.  </li> <li>[ ] Escrever abstract rascunho (200\u2013300 palavras) para cada venue.</li> </ul>"},{"location":"quality/nala-iq-relator/","title":"Relat\u00f3rio Diagn\u00f3stico Nala-IQ","text":""},{"location":"quality/nala-iq-relator/#nome-da-api-avaliada","title":"[Nome da API Avaliada]","text":"<ul> <li>Data da Avalia\u00e7\u00e3o: [DD/MM/AAAA]</li> <li>Vers\u00e3o do Framework: Nala-IQ Core v1</li> <li>Avaliador(es) Principal(is): [Nome(s)]</li> <li>Equipe/Produto Avaliado: [Nome da Equipe/Produto]</li> </ul>"},{"location":"quality/nala-iq-relator/#1-capa-folha-de-rosto","title":"1. Capa / Folha de Rosto","text":"<p>[Logo Nalaminds]</p> <p>T\u00edtulo: Relat\u00f3rio Diagn\u00f3stico Nala-IQ</p> <p>Nome da API, data, avaliadores e equipe.</p>"},{"location":"quality/nala-iq-relator/#2-sumario-executivo","title":"2. Sum\u00e1rio Executivo","text":""},{"location":"quality/nala-iq-relator/#21-proposito-e-escopo-da-avaliacao","title":"2.1 Prop\u00f3sito e Escopo da Avalia\u00e7\u00e3o","text":"<p>[Descrever brevemente o objetivo e o contexto do diagn\u00f3stico.]</p>"},{"location":"quality/nala-iq-relator/#22-metodologia-resumida","title":"2.2 Metodologia Resumida","text":"<p>Breve descri\u00e7\u00e3o da abordagem h\u00edbrida do Nala-IQ: question\u00e1rio, evid\u00eancias documentais e automatizadas, entrevistas/workshops.</p>"},{"location":"quality/nala-iq-relator/#23-destaques-da-maturidade-geral","title":"2.3 Destaques da Maturidade Geral","text":"<ul> <li>Inserir Radar Chart mostrando o n\u00edvel das 14 dimens\u00f5es.</li> <li>Listar principais pontos fortes.</li> <li>Listar principais \u00e1reas de oportunidade / risco.</li> </ul>"},{"location":"quality/nala-iq-relator/#24-recomendacoes-chave-top-3-5","title":"2.4 Recomenda\u00e7\u00f5es Chave (Top 3-5)","text":"<ul> <li>[R1]</li> <li>[R2]</li> <li>[R3]</li> </ul>"},{"location":"quality/nala-iq-relator/#25-proximos-passos-sugeridos","title":"2.5 Pr\u00f3ximos Passos Sugeridos","text":"<p>Breve roadmap ou orienta\u00e7\u00f5es.</p>"},{"location":"quality/nala-iq-relator/#3-introducao-detalhada","title":"3. Introdu\u00e7\u00e3o Detalhada","text":""},{"location":"quality/nala-iq-relator/#31-contexto-da-api-avaliada","title":"3.1 Contexto da API Avaliada","text":"<p>[Descri\u00e7\u00e3o breve da API, seu papel e import\u00e2ncia.]</p>"},{"location":"quality/nala-iq-relator/#32-objetivos-da-avaliacao","title":"3.2 Objetivos da Avalia\u00e7\u00e3o","text":"<p>[Quais problemas queriam resolver ou entender?]</p>"},{"location":"quality/nala-iq-relator/#33-escopo-detalhado","title":"3.3 Escopo Detalhado","text":"<p>[O que foi inclu\u00eddo / exclu\u00eddo.]</p>"},{"location":"quality/nala-iq-relator/#34-periodo-da-avaliacao-e-participantes","title":"3.4 Per\u00edodo da Avalia\u00e7\u00e3o e Participantes","text":"<p>[Datas e quem participou.]</p>"},{"location":"quality/nala-iq-relator/#35-metodologia-nala-iq-aplicada","title":"3.5 Metodologia Nala-IQ Aplicada","text":"<p>Descri\u00e7\u00e3o da metodologia h\u00edbrida.</p>"},{"location":"quality/nala-iq-relator/#36-modelo-de-maturidade-nala-iq","title":"3.6 Modelo de Maturidade Nala-IQ","text":"N\u00edvel Descri\u00e7\u00e3o 1 Inicial 2 Em Desenvolvimento 3 Definido 4 Gerenciado 5 Otimizado / L\u00edder"},{"location":"quality/nala-iq-relator/#4-resultados-da-avaliacao-por-dimensao","title":"4. Resultados da Avalia\u00e7\u00e3o por Dimens\u00e3o","text":"<p>(Repete para cada dimens\u00e3o)</p>"},{"location":"quality/nala-iq-relator/#4x-dimensao-nome","title":"4.X Dimens\u00e3o [Nome]","text":""},{"location":"quality/nala-iq-relator/#4x1-nivel-de-maturidade","title":"4.X.1 N\u00edvel de Maturidade","text":"<p>[N\u00edvel + M\u00e9dia calculada]</p>"},{"location":"quality/nala-iq-relator/#4x2-analise-geral","title":"4.X.2 An\u00e1lise Geral","text":"<p>[Resumo qualitativo.]</p>"},{"location":"quality/nala-iq-relator/#4x3-pontos-fortes","title":"4.X.3 Pontos Fortes","text":"<ul> <li>[Itens]</li> </ul>"},{"location":"quality/nala-iq-relator/#4x4-oportunidades-de-melhoria-e-riscos","title":"4.X.4 Oportunidades de Melhoria e Riscos","text":"<ul> <li>[Itens]</li> </ul>"},{"location":"quality/nala-iq-relator/#4x5-tabela-resumo-sub-dimensoes","title":"4.X.5 Tabela Resumo Sub-dimens\u00f5es","text":"Sub-dimens\u00e3o Pontua\u00e7\u00e3o M\u00e9dia N\u00edvel [nome] [valor] [n\u00edvel]"},{"location":"quality/nala-iq-relator/#5-recomendacoes-priorizadas-e-plano-de-acao","title":"5. Recomenda\u00e7\u00f5es Priorizadas e Plano de A\u00e7\u00e3o","text":""},{"location":"quality/nala-iq-relator/#51-introducao","title":"5.1 Introdu\u00e7\u00e3o","text":"<p>[Como foram definidas e priorizadas.]</p>"},{"location":"quality/nala-iq-relator/#52-tabela-de-recomendacoes","title":"5.2 Tabela de Recomenda\u00e7\u00f5es","text":"ID Recomenda\u00e7\u00e3o Dimens\u00e3o Prioridade Esfor\u00e7o Impacto Observa\u00e7\u00f5es R1 ... II, V Alta M Alto ..."},{"location":"quality/nala-iq-relator/#53-roadmap-sugerido","title":"5.3 Roadmap Sugerido","text":"<p>[Curto plano faseado.]</p>"},{"location":"quality/nala-iq-relator/#6-conclusao","title":"6. Conclus\u00e3o","text":"<p>[Resumo do diagn\u00f3stico, refor\u00e7ando pontos fortes, riscos e o convite \u00e0 melhoria cont\u00ednua.]</p>"},{"location":"quality/nala-iq-relator/#7-apendices","title":"7. Ap\u00eandices","text":""},{"location":"quality/nala-iq-relator/#apendice-a-scorecard-detalhado-por-criterio","title":"Ap\u00eandice A: Scorecard Detalhado por Crit\u00e9rio","text":"<p>[Link para planilha ou tabela destacando todos os crit\u00e9rios e notas.]</p>"},{"location":"quality/nala-iq-relator/#apendice-b-evidencias-documentais","title":"Ap\u00eandice B: Evid\u00eancias Documentais","text":"<p>[Listar documentos coletados.]</p>"},{"location":"quality/nala-iq-relator/#apendice-c-dados-automatizados","title":"Ap\u00eandice C: Dados Automatizados","text":"<p>[Listar relat\u00f3rios t\u00e9cnicos, scans etc.]</p>"},{"location":"quality/nala-iq-relator/#apendice-d-glossario","title":"Ap\u00eandice D: Gloss\u00e1rio","text":"<p>[Opcional.]</p>"},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/","title":"Relat\u00f3rio Diagn\u00f3stico Nala-IQ: [Nome da API Avaliada]","text":"<p>Data da Avalia\u00e7\u00e3o: [DD/MM/AAAA] Vers\u00e3o do Framework Nala-IQ Utilizada: Nala-IQ Core v1 Avaliador(es) Principal(is): [Nome(s)] Equipe/Produto Avaliado: [Nome da Equipe/Produto]  </p>"},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/#1-sumario-executivo","title":"1. Sum\u00e1rio Executivo","text":""},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/#11-proposito-e-escopo-da-avaliacao","title":"1.1 Prop\u00f3sito e Escopo da Avalia\u00e7\u00e3o","text":"<p>[Descri\u00e7\u00e3o breve]</p>"},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/#12-metodologia-resumida","title":"1.2 Metodologia Resumida","text":"<p>[Descri\u00e7\u00e3o da abordagem h\u00edbrida]</p>"},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/#13-destaques-da-maturidade-geral","title":"1.3 Destaques da Maturidade Geral","text":"<ul> <li>Radar Chart com n\u00edveis de maturidade das 14 dimens\u00f5es</li> <li>Principais pontos fortes</li> <li>Principais \u00e1reas de oportunidade/risco</li> </ul>"},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/#14-recomendacoes-chave","title":"1.4 Recomenda\u00e7\u00f5es Chave","text":"<p>[Listagem Top 3-5 recomenda\u00e7\u00f5es mais impactantes]</p>"},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/#15-proximos-passos-sugeridos","title":"1.5 Pr\u00f3ximos Passos Sugeridos","text":"<p>[Orienta\u00e7\u00e3o para roadmap de melhoria cont\u00ednua]</p>"},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/#2-introducao-detalhada","title":"2. Introdu\u00e7\u00e3o Detalhada","text":""},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/#21-contexto-da-api-avaliada","title":"2.1 Contexto da API Avaliada","text":"<p>[Descri\u00e7\u00e3o da API, prop\u00f3sito, arquitetura]</p>"},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/#22-objetivos-da-avaliacao-nala-iq","title":"2.2 Objetivos da Avalia\u00e7\u00e3o Nala-IQ","text":""},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/#23-escopo-detalhado","title":"2.3 Escopo Detalhado","text":""},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/#24-periodo-da-avaliacao-e-participantes","title":"2.4 Per\u00edodo da Avalia\u00e7\u00e3o e Participantes","text":""},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/#25-metodologia-nala-iq-aplicada","title":"2.5 Metodologia Nala-IQ Aplicada","text":""},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/#26-modelo-de-maturidade-nala-iq","title":"2.6 Modelo de Maturidade Nala-IQ","text":"<ul> <li>N\u00edvel 1: Inicial</li> <li>N\u00edvel 2: Em Desenvolvimento</li> <li>N\u00edvel 3: Definido</li> <li>N\u00edvel 4: Gerenciado</li> <li>N\u00edvel 5: Otimizado/L\u00edder</li> </ul>"},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/#3-resultados-da-avaliacao-de-maturidade-por-dimensao","title":"3. Resultados da Avalia\u00e7\u00e3o de Maturidade por Dimens\u00e3o","text":""},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/#3x-dimensao-nome-da-dimensao","title":"3.X Dimens\u00e3o [Nome da Dimens\u00e3o]","text":""},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/#3x1-nivel-de-maturidade-alcancado","title":"3.X.1 N\u00edvel de Maturidade Alcan\u00e7ado","text":"<p>[Nome do n\u00edvel] (Pontua\u00e7\u00e3o M\u00e9dia: [0.00-3.00])</p>"},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/#3x2-analise-geral-da-dimensao","title":"3.X.2 An\u00e1lise Geral da Dimens\u00e3o","text":""},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/#3x3-pontos-fortes-detalhados","title":"3.X.3 Pontos Fortes Detalhados","text":""},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/#3x4-oportunidades-de-melhoria-e-riscos-identificados","title":"3.X.4 Oportunidades de Melhoria e Riscos Identificados","text":""},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/#3x5-tabela-resumo-da-pontuacao-por-sub-dimensao","title":"3.X.5 Tabela Resumo da Pontua\u00e7\u00e3o por Sub-dimens\u00e3o","text":"Sub-dimens\u00e3o Pontua\u00e7\u00e3o M\u00e9dia N\u00edvel de Maturidade [Exemplo] [Valor] [N\u00edvel]"},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/#4-recomendacoes-priorizadas-e-plano-de-acao-sugerido","title":"4. Recomenda\u00e7\u00f5es Priorizadas e Plano de A\u00e7\u00e3o Sugerido","text":""},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/#41-introducao-as-recomendacoes","title":"4.1 Introdu\u00e7\u00e3o \u00e0s Recomenda\u00e7\u00f5es","text":""},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/#42-tabela-de-recomendacoes","title":"4.2 Tabela de Recomenda\u00e7\u00f5es","text":"ID Recomenda\u00e7\u00e3o Dimens\u00e3o(\u00f5es) Nala-IQ Prioridade Esfor\u00e7o Estimado Impacto Esperado Observa\u00e7\u00f5es R1 [Descri\u00e7\u00e3o] [Ex: II, V] Alta P Alto [Detalhes]"},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/#43-sugestoes-para-um-roadmap-de-melhoria-continua","title":"4.3 Sugest\u00f5es para um Roadmap de Melhoria Cont\u00ednua","text":""},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/#5-conclusao","title":"5. Conclus\u00e3o","text":"<p>[Resumo geral dos achados e import\u00e2ncia da melhoria cont\u00ednua]</p>"},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/#6-apendices","title":"6. Ap\u00eandices","text":""},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/#apendice-a-scorecard-detalhado-por-criterio","title":"Ap\u00eandice A: Scorecard Detalhado por Crit\u00e9rio","text":""},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/#apendice-b-lista-de-evidencias-documentais-e-ferramentas-analisadas","title":"Ap\u00eandice B: Lista de Evid\u00eancias Documentais e Ferramentas Analisadas","text":""},{"location":"quality/nala-iq-relatorio-diagnostico-estrutura/#apendice-c-glossario-de-termos-nala-iq","title":"Ap\u00eandice C: Gloss\u00e1rio de Termos Nala-IQ","text":""},{"location":"quality/nala-iq-submission/","title":"Plano de Submiss\u00e3o (90 dias) \u2014 Nala-IQ: Score e Framework \u201cal\u00e9m do estado da arte\u201d","text":"<p>Objetivo: transformar o Nala-IQ em refer\u00eancia de mercado e acad\u00eamica, publicando (i) artigo de experi\u00eancia e (ii) artigo t\u00e9cnico com valida\u00e7\u00e3o emp\u00edrica, com pacote de replica\u00e7\u00e3o aberto (open-science) e estrat\u00e9gia de IP definida.</p>"},{"location":"quality/nala-iq-submission/#1-metas-e-produtos-finais","title":"1) Metas e produtos finais","text":"<p>Em 90 dias, entregar: - \u2705 Artigo \u201cExperience/Practice\u201d (execu\u00e7\u00e3o industrial e impacto) \u2014 pronto para um peri\u00f3dico/venue applied (ex.: IEEE Software / experi\u00eancia industrial) OU talk em evento (apidays/OWASP/SREcon). - \u2705 Artigo t\u00e9cnico (m\u00e9todo + dados) com avalia\u00e7\u00e3o emp\u00edrica, an\u00e1lises ablation e pacote de replica\u00e7\u00e3o (c\u00f3digo + dados anonimizados + Docker). - \u2705 Pacote de replica\u00e7\u00e3o (open): reposit\u00f3rio p\u00fablico com DOI (ex.: Zenodo) contendo c\u00f3digo, dados anonimizados, instru\u00e7\u00f5es e scripts de an\u00e1lise. - \u2705 Kit de comunica\u00e7\u00e3o: p\u00e1gina no site, infogr\u00e1ficos, video-short (2\u20133 min). - \u2705 IP: pedido de marca \u201cNala-IQ\u201d no INPI, registro de software no INPI (c\u00f3digo do scoring), pol\u00edtica de licen\u00e7as (open-core).</p>"},{"location":"quality/nala-iq-submission/#2-publicos-e-venues-alvo-sem-travar-em-datas","title":"2) P\u00fablicos e venues alvo (sem travar em datas)","text":"<ul> <li>Peri\u00f3dicos / applied: IEEE Software (experience/insights), JSS (Journal of Systems and Software), EMSE (Empirical Software Engineering).  </li> <li>Confer\u00eancias: ICSA (arquitetura de software), IEEE SERVICES/ICWS (servi\u00e7os e APIs), SREcon (SLO/error budget), OWASP Global AppSec (seguran\u00e7a de APIs), apidays (pr\u00e1tica de APIs).   <p>Datas de CFP variam; checar janelas correntes na Semana 1 e escolher 2 alvos principais (1 peri\u00f3dico + 1 confer\u00eancia) de acordo com o cronograma.</p> </li> </ul>"},{"location":"quality/nala-iq-submission/#3-contribuicao-cientificatecnica-mensagem-central","title":"3) Contribui\u00e7\u00e3o cient\u00edfica/t\u00e9cnica (mensagem central)","text":"<ul> <li>Score em camadas + penalidade assim\u00e9trica + gating, com Trust Delta (self vs. evidence), Percent-to-Target, Anti-Goodhart, propaga\u00e7\u00e3o por depend\u00eancias e intervalo de confian\u00e7a.  </li> <li>Valida\u00e7\u00e3o emp\u00edrica em m\u00faltiplas empresas/setores; correla\u00e7\u00e3o com SLO/OTel, MTTR, incidentes e m\u00e9tricas DORA.  </li> <li>Reprodutibilidade: pacote de replica\u00e7\u00e3o completo e crit\u00e9rios verific\u00e1veis (linters, scanners, queries OTel).</li> </ul>"},{"location":"quality/nala-iq-submission/#4-trilhas-de-trabalho-workstreams","title":"4) Trilhas de trabalho (workstreams)","text":""},{"location":"quality/nala-iq-submission/#ws1-metodo-scoring","title":"WS1 \u2014 M\u00e9todo &amp; Scoring","text":"<ul> <li>Formalizar as f\u00f3rmulas (Base/Temporal/Environmental + FMEA + Gating + PT-Target + Goodhart Guard).  </li> <li>Definir parametriza\u00e7\u00e3o padr\u00e3o (\u03b1, thresholds, \u03bb, perfis).  </li> <li>Implementar <code>/v1/score</code> com explainability (decomposi\u00e7\u00e3o da pontua\u00e7\u00e3o, penalidades, flags, IC).</li> </ul>"},{"location":"quality/nala-iq-submission/#ws2-evidencias-coleta","title":"WS2 \u2014 Evid\u00eancias &amp; Coleta","text":"<ul> <li>Conectores de evid\u00eancia (TLS scanner, OpenAPI lint, SLO via OTel, supply-chain/SBOM).  </li> <li>Pipeline de anonymiza\u00e7\u00e3o (IDs, hashing, filtros).  </li> <li>Question\u00e1rio de Self-Report mapeado aos crit\u00e9rios.</li> </ul>"},{"location":"quality/nala-iq-submission/#ws3-dados-benchmark","title":"WS3 \u2014 Dados &amp; Benchmark","text":"<ul> <li>Selecionar 3\u20135 empresas (opt-in, NDA) em setores distintos.  </li> <li>Medir score baseline e retest (30\u201360 dias) ap\u00f3s corre\u00e7\u00f5es.  </li> <li>Calcular quartis/medianas por dimens\u00e3o (baseline Nala-IQ) e correlacionar com SLO/DORA.</li> </ul>"},{"location":"quality/nala-iq-submission/#ws4-experimentos-analises","title":"WS4 \u2014 Experimentos &amp; An\u00e1lises","text":"<ul> <li>Ablation (retirar penalidade/gating/anti-Goodhart e observar piora na prioriza\u00e7\u00e3o).  </li> <li>Inter-rater reliability (consist\u00eancia entre avaliadores).  </li> <li>An\u00e1lises estat\u00edsticas (efeito, IC, gr\u00e1ficos).</li> </ul>"},{"location":"quality/nala-iq-submission/#ws5-artigos-artefatos","title":"WS5 \u2014 Artigos &amp; Artefatos","text":"<ul> <li>Paper t\u00e9cnico (m\u00e9todo + dados) e experience paper (hist\u00f3rias, impacto).  </li> <li>Figuras e tabelas (ver se\u00e7\u00e3o 7).  </li> <li>Pacote de replica\u00e7\u00e3o com DOI + guia \u201cReproduza os resultados\u201d.</li> </ul>"},{"location":"quality/nala-iq-submission/#ws6-ip-licencas","title":"WS6 \u2014 IP &amp; Licen\u00e7as","text":"<ul> <li>Marca Nala-IQ (INPI), registro de software (INPI), licen\u00e7as:  </li> <li>C\u00f3digo: Apache-2.0 (ou BSL se preferir open-core).  </li> <li>Conte\u00fado/figuras: CC-BY-NC.  </li> <li>Modelos/benchmarks premium: segredo de neg\u00f3cio.</li> </ul>"},{"location":"quality/nala-iq-submission/#ws7-comms-site","title":"WS7 \u2014 Comms &amp; Site","text":"<ul> <li>P\u00e1gina \u201cResearch\u201d com resumo, infogr\u00e1ficos, e link para replica\u00e7\u00e3o.  </li> <li>Blog post \u201cpor tr\u00e1s do score\u201d + \u201cresultados preliminares\u201d.  </li> <li>Slides deck e v\u00eddeo 2\u20133 min.</li> </ul>"},{"location":"quality/nala-iq-submission/#5-linha-do-tempo-90-dias","title":"5) Linha do tempo (90 dias)","text":""},{"location":"quality/nala-iq-submission/#semanas-12","title":"Semanas 1\u20132","text":"<ul> <li>Travar venues alvo e requisitos; montar protocolo de estudo (m\u00e9todo/\u00e9tica/anonimiza\u00e7\u00e3o).  </li> <li>Fechar <code>scoring_config.yml</code> e <code>/v1/score</code> (MVP).  </li> <li>Selecionar 3\u20135 empresas piloto e assinar NDAs.  </li> <li>Instrumentar conectores (TLS/OpenAPI/OTel).  </li> <li>Sa\u00eddas: Spec v1, protocolo, NDA, lista de pilotos, endpoint <code>/v1/score</code> MVP.</li> </ul>"},{"location":"quality/nala-iq-submission/#semanas-34","title":"Semanas 3\u20134","text":"<ul> <li>Coletar baseline (Self + Evidence) nas empresas piloto.  </li> <li>Rodar primeiro batch de an\u00e1lises explorat\u00f3rias.  </li> <li>Esqueleto do paper t\u00e9cnico e do experience paper.  </li> <li>Sa\u00eddas: dataset v0 (anonimizado), outline de artigos, primeiros gr\u00e1ficos.</li> </ul>"},{"location":"quality/nala-iq-submission/#semanas-58","title":"Semanas 5\u20138","text":"<ul> <li>Execu\u00e7\u00e3o dos experimentos (ablation, inter-rater, correla\u00e7\u00e3o SLO/DORA).  </li> <li>Implementar IC (bootstrap) e Goodhart Guard no score.  </li> <li>Preparar figuras finais e tabelas.  </li> <li>Sa\u00eddas: resultados consolidados, figuras/tabelas, pacote de replica\u00e7\u00e3o v0.</li> </ul>"},{"location":"quality/nala-iq-submission/#semanas-910","title":"Semanas 9\u201310","text":"<ul> <li>Reda\u00e7\u00e3o completa (R1) dos dois artigos.  </li> <li>Revis\u00e3o interna + revis\u00e3o por pares externos (2\u20133 reviewers).  </li> <li>Polir replication package (Docker + scripts reprodut\u00edveis + DOI).</li> </ul>"},{"location":"quality/nala-iq-submission/#semanas-1112","title":"Semanas 11\u201312","text":"<ul> <li>Vers\u00e3o final (R2), checagem de checklists dos venues.  </li> <li>Submiss\u00e3o 1 (peri\u00f3dico) + Submiss\u00e3o 2 (confer\u00eancia/talk).  </li> <li>Publicar landing research no site + blog post + infogr\u00e1ficos.</li> </ul>"},{"location":"quality/nala-iq-submission/#6-entregaveis-checklist","title":"6) Entreg\u00e1veis (checklist)","text":"<ul> <li>[ ] <code>scoring_config.yml</code> final  </li> <li>[ ] API <code>/v1/score</code> (com explainability + IC)  </li> <li>[ ] Conectores de evid\u00eancia (TLS, OpenAPI, OTel)  </li> <li>[ ] Dataset anonimizados (v1) + dicion\u00e1rio de dados  </li> <li>[ ] Relat\u00f3rios estat\u00edsticos (notebooks/scripts)  </li> <li>[ ] Paper t\u00e9cnico (PDF) + experience paper (PDF)  </li> <li>[ ] Replication package (c\u00f3digo + dados + Docker + DOI)  </li> <li>[ ] Infogr\u00e1ficos e slides deck  </li> <li>[ ] P\u00e1gina \u201cResearch\u201d no site  </li> <li>[ ] Pedido de marca (INPI) + registro de software</li> </ul>"},{"location":"quality/nala-iq-submission/#7-figuras-e-tabelas-pre-planejadas","title":"7) Figuras e Tabelas (pr\u00e9-planejadas)","text":"<p>Figuras 1. Arquitetura do score (camadas) 2. Pipeline de evid\u00eancias (Self vs Evidence) + Trust Delta 3. Mecanismo de penalidade FMEA + Gating (fluxo) 4. Anti-Goodhart (exemplo de dampening) 5. Radar por dimens\u00e3o (antes/depois) 6. Gr\u00e1ficos de correla\u00e7\u00e3o (score \u00d7 SLO/DORA/MTTR)</p> <p>Tabelas 1. Par\u00e2metros do score (\u03b1, thresholds, \u03bb, perfis) 2. Matriz S/O/D por crit\u00e9rio-chave 3. Resultados por empresa (anon) e por dimens\u00e3o 4. Ablation study (efeito de remover componentes) 5. Inter-rater reliability (\u03ba/ICC) 6. Percent-to-Target por dimens\u00e3o e quartis (benchmark)</p>"},{"location":"quality/nala-iq-submission/#8-pacote-de-replicacao-open-science","title":"8) Pacote de replica\u00e7\u00e3o (open-science)","text":"<p>Conte\u00fado m\u00ednimo: - <code>/code</code>: implementa\u00e7\u00e3o do score + scripts de an\u00e1lise - <code>/data</code>: datasets anonimizados (CSV/Parquet) + README + data-dict - <code>/docker</code>: Dockerfile + <code>make reproduce</code> - <code>/notebooks</code>: explora\u00e7\u00e3o, ablation, estat\u00edstica, gera\u00e7\u00e3o de figuras - <code>/artifact-metadata</code>: guia de avalia\u00e7\u00e3o, vers\u00e3o, hash, instru\u00e7\u00f5es - DOI (ex.: Zenodo) e licen\u00e7a: code Apache-2.0 / data CC-BY-NC</p>"},{"location":"quality/nala-iq-submission/#9-ip-etica-e-compliance","title":"9) IP, \u00e9tica e compliance","text":"<ul> <li>Marca \u201cNala-IQ\u201d (INPI) \u2014 pedido imediato.  </li> <li>Registro de software (INPI) \u2014 c\u00f3digo do score/pipelines.  </li> <li>Licen\u00e7as: c\u00f3digo Apache-2.0 (ou BSL), conte\u00fado CC-BY-NC, benchmarks premium como segredo.  </li> <li>\u00c9tica/privacidade: NDA com pilotos, anonimiza\u00e7\u00e3o, remo\u00e7\u00e3o de PII, checklist LGPD.  </li> <li>Defensive publication: subir preprint (ex.: arXiv/Zenodo) ap\u00f3s submiss\u00e3o formal.</li> </ul>"},{"location":"quality/nala-iq-submission/#10-estrutura-dos-artigos-resumo","title":"10) Estrutura dos artigos (resumo)","text":""},{"location":"quality/nala-iq-submission/#paper-tecnico-metodo-dados","title":"Paper t\u00e9cnico (m\u00e9todo + dados)","text":"<ol> <li>Introdu\u00e7\u00e3o (problema, lacunas, contribui\u00e7\u00f5es)  </li> <li>Trabalhos relacionados (SAMM/BSIMM, CVSS, DORA, SRE)  </li> <li>M\u00e9todo Nala-IQ (f\u00f3rmulas, componentes, IC, anti-Goodhart)  </li> <li>Dataset e protocolo (empresas, coleta, anonimiza\u00e7\u00e3o)  </li> <li>Resultados (correla\u00e7\u00f5es, ablation, \u03ba/ICC, PT-Target)  </li> <li>Amea\u00e7as \u00e0 validade e limita\u00e7\u00f5es  </li> <li>Conclus\u00f5es e disponibilidade (replication package, DOI)</li> </ol>"},{"location":"quality/nala-iq-submission/#experience-paper-talk","title":"Experience paper / Talk","text":"<ul> <li>Contexto, dores, desenho do score, \u201ccomo implementamos\u201d, resultados pr\u00e1ticos (antes/depois), ROI/tempo, li\u00e7\u00f5es aprendidas.</li> </ul>"},{"location":"quality/nala-iq-submission/#11-riscos-e-mitigacao","title":"11) Riscos e mitiga\u00e7\u00e3o","text":"<ul> <li>Baixa ades\u00e3o de pilotos \u2192 ampliar funil e incentivos (diagn\u00f3stico gratuito; insights exclusivos).  </li> <li>Dados ruidosos ou parciais \u2192 refor\u00e7ar evid\u00eancia autom\u00e1tica e normaliza\u00e7\u00e3o; IC para incerteza.  </li> <li>Prazos/CFP \u2192 planejar vers\u00e3o \u201cjournal-first\u201d (rolling) + talk (rolling CFPs em eventos de API).  </li> <li>Revisores pedirem mais evid\u00eancia \u2192 manter backlog de empresas e preparar 2\u00ba batch.</li> </ul>"},{"location":"quality/nala-iq-submission/#12-proximos-passos-imediatos-esta-semana","title":"12) Pr\u00f3ximos passos imediatos (esta semana)","text":"<ul> <li>[ ] Escolher 1 peri\u00f3dico + 1 confer\u00eancia/talk alvo e baixar os templates.  </li> <li>[ ] Finalizar <code>scoring_config.yml</code> e publicar <code>/v1/score</code> (MVP com explainability).  </li> <li>[ ] Fechar 3\u20135 pilotos (NDA) e agenda de coleta.  </li> <li>[ ] Criar repo \u201cnala-iq-replication\u201d e esqueleto de pastas.  </li> <li>[ ] Escrever abstract rascunho (200\u2013300 palavras) para cada venue.</li> </ul>"},{"location":"quality/nala-iqpy-automated/","title":"\ud83e\udde0 Nala-IQPY \u2014 Fully Automated Quality Framework","text":"<p>This document describes how the Nala-IQPY framework works end-to-end: scoring logic, automation via GitHub Actions, and how to extend it to new modules or dimensions.</p>"},{"location":"quality/nala-iqpy-automated/#structure","title":"\ud83c\udfd7\ufe0f Structure","text":"<pre><code>helpers/quality/\n\u251c\u2500\u2500 evaluate_module.py   # Evaluates a single module\n\u251c\u2500\u2500 evaluate_all.py      # Runs evaluation on all modules\ndocs/quality/\n\u2514\u2500\u2500 index.md             # Generated report published via MkDocs\n</code></pre>"},{"location":"quality/nala-iqpy-automated/#dimensions-tracked-v1","title":"\u2705 Dimensions Tracked (v1)","text":"Dimension Signals used for automation Documentation Presence of docstrings Testing Coverage % from coverage.xml Resilience Usage of <code>@fallback_handler</code> Observability Usage of <code>@with_span</code>, <code>@with_observability</code>"},{"location":"quality/nala-iqpy-automated/#scoring-logic","title":"\u2699\ufe0f Scoring Logic","text":"<p>Each module gets evaluated with signals parsed via AST and coverage reports. Example rules:</p> <ul> <li>\u2705 <code>docstrings &gt;= 5</code> \u2192 full score for docs</li> <li>\u2705 <code>@fallback_handler</code> found \u2192 resilience ok</li> <li>\u2705 <code>coverage &gt;= 80%</code> \u2192 testing is strong</li> <li>\u274c No decorators or &lt;50% coverage \u2192 warnings/suggestions</li> </ul> <p>Scores are not stored \u2014 they are calculated dynamically.</p>"},{"location":"quality/nala-iqpy-automated/#output","title":"\ud83d\udcc4 Output","text":"<p>Example result in <code>docs/quality/index.md</code>:</p> <pre><code>\ud83d\udce6 Module: `nala.athomic.observability`\n\n\u2705 Documentation: Good usage of docstrings\n\u2705 Resilience: Fallback handlers present\n\u26a0\ufe0f  Observability: No spans detected\n\u2705 Testing: 87% coverage\n\n\ud83e\udde0 Recommendations:\n- Add `@with_span` for better tracing\n</code></pre>"},{"location":"quality/nala-iqpy-automated/#ci-automation","title":"\ud83e\udd16 CI Automation","text":"<p>Workflow: <code>.github/workflows/quality-report.yml</code></p> <p>Runs on every push to <code>main</code>: 1. Runs coverage 2. Runs <code>evaluate_all.py</code> 3. Overwrites <code>docs/quality/index.md</code> 4. Commits updated report to Git</p>"},{"location":"quality/nala-iqpy-automated/#roadmap","title":"\ud83d\ude80 Roadmap","text":"<ul> <li>[x] CLI for single and multi-module evaluation</li> <li>[x] GitHub Actions integration</li> <li>[ ] Add more dimensions: <code>security</code>, <code>configurability</code>, <code>performance</code></li> <li>[ ] Export as <code>.csv</code>, <code>.json</code>, or <code>.html</code></li> <li>[ ] Add badge for overall quality per module</li> </ul>"},{"location":"quality/nala-iqpy-automated/#how-to-extend","title":"\ud83d\udcda How to Extend","text":"<p>To add a new dimension: 1. Open <code>evaluate_module.py</code> 2. Add AST/coverage logic to extract signals 3. Add lines to scoring + recommendation block 4. Re-run <code>evaluate_all.py</code></p> <p>Tip: Use <code>ast</code>, <code>re</code>, or Git log data to build advanced signals.</p>"},{"location":"quality/nala-iqpy-automated/#future-possibilities","title":"\ud83e\udde9 Future Possibilities","text":"<ul> <li>Integration with MkDocs pages (<code>/quality</code>)</li> <li>Dashboard (Streamlit or Plotly)</li> <li>CLI for trend history (versioned reports)</li> </ul>"},{"location":"quality/nala-iqpy/","title":"\ud83c\udfc6 Nala-IQPY \u2014 Quality Evaluation Framework","text":"<p>Nala-IQPY is the internal framework used in the <code>athomic-docs</code> to assess and guide the quality of core modules across various dimensions such as security, resilience, observability, testing, and developer experience.</p>"},{"location":"quality/nala-iqpy/#purpose","title":"\ud83c\udfaf Purpose","text":"<ul> <li>Promote engineering excellence and sustainable architecture</li> <li>Create visibility into technical debt and opportunities</li> <li>Guide refactor, testing and documentation priorities</li> <li>Enable scorecards and dashboards for teams and governance</li> </ul>"},{"location":"quality/nala-iqpy/#core-dimensions","title":"\ud83e\uddf1 Core Dimensions","text":"<p>Each module is evaluated across the following quality dimensions:</p> Dimension Description <code>documentation</code> Presence and clarity of docstrings, <code>.md</code> files, public contracts <code>security</code> Masking, secrets management, auth flows, headers <code>resilience</code> Fallbacks, circuit breaker, retry, throttling <code>observability</code> Logs, metrics, traces, health checks <code>testing</code> Unit + integration test coverage, edge case tests <code>dx</code> Developer experience: registries, readability, CLI tools <code>configurability</code> Usage of settings, schema validation, env awareness <code>performance</code> Caching, efficient logic, async support"},{"location":"quality/nala-iqpy/#evaluation-model","title":"\ud83e\uddea Evaluation Model","text":"<p>Each module is defined via a structured YAML file:</p> <pre><code>module: nala.athomic.security.secrets\ndimensions:\n  documentation:\n    score: 1\n    total: 1\n  security:\n    score: 3\n    total: 4\n  testing:\n    score: 2\n    total: 3\n</code></pre> <p>Optional: add <code>weight</code> for dimensions if prioritization is needed.</p>"},{"location":"quality/nala-iqpy/#scoring","title":"\ud83d\udcca Scoring","text":""},{"location":"quality/nala-iqpy/#per-dimension","title":"Per Dimension:","text":"<pre><code>dimension_score = score / total\n</code></pre>"},{"location":"quality/nala-iqpy/#per-module","title":"Per Module:","text":"<pre><code>module_score = sum(d.score) / sum(d.total)\n</code></pre>"},{"location":"quality/nala-iqpy/#weighted-score","title":"Weighted Score:","text":"<pre><code>weighted_score = \u03a3((score/total) * weight) / \u03a3(weights)\n</code></pre>"},{"location":"quality/nala-iqpy/#roadmap","title":"\ud83d\udd04 Roadmap","text":"<ul> <li>[x] Design quality dimensions and scoring model</li> <li>[x] CLI <code>generate_report.py</code> with summary per module</li> <li>[ ] Export <code>report.md</code>, <code>report.csv</code> or <code>report.html</code></li> <li>[ ] Integration with GitHub Actions</li> <li>[ ] Optional: automate score based on static analysis</li> </ul>"},{"location":"quality/nala-iqpy/#structure","title":"\ud83d\udcc1 Structure","text":"<pre><code>quality/\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 observability.yaml\n\u2502   \u251c\u2500\u2500 secrets.yaml\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 generate_report.py\n\u2514\u2500\u2500 report.md  # output\n</code></pre>"},{"location":"quality/nala-iqpy/#why-this-matters","title":"\ud83e\udde0 Why This Matters","text":"<p>By embedding quality evaluation in the engineering process, we ensure the Athomic Docs remains scalable, maintainable and secure across all its critical dimensions.</p>"},{"location":"quality/nala-loggin-exception-best-praticies/","title":"\ud83e\uddfe Boas Pr\u00e1ticas de Logging de Exce\u00e7\u00f5es em Python","text":""},{"location":"quality/nala-loggin-exception-best-praticies/#1-loggerexception-vs-loggererror","title":"1. <code>logger.exception()</code> vs <code>logger.error()</code>","text":"Situa\u00e7\u00e3o M\u00e9todo recomendado Traceback inclu\u00eddo? Dentro de um <code>except</code> e deseja logar o erro completo <code>logger.exception()</code> \u2705 Sim Fora de <code>except</code>, mas ocorreu uma falha <code>logger.error()</code> \u274c N\u00e3o (a menos que use <code>exc_info=True</code>) Dentro de <code>except</code>, mas n\u00e3o quer o traceback completo <code>logger.error(..., exc_info=False)</code> \u274c N\u00e3o"},{"location":"quality/nala-loggin-exception-best-praticies/#2-por-que-evitar-stre-ou-e-na-mensagem","title":"2. Por que evitar <code>str(e)</code> ou <code>- {e}</code> na mensagem?","text":"<pre><code># Evitar:\nlogger.exception(f\"Erro: {e}\")\n\n# Correto:\nlogger.exception(\"Erro durante opera\u00e7\u00e3o\")\n</code></pre> <p>\ud83d\udd0d <code>logger.exception()</code> j\u00e1 inclui o <code>str(e)</code> e o <code>traceback</code>, ent\u00e3o repetir <code>e</code> polui o log e causa duplica\u00e7\u00e3o.</p>"},{"location":"quality/nala-loggin-exception-best-praticies/#3-como-adicionar-contexto-util-use-extra-logging-ou-bind-loguru","title":"3. Como adicionar contexto \u00fatil? Use <code>extra</code> (logging) ou <code>bind()</code> (loguru)","text":""},{"location":"quality/nala-loggin-exception-best-praticies/#exemplo-com-logging-padrao-python","title":"Exemplo com <code>logging</code> (padr\u00e3o Python):","text":"<pre><code>logger.error(\"Erro ao salvar item\", extra={\"item_id\": item.id, \"user_id\": user.id})\n</code></pre> <p>\u26a0\ufe0f Requer formatter compat\u00edvel com <code>extra</code> para exibir no log.</p>"},{"location":"quality/nala-loggin-exception-best-praticies/#exemplo-com-loguru-recomendado","title":"Exemplo com <code>loguru</code> (recomendado):","text":"<pre><code>from loguru import logger\n\ntry:\n    ...\nexcept Exception:\n    logger.bind(model=\"UserDocument\", criteria={\"email\": \"teste@...\"}) \\\n          .exception(\"Erro durante Beanie find_one\")\n</code></pre>"},{"location":"quality/nala-loggin-exception-best-praticies/#4-loguru-com-json-estruturado-otimo-para-gcp-elk-etc","title":"4. Loguru com JSON estruturado (\u00f3timo para GCP, ELK, etc)","text":"<pre><code>from loguru import logger\nimport sys\n\nlogger.remove()\nlogger.add(sys.stdout, serialize=True)  # Logs em JSON\n</code></pre>"},{"location":"quality/nala-loggin-exception-best-praticies/#5-exemplo-completo-com-boas-praticas","title":"5. Exemplo completo com boas pr\u00e1ticas","text":"<pre><code>from loguru import logger\n\ndef buscar_usuario(email):\n    try:\n        raise RuntimeError(\"Usu\u00e1rio n\u00e3o encontrado\")\n    except Exception:\n        logger.bind(model=\"UserDocument\", operation=\"find_one\", criteria={\"email\": email}) \\\n              .exception(\"Erro ao buscar usu\u00e1rio\")\n</code></pre>"},{"location":"quality/nala-loggin-exception-best-praticies/#conclusoes","title":"\u2705 Conclus\u00f5es","text":"<ul> <li>Use <code>logger.exception()</code> apenas dentro de <code>except</code>.</li> <li>Evite incluir <code>str(e)</code> na mensagem: j\u00e1 est\u00e1 no traceback.</li> <li>Prefira logs estruturados com <code>extra</code> ou <code>bind()</code>.</li> <li>Loguru facilita muito logs avan\u00e7ados com menos c\u00f3digo.</li> <li>Se poss\u00edvel, use <code>serialize=True</code> para JSON logs em ambientes cloud.</li> </ul>"},{"location":"quality/nala-module-design-guideline/","title":"\ud83e\udde0 Guia para Planejamento de M\u00f3dulos no <code>athomic-docs</code>","text":"<p>Um m\u00f3dulo bem planejado \u00e9 aquele que resolve um problema claro, respeita os princ\u00edpios de arquitetura do projeto e pode evoluir com seguran\u00e7a e clareza.</p>"},{"location":"quality/nala-module-design-guideline/#1-perguntas-fundamentais-para-iniciar","title":"\ud83e\udded 1. Perguntas fundamentais para iniciar","text":"<p>Antes de qualquer c\u00f3digo, pergunte:</p> Pergunta Por que \u00e9 importante? \u2705 Qual problema este m\u00f3dulo resolve? Evita m\u00f3dulos \"gen\u00e9ricos\" ou sem prop\u00f3sito claro \u2705 Quem ir\u00e1 consumir este m\u00f3dulo? Define interface e n\u00edvel de acoplamento \u2705 Ele ser\u00e1 usado diretamente ou via integra\u00e7\u00e3o? Afeta como ele ser\u00e1 exposto (ex: API vs Interno) \u2705 Este m\u00f3dulo ter\u00e1 extens\u00f5es espec\u00edficas por recurso? Indica necessidade de abstra\u00e7\u00f5es e plugin points \u2705 Ele tem depend\u00eancias internas entre camadas (ex: API, infra)? Detecta riscos de depend\u00eancia circular"},{"location":"quality/nala-module-design-guideline/#2-definicao-de-responsabilidades","title":"\ud83e\uddf1 2. Defini\u00e7\u00e3o de responsabilidades","text":"<p>Cada m\u00f3dulo deve responder claramente:</p> Quest\u00e3o Exemplo \ud83c\udfaf Qual \u00e9 sua responsabilidade \u00fanica? <code>observability</code>: fornecer m\u00e9tricas, tracing e readiness \ud83d\udd0c O que ele fornece como contrato p\u00fablico? Fun\u00e7\u00f5es, classes, decorators, registries \ud83d\udce6 Que tipos de componentes ele exp\u00f5e? <code>MetricProbe</code>, <code>ReadinessCheck</code>, <code>@with_span</code> \ud83e\uddea Como ele ser\u00e1 testado de forma isolada? Sem depender de servi\u00e7os reais \u2699\ufe0f Ele se adapta a diferentes configura\u00e7\u00f5es (<code>enabled</code>)? Por recurso, por modo, por ambiente"},{"location":"quality/nala-module-design-guideline/#3-agrupamento-por-comportamento-e-abstracao","title":"\ud83e\udde9 3. Agrupamento por comportamento e abstra\u00e7\u00e3o","text":"<p>Ao observar o m\u00f3dulo, reflita:</p> <ul> <li>H\u00e1 comportamentos que se repetem por recurso?</li> <li>\u2192 Crie interfaces/Protocolos (<code>ReadinessCheck</code>, <code>MetricProbe</code>)</li> <li>H\u00e1 c\u00f3digo condicional baseado em config?</li> <li>\u2192 Separe em enabled-guards (evitar <code>if settings.X.enabled</code> em todo lugar)</li> <li>H\u00e1 opera\u00e7\u00f5es peri\u00f3dicas?</li> <li>\u2192 Use schedulers (ex: <code>MetricScheduler</code>)</li> <li>O m\u00f3dulo precisa registrar extens\u00f5es espec\u00edficas?</li> <li>\u2192 Use registries, n\u00e3o listas manuais</li> <li>Ele depende de recursos como Redis, DB...?</li> <li>\u2192 Certifique-se que as depend\u00eancias n\u00e3o criam ciclos</li> </ul>"},{"location":"quality/nala-module-design-guideline/#4-componentes-tipicos-em-modulos-maduros","title":"\ud83e\uddf0 4. Componentes t\u00edpicos em m\u00f3dulos maduros","text":"Componente Responsabilidade <code>interfaces.py</code> Protocolos/contratos base (ex: <code>Check</code>) <code>registry.py</code> Ponto central de registro e execu\u00e7\u00e3o <code>resources/</code> Implementa\u00e7\u00f5es espec\u00edficas por recurso <code>metrics/</code> M\u00e9tricas que esse m\u00f3dulo coleta ou exp\u00f5e <code>instrumentation/</code> C\u00f3digos que integram com outros sistemas <code>bootstrap.py</code> Fun\u00e7\u00e3o que registra todos os componentes"},{"location":"quality/nala-module-design-guideline/#5-evitar-code-smells-classicos","title":"\ud83e\udde0 5. Evitar code smells cl\u00e1ssicos","text":"Code smell Alternativa <code>if settings.enabled:</code> em todo lugar Use <code>.enabled()</code> no componente Muitos <code>imports</code> cruzados Use <code>import lazy</code> ou <code>registry</code> Fun\u00e7\u00f5es utilit\u00e1rias soltas Agrupe por comportamento em classes ou m\u00f3dulos L\u00f3gica de m\u00e9trica junto da l\u00f3gica funcional Separe l\u00f3gica de coleta vs funcional"},{"location":"quality/nala-module-design-guideline/#6-checklist-antes-de-fechar-o-modulo","title":"\ud83d\udd01 6. Checklist antes de \"fechar\" o m\u00f3dulo","text":"<ul> <li>[ ] O nome do m\u00f3dulo representa claramente seu papel?</li> <li>[ ] O m\u00f3dulo possui responsabilidade \u00fanica?</li> <li>[ ] As partes configur\u00e1veis s\u00e3o lidas centralizadamente?</li> <li>[ ] Os recursos est\u00e3o organizados por tipo ou fun\u00e7\u00e3o?</li> <li>[ ] O m\u00f3dulo pode ser estendido sem altera\u00e7\u00e3o no n\u00facleo?</li> <li>[ ] \u00c9 f\u00e1cil testar os componentes de forma isolada?</li> <li>[ ] Nenhum ciclo de depend\u00eancia foi introduzido?</li> </ul>"},{"location":"quality/nala-module-design-guideline/#exemplo-de-aplicacao-observability","title":"\u2728 Exemplo de aplica\u00e7\u00e3o: <code>observability</code>","text":"<ul> <li>Identificamos os comportamentos: <code>check</code>, <code>trace</code>, <code>metric</code></li> <li>Criamos <code>Protocolos</code> para <code>Check</code> e <code>MetricProbe</code></li> <li>Isolamos instrumenta\u00e7\u00e3o e l\u00f3gica de coleta</li> <li>Evitamos ciclos com <code>import lazy</code></li> <li>Plugamos por registries</li> <li>Expomos apenas o necess\u00e1rio na API</li> </ul>"},{"location":"quality/nala-module-design-guideline/#conclusao","title":"\ud83d\ude80 Conclus\u00e3o","text":"<p>Esse guia n\u00e3o dita como deve ser o m\u00f3dulo, mas como pensar sobre ele antes, durante e depois do desenvolvimento.</p> <p>Um bom m\u00f3dulo deve ser f\u00e1cil de entender, f\u00e1cil de usar e dif\u00edcil de quebrar.</p>"},{"location":"quality/nala-new-module-checklist/","title":"\u2705 Checklist de Qualidade por M\u00f3dulo - athomic-docs","text":"<p>Este checklist garante consist\u00eancia, escalabilidade e boas pr\u00e1ticas no desenvolvimento de cada m\u00f3dulo da <code>athomic-docs</code>.</p>"},{"location":"quality/nala-new-module-checklist/#estrutura-interna-do-modulo","title":"\ud83d\udce6 Estrutura Interna do M\u00f3dulo","text":"<ul> <li>[ ] Classe <code>Settings</code> para configura\u00e7\u00f5es validadas (ex: via <code>pydantic</code>)</li> <li>[ ] Logging estruturado com mascaramento de dados sens\u00edveis (configur\u00e1vel)</li> <li>[ ] Retry para chamadas externas com pol\u00edtica configur\u00e1vel</li> <li>[ ] Fallback com cadeia configur\u00e1vel e logs</li> <li>[ ] Cache com TTL configur\u00e1vel (in-memory, Redis, etc.)</li> <li>[ ] Tracing com suporte a OpenTelemetry (decoradores ou middleware)</li> <li>[ ] Rate Limiting via <code>limits</code>, configur\u00e1vel por chave/contexto</li> <li>[ ] Circuit Breaker (<code>Breakout</code>) com backoff e monitoramento</li> <li>[ ] Observabilidade (m\u00e9tricas, health checks, etc.)</li> <li>[ ] Thread-safe / async-safe garantido</li> <li>[ ] Pool de recursos (ex: conex\u00f5es DB, Redis) com gerenciamento configur\u00e1vel</li> <li>[ ] Singleton (<code>@singleton</code>, <code>@async_singleton</code>) onde aplic\u00e1vel</li> <li>[ ] Suporte \u00e0 Inje\u00e7\u00e3o de Depend\u00eancia (com fallback e override)</li> <li>[ ] Internacionaliza\u00e7\u00e3o (i18n) para mensagens de erro e logs</li> <li>[ ] Resili\u00eancia</li> <li>[ ] Escalabilidade</li> <li>[ ] Documenta\u00e7\u00e3o</li> <li>[ ] Seguran\u00e7a</li> <li>[ ] Performance</li> <li>[ ] Versioamento</li> <li>[ ] Flexibilidade/adaptabilidade/dependency injection</li> <li>[ ] Extensivibilidade </li> <li>[ ] Testes unit\u00e1rios/integra\u00e7\u00e3o/e2e/chaos/stress/cobertura/benchmark/seguran\u00e7a</li> <li>[ ] Examples codes</li> </ul>"},{"location":"quality/nala-new-module-checklist/#qualidade-no-desenvolvimento","title":"\ud83e\uddea Qualidade no Desenvolvimento","text":"<ul> <li>[ ] Aplica\u00e7\u00e3o de Design Patterns apropriados (Adapter, Strategy, etc.)</li> <li>[ ] Clean Architecture (interface, dom\u00ednio, aplica\u00e7\u00e3o, infraestrutura)</li> <li>[ ] C\u00f3digo limpo e autoexplicativo</li> <li>[ ] Testes unit\u00e1rios</li> <li>[ ] Testes de integra\u00e7\u00e3o</li> <li>[ ] Testes E2E (quando aplic\u00e1vel)</li> <li>[ ] Documenta\u00e7\u00e3o atualizada (docstrings e markdowns)</li> <li>[ ] Configura\u00e7\u00f5es multi-ambiente isoladas (<code>dev</code>, <code>stage</code>, <code>prod</code>)</li> <li>[ ] Testes de carga/stress</li> <li>[ ] Testes de seguran\u00e7a (headers, injection, autentica\u00e7\u00e3o)</li> <li>[ ] Suporte a Profiling Configur\u00e1vel</li> </ul> <p>Sugest\u00e3o: automatizar essa estrutura com um CLI: <code>nala init module &lt;nome&gt;</code></p>"},{"location":"quality/nala-quality-framework/","title":"\ud83d\udcd0 Framework de Avalia\u00e7\u00e3o de Qualidade para APIs","text":"<p>Este framework foi criado para garantir que APIs desenvolvidas atendam aos mais altos padr\u00f5es de qualidade, escalabilidade, seguran\u00e7a e manuten\u00e7\u00e3o. Ele \u00e9 voltado especialmente para sistemas distribu\u00eddos, que lidam com m\u00faltiplos ambientes e precisam de um grau elevado de confiabilidade.</p>"},{"location":"quality/nala-quality-framework/#1-boas-praticas-de-engenharia","title":"\u2705 1. Boas pr\u00e1ticas de engenharia","text":"Crit\u00e9rio Avalia\u00e7\u00e3o \ud83d\udd04 Single Responsibility Cada m\u00f3dulo (provider, handler, registry) tem responsabilidade \u00fanica. \ud83d\udca1 Interface-based Design Uso extensivo de interfaces (<code>SecretProvider</code>, <code>RotatableSecretProvider</code>). \u267b\ufe0f DRY &amp; Reusabilidade Handlers como retry, fallback, cache, separando l\u00f3gica de neg\u00f3cio. \ud83e\uddea Testabilidade Testes unit\u00e1rios bem definidos, uso de mocks e cobertura adequada. \ud83d\udce6 Organiza\u00e7\u00e3o de pacotes Estrutura clara: <code>providers</code>, <code>crypto</code>, <code>registry</code>, <code>decorators</code>, etc. \u26a0\ufe0f Manejo de erros e retry Uso de <code>tenacity</code>, tratamento centralizado e seguro de falhas. \u2705 Nota: Excelente uso de princ\u00edpios SOLID e design orientado a interfaces."},{"location":"quality/nala-quality-framework/#2-escalabilidade-e-performance","title":"\ud83d\ude80 2. Escalabilidade e performance","text":"Crit\u00e9rio Avalia\u00e7\u00e3o \ud83e\udde0 Cache com TTL Evita chamadas repetidas a backends remotos, melhora resposta. \u23f1 TTL por chave Controle refinado de cache, ideal para chaves cr\u00edticas. \u2699\ufe0f Fallback local Suporte a fallback de m\u00faltiplas fontes: Vault, .env, arquivos, etc. \ud83e\udde9 Integra\u00e7\u00e3o com Vault Compat\u00edvel com versionamento e rota\u00e7\u00e3o autom\u00e1tica. \u2705 Nota: Projeto est\u00e1vel e preparado para carga elevada e sistemas distribu\u00eddos."},{"location":"quality/nala-quality-framework/#3-seguranca","title":"\ud83d\udd10 3. Seguran\u00e7a","text":"Crit\u00e9rio Avalia\u00e7\u00e3o \ud83d\udd10 Vault como backend seguro Evita segredos hardcoded, autentica\u00e7\u00e3o via AppRole e policies. \ud83e\uddfe Arquivos tempor\u00e1rios para certs Evita persist\u00eancia de dados sens\u00edveis no disco. \ud83e\uddea Restri\u00e7\u00f5es e valida\u00e7\u00f5es Campos obrigat\u00f3rios, checagens com <code>is_authenticated()</code>, etc. \ud83d\udd01 Retry com limites e logs Evita loop infinito e permite observa\u00e7\u00e3o segura de falhas. \u2705 Nota: Excelente seguran\u00e7a base. Criptografia local com Fernet est\u00e1 no radar."},{"location":"quality/nala-quality-framework/#4-modularidade-e-extensibilidade","title":"\ud83e\uddf1 4. Modularidade e extensibilidade","text":"Crit\u00e9rio Avalia\u00e7\u00e3o \ud83d\udd0c Providers plug\u00e1veis Arquivo, Vault, Env, GCP, AWS, Azure, facilmente intercambi\u00e1veis. \ud83e\udde0 Composi\u00e7\u00e3o via Decorators Cache, fallback e retry empilh\u00e1veis com facilidade. \ud83e\uddf1 Compat\u00edvel com DI Providers s\u00e3o f\u00e1ceis de registrar/injetar dinamicamente. \ud83d\udd27 Suporte a ambientes Configur\u00e1vel por settings.toml, .env, secrets.toml, etc. \u2705 Nota: Sistema pronto para expans\u00e3o segura em ambientes diversos."},{"location":"quality/nala-quality-framework/#5-criterios-adicionais-recomendados","title":"\ud83e\uddea 5. Crit\u00e9rios adicionais recomendados","text":"Crit\u00e9rio Por que \u00e9 importante \ud83d\udcca Observabilidade M\u00e9tricas de cache hits, fallback hits, erro de providers. \ud83d\udcc1 Suporte a chaves bin\u00e1rias <code>.p12</code>, base64 ou bytes \u2013 necess\u00e1rio para certificados, blobs, etc. \ud83d\udd04 Rota\u00e7\u00e3o ativa real Vault Transit/PKI e Key rotation com TTL autom\u00e1tico. \ud83d\udd10 Pol\u00edticas de acesso Integra\u00e7\u00e3o com RBAC, pol\u00edticas Vault e controle por servi\u00e7o. \u2601\ufe0f Cloud-native providers Suporte nativo a AWS, GCP, Azure e suas pol\u00edticas espec\u00edficas. \ud83d\udd00 Registry din\u00e2mico Auto-registro e fallback entre m\u00faltiplos providers dinamicamente. \ud83e\uddf0 Configura\u00e7\u00e3o robusta Uso de Dynaconf, .env, .secrets.toml e separa\u00e7\u00e3o clara de ambientes."},{"location":"quality/nala-quality-framework/#objetivo-do-framework","title":"\ud83c\udfaf Objetivo do Framework","text":"<p>Este framework serve como uma r\u00e9gua de qualidade para APIs que lidam com segredos, autentica\u00e7\u00e3o e dados sens\u00edveis. Ele \u00e9 ideal para projetos que exigem:</p> <ul> <li>Alta disponibilidade</li> <li>Seguran\u00e7a corporativa (Vault, AppRole, pol\u00edticas)</li> <li>Testabilidade desde a V0</li> <li>Extensibilidade com novos providers, formatos ou integra\u00e7\u00f5es</li> <li>Robustez contra falhas transit\u00f3rias</li> </ul>"},{"location":"quality/nala-quality-framework/#conclusao","title":"\ud83d\udccc Conclus\u00e3o","text":"<p>O uso consistente deste framework permite avaliar e evoluir projetos com base em crit\u00e9rios objetivos, garantindo qualidade t\u00e9cnica, seguran\u00e7a e sustentabilidade de longo prazo. Pode ser usado como guia de arquitetura, checklists de PR ou auditoria t\u00e9cnica.</p>"},{"location":"quality/plano-core-data-service-nala-iq/","title":"\ud83d\udccc Plano de Execu\u00e7\u00e3o \u2013 Core Data Service / Knowledge API do Nala-IQ","text":"<p>Fonte \u00fanica de verdade dos crit\u00e9rios, dimens\u00f5es, rela\u00e7\u00f5es, vers\u00f5es e m\u00e9tricas do Nala-IQ. Consumido por: site de docs, scorecards, agente de AI, dashboards e integra\u00e7\u00f5es.</p>"},{"location":"quality/plano-core-data-service-nala-iq/#objetivos","title":"\ud83c\udfaf Objetivos","text":"<ul> <li>Centralizar conte\u00fado e metadados do framework.</li> <li>Servir via API (est\u00e1vel, versionada, com RBAC/premium).</li> <li>Permitir an\u00e1lises/score e planejamento 30/60/90 a partir dos gaps.</li> <li>Manter f\u00e1cil (autoria simples, versionamento e evolu\u00e7\u00e3o do schema sem dor).</li> </ul>"},{"location":"quality/plano-core-data-service-nala-iq/#principios","title":"\ud83e\udded Princ\u00edpios","text":"<ul> <li>Autorias leg\u00edveis (Markdown + YAML).</li> <li>Versionamento em Git (PRs, revis\u00e3o, changelog).</li> <li>Esquema flex\u00edvel (PostgreSQL + JSONB).</li> <li>Rela\u00e7\u00f5es expl\u00edcitas (depende de, relacionado, refor\u00e7a).</li> <li>APIs previs\u00edveis (OpenAPI + testes de contrato).</li> </ul>"},{"location":"quality/plano-core-data-service-nala-iq/#arquitetura-alto-nivel","title":"\ud83c\udfd7\ufe0f Arquitetura (alto n\u00edvel)","text":"<pre><code>flowchart LR\n  A[MD + YAML (Git)] --&gt; B[CI: validar + transformar]\n  B --&gt; C[(PostgreSQL JSONB)]\n  C --&gt; D[[Core Data Service API]]\n  D --&gt; E[Site de Docs]\n  D --&gt; F[Scorecards/Dash]\n  D --&gt; G[Agente de AI (RAG + fun\u00e7\u00f5es)]\n  C --&gt; H[(Opcional: Neo4j)]\n</code></pre> <p>Stack sugerida: - Service: FastAPI (Py) ou NestJS (Node). - DB: PostgreSQL (JSONB + pg_trgm/pgvector). - Search: pgvector/Qdrant (futuro). - Grafo (opcional): Neo4j (espelhado). - Infra: Docker + GitHub Actions + Cloud Run/K8s.</p>"},{"location":"quality/plano-core-data-service-nala-iq/#autoria-conteudo-fonte","title":"\ud83d\uddc2\ufe0f Autoria (conte\u00fado fonte)","text":"<p>Um crit\u00e9rio = 1 arquivo Markdown com front-matter YAML. Ex.: <code>content/criteria/2.3-tls-1-3.md</code></p> <pre><code>id: \"2.3-tls-1-3\"\ntitulo: \"Transporte seguro (TLS 1.3)\"\ndimensao: \"Seguran\u00e7a\"\nsubdimensao: \"Transporte\"\nimpacto: \"alto\"          # baixo | medio | alto\npeso: 5                  # 1..5\ncriticidade: \"obrigatorio\" # obrigatorio | recomendado | diferencial\naplicabilidade: [\"APIs_publicas\",\"APIs_internas\",\"Reguladas\"]\nmaturidade: [\"N1\",\"N3\",\"N5\"]\nbeneficios:\n  curto_prazo: [\"Mitiga MITM\"]\n  longo_prazo: [\"Alinha compliance\"]\nriscos:\n  curto_prazo: [\"Exposi\u00e7\u00e3o de tr\u00e1fego\"]\n  longo_prazo: [\"San\u00e7\u00f5es LGPD/GDPR\"]\ndependencias: [\"2.1-autenticacao\",\"2.2-autorizacao\"]\nrelacionados: [\"9.2-logging-seguro\",\"12.4-slas\"]\nmetricas: [\"% de endpoints com TLS &gt;= 1.3\"]\nperguntas_diagnostico:\n  - \"Todos os endpoints exigem TLS 1.3?\"\naceitacao:\n  - \"Scan externo comprova TLS &gt;= 1.3 em 100% dos hosts\"\nacoes_recomendadas:\n  - \"Habilitar TLS 1.3 no LB e automatizar renova\u00e7\u00e3o (ACME)\"\nexemplos_validos:\n  - tipo: \"nginx\"\n    snippet: \"ssl_protocols TLSv1.3;\"\nlinks:\n  - \"https://owasp.org/API-Security/\"\n  - \"https://nginx.org/en/docs/http/configuring_https_servers.html\"\npremium: false\nversao: \"1.0.0\"\n</code></pre> <p>O corpo do <code>.md</code> cont\u00e9m narrativa detalhada, diagramas e observa\u00e7\u00f5es.</p>"},{"location":"quality/plano-core-data-service-nala-iq/#modelo-de-dados-postgresql","title":"\ud83d\uddc4\ufe0f Modelo de Dados (PostgreSQL)","text":"<pre><code>-- crit\u00e9rios, campos \u201cfixos\u201d\nCREATE TABLE criteria (\n  id TEXT PRIMARY KEY,\n  titulo TEXT NOT NULL,\n  dimensao TEXT NOT NULL,\n  subdimensao TEXT,\n  impacto TEXT,            -- baixo|medio|alto\n  peso INT,                -- 1..5\n  criticidade TEXT,        -- obrigatorio|recomendado|diferencial\n  aplicabilidade TEXT[],   -- ex: {APIs_publicas,Reguladas}\n  premium BOOLEAN DEFAULT FALSE,\n  versao TEXT,\n  conteudo_md TEXT,        -- markdown completo\n  metadata JSONB,          -- beneficios, riscos, perguntas, aceitacao, acoes, exemplos, links...\n  created_at TIMESTAMPTZ DEFAULT now(),\n  updated_at TIMESTAMPTZ DEFAULT now()\n);\n\n-- rela\u00e7\u00f5es entre crit\u00e9rios\nCREATE TABLE relations (\n  source_id TEXT REFERENCES criteria(id),\n  target_id TEXT REFERENCES criteria(id),\n  tipo TEXT CHECK (tipo IN ('depende_de','relacionado','reforca')),\n  PRIMARY KEY (source_id, target_id, tipo)\n);\n\n-- refer\u00eancias externas (normas/links)\nCREATE TABLE refs (\n  criteria_id TEXT REFERENCES criteria(id),\n  fonte TEXT,\n  url TEXT\n);\n\n-- tags livres\nCREATE TABLE tags (tag TEXT PRIMARY KEY);\nCREATE TABLE criteria_tags (\n  criteria_id TEXT REFERENCES criteria(id),\n  tag TEXT REFERENCES tags(tag),\n  PRIMARY KEY (criteria_id, tag)\n);\n\n-- changelog\nCREATE TABLE changelogs (\n  criteria_id TEXT REFERENCES criteria(id),\n  versao TEXT,\n  autor TEXT,\n  nota TEXT,\n  data TIMESTAMPTZ DEFAULT now()\n);\n\n-- \u00edndices \u00fateis\nCREATE INDEX ON criteria USING GIN (metadata jsonb_path_ops);\nCREATE INDEX criteria_title_trgm ON criteria USING GIN (titulo gin_trgm_ops);\n</code></pre> <p>JSONB permite adicionar novos campos sem migra\u00e7\u00e3o pesada. relations habilita an\u00e1lise de depend\u00eancias (e pode ser espelhado no Neo4j depois).</p>"},{"location":"quality/plano-core-data-service-nala-iq/#superficies-da-api-v1","title":"\ud83d\udd0c Superf\u00edcies da API (v1)","text":""},{"location":"quality/plano-core-data-service-nala-iq/#leitura","title":"Leitura","text":"<ul> <li><code>GET /v1/criteria?dimensao=Seguranca&amp;impacto=alto&amp;premium=false</code></li> <li><code>GET /v1/criteria/{id}</code></li> <li><code>GET /v1/dimensions</code> \u2022 <code>GET /v1/dimensions/{id}</code></li> <li><code>GET /v1/relations/{id}</code> \u2192 <code>{depende_de:[...],dependentes:[...],relacionados:[...]}</code> </li> <li><code>GET /v1/search?q=tls%201.3&amp;top_k=10</code> (texto/sem\u00e2ntico)</li> <li><code>GET /v1/export?format=json|csv</code></li> </ul>"},{"location":"quality/plano-core-data-service-nala-iq/#calculo","title":"C\u00e1lculo","text":"<ul> <li><code>POST /v1/score</code></li> </ul> <pre><code>{\n  \"profile\": \"externa_regulada\",\n  \"answers\": [\n    {\"criterion_id\":\"2.3-tls-1-3\", \"value\":5},\n    {\"criterion_id\":\"9.2-logging-seguro\", \"value\":3}\n  ]\n}\n</code></pre> <p>Resposta (exemplo):</p> <pre><code>{\n  \"score_total\": 0.82,\n  \"por_dimensao\": [{\"dim\":\"Seguran\u00e7a\",\"score\":0.91}],\n  \"gaps\": [{\"criterion_id\":\"9.2-logging-seguro\",\"impacto\":\"alto\",\"peso\":5}]\n}\n</code></pre> <ul> <li><code>POST /v1/plan</code> (30/60/90)</li> </ul> <pre><code>{\n  \"gaps\":[{\"criterion_id\":\"9.2-logging-seguro\",\"impacto\":\"alto\",\"peso\":5}],\n  \"constraints\":{\"prazo_dias\":90,\"budget\":\"medio\"}\n}\n</code></pre>"},{"location":"quality/plano-core-data-service-nala-iq/#admin-privado","title":"Admin (privado)","text":"<ul> <li><code>POST /v1/ingest/sync</code> (gatilho de sincroniza\u00e7\u00e3o p\u00f3s-merge)</li> <li>(Opcional) <code>POST/PATCH /v1/criteria</code> (edi\u00e7\u00e3o via UI interna)</li> </ul>"},{"location":"quality/plano-core-data-service-nala-iq/#scoring-sugestao-pratica","title":"\ud83e\uddee Scoring (sugest\u00e3o pr\u00e1tica)","text":"<ul> <li>Por crit\u00e9rio: <code>nota \u00d7 peso \u00d7 fator_impacto \u00d7 aplicabilidade</code></li> <li><code>fator_impacto</code>: baixo=1.0 | m\u00e9dio=1.25 | alto=1.5  </li> <li><code>aplicabilidade</code>: 0 (n\u00e3o se aplica) | 1 (aplica)</li> <li>Ajuste por perfil (ex.: regulada \u2191 seguran\u00e7a/compliance; p\u00fablica \u2191 DX/perf).</li> <li>Confian\u00e7a: estimar confian\u00e7a (alta/m\u00e9dia/baixa) baseada nas evid\u00eancias fornecidas.</li> </ul>"},{"location":"quality/plano-core-data-service-nala-iq/#acesso-planos-freepremium","title":"\ud83d\udd10 Acesso &amp; Planos (free/premium)","text":"<ul> <li>Campo <code>premium</code> nos crit\u00e9rios controla visibilidade.</li> <li>Auth via API keys/OAuth2; RBAC (<code>public</code>, <code>customer</code>, <code>admin</code>).</li> <li>Rate limit por plano; ETag/If-None-Match em recursos est\u00e1ticos.</li> </ul>"},{"location":"quality/plano-core-data-service-nala-iq/#performance-confiabilidade","title":"\u26a1 Performance &amp; Confiabilidade","text":"<ul> <li>Cache de leitura (CDN/APIGW) para endpoints p\u00fablicos.</li> <li>\u00cdndices GIN (JSONB), trgm (texto), pgvector (busca sem\u00e2ntica futura).</li> <li>Deploy stateless (Blue/Green), logs estruturados, m\u00e9tricas (Prometheus), tracing.</li> </ul>"},{"location":"quality/plano-core-data-service-nala-iq/#cicd-pipeline","title":"\ud83d\udd04 CI/CD (pipeline)","text":"<ol> <li>Lint/Schema: validar YAML (JSON Schema), IDs, campos obrigat\u00f3rios.  </li> <li>Build: transformar MD+YAML \u2192 JSON normalizado.  </li> <li>Ingest: upsert no Postgres; criar/atualizar rela\u00e7\u00f5es/refs/tags.  </li> <li>Contrato: testes de API (OpenAPI + Dredd/Postman).  </li> <li>Publish: redeploy do servi\u00e7o + rebuild do site de docs.</li> </ol>"},{"location":"quality/plano-core-data-service-nala-iq/#monorepo-sugestao","title":"\ud83d\udce6 Monorepo (sugest\u00e3o)","text":"<pre><code>/content      # MD+YAML (fonte)\n  /criteria\n/ingest       # scripts de valida\u00e7\u00e3o e carga\n/service      # API (FastAPI/NestJS)\n/site         # Docusaurus/MkDocs (docs)\n</code></pre>"},{"location":"quality/plano-core-data-service-nala-iq/#roadmap-4-sprints","title":"\ud83d\uddfa\ufe0f Roadmap (4 sprints)","text":"Sprint Foco Entregas S1 \u2013 Fundacional Schema + leitura Postgres, ingest\u00e3o MD\u2192DB, <code>GET /criteria</code>, <code>GET /dimensions</code>, OpenAPI S2 \u2013 Rela\u00e7\u00f5es &amp; Busca Contexto <code>relations</code>, <code>GET /relations/{id}</code>, search trgm, export JSON/CSV S3 \u2013 Score &amp; Plano Valor <code>POST /score</code>, <code>POST /plan</code>, perfis e pesos S4 \u2013 Premium &amp; Obs Produto RBAC/keys, m\u00e9tricas/tracing, rate limit, cache/CDN"},{"location":"quality/plano-core-data-service-nala-iq/#indicadores-de-sucesso","title":"\ud83d\udcca Indicadores de Sucesso","text":"<ul> <li>T\u00e9cnicos: lat\u00eancia p95, erros &lt;0.1%, cobertura de testes, sucesso de ingest\u00f5es.  </li> <li>Produto: n\u00ba de crit\u00e9rios, consumo por consumidores (site/agent), gera\u00e7\u00e3o de planos, leads premium.  </li> </ul>"},{"location":"quality/plano-core-data-service-nala-iq/#extensoes-futuras","title":"\ud83d\udd2e Extens\u00f5es futuras","text":"<ul> <li>Neo4j para an\u00e1lise de depend\u00eancias (impacto em cascata, caminhos m\u00ednimos).  </li> <li>Benchmarks setoriais (opt-in) e relat\u00f3rios executivos.  </li> <li>Integra\u00e7\u00f5es (Apigee/Kong/Postman) para coleta semi-autom\u00e1tica de evid\u00eancias.  </li> </ul> <p>Resumo: MD+YAML (autoria) \u2192 Postgres JSONB (verdade) \u2192 API est\u00e1vel (consumo). Simples para come\u00e7ar, forte para escalar e perfeito para ligar site, scorecards e agente de AI.</p>"},{"location":"quality/roadmap/","title":"\ud83d\udccc Roadmap Priorit\u00e1rio de Evolu\u00e7\u00e3o do Nala-IQ","text":"<p>Estruturado por vers\u00f5es (v2, v2.1, v3, v4+), considerando impacto, esfor\u00e7o e alinhamento estrat\u00e9gico.</p>"},{"location":"quality/roadmap/#nala-iq-v2-2025-alta-prioridade-quick-wins","title":"\ud83d\ude80 Nala-IQ v2 (2025) \u2013 Alta Prioridade / Quick Wins","text":"<p>Objetivo: Expandir cobertura em \u00e1reas cr\u00edticas de custo, pessoas e inclus\u00e3o. Foco: Entregas de alto impacto e esfor\u00e7o moderado.</p> <ul> <li>FinOps Framework</li> <li>Introduzir sub-dimens\u00e3o de FinOps (custos, TCO, ROI).</li> <li>Impacto: Muito alto | Esfor\u00e7o: M\u00e9dio.</li> <li> <p>Por qu\u00ea: torna a API um ativo financeiro transparente.</p> </li> <li> <p>Cultura &amp; Pessoas (nova Dimens\u00e3o XVI)</p> </li> <li>Avaliar habilidades, ownership, colabora\u00e7\u00e3o e melhoria cont\u00ednua.</li> <li>Impacto: Muito alto | Esfor\u00e7o: M\u00e9dio.</li> <li> <p>Por qu\u00ea: visibilidade estrat\u00e9gica ao fator humano.</p> </li> <li> <p>WCAG (Acessibilidade \u2013 DX)</p> </li> <li>Sub-dimens\u00e3o dentro de Experi\u00eancia do Desenvolvedor.</li> <li>Impacto: M\u00e9dio-alto | Esfor\u00e7o: Baixo.</li> <li>Por qu\u00ea: inclus\u00e3o + compliance internacional.</li> </ul>"},{"location":"quality/roadmap/#nala-iq-v21-2026-consolidacao-tecnica","title":"\ud83d\udcc8 Nala-IQ v2.1 (2026) \u2013 Consolida\u00e7\u00e3o T\u00e9cnica","text":"<p>Objetivo: Fortalecer robustez, confiabilidade e qualidade de dados. Foco: Expandir crit\u00e9rios t\u00e9cnicos e de governan\u00e7a.</p> <ul> <li>Gest\u00e3o de Riscos (ISO 31000 / NIST RMF)</li> <li>Crit\u00e9rios expl\u00edcitos de riscos para APIs (financeiros, operacionais, regulat\u00f3rios).</li> <li> <p>Impacto: Alto | Esfor\u00e7o: M\u00e9dio.</p> </li> <li> <p>ISO/IEC 25012 (Qualidade de Dados)</p> </li> <li>Crit\u00e9rios para integridade, acur\u00e1cia, consist\u00eancia dos dados.</li> <li> <p>Impacto: Alto | Esfor\u00e7o: M\u00e9dio.</p> </li> <li> <p>SRE Practices (SLOs e Error Budgets)</p> </li> <li>Gest\u00e3o ativa de confiabilidade e or\u00e7amentos de erro.</li> <li>Impacto: Muito alto | Esfor\u00e7o: M\u00e9dio.</li> </ul>"},{"location":"quality/roadmap/#nala-iq-v3-2027-expansao-estrategica","title":"\ud83c\udf0d Nala-IQ v3 (2027) \u2013 Expans\u00e3o Estrat\u00e9gica","text":"<p>Objetivo: Incorporar frameworks de seguran\u00e7a avan\u00e7ada, arquitetura e ESG. Foco: Atender demandas regulat\u00f3rias e de grandes organiza\u00e7\u00f5es.</p> <ul> <li>Zero Trust (NIST 800-207)</li> <li>Arquitetura \u201cnever trust, always verify\u201d aplicada a APIs.</li> <li> <p>Impacto: Muito alto | Esfor\u00e7o: Alto.</p> </li> <li> <p>ISO 27001 / 27701</p> </li> <li>Seguran\u00e7a da informa\u00e7\u00e3o + privacidade organizacional.</li> <li> <p>Impacto: Muito alto | Esfor\u00e7o: Alto.</p> </li> <li> <p>TOGAF</p> </li> <li>Crit\u00e9rios de alinhamento arquitetural com a organiza\u00e7\u00e3o.</li> <li> <p>Impacto: Alto | Esfor\u00e7o: Alto.</p> </li> <li> <p>GHG Protocol / ISO 14064 + SBTi</p> </li> <li>ESG avan\u00e7ado: carbono digital e metas sustent\u00e1veis.</li> <li> <p>Impacto: M\u00e9dio-alto | Esfor\u00e7o: Alto.</p> </li> <li> <p>C4 Model / Arc42</p> </li> <li>Crit\u00e9rios de documenta\u00e7\u00e3o arquitetural.</li> <li>Impacto: M\u00e9dio | Esfor\u00e7o: Baixo.</li> </ul>"},{"location":"quality/roadmap/#nala-iq-v4-2028-em-diante-visao-de-futuro","title":"\ud83e\udded Nala-IQ v4+ (2028 em diante) \u2013 Vis\u00e3o de Futuro","text":"<p>Objetivo: Transformar o Nala-IQ em padr\u00e3o global, conectando APIs a estrat\u00e9gia e inova\u00e7\u00e3o. Foco: Neg\u00f3cio, plataformas digitais e inova\u00e7\u00e3o.</p> <ul> <li>Balanced Scorecard (BSC)</li> <li>Alinhar APIs com objetivos estrat\u00e9gicos do neg\u00f3cio.</li> <li> <p>Impacto: Alto | Esfor\u00e7o: M\u00e9dio.</p> </li> <li> <p>Porter\u2019s Value Chain</p> </li> <li>Mostrar APIs como habilitadoras da cadeia de valor.</li> <li> <p>Impacto: Alto | Esfor\u00e7o: M\u00e9dio.</p> </li> <li> <p>Platform Design Toolkit</p> </li> <li>APIs como parte de ecossistemas digitais e marketplaces.</li> <li> <p>Impacto: Alto | Esfor\u00e7o: M\u00e9dio.</p> </li> <li> <p>OKRs</p> </li> <li>Conectar APIs a objetivos organizacionais.</li> <li> <p>Impacto: M\u00e9dio | Esfor\u00e7o: Baixo.</p> </li> <li> <p>ISO 29119 (Software Testing)</p> </li> <li>Padr\u00f5es de testes de software como crit\u00e9rio de maturidade.</li> <li> <p>Impacto: Alto | Esfor\u00e7o: M\u00e9dio.</p> </li> <li> <p>Mutation Testing</p> </li> <li>Crit\u00e9rios de robustez em testes avan\u00e7ados.</li> <li> <p>Impacto: M\u00e9dio | Esfor\u00e7o: Baixo.</p> </li> <li> <p>NIST AI RMF</p> </li> <li>Avalia\u00e7\u00e3o de riscos em APIs que exp\u00f5em modelos de AI.</li> <li>Impacto: M\u00e9dio-alto | Esfor\u00e7o: Alto.</li> </ul>"},{"location":"quality/roadmap/#resumo-estrategico","title":"\ud83c\udfaf Resumo estrat\u00e9gico","text":"<ul> <li>v2 (w1): FinOps, Cultura &amp; Pessoas, Acessibilidade \u2192 alto impacto imediato.  </li> <li>v2.1 (w2): Riscos, Qualidade de Dados, SRE \u2192 consolida\u00e7\u00e3o t\u00e9cnica.  </li> <li>v3 (w3): Zero Trust, ISO 27001, ESG avan\u00e7ado, Arquitetura \u2192 seguran\u00e7a e grandes empresas.  </li> <li>v4+ (w4): Neg\u00f3cio, inova\u00e7\u00e3o e AI \u2192 APIs como ativos estrat\u00e9gicos globais.</li> </ul>"},{"location":"quality/benckmark/athomic-code-quality-methodology/","title":"\ud83e\udde0 Athomic Code Quality &amp; Architecture Review Methodology","text":"<p>Este documento define uma metodologia estruturada para an\u00e1lise da qualidade de c\u00f3digo e arquitetura de m\u00f3dulos da Athomic \u2014 ou qualquer framework moderno.</p>"},{"location":"quality/benckmark/athomic-code-quality-methodology/#objetivo","title":"\ud83c\udfaf Objetivo","text":"<p>Avaliar se as decis\u00f5es t\u00e9cnicas, padr\u00f5es de projeto e estruturas aplicadas em cada m\u00f3dulo est\u00e3o alinhadas com:</p> <ul> <li>Boas pr\u00e1ticas de engenharia de software</li> <li>Escalabilidade, manutenibilidade e extensibilidade</li> <li>Solu\u00e7\u00f5es adotadas por frameworks enterprise (Django, Spring, FastAPI, etc.)</li> </ul>"},{"location":"quality/benckmark/athomic-code-quality-methodology/#criterios-de-avaliacao-por-modulo","title":"\ud83e\uddf1 Crit\u00e9rios de Avalia\u00e7\u00e3o por M\u00f3dulo","text":"Crit\u00e9rio Descri\u00e7\u00e3o Design Pattern aplicado Padr\u00f5es utilizados (e.g., Factory, Decorator, Strategy) Responsabilidade \u00fanica (SRP) O componente respeita a coes\u00e3o ou mistura responsabilidades? Extensibilidade (OCP) O c\u00f3digo \u00e9 facilmente estendido sem ser modificado? Acoplamento e coes\u00e3o Avalia\u00e7\u00e3o do grau de depend\u00eancia entre m\u00f3dulos Testabilidade \u00c9 f\u00e1cil isolar, testar e mockar o comportamento? Observabilidade Existem m\u00e9tricas e logs rastre\u00e1veis por componente? Composi\u00e7\u00e3o As funcionalidades s\u00e3o combin\u00e1veis ou s\u00e3o \"caixas fechadas\"? Contratos claros (Interfaces) Existem interfaces/protocolos ou o c\u00f3digo est\u00e1 acoplado? Equival\u00eancia com frameworks refer\u00eancia Como grandes frameworks resolveram problemas semelhantes? Clareza e sem\u00e2ntica do c\u00f3digo O c\u00f3digo expressa claramente sua inten\u00e7\u00e3o?"},{"location":"quality/benckmark/athomic-code-quality-methodology/#frameworks-de-comparacao","title":"\ud83e\uddea Frameworks de Compara\u00e7\u00e3o","text":"Framework Padr\u00f5es \u00fateis observados Django AppConfig, signals, middleware, configura\u00e7\u00e3o modular Spring Boot Beans, <code>@DependsOn</code>, <code>@Component</code>, Lifecycle Hooks FastAPI Dependency injection, Pydantic, APIRouter NestJS Modules, Guards, Interceptors, Providers Resilience4j Builder + Decorator + Registry + Metrics"},{"location":"quality/benckmark/athomic-code-quality-methodology/#exemplo-de-analise-resilience","title":"\ud83d\udd04 Exemplo de An\u00e1lise (Resilience)","text":"Aspecto Athomic Resilience4j Status Padr\u00e3o aplicado Decorator Decorator fluente + Registry \ud83d\udfe1 Composi\u00e7\u00e3o de estrat\u00e9gias Parcial (encadeamento manual) Totalmente compos\u00e1vel \ud83d\udd34 Contrato/Interface Impl\u00edcito Expl\u00edcito (<code>Retry</code>, <code>CircuitBreaker</code>) \ud83d\udd36 Observabilidade por componente B\u00e1sica (logs) Avan\u00e7ada (Micrometer) \ud83d\udfe1"},{"location":"quality/benckmark/athomic-code-quality-methodology/#como-utilizar","title":"\u2705 Como utilizar","text":"<ol> <li>Selecionar m\u00f3dulo (ex: messaging, config, observability)</li> <li>Aplicar os crit\u00e9rios acima</li> <li>Comparar com solu\u00e7\u00e3o de refer\u00eancia</li> <li>Listar sugest\u00f5es pr\u00e1ticas de refatora\u00e7\u00e3o e evolu\u00e7\u00e3o</li> <li>Definir plano incremental de melhoria</li> </ol>"},{"location":"quality/benckmark/athomic-code-quality-methodology/#sugestao-de-formato-de-relatorio","title":"\ud83d\udccc Sugest\u00e3o de Formato de Relat\u00f3rio","text":"<pre><code>## M\u00f3dulo: observability\n\n### \u2705 Padr\u00f5es aplicados:\n- Decorator\n- Registry\n\n### \u26a0\ufe0f Oportunidades:\n- Falta isolamento de m\u00e9trica por plugin\n- Logging ainda acoplado \u00e0 configura\u00e7\u00e3o global\n\n### \ud83e\uddea Comparativo com NestJS:\n- Athomic usa decorators funcionais\n- NestJS usa interceptors e inje\u00e7\u00e3o de contexto\n\n### \u2705 Sugest\u00f5es:\n- Introduzir `ObservabilityContext`\n- Separar logger por tenant/plugin\n- CLI: `athomic trace &lt;id&gt;`\n\n</code></pre> <p>Essa metodologia pode ser usada internamente para revis\u00e3o t\u00e9cnica ou como guia de arquitetura evolutiva em projetos open source ou corporativos.</p>"},{"location":"quality/benckmark/athomic_big_picture/","title":"\ud83c\udf10 Athomic Big Picture \u2013 Mapeamento de Componentes vs Frameworks Enterprise","text":"<p>Este documento apresenta uma vis\u00e3o panor\u00e2mica dos principais m\u00f3dulos, subm\u00f3dulos e componentes da Athomic comparados com frameworks de refer\u00eancia como:</p> <ul> <li>\ud83d\udfe9 Spring Boot (Java)</li> <li>\ud83d\udfe6 NestJS (NodeJS)</li> <li>\ud83d\udfe8 Django (Python)</li> <li>\ud83d\udfe5 FastAPI (Python)</li> </ul>"},{"location":"quality/benckmark/athomic_big_picture/#matriz-de-comparacao","title":"\ud83d\udcca Matriz de Compara\u00e7\u00e3o","text":"Categoria / Componente Athomic Spring Boot NestJS Django / FastAPI Configura\u00e7\u00e3o Central Dynaconf + Pydantic + Contracts <code>@ConfigurationProperties</code> <code>ConfigService</code>, <code>.env</code> settings.py / Pydantic Secrets Management Vault, Env, Fallback Spring Vault Custom Provider None (manual/env) Feature Flags Modular + Redis + Decorators Togglz / FF4J LaunchDarkly / Unleash (via lib) None / Third-party Resili\u00eancia Retry, Circuit, Timeout, Fallback Resilience4j Interceptors, Guards <code>httpx</code>, custom decorators Rate Limiter limits lib + decorators Bucket4j Rate-limiter middleware <code>slowapi</code> / custom Observabilidade Logging, Maskers, Tracing, Metrics Micrometer + Sleuth <code>Interceptor + Prometheus</code> Prometheus + OTLP (manual) Messaging Kafka, RabbitMQ, Delay, Retry Spring Cloud Stream ClientsModule + Kafka Celery / Kombu / Pika Outbox Mongo-based with retry &amp; dlq Outbox Pattern (JPA/Eventuate) Manual Manual (via DB/queue) Plugins Plugin registry in progress Spring Plugins NestJS Modules Django Apps / FastAPI Routers Lifecycle FastAPI lifespan + registry Lifecycle Beans / @PostConstruct <code>onModuleInit</code>, <code>onAppStart</code> <code>AppConfig.ready()</code> Security/Auth JWT, IP Filters, Roles Spring Security Guards + JWT Module Django Auth / FastAPI JWT HTTP Client Async HTTPX + auth strategies WebClient / RestTemplate Axios + Interceptors HTTPX + retry KVStore Redis/Memory, Wrappers, TTL RedisTemplate / Lettuce <code>@Cacheable()</code> <code>aioredis</code> / <code>cachetools</code> Caching Decorators + Redis + Brotli Spring Cache + Micrometer CacheModule <code>fastapi-cache</code> / Manual Compress\u00e3o Brotli / GZIP providers Manual or via filters Middleware <code>brotli-asgi</code> Events Redis pub/sub / Local ApplicationEventPublisher EventEmitter / Observable Signals / FastAPI events Database - KV/Docs Mongo, Redis, Local Spring Data / Repositories Mongoose / TypeORM ODMantic / SQLModel HTTP Services HTTP client + strategy RestTemplate / WebClient HttpModule + Interceptors HTTPX / Requests Credential Proxy Chainable vault/env/file proxy Secrets Bean None (manual) None (manual)"},{"location":"quality/benckmark/athomic_big_picture/#parecer-por-dominio","title":"\ud83e\udde0 Parecer por Dom\u00ednio","text":"<ul> <li>Configura\u00e7\u00e3o: estrutura modular, contratos e valida\u00e7\u00e3o superiores a muitos frameworks Python.</li> <li>Seguran\u00e7a: JWT, IP e escopo com decorators s\u00e3o fortes, mas faltam integra\u00e7\u00f5es OAuth2 / SSO.</li> <li>Observabilidade: logging estruturado com masking \u00e9 destaque; m\u00e9tricas precisam padroniza\u00e7\u00e3o e amplitude.</li> <li>Mensageria: interface e providers robustos. Faltam visualiza\u00e7\u00e3o e schema registry unificado.</li> <li>Performance: decorators, cache e compress\u00e3o bem implementados, prontos para produ\u00e7\u00e3o.</li> <li>Resili\u00eancia: boa base funcional, precisa composi\u00e7\u00e3o fluente e estrat\u00e9gia unificada.</li> <li>Plugins/Lifecycle: proposta promissora; implementa\u00e7\u00e3o ainda em andamento.</li> <li>Secrets/FeatureFlags: arquitetura poderosa e segura; podem evoluir com CLI e visibilidade.</li> </ul> <p>A Athomic se posiciona como uma alternativa Python extens\u00edvel e enterprise-grade com conceitos inspirados nos melhores frameworks do mercado, mantendo uma separa\u00e7\u00e3o clara por m\u00f3dulo e forte ader\u00eancia a boas pr\u00e1ticas.</p>"},{"location":"quality/benckmark/athomic_config_quality_review/","title":"\u2699\ufe0f Athomic \u2013 Code Quality &amp; Architecture Review: M\u00f3dulo Config/Settings","text":"<p>Este relat\u00f3rio aplica a metodologia de revis\u00e3o arquitetural ao m\u00f3dulo <code>config</code>, respons\u00e1vel por centralizar a configura\u00e7\u00e3o da aplica\u00e7\u00e3o com Dynaconf e valida\u00e7\u00e3o via Pydantic.</p>"},{"location":"quality/benckmark/athomic_config_quality_review/#contexto-do-modulo","title":"\ud83d\udce6 Contexto do M\u00f3dulo","text":"<p>O m\u00f3dulo define settings centralizados com suporte a m\u00faltiplos ambientes (<code>.env</code>, <code>.secrets.toml</code>, <code>settings.toml</code>) usando Dynaconf. Contratos s\u00e3o implementados com Pydantic por dom\u00ednio (e.g., <code>MessagingSettings</code>, <code>AuthSettings</code>) e validados dinamicamente.</p>"},{"location":"quality/benckmark/athomic_config_quality_review/#avaliacao-por-criterio","title":"\u2705 Avalia\u00e7\u00e3o por Crit\u00e9rio","text":"Crit\u00e9rio Status Coment\u00e1rio Design Patterns aplicados \ud83d\udfe2 Settings Registry, Strategy, Contracts, Validation Layer. SRP (Responsabilidade \u00fanica) \ud83d\udfe2 Cada m\u00f3dulo tem seus pr\u00f3prios contratos e configura\u00e7\u00e3o dedicada. Extensibilidade (OCP) \ud83d\udfe2 F\u00e1cil de adicionar novos blocos de configura\u00e7\u00e3o com valida\u00e7\u00e3o. Contratos claros (Interface) \ud83d\udfe2 Uso extensivo de <code>BaseSettings</code> do Pydantic. Testabilidade \ud83d\udfe2 Testes por dom\u00ednio s\u00e3o simples e previs\u00edveis. Observabilidade \ud83d\udd36 N\u00e3o h\u00e1 CLI ou endpoint para visualizar settings em tempo de execu\u00e7\u00e3o. Composi\u00e7\u00e3o por plugin \ud83d\udd36 Plugins ainda n\u00e3o registram seus pr\u00f3prios settings dinamicamente. Compara\u00e7\u00e3o com frameworks \ud83d\udfe2 Modelo semelhante ao Spring Boot <code>@ConfigurationProperties</code>. Clareza e sem\u00e2ntica do c\u00f3digo \ud83d\udfe2 Tipagem forte, valida\u00e7\u00e3o estruturada, fallback claro."},{"location":"quality/benckmark/athomic_config_quality_review/#comparacao-com-frameworks-enterprise","title":"\ud83e\uddea Compara\u00e7\u00e3o com frameworks enterprise","text":"Aspecto Athomic Spring Boot NestJS + ConfigService Multi-fonte \u2705 Dynaconf \u2705 YAML, <code>.env</code>, profiles \u2705 via <code>.env</code>, arquivos Valida\u00e7\u00e3o tipada \u2705 Pydantic \u2705 via Java Beans \u26a0\ufe0f manual Override por ambiente \u2705 <code>env_switcher</code> \u2705 <code>@Profile</code> \u26a0\ufe0f condicional manual Registry/configs por m\u00f3dulo \u2705 Contracts por dom\u00ednio \u2705 <code>@ConfigurationProperties</code> \u26a0\ufe0f flat ou global CLI/visualiza\u00e7\u00e3o \u274c \u2705 Spring Boot Actuator \u274c"},{"location":"quality/benckmark/athomic_config_quality_review/#padroes-de-projeto-observados","title":"\u2705 Padr\u00f5es de Projeto Observados","text":"<ul> <li>Validation Layer (com Pydantic)</li> <li>Settings Registry informal (por dom\u00ednio)</li> <li>Strategy-like para fallback de fonte</li> <li>Environment Switch Pattern (via env_switcher)</li> <li>Schema por contexto (JWT, Redis, Messaging, etc.)</li> </ul>"},{"location":"quality/benckmark/athomic_config_quality_review/#oportunidades-de-melhoria","title":"\u26a0\ufe0f Oportunidades de Melhoria","text":"<ul> <li>CLI <code>athomic config show</code>, <code>config validate</code>, <code>config diff</code></li> <li>Plugin registrar settings dinamicamente (<code>plugin.register_config(...)</code>)</li> <li>Settings visualiz\u00e1veis com m\u00e1scara via <code>/diagnostics/settings</code></li> <li>Documenta\u00e7\u00e3o gerada automaticamente (tipo Swagger para settings)</li> </ul>"},{"location":"quality/benckmark/athomic_config_quality_review/#roadmap-sugerido","title":"\ud83c\udfaf Roadmap sugerido","text":"<ul> <li>[ ] Registry de <code>SettingsContract</code> por m\u00f3dulo</li> <li>[ ] CLI: <code>config show</code>, <code>config diff</code></li> <li>[ ] Endpoint com mask de sensitive fields</li> <li>[ ] ScopedSettings por tenant ou plugin</li> <li>[ ] Exporta\u00e7\u00e3o para painel web (Backstage, etc.)</li> </ul> <p>O m\u00f3dulo <code>config</code> \u00e9 robusto e bem estruturado. Pode liderar a orquestra\u00e7\u00e3o de configura\u00e7\u00f5es din\u00e2micas e se tornar um hub declarativo e audit\u00e1vel da plataforma.</p>"},{"location":"quality/benckmark/athomic_design_patterns_matrix/","title":"\ud83e\uddf1 Athomic &amp; Frameworks \u2013 Matriz de Design Patterns Modernos","text":"<p>Esta matriz compara os principais padr\u00f5es de design aplicados na Athomic com sua presen\u00e7a ou uso equivalente em frameworks enterprise de refer\u00eancia: Spring Boot, NestJS, Django e FastAPI.</p>"},{"location":"quality/benckmark/athomic_design_patterns_matrix/#matriz-de-design-patterns","title":"\ud83d\udcca Matriz de Design Patterns","text":"Design Pattern / Conceito Athomic Spring Boot NestJS Django / FastAPI Observa\u00e7\u00f5es Athomic Registry \u2705 \u2705 Beans/Context \u2705 DI Container \u26a0\ufe0f parcial (<code>apps</code>) Amplo uso em <code>secrets</code>, <code>kvstore</code>, <code>messaging</code> Factory \u2705 \u2705 Beans/Factory \u2705 Providers \u26a0\ufe0f manual <code>SecretsProviderFactory</code>, <code>KVStoreFactory</code> Adapter / Wrapper \u2705 \u2705 <code>RedisTemplate</code> \u2705 Custom providers \u2705 com custom classes Usado em cache, HTTP, messaging Decorator \u2705 \u2705 <code>@Retryable</code> \u2705 Interceptors \u2705 Decorators nativos <code>@cached</code>, <code>@trace_span</code>, <code>@rate_limited</code> Strategy \u2705 \u2705 Policy Beans \u2705 Custom Guards \u26a0\ufe0f manual <code>FeatureFlag</code>, <code>RateLimiter</code>, <code>SecretsProvider</code> Chain of Responsibility \u2705 \u2705 Filters \u26a0\ufe0f middleware \u26a0\ufe0f limitado <code>fallback_handler</code> + <code>secrets</code> Command \u26a0\ufe0f \u2705 Scheduled Cmds \u2705 CLI Commands \u2705 Mgmt Commands Em estrutura\u00e7\u00e3o via CLI (<code>athomic command</code>) Observer / Event Bus \u26a0\ufe0f \u2705 Events \u2705 EventEmitter \u2705 Django Signals Iniciado em plugins e lifecycle Builder (fluent APIs) \u26a0\ufe0f \u2705 Decorator.of() \u26a0\ufe0f poucos casos \u274c Planejado para <code>resilience</code> Lifecycle / Hooks \ud83d\udd36 \u2705 LifecycleBeans \u2705 onModuleInit \u2705 <code>AppConfig.ready()</code> Parcial com <code>lifespan</code>, em evolu\u00e7\u00e3o Context Object \u2705 \u2705 SecurityContext \u2705 RequestContext \u2705 Request + contextvars <code>AuthContext</code>, <code>TraceContext</code>, <code>SecurityContext</code> Proxy / Gateway \u2705 \u2705 RestTemplate \u2705 Axios \u26a0\ufe0f HTTPX manual HTTP clients com fallback e tracing Plugin/Module Loader \ud83d\udd36 \u2705 Scan + Profile \u2705 Modules \u2705 Django Apps Em desenvolvimento: <code>PluginBase</code>, <code>PluginManager</code> Configuration Object \u2705 \u2705 <code>@ConfigProps</code> \u2705 <code>.env</code> + DI \u2705 Pydantic / settings <code>MessagingSettings</code>, <code>SecuritySettings</code>, etc. Provider Pattern \u2705 \u2705 for config \u2705 widely used \u26a0\ufe0f parcial <code>BaseSecretsProvider</code>, <code>BaseKVStore</code>, <code>RateLimiter</code> Validation Layer \u2705 \u2705 Validator API \u26a0\ufe0f manual \u2705 via Pydantic Contratos por dom\u00ednio com valida\u00e7\u00e3o completa Service Locator (evitado) \u26a0\ufe0f \u26a0\ufe0f Beans \u26a0\ufe0f DI workaround \u26a0\ufe0f usado informalmente Apenas no <code>registry</code> para componentes comuns Retry Policy / Resilience Pattern \u2705 \u2705 Resilience4j \u26a0\ufe0f manual/intercept \u26a0\ufe0f decorador manual Presente, mas composi\u00e7\u00e3o fluente ainda ausente"},{"location":"quality/benckmark/athomic_design_patterns_matrix/#conclusoes","title":"\ud83e\udde0 Conclus\u00f5es","text":"<ul> <li>A Athomic aplica a maioria dos principais design patterns modernos, especialmente Decorator, Strategy, Registry, Adapter, e Context Object.</li> <li>Pontos fortes: separa\u00e7\u00e3o modular, extensibilidade, uso de contracts e validadores.</li> <li>A evoluir: composi\u00e7\u00e3o fluente (Builder), CLI Command Pattern, Plugin lifecycle formal, Event Bus.</li> </ul> <p>A Athomic constr\u00f3i uma arquitetura s\u00f3lida e extens\u00edvel ao adotar e adaptar padr\u00f5es avan\u00e7ados de design \u2014 compar\u00e1vel aos melhores frameworks enterprise modernos.</p>"},{"location":"quality/benckmark/athomic_kvstore_quality_review/","title":"\ud83e\udde0 Athomic Architecture &amp; Code Quality Review \u2013 KVStore Module","text":"<p>Este relat\u00f3rio aplica a metodologia de avalia\u00e7\u00e3o arquitetural e qualidade de c\u00f3digo ao m\u00f3dulo <code>KVStore</code> da Athomic, utilizando como refer\u00eancia padr\u00f5es de projeto consagrados e frameworks enterprise.</p>"},{"location":"quality/benckmark/athomic_kvstore_quality_review/#contexto-do-modulo","title":"\ud83d\udce6 Contexto do M\u00f3dulo","text":"<p>O m\u00f3dulo <code>KVStore</code> fornece uma abstra\u00e7\u00e3o para armazenamento chave-valor ass\u00edncrono. Ele suporta m\u00faltiplos backends (Redis, Local), wrappers (TTL, key-resolving), integra\u00e7\u00e3o com observabilidade (m\u00e9tricas, tracing), inje\u00e7\u00e3o de depend\u00eancia e ciclo de vida.</p>"},{"location":"quality/benckmark/athomic_kvstore_quality_review/#avaliacao-por-criterio","title":"\u2705 Avalia\u00e7\u00e3o por Crit\u00e9rio","text":"Crit\u00e9rio Status Coment\u00e1rio Design Patterns aplicados \ud83d\udfe2 Uso consistente de <code>Factory</code>, <code>Registry</code>, <code>Decorator</code>, <code>Protocol</code>, <code>Adapter</code>. SRP (Responsabilidade \u00fanica) \ud83d\udfe2 <code>BaseKVStore</code>, <code>KVStoreFactory</code>, <code>KVStoreRegistry</code>, <code>Wrappers</code> t\u00eam responsabilidades bem separadas. Extensibilidade (OCP) \ud83d\udfe2 Suporte din\u00e2mico a novos providers e wrappers via registries. Contratos claros (Interface) \ud83d\udfe2 <code>KVStoreProtocol</code>, <code>BaseKVStore</code> fornecem contratos robustos e test\u00e1veis. Testabilidade \ud83d\udfe2 C\u00f3digo desacoplado, baseado em DI. Wrappers podem ser testados isoladamente. Observabilidade \ud83d\udfe2 M\u00e9tricas Prometheus + Tracing com spans por opera\u00e7\u00e3o. Composi\u00e7\u00e3o / Encadeamento \ud83d\udfe2 Wrappers aninhados (<code>DefaultTTL</code>, <code>KeyResolving</code>) seguem padr\u00e3o decorador. Acoplamento / Coes\u00e3o \ud83d\udfe2 Alto grau de coes\u00e3o e baixo acoplamento entre camadas. Compara\u00e7\u00e3o com frameworks \ud83d\udfe2 Arquitetura similar ao <code>Spring Data Redis</code> com interface, factory, injection, wrapper. Documenta\u00e7\u00e3o e clareza sem\u00e2ntica \ud83d\udfe2 Docstrings exemplares, argumentos tipados, erros tratados."},{"location":"quality/benckmark/athomic_kvstore_quality_review/#comparacao-com-frameworks-enterprise","title":"\ud83e\uddea Compara\u00e7\u00e3o com frameworks enterprise","text":"Aspecto Athomic KVStore Spring Data Redis NestJS CacheModule Interface baseada em contrato \u2705 <code>KVStoreProtocol</code> \u2705 <code>ReactiveRedisConnectionFactory</code> \u26a0\ufe0f via DI / n\u00e3o formal Wrappers e decorators \u2705 Encade\u00e1veis (<code>DefaultTTL</code>) \u2705 <code>RedisTemplate</code> wrappers \u26a0\ufe0f limitado Tracing e m\u00e9tricas \u2705 Prometheus + spans \u2705 Micrometer \u26a0\ufe0f manual Suporte multi-provider \u2705 Registry \u2705 profile + bean \u26a0\ufe0f n\u00e3o por padr\u00e3o"},{"location":"quality/benckmark/athomic_kvstore_quality_review/#padroes-de-projeto-observados","title":"\u2705 Padr\u00f5es de Projeto Observados","text":"<ul> <li>Factory (<code>KVStoreFactory</code>)</li> <li>Registry (<code>kv_store_registry</code>, <code>wrapper_registry</code>)</li> <li>Adapter/Wrapper (<code>KeyResolvingKVClient</code>, <code>DefaultTTLKvClient</code>)</li> <li>Decorator (via wrappers com mesma interface)</li> <li>Protocol + ABC (<code>KVStoreProtocol</code>, <code>BaseKVStore</code>)</li> <li>Dependency Injection ready</li> <li>Service Lifecycle Pattern (<code>BaseService</code>)</li> </ul>"},{"location":"quality/benckmark/athomic_kvstore_quality_review/#sugestoes-finais","title":"\ud83d\udccc Sugest\u00f5es Finais","text":"<ul> <li>\u2714\ufe0f Nenhuma mudan\u00e7a cr\u00edtica necess\u00e1ria.</li> <li>\ud83d\udd0d Pode-se eventualmente introduzir:</li> <li>CacheClientContext para rastreio de origem (e.g., observabilidade por escopo)</li> <li>Invers\u00e3o de controle para composi\u00e7\u00e3o de wrappers (com builder/factory fluente)</li> <li>Registry visualiz\u00e1vel via CLI (<code>athomic kvstore show</code>)</li> </ul>"},{"location":"quality/benckmark/athomic_kvstore_quality_review/#conclusao","title":"\ud83c\udfc1 Conclus\u00e3o","text":"<p>O m\u00f3dulo <code>KVStore</code> da Athomic \u00e9 altamente modular, bem estruturado e adota padr\u00f5es modernos de forma exemplar. Serve como refer\u00eancia para os demais m\u00f3dulos.</p>"},{"location":"quality/benckmark/athomic_lifecycle_quality_review/","title":"\ud83d\udd01 Athomic \u2013 Code Quality &amp; Architecture Review: M\u00f3dulo Lifecycle/Core","text":"<p>Este relat\u00f3rio aplica a metodologia de revis\u00e3o arquitetural ao m\u00f3dulo <code>lifecycle</code>, respons\u00e1vel por orquestrar o ciclo de vida da aplica\u00e7\u00e3o: startup, shutdown e integra\u00e7\u00e3o de componentes.</p>"},{"location":"quality/benckmark/athomic_lifecycle_quality_review/#contexto-do-modulo","title":"\ud83d\udce6 Contexto do M\u00f3dulo","text":"<p>O m\u00f3dulo <code>lifecycle/core</code> organiza o carregamento e encerramento de servi\u00e7os externos (ex: Consul, Kafka, Vault), utilizando a fun\u00e7\u00e3o <code>lifespan()</code> do FastAPI. H\u00e1 planos de evoluir para um sistema formal de plugins com <code>on_startup</code>, <code>on_shutdown</code>, controle de ordem e retry autom\u00e1tico.</p>"},{"location":"quality/benckmark/athomic_lifecycle_quality_review/#avaliacao-por-criterio","title":"\u2705 Avalia\u00e7\u00e3o por Crit\u00e9rio","text":"Crit\u00e9rio Status Coment\u00e1rio Design Patterns aplicados \ud83d\udd36 Uso atual de <code>lifespan</code>, com inten\u00e7\u00e3o de introduzir <code>LifecycleParticipant</code> e <code>Manager</code>. SRP (Responsabilidade \u00fanica) \ud83d\udfe1 A inicializa\u00e7\u00e3o est\u00e1 acoplada ao FastAPI, dificultando reuso isolado. Extensibilidade (OCP) \ud83d\udd36 Ainda limitado; novos m\u00f3dulos precisam ser adicionados manualmente. Contratos claros (Interface) \ud83d\udd34 N\u00e3o h\u00e1 interface <code>LifecycleParticipant</code>, <code>on_startup</code>, <code>on_shutdown</code>. Testabilidade \ud83d\udfe1 Dif\u00edcil de testar a orquestra\u00e7\u00e3o de startup sem instanciar app completa. Observabilidade \ud83d\udd34 Sem m\u00e9tricas ou logs por recurso (ex: tempo de startup por m\u00f3dulo). Orquestra\u00e7\u00e3o din\u00e2mica \ud83d\udd34 Sem registro de ordem, prioridade, depend\u00eancia ou falha por m\u00f3dulo. Compara\u00e7\u00e3o com frameworks \ud83d\udd36 Spring Boot, NestJS e Django usam estrat\u00e9gias mais formalizadas. Clareza e sem\u00e2ntica do c\u00f3digo \ud83d\udfe1 Funcional, mas n\u00e3o modularizado \u2014 mistura l\u00f3gica de inicializa\u00e7\u00e3o e aplica\u00e7\u00e3o."},{"location":"quality/benckmark/athomic_lifecycle_quality_review/#comparacao-com-frameworks-enterprise","title":"\ud83e\uddea Compara\u00e7\u00e3o com frameworks enterprise","text":"Aspecto Athomic Spring Boot NestJS Django Lifecycle por m\u00f3dulo \u274c manual \u2705 <code>@PostConstruct</code>, <code>@DependsOn</code> \u2705 <code>onModuleInit()</code> \u2705 <code>AppConfig.ready()</code> Ordem de depend\u00eancia \u274c \u2705 via anota\u00e7\u00f5es \u2705 via metadados \u26a0\ufe0f ordem por arquivo Retry/Timeout startup \u274c \u2705 Retry por recurso \u26a0\ufe0f via l\u00f3gica manual \u274c Observabilidade do ciclo \u274c \u2705 actuator <code>/startup</code> \u26a0\ufe0f plugins externos \u274c CLI de status \u274c \u2705 <code>spring status</code> \u274c \u274c"},{"location":"quality/benckmark/athomic_lifecycle_quality_review/#padroes-de-projeto-a-serem-aplicados","title":"\u2705 Padr\u00f5es de Projeto a serem aplicados","text":"<ul> <li>LifecycleParticipant Interface</li> <li>LifecycleManager com ordem e retry</li> <li>Orquestra\u00e7\u00e3o baseada em metadados</li> <li>Event bus para <code>on_ready</code>, <code>on_shutdown</code></li> </ul>"},{"location":"quality/benckmark/athomic_lifecycle_quality_review/#oportunidades-de-melhoria","title":"\u26a0\ufe0f Oportunidades de Melhoria","text":"<ul> <li>Implementar <code>LifecycleParticipant</code> com <code>on_startup</code>, <code>on_shutdown</code></li> <li>Permitir registro modular com prioridade</li> <li>Middleware para registrar tempo de startup</li> <li>CLI: <code>athomic lifecycle status</code>, <code>athomic diagnostics</code></li> <li>Exportar m\u00e9tricas: <code>startup_duration_seconds</code>, <code>startup_failures_total</code></li> <li>Separar FastAPI startup da l\u00f3gica de backend</li> </ul>"},{"location":"quality/benckmark/athomic_lifecycle_quality_review/#roadmap-sugerido","title":"\ud83c\udfaf Roadmap sugerido","text":"<ul> <li>[ ] <code>LifecycleParticipant</code>, <code>LifecycleManager</code></li> <li>[ ] Hooks por m\u00f3dulo, com fallback em falha</li> <li>[ ] CLI de status de inicializa\u00e7\u00e3o</li> <li>[ ] Observabilidade: tempos, falhas, ordem</li> <li>[ ] Registry visual ou endpoint <code>/diagnostics/lifecycle</code></li> </ul> <p>O m\u00f3dulo <code>lifecycle</code> \u00e9 o ponto de orquestra\u00e7\u00e3o de todo o framework. Com padroniza\u00e7\u00e3o e observabilidade, pode se tornar a espinha dorsal de runtime confi\u00e1vel.</p>"},{"location":"quality/benckmark/athomic_messaging_quality_review/","title":"\ud83d\udcec Athomic \u2013 Code Quality &amp; Architecture Review: M\u00f3dulo Messaging","text":"<p>Este relat\u00f3rio aplica a metodologia de revis\u00e3o arquitetural ao m\u00f3dulo <code>messaging</code>, respons\u00e1vel por abstra\u00e7\u00f5es de comunica\u00e7\u00e3o ass\u00edncrona com suporte a Kafka, RabbitMQ, entre outros.</p>"},{"location":"quality/benckmark/athomic_messaging_quality_review/#contexto-do-modulo","title":"\ud83d\udce6 Contexto do M\u00f3dulo","text":"<p>O m\u00f3dulo fornece producers e consumers desacoplados via interfaces, com suporte a m\u00faltiplos backends (Kafka, RabbitMQ), configura\u00e7\u00e3o via Pydantic, registry de providers e serializa\u00e7\u00e3o customizada.</p>"},{"location":"quality/benckmark/athomic_messaging_quality_review/#avaliacao-por-criterio","title":"\u2705 Avalia\u00e7\u00e3o por Crit\u00e9rio","text":"Crit\u00e9rio Status Coment\u00e1rio Design Patterns aplicados \ud83d\udfe2 Uso correto de <code>Factory</code>, <code>Registry</code>, <code>Protocol</code>, <code>Adapter</code>. SRP (Responsabilidade \u00fanica) \ud83d\udfe2 Separation clara entre interface, configura\u00e7\u00e3o, e provider. Extensibilidade (OCP) \ud83d\udfe2 Suporte a novos backends sem alterar o core. Contratos claros (Interface) \ud83d\udfe2 Uso de <code>BaseProducer</code>, <code>BaseConsumer</code>, tipados e ass\u00edncronos. Testabilidade \ud83d\udfe2 F\u00e1cil de mockar com base nas interfaces. Testes com mocks j\u00e1 aplicados. Observabilidade \ud83d\udfe1 H\u00e1 tracing b\u00e1sico, mas falta m\u00e9tricas por t\u00f3pico/backend. Composi\u00e7\u00e3o / Middleware \ud83d\udd36 Falta <code>message_middleware</code> padr\u00e3o para encadeamento de l\u00f3gica. Acoplamento / Coes\u00e3o \ud83d\udfe2 Boa separa\u00e7\u00e3o entre providers e clientes. Compara\u00e7\u00e3o com frameworks \ud83d\udfe1 Compar\u00e1vel ao NestJS <code>ClientsModule</code> ou Spring Cloud Stream. Clareza e sem\u00e2ntica do c\u00f3digo \ud83d\udfe2 Expressivo, especialmente na factory e inst\u00e2ncia de configura\u00e7\u00e3o."},{"location":"quality/benckmark/athomic_messaging_quality_review/#comparacao-com-frameworks-enterprise","title":"\ud83e\uddea Compara\u00e7\u00e3o com frameworks enterprise","text":"Aspecto Athomic Spring Cloud Stream NestJS Microservices Interface desacoplada \u2705 \u2705 \u2705 via Provider Registry de backends \u2705 \u2705 \u2705 Middleware \u274c \u2705 Interceptors \u2705 Tracing / Observabilidade \u26a0\ufe0f parcial \u2705 Micrometer + Actuator \u26a0\ufe0f manual Schema registry / valida\u00e7\u00e3o \u274c \u2705 Avro \u26a0\ufe0f parcial CLI / Monitoramento \u274c \u2705 actuator \u274c"},{"location":"quality/benckmark/athomic_messaging_quality_review/#padroes-de-projeto-observados","title":"\u2705 Padr\u00f5es de Projeto Observados","text":"<ul> <li>Factory (<code>MessagingFactory</code>)</li> <li>Registry (<code>provider_registry</code>)</li> <li>Protocol (<code>BaseProducer</code>, <code>BaseConsumer</code>)</li> <li>Adapter (implementa\u00e7\u00f5es espec\u00edficas por backend)</li> <li>Configuration Contract (<code>KafkaSettings</code>, <code>LocalMessagingSettings</code>)</li> <li>Service Interface Injection Ready</li> </ul>"},{"location":"quality/benckmark/athomic_messaging_quality_review/#oportunidades-de-melhoria","title":"\u26a0\ufe0f Oportunidades de Melhoria","text":"<ul> <li>Criar <code>message_middleware</code> encade\u00e1vel (before_send, after_receive, on_error)</li> <li>Expor m\u00e9tricas por t\u00f3pico/backend: <code>msg_published_total</code>, <code>msg_error_total</code></li> <li>CLI para <code>athomic messaging status</code>, <code>replay</code>, <code>test</code></li> <li>Tracing completo com spans nomeados por t\u00f3pico</li> <li>Suporte a schema validation (JSONSchema, Avro, Pydantic)</li> <li>Dead Letter Queue como provider ou fallback</li> </ul>"},{"location":"quality/benckmark/athomic_messaging_quality_review/#roadmap-sugerido","title":"\ud83c\udfaf Roadmap sugerido","text":"<ul> <li>[ ] Middleware padr\u00e3o para intercepta\u00e7\u00e3o de mensagens</li> <li>[ ] CLI: replay, status, consumer logs</li> <li>[ ] Observabilidade granular por t\u00f3pico</li> <li>[ ] Registry para reprocessadores e validadores</li> <li>[ ] Schema registry com valida\u00e7\u00e3o antes do envio</li> <li>[ ] RetryPolicy configur\u00e1vel no decorator do consumer</li> </ul> <p>O m\u00f3dulo <code>messaging</code> da Athomic est\u00e1 maduro e segue boas pr\u00e1ticas modernas, com espa\u00e7o para crescer em observabilidade, tooling e extensibilidade funcional.</p>"},{"location":"quality/benckmark/athomic_quality_summary/","title":"\ud83e\udde9 Athomic \u2013 Sum\u00e1rio Executivo da Avalia\u00e7\u00e3o Arquitetural e de Qualidade de C\u00f3digo","text":"<p>Este documento resume os principais insights obtidos na revis\u00e3o detalhada de todos os m\u00f3dulos da Athomic, com base na metodologia de avalia\u00e7\u00e3o arquitetural, compara\u00e7\u00e3o com frameworks enterprise e boas pr\u00e1ticas de engenharia de software.</p>"},{"location":"quality/benckmark/athomic_quality_summary/#visao-geral-por-modulo","title":"\u2705 Vis\u00e3o Geral por M\u00f3dulo","text":"M\u00f3dulo Status Arquitetural Destaques Positivos Pontos de Melhoria kvstore \ud83d\udfe2 Excelente Contracts claros, extens\u00edvel, observ\u00e1vel Poucas melhorias \u2014 refer\u00eancia de qualidade resilience \ud83d\udfe1 Boa base Decorators \u00fateis, tracing b\u00e1sico Faltam contratos, composi\u00e7\u00e3o e m\u00e9tricas messaging \ud83d\udfe2 S\u00f3lido Registry de providers, producers/consumers tipados Middleware e observabilidade granular observability \ud83d\udfe1 Robusto Logging estruturado, masking, registry de maskers M\u00e9tricas, spans detalhados, CLI secrets \ud83d\udfe2 Avan\u00e7ado Fallback chain, rotatable/cached provider, Vault seguro Auditoria, CLI, m\u00e9tricas rate_limiter \ud83d\udfe1 Funcional Uso da lib <code>limits</code>, decorators bem definidos Observabilidade e CLI auth/security \ud83d\udfe1 Consolidado JWT + IP + contextos seguros PolicyManager, logging, integra\u00e7\u00e3o OAuth config \ud83d\udfe2 Robusto Dynaconf + Pydantic, contratos por dom\u00ednio CLI, scoped settings, plugin aware plugin \ud83d\udd36 Em estrutura\u00e7\u00e3o Boa proposta, planejamento claro Falta PluginBase, lifecycle, discovery lifecycle \ud83d\udd36 Funcional inicial Usa <code>lifespan</code>, inicia servi\u00e7os externos Modulariza\u00e7\u00e3o, retry, m\u00e9tricas performance \ud83d\udfe2 Madura Cache + locking com TTL, decorators eficientes M\u00e9tricas, prewarming, CLI feature_flags \ud83d\udd34 Em est\u00e1gio inicial Planejado uso de providers, escopo e decorators Faltam contratos, CLI, observabilidade"},{"location":"quality/benckmark/athomic_quality_summary/#avaliacao-geral","title":"\ud83d\udcca Avalia\u00e7\u00e3o Geral","text":""},{"location":"quality/benckmark/athomic_quality_summary/#pontos-fortes-da-athomic","title":"\u2705 Pontos Fortes da Athomic","text":"<ul> <li>Padr\u00f5es s\u00f3lidos: Registry, Strategy, Decorator, Adapter amplamente utilizados</li> <li>Separa\u00e7\u00e3o clara por m\u00f3dulo e escopo</li> <li>Uso consistente de tipagem forte, Pydantic e valida\u00e7\u00e3o</li> <li>Extensibilidade embutida por design (especialmente em <code>kvstore</code>, <code>secrets</code>, <code>messaging</code>)</li> <li>Tracing e contextualiza\u00e7\u00e3o j\u00e1 integrados</li> </ul>"},{"location":"quality/benckmark/athomic_quality_summary/#principais-oportunidades-de-evolucao","title":"\u26a0\ufe0f Principais Oportunidades de Evolu\u00e7\u00e3o","text":"<ol> <li>Formaliza\u00e7\u00e3o de contratos (Interfaces) em m\u00f3dulos como <code>resilience</code>, <code>rate_limiter</code>, <code>plugin</code>, <code>feature_flags</code></li> <li>Orquestra\u00e7\u00e3o modular via <code>LifecycleParticipant</code>, ordena\u00e7\u00e3o e retry</li> <li>CLI padronizada por m\u00f3dulo: <code>athomic config</code>, <code>athomic secrets</code>, <code>athomic messaging</code>, etc.</li> <li>Observabilidade granular por escopo (plugin, tenant, rota, estrat\u00e9gia)</li> <li>Integra\u00e7\u00e3o de painel/diagn\u00f3stico para gest\u00e3o visual dos subsistemas</li> </ol>"},{"location":"quality/benckmark/athomic_quality_summary/#conclusao","title":"\ud83c\udfc1 Conclus\u00e3o","text":"<p>A Athomic apresenta arquitetura modular de alta qualidade, com pr\u00e1ticas maduras de extensibilidade, separa\u00e7\u00e3o de responsabilidades e testabilidade. Com a evolu\u00e7\u00e3o do sistema de plugins, CLI e observabilidade avan\u00e7ada, est\u00e1 pronta para se posicionar como um framework open-core robusto e competitivo.</p>"},{"location":"quality/benckmark/athomic_rate_limiter_quality_review/","title":"\ud83d\udea6 Athomic \u2013 Code Quality &amp; Architecture Review: M\u00f3dulo Rate Limiter","text":"<p>Este relat\u00f3rio aplica a metodologia de revis\u00e3o arquitetural ao m\u00f3dulo <code>rate_limiter</code>, respons\u00e1vel por controle de uso baseado em limites configur\u00e1veis, com suporte a m\u00faltiplos backends.</p>"},{"location":"quality/benckmark/athomic_rate_limiter_quality_review/#contexto-do-modulo","title":"\ud83d\udce6 Contexto do M\u00f3dulo","text":"<p>O m\u00f3dulo utiliza a biblioteca <code>limits</code> como base e abstrai diferentes backends (<code>MemoryStorage</code>, <code>RedisStorage</code>). Implementa inje\u00e7\u00e3o de depend\u00eancia, suporte a m\u00faltiplas pol\u00edticas e decorators (<code>@rate_limited</code>) para uso por rota, servi\u00e7o, tenant, etc.</p>"},{"location":"quality/benckmark/athomic_rate_limiter_quality_review/#avaliacao-por-criterio","title":"\u2705 Avalia\u00e7\u00e3o por Crit\u00e9rio","text":"Crit\u00e9rio Status Coment\u00e1rio Design Patterns aplicados \ud83d\udfe2 Adapter (backends), Factory, Registry, Decorator, Strategy. SRP (Responsabilidade \u00fanica) \ud83d\udfe2 Providers, policies, decorators, e settings est\u00e3o bem separados. Extensibilidade (OCP) \ud83d\udfe2 Novos providers podem ser registrados facilmente. Contratos claros (Interface) \ud83d\udfe1 Poderia ter uma interface expl\u00edcita como <code>RateLimiterProtocol</code>. Testabilidade \ud83d\udfe2 C\u00f3digo baseado em DI com testes bem isol\u00e1veis. Observabilidade \ud83d\udd36 Logs m\u00ednimos; faltam m\u00e9tricas como <code>rate_limit_hit</code>, <code>rate_limit_exceeded</code>. Composi\u00e7\u00e3o e controle \ud83d\udfe1 Policies e keys s\u00e3o configur\u00e1veis, mas podem se beneficiar de custom middleware. Compara\u00e7\u00e3o com frameworks \ud83d\udfe1 Similar a Django Ratelimit ou Flask-Limiter, com maior modularidade. Clareza e sem\u00e2ntica do c\u00f3digo \ud83d\udfe2 Decorators claros e autoexplicativos. Boas mensagens de erro."},{"location":"quality/benckmark/athomic_rate_limiter_quality_review/#comparacao-com-frameworks-enterprise","title":"\ud83e\uddea Compara\u00e7\u00e3o com frameworks enterprise","text":"Aspecto Athomic Flask-Limiter / Django Ratelimit Spring RateLimiter / Micronaut Backends m\u00faltiplos \u2705 Redis, mem\u00f3ria via <code>limits</code> \u26a0\ufe0f limitado \u2705 Redis + Configurable Decorator <code>@rate_limited</code> \u2705 configur\u00e1vel via policy/key \u2705 \u26a0\ufe0f via filtros complexos M\u00e9tricas \u274c \u26a0\ufe0f logs ou plugins externos \u2705 Micrometer Pol\u00edticas din\u00e2micas \u26a0\ufe0f configur\u00e1veis via c\u00f3digo \u26a0\ufe0f regex por rota \u2705 via <code>RateLimiterConfig</code> CLI ou painel \u274c \u274c \u2705 via Actuator"},{"location":"quality/benckmark/athomic_rate_limiter_quality_review/#padroes-de-projeto-observados","title":"\u2705 Padr\u00f5es de Projeto Observados","text":"<ul> <li>Adapter (<code>LimitsRateLimiter</code>, etc.)</li> <li>Factory + Registry (<code>RateLimiterRegistry</code>)</li> <li>Strategy (por policy: <code>ip</code>, <code>route</code>, <code>tenant</code>)</li> <li>Decorator (<code>@rate_limited</code>)</li> <li>Provider Pattern com storage configur\u00e1vel</li> </ul>"},{"location":"quality/benckmark/athomic_rate_limiter_quality_review/#oportunidades-de-melhoria","title":"\u26a0\ufe0f Oportunidades de Melhoria","text":"<ul> <li>Introduzir <code>RateLimiterProtocol</code> como contrato para providers</li> <li>Criar m\u00e9tricas por pol\u00edtica:</li> <li><code>rate_limit_hit_total</code></li> <li><code>rate_limit_exceeded_total</code></li> <li>Middleware para chave din\u00e2mica autom\u00e1tica (<code>key_func</code>)</li> <li>CLI: <code>athomic ratelimit status</code>, <code>flush</code>, <code>reset</code></li> <li>Diagn\u00f3stico por rota/policy via <code>/diagnostics/ratelimit</code></li> </ul>"},{"location":"quality/benckmark/athomic_rate_limiter_quality_review/#roadmap-sugerido","title":"\ud83c\udfaf Roadmap sugerido","text":"<ul> <li>[ ] Interface formal <code>BaseRateLimiter</code></li> <li>[ ] CLI para status e flush</li> <li>[ ] M\u00e9tricas Prometheus por policy/key</li> <li>[ ] Middleware para uso program\u00e1tico (sem decorator)</li> <li>[ ] Suporte a pol\u00edtica custom por tenant</li> </ul> <p>O m\u00f3dulo <code>rate_limiter</code> da Athomic \u00e9 funcional, test\u00e1vel e extens\u00edvel. Para se tornar enterprise-grade, deve evoluir em observabilidade e controle din\u00e2mico.</p>"},{"location":"quality/benckmark/athomic_resilience_quality_review/","title":"\ud83d\udee1\ufe0f Athomic \u2013 Code Quality &amp; Architecture Review: M\u00f3dulo Resilience","text":"<p>Este relat\u00f3rio aplica a metodologia de revis\u00e3o arquitetural ao m\u00f3dulo <code>resilience</code>, que oferece mecanismos de toler\u00e2ncia a falhas como retry, fallback, timeout, circuit breaker e bulkhead.</p>"},{"location":"quality/benckmark/athomic_resilience_quality_review/#contexto-do-modulo","title":"\ud83d\udce6 Contexto do M\u00f3dulo","text":"<p>O m\u00f3dulo <code>resilience</code> centraliza decoradores para resili\u00eancia de chamadas externas, servi\u00e7os cr\u00edticos e integra\u00e7\u00e3o com observabilidade.</p>"},{"location":"quality/benckmark/athomic_resilience_quality_review/#avaliacao-por-criterio","title":"\u2705 Avalia\u00e7\u00e3o por Crit\u00e9rio","text":"Crit\u00e9rio Status Coment\u00e1rio Design Patterns aplicados \ud83d\udfe1 Decorators utilizados corretamente, mas aus\u00eancia de abstra\u00e7\u00f5es expl\u00edcitas como <code>RetryStrategy</code> ou <code>FallbackHandler</code>. SRP (Responsabilidade \u00fanica) \ud83d\udfe1 Algumas fun\u00e7\u00f5es t\u00eam m\u00faltiplas l\u00f3gicas internas (ex: fallback + log + span em uma s\u00f3 fun\u00e7\u00e3o). Extensibilidade (OCP) \ud83d\udd36 Dificuldade em aplicar m\u00faltiplas estrat\u00e9gias encadeadas de forma configur\u00e1vel. Contratos claros (Interface) \ud83d\udd34 N\u00e3o h\u00e1 uma interface comum para estrat\u00e9gias de resili\u00eancia. Testabilidade \ud83d\udfe1 Test\u00e1vel, mas sem mocks ou estrat\u00e9gias isoladas como unidades. Observabilidade \ud83d\udfe2 Integra\u00e7\u00e3o com spans e tracing estruturado. Composi\u00e7\u00e3o / Encadeamento \ud83d\udd34 Decorators n\u00e3o comp\u00f5em bem (n\u00e3o h\u00e1 <code>@with_resilience([...])</code>). Acoplamento / Coes\u00e3o \ud83d\udfe1 Cada decorator \u00e9 coeso, mas o sistema geral est\u00e1 acoplado a detalhes. Compara\u00e7\u00e3o com frameworks \ud83d\udfe1 Resilience4j usa <code>DecoratorBuilder</code> e estrat\u00e9gia modular via <code>registry</code>. Clareza e sem\u00e2ntica do c\u00f3digo \ud83d\udfe1 Claro, mas \u00e0s vezes misturam responsabilidade de resili\u00eancia + tracing/logging."},{"location":"quality/benckmark/athomic_resilience_quality_review/#comparacao-com-frameworks-enterprise","title":"\ud83e\uddea Compara\u00e7\u00e3o com frameworks enterprise","text":"Aspecto Athomic Resilience4j Spring Retry Padr\u00e3o Decorator \u2705 \u2705 Decorators fluentes \u2705 via <code>@Retryable</code> Composi\u00e7\u00e3o de estrat\u00e9gias \u274c \u2705 encadeamento via <code>Decorators</code> \u26a0\ufe0f limitado Interface/Contratos \u274c \u2705 <code>Retry</code>, <code>CircuitBreaker</code>, etc \u2705 RetryPolicy Observabilidade \u2705 Tracing/Spans \u2705 Micrometer + events \u2705 Actuator"},{"location":"quality/benckmark/athomic_resilience_quality_review/#padroes-de-projeto-observados","title":"\u2705 Padr\u00f5es de Projeto Observados","text":"<ul> <li>Decorator (uso em <code>@with_retry</code>, <code>@with_fallback</code>, etc.)</li> <li>Boas pr\u00e1ticas de observabilidade via spans</li> <li>Abordagem funcional com fallback executado dinamicamente</li> </ul>"},{"location":"quality/benckmark/athomic_resilience_quality_review/#oportunidades-de-melhoria","title":"\u26a0\ufe0f Oportunidades de Melhoria","text":"<ul> <li>Introduzir classes como <code>RetryStrategy</code>, <code>FallbackStrategy</code>, <code>CircuitBreakerStrategy</code></li> <li>Criar <code>ResilienceRegistry</code> para registrar e encadear estrat\u00e9gias</li> <li>Adicionar builder fluente ou lista de estrat\u00e9gias:</li> </ul> <pre><code>@with_resilience([\n    RetryStrategy(retries=3),\n    CircuitBreakerStrategy(threshold=0.5)\n])\n</code></pre> <ul> <li>Separar tracing/logging/resili\u00eancia em camadas distintas</li> <li>CLI para listar e testar estrat\u00e9gias registradas</li> </ul>"},{"location":"quality/benckmark/athomic_resilience_quality_review/#roadmap-sugerido","title":"\ud83c\udfaf Roadmap sugerido","text":"<ul> <li>[ ] Introduzir interfaces: <code>BaseResilienceStrategy</code></li> <li>[ ] Registrar estrat\u00e9gias dinamicamente (<code>strategy_registry</code>)</li> <li>[ ] Criar decorator unificado <code>@with_resilience([ ... ])</code></li> <li>[ ] Desacoplar tracing/logging da execu\u00e7\u00e3o</li> <li>[ ] CLI: <code>athomic resilience list</code>, <code>resilience test --strategy=...</code></li> </ul> <p>O m\u00f3dulo j\u00e1 entrega valor real, mas pode evoluir de uma abordagem funcional acoplada para uma arquitetura extens\u00edvel e configur\u00e1vel ao n\u00edvel de Resilience4j.</p>"},{"location":"quality/benckmark/athomic_secrets_quality_review/","title":"\ud83d\udd10 Athomic \u2013 Code Quality &amp; Architecture Review: M\u00f3dulo Secrets","text":"<p>Este relat\u00f3rio aplica a metodologia de revis\u00e3o arquitetural ao m\u00f3dulo <code>secrets</code>, respons\u00e1vel por gerenciamento seguro de segredos, fallback e rota\u00e7\u00e3o.</p>"},{"location":"quality/benckmark/athomic_secrets_quality_review/#contexto-do-modulo","title":"\ud83d\udce6 Contexto do M\u00f3dulo","text":"<p>O m\u00f3dulo <code>secrets</code> da Athomic abstrai o acesso seguro a segredos de ambiente, arquivos e Vault. Implementa um sistema de fallback configur\u00e1vel, rota\u00e7\u00e3o e cache, com suporte a m\u00faltiplos providers atrav\u00e9s de registries.</p>"},{"location":"quality/benckmark/athomic_secrets_quality_review/#avaliacao-por-criterio","title":"\u2705 Avalia\u00e7\u00e3o por Crit\u00e9rio","text":"Crit\u00e9rio Status Coment\u00e1rio Design Patterns aplicados \ud83d\udfe2 Registry, Factory, Adapter, Fallback Chain, Strategy. SRP (Responsabilidade \u00fanica) \ud83d\udfe2 Separa\u00e7\u00e3o clara entre providers (<code>Vault</code>, <code>Env</code>, <code>File</code>) e rota\u00e7\u00f5es. Extensibilidade (OCP) \ud83d\udfe2 F\u00e1cil de adicionar novos providers com base em contrato. Contratos claros (Interface) \ud83d\udfe2 <code>BaseSecretsProvider</code>, <code>RotatableSecretsProvider</code> bem definidos. Testabilidade \ud83d\udfe2 M\u00f3dulo altamente test\u00e1vel com mocks de provider e rota\u00e7\u00e3o. Observabilidade \ud83d\udfe1 Logs de erro e tracing parcial, mas faltam m\u00e9tricas e eventos de rota\u00e7\u00e3o. Fallback/Chain-of-responsibility \ud83d\udfe2 Encadeamento configur\u00e1vel com prioridade e recupera\u00e7\u00e3o segura. Cache e rota\u00e7\u00e3o controlada \u2705 TTL por segredo e <code>CachedSecretsProvider</code> com controle fino. Compara\u00e7\u00e3o com frameworks \ud83d\udfe1 Compar\u00e1vel ao Spring Cloud Vault, com potencial para CLI e audit trail. Clareza e sem\u00e2ntica do c\u00f3digo \ud83d\udfe2 Interfaces e docstrings muito claras. Alto n\u00edvel de coes\u00e3o."},{"location":"quality/benckmark/athomic_secrets_quality_review/#comparacao-com-frameworks-enterprise","title":"\ud83e\uddea Compara\u00e7\u00e3o com frameworks enterprise","text":"Aspecto Athomic Spring Cloud Vault AWS Secrets Manager SDK M\u00faltiplos backends \u2705 <code>vault</code>, <code>env</code>, <code>file</code> \u2705 profile/config-based \u26a0\ufe0f apenas AWS Rota\u00e7\u00e3o autom\u00e1tica/manual \u2705 via <code>RotatableSecretsProvider</code> \u2705 com agendador e webhook \u2705 TTL e versioning Cache e TTL \u2705 com <code>CachedSecretsProvider</code> \u26a0\ufe0f parcial \u2705 Fallback Chain \u2705 via decorator <code>@fallback_handler</code> \u274c \u26a0\ufe0f limitado CLI \u274c \u2705 <code>vault kv get</code> \u2705 <code>aws secretsmanager</code> Observabilidade \u26a0\ufe0f logs e erros \u2705 audit trail \u2705 IAM audit + logs"},{"location":"quality/benckmark/athomic_secrets_quality_review/#padroes-de-projeto-observados","title":"\u2705 Padr\u00f5es de Projeto Observados","text":"<ul> <li>Strategy / Adapter (Vault, Env, File)</li> <li>Factory (<code>SecretsProviderFactory</code>)</li> <li>Registry (<code>secrets_registry</code>, <code>rotatable_registry</code>)</li> <li>Chain of Responsibility (<code>fallback_handler</code>)</li> <li>Decorator para retry e fallback</li> <li>Cache + TTL Pattern</li> </ul>"},{"location":"quality/benckmark/athomic_secrets_quality_review/#oportunidades-de-melhoria","title":"\u26a0\ufe0f Oportunidades de Melhoria","text":"<ul> <li>Adicionar eventos de rota\u00e7\u00e3o com <code>on_rotate</code></li> <li>CLI: <code>athomic secrets list</code>, <code>secrets test</code>, <code>rotate</code></li> <li>Expor m\u00e9tricas Prometheus:</li> <li><code>secrets_lookup_total</code>, <code>rotation_success_total</code>, <code>secrets_cache_hit_total</code></li> <li>Auditoria: <code>last_accessed</code>, <code>last_rotated</code>, <code>source_provider</code></li> <li>Integra\u00e7\u00e3o com sistemas como Azure Key Vault, GCP Secrets Manager</li> </ul>"},{"location":"quality/benckmark/athomic_secrets_quality_review/#roadmap-sugerido","title":"\ud83c\udfaf Roadmap sugerido","text":"<ul> <li>[ ] CLI: secrets list/test/rotate/status</li> <li>[ ] Events: hook <code>on_rotate()</code>, <code>on_fallback()</code></li> <li>[ ] Registry visualiz\u00e1vel via API <code>/diagnostics/secrets</code></li> <li>[ ] Suporte a pol\u00edticas de expira\u00e7\u00e3o/configura\u00e7\u00e3o via arquivo</li> <li>[ ] Integra\u00e7\u00e3o com mais backends (GCP, Azure, etc)</li> </ul> <p>O m\u00f3dulo <code>secrets</code> da Athomic \u00e9 altamente maduro, extens\u00edvel e seguro \u2014 e pode evoluir ainda mais com CLI, auditabilidade e integra\u00e7\u00f5es empresariais.</p>"},{"location":"quality/benckmark/athomic_security_quality_review/","title":"\ud83d\udd10 Athomic \u2013 Code Quality &amp; Architecture Review: M\u00f3dulo Auth/Security","text":"<p>Este relat\u00f3rio aplica a metodologia de revis\u00e3o arquitetural ao m\u00f3dulo <code>auth/security</code>, que implementa autentica\u00e7\u00e3o via JWT, controle de acesso, IP filtering e contexto seguro de requisi\u00e7\u00e3o.</p>"},{"location":"quality/benckmark/athomic_security_quality_review/#contexto-do-modulo","title":"\ud83d\udce6 Contexto do M\u00f3dulo","text":"<p>O m\u00f3dulo <code>auth</code> prov\u00ea autentica\u00e7\u00e3o baseada em JWT, gerenciamento de contexto (<code>AuthContext</code>, <code>SecurityContext</code>), decorators para acesso p\u00fablico, bloqueio por IP e valida\u00e7\u00e3o de escopos. Tamb\u00e9m lida com <code>SecuritySettings</code>, inje\u00e7\u00e3o de seguran\u00e7a por rota e acesso seguro a credenciais.</p>"},{"location":"quality/benckmark/athomic_security_quality_review/#avaliacao-por-criterio","title":"\u2705 Avalia\u00e7\u00e3o por Crit\u00e9rio","text":"Crit\u00e9rio Status Coment\u00e1rio Design Patterns aplicados \ud83d\udfe2 Decorator, ContextObject, Provider, Strategy, Filter. SRP (Responsabilidade \u00fanica) \ud83d\udfe1 Algumas classes acumulam m\u00faltiplas fun\u00e7\u00f5es: parsing JWT + resolu\u00e7\u00e3o de permiss\u00f5es + log de IP. Extensibilidade (OCP) \ud83d\udfe2 Suporte a estrat\u00e9gias futuras de controle (<code>requires_policy</code>, <code>roles</code>). Contratos claros (Interface) \ud83d\udfe1 Contextos bem definidos, mas falta interface clara para pol\u00edtica ou provider. Testabilidade \ud83d\udfe2 Testes de JWT, contextos e verifica\u00e7\u00e3o por IP s\u00e3o vi\u00e1veis e isol\u00e1veis. Observabilidade \ud83d\udd36 Falta log estruturado por tipo de nega\u00e7\u00e3o/autoriza\u00e7\u00e3o, sem m\u00e9tricas. Composi\u00e7\u00e3o e controle \ud83d\udfe1 IP blocking + JWT + user_scope ainda acoplados. Poderia haver <code>AccessChain</code>. Compara\u00e7\u00e3o com frameworks \ud83d\udfe1 Equivalente a Auth0/JWTMiddleware/NestJS Guards em funcionalidade, mas com menos orquestra\u00e7\u00e3o e painel. Clareza e sem\u00e2ntica do c\u00f3digo \ud83d\udfe2 C\u00f3digo limpo e estruturado por escopo, decorators claros."},{"location":"quality/benckmark/athomic_security_quality_review/#comparacao-com-frameworks-enterprise","title":"\ud83e\uddea Compara\u00e7\u00e3o com frameworks enterprise","text":"Aspecto Athomic NestJS Auth/Guards Spring Security JWT integrado \u2705 via JWTHandler, AuthContext \u2705 <code>JwtAuthGuard</code> \u2705 com filters Controle de IP \u2705 por decorator/config \u26a0\ufe0f custom \u26a0\ufe0f manual Roles e pol\u00edticas \u26a0\ufe0f limitado, sem <code>@requires_role</code> \u2705 via decorator + metadata \u2705 <code>@PreAuthorize</code> Contexto e escopo \u2705 <code>AuthContext</code>, <code>trace_id</code> \u2705 via <code>RequestContext</code> \u2705 <code>SecurityContext</code> Observabilidade \u26a0\ufe0f logs manuais \u26a0\ufe0f logs manuais \u2705 Actuator + audit Integra\u00e7\u00e3o com IdP (OAuth2) \u274c \u2705 poss\u00edvel \u2705 com config e beans"},{"location":"quality/benckmark/athomic_security_quality_review/#padroes-de-projeto-observados","title":"\u2705 Padr\u00f5es de Projeto Observados","text":"<ul> <li>Decorator (<code>@public_route</code>, <code>@requires_auth</code>)</li> <li>Strategy-like (para controle por IP, permiss\u00f5es)</li> <li>Context Object (<code>AuthContext</code>, <code>SecurityContext</code>)</li> <li>Filter Chain (manual) \u2013 parsing JWT \u2192 IP check \u2192 scope</li> </ul>"},{"location":"quality/benckmark/athomic_security_quality_review/#oportunidades-de-melhoria","title":"\u26a0\ufe0f Oportunidades de Melhoria","text":"<ul> <li>Criar <code>PolicyManager</code> e decorator <code>@requires_policy(\"admin:delete\")</code></li> <li>CLI: <code>athomic auth show</code>, <code>revoke-token</code>, <code>whoami</code></li> <li>Logging de eventos de seguran\u00e7a (token inv\u00e1lido, bloqueio, login)</li> <li>M\u00e9tricas: <code>auth_success_total</code>, <code>auth_denied_total</code>, <code>token_revoked_total</code></li> <li>Painel visual ou endpoint <code>/diagnostics/auth</code></li> <li>Suporte a OAuth2 / IdPs externos via providers</li> </ul>"},{"location":"quality/benckmark/athomic_security_quality_review/#roadmap-sugerido","title":"\ud83c\udfaf Roadmap sugerido","text":"<ul> <li>[ ] Decorator <code>@requires_policy</code> + <code>PolicyManager</code></li> <li>[ ] Logging + eventos estruturados de seguran\u00e7a</li> <li>[ ] CLI para usu\u00e1rios/token</li> <li>[ ] M\u00e9tricas Prometheus por status</li> <li>[ ] Suporte a integra\u00e7\u00e3o com IdP externo (Auth0, Keycloak)</li> <li>[ ] Hook <code>on_auth_failure</code> por tipo (IP, JWT, role)</li> </ul> <p>O m\u00f3dulo auth da Athomic \u00e9 s\u00f3lido, mas pode crescer muito em auditoria, extensibilidade e visibilidade \u2014 pilares de seguran\u00e7a moderna.</p>"}]}